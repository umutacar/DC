\documentclass{course}
\title{Parallel and Sequential Algorithms}

% Course number must be unique in the database
\coursenumber{15210}

\semester{Spring 2018}
\picture{/210/course/air-pavilion.jpg}
\website{http://www.cs.cmu.edu/~15210}

% Provides book
% This must be provided
% The name should be relative to course number.
\providesbook{S18}

% Start counting chapters from 
% This is optional. Will start counting at 1.
\provideschapter{18}
\providessection{1}

15-210 aims to teach methods for designing, analyzing, and programming
sequential and parallel algorithms and data structures. The emphasis
is on teaching fundamental concepts applicable across a wide variety
of problem domains, and transferable across a reasonably broad set of
programming languages and computer architectures. This course also
includes a significant programming component in which students will
program concrete examples from domains such as engineering, scientific
computing, graphics, data mining, and information retrieval (web
search).

Unlike a traditional introduction to algorithms and data structures,
this course puts an emphasis on parallel thinking â€” i.e., thinking
about how algorithms can do multiple things at once instead of one at
a time. The course follows up on material learned in 15-122 and 15-150
but goes into significantly more depth on algorithmic issues. 


\begin{book}
\title{Algorithm Design: Parallel and Sequential}
\authors{Umut A. Acar and Guy Blelloch}

\begin{chapter}[Minimum Spanning Trees]
\label{ch:mst}

\picture{./media/sydney-harbour-bridge.jpg}

This chapter covers the problem of finding a minimal set of edges that
span a graph, a.k.a., the \defn{Minimum Spanning Tree} or \defs{MST}
problem.

\begin{section}[Spanning Trees]


\begin{unit}

\begin{teachnote}
TODO: THROUGHOUT THIS CHAPTER The arguments for correctness and
complexity needs to be improved. Umut made some updates in Spring 2018
but more work is needed.
\end{teachnote}

\begin{teachnote}

Some puzzles to use

\begin{itemize}

\item Puzzle: you moved to a new city and don't feel safe.  How would
  you travel. Learn some small number of streets and stick to them.
  That is a spanning tree.  If you also want to minimize how much you
  walk that is an MST.

\item Puzzle 1: you have 6 cities as shown in the example and you can
  build roads between some of these cities as shown. Your task is to
  connect all cities by building the smallest number of road segments.
  How many edges you need?  5 because all you need is a tree.

\item Puzzle 2: suppose now you have edge weights.  you want to build
  the spanning tree that has the minumum total weight.

\item Puzzle: How to use graph search:  students usually discover prim

\item Puzzle: suppose I sort the edges. students usually discover kruskal.

\end{itemize}


Algorithms for Spanning trees.
\begin{itemize}

\item BFS, DFS

\item Graph contraction based algorithm. Select star edges as tree edges.

\item Iterative algorithm: add each edge one by one unless the
  endpoints are in the same component.  Excellent way to introduce
  union find. TODO: Show how to implement union find on log work/span.

\end{itemize}



Algorithms for  Minimum Spanning Trees
\begin{itemize}

\item Introduce light edge rule by considering a cut consisting of a
  single vertex.  The lightest edge on that vertex v must be in the
  MST.  Suppose not.  Consider adding the lightest edge.  There is a
  cycle, because there is an edge from (u1, v) and the lightest edge
  (u2,v).  Replace (u1,v) with (u2,v).  You have  a tree with smaller
  weight.

\item Go back to your example and check that this is the case.

\item Now generalize.  the point is that you can consider any cut and
  replace the heaving edge crossing the cut with the lightest. You
  still have a spanning tree because when you delete the heavy edge.
  you get two trees, which then you connect by inserting the lightest
  edge. 


\item Now go trough, Prim, Kruskal, and Boruvka in that order.  Use
  puzzles. The students usually discover Prim and Kruskal.


\item For Boruvka: establish that each part is a tree.  By arguing why
  you can't have cycles. A cycle would violate the ordering relation
  assumed for picking joiners.

\end{itemize}

\end{teachnote}

\begin{gram}
Recall that we say that an undirected graph is a forest if it has no
cycles and a tree if it is also connected. Given a connected,
undirected graph, we might want to identify a subset of the edges that
form a tree, while including all the vertices.  We call such a tree a
spanning tree.
%
\end{gram}

\begin{group}
\begin{definition}[Spanning Tree]
For a connected undirected graph $G = (V,E)$, a spanning tree is a
tree $T = (V,E')$ with $E' \subseteq E$.
\end{definition}

\begin{note}
A graph can have many spanning trees, but all have $|V|$ vertices and
$|V|-1$ edges.
\end{note}

\begin{example}
A graph on the left, and two possible spanning trees.
\begin{center}
\includegraphics[width=1.7in]{./media/graph-pistol.jpg}
\hspace{.1in}
\includegraphics[width=1.7in]{./media/graph-pistol-st-1.jpg}
\hspace{.1in}
\includegraphics[width=1.7in]{./media/graph-pistol-st-2.jpg}
\end{center}
\end{example}
\end{group}

\begin{teachask}
Design an algorithm for finding a spanning tree of a connected,
undirected graph?
\end{teachask}

\begin{gram}[Sequential Algorithms for Spanning Trees]
One way to generate a spanning tree is simply to use a graph search.
%
\begin{itemize}
\item A DFS-tree is a spanning tree, because it includes a path from
  the source to all the vertices in the graph. 

\item We can construct a spanning tree based on BFS, by including in
  the spanning tree each edge that leads to the discovery of an
  unvisited vertex. 
\end{itemize}

 DFS and BFS are work-efficient algorithms for computing spanning
 trees but as we discussed they are not good parallel algorithms.
\end{gram}

\begin{teachask}
Can you think of an algorithm with polylogarithmic span for finding a
spanning tree of a connected undirected graph? 
\end{teachask}

\begin{gram}[Parallel Algorithms for Spanning Trees]
Another way to generate a spanning tree is to use graph contraction,
which as we have seen can be done in parallel.  The idea is to use
star contraction and add all the edges that are selected to define the
stars throughout the algorithm to the spanning tree.
\end{gram}

\end{unit}
\end{section}

\begin{section}[Minimum Spanning Trees]
\label{sec:mst::mst}

\begin{unit}

\begin{gram}
A graph can have many spanning trees.  In same cases, such as in
weighted graphs, we may be interested in finding the spanning tree
with the smallest total weight (i.e., sum of the weights of its
edges).
\end{gram}

\begin{group}
\begin{definition}[Minimum Spanning Trees]
Given a connected, undirected weighted graph $G = (V,E,w)$, the
minimum (weight) spanning tree (MST) problem requires finding a
spanning tree of minimum weight, where the weight of a tree $T$ is
defined as
\[
w(T) = \sum_{e \in E(T)} w(e).
\]
\end{definition}

\begin{example}
A graph (left) and its MST (right). 
\begin{center}
\includegraphics[width=2in]{./media/graph-pistol-w.jpg}
%graph-pistol-w-cut-1.pdf 
%
\hspace{1in}
%
\includegraphics[width=2in]{./media/graph-pistol-w-mst.jpg}
\end{center}
\end{example}
\end{group}


\begin{example}[Network Design]
\label{ex:mst::network-design}
Minimum spanning trees have many interesting applications in network
design, i.e., in the design of a network that includes vertices and
connections between them.
%
In such network design problems, it can be important to minimize some
cost function, defined in terms of the connections in the network.
%
As an example, suppose that you are wiring a building so that all the
rooms are connected via bidirectional communication wires. 
%
Suppose
that you can connect any two rooms at the cost of the wire connecting
the rooms, which depends on the specifics of the building and the
rooms but is always a positive real number.  
%
We can represent this problem as a minimization problem over a graph,
where vertices represent rooms and weighted edges represent possible
connections and their cost, attached to the graph as weights. 
%
Minimize the cost of the wiring corresponds to finding a minimum
spanning tree of the graph.
%

One of the algorithms that we cover in this chapter (\boruvka{}'s
algorithm) was discovered when developing the electric network for the
historical country of Moravia.
%
Today Moravia is part of Czech Republic. 
\end{example}
\end{unit}
\end{section}


\begin{section}[Light-Edge Property]

There are several algorithms for computing minimum spanning trees.
%
These algorithms are all based on the same underlying property about
cuts in a graph, which we will refer to as the~\defn{light-edge
  property}.  
%
Intuitively, the light-edge property states that if you partition the
graph into two blocks, the minimum edge between the two parts has to
be in an MST.  The light-edge property gives a way to identify
algorithmically the edges of an MST.


\begin{unit}

\begin{gram}[Distinct Edge-Weights Assumption]
In this section, we assume that all edges in a graph have distinct
weights. 
%
This assumption causes no loss-of-generality, because we can always
break ties between edges by ordering them arbitrarily as long as the
ordering is deterministic.  
%
There are multiple ways to achieve this.
%
For example, we can order edges based on their end-points or assign a
unique label to each and break ties by comparing the labels.
%
We can also tweak the edge weights to ensure uniqueness of the weights
without altering the ordering of edges with distinct weights.
% 
\end{gram}

\begin{group}
\begin{definition}[Graph Cut]
\label{def:graphs::graph-cut}
For a graph $G = (V,E)$, a cut is defined in terms of a non-empty
proper subset $U \subsetneq V$.  The set $U$ partitions the graph into
blocks induced by the vertex set $U$ and the vertex set $V \setminus
U$, which together are called the~\defn{cut} and written as the cut
$(U, V \setminus U)$.
%
We refer to the edges between the two parts as the~\defn{cut edges}
written $E(U, V \setminus U)$.
%
We sometimes say that a cut edge ~\defn{crosses} the cut defined by
its end-points.
\end{definition}

\begin{example}
If the subset $U$ in the definition of graph-cuts is a single vertex
$v$ of the graph.
%
The cut edges consist of all edges incident on $v$.
\end{example}
\end{group}

\begin{group}
\begin{lemma}[Light-Edge Property] 
\label{lem:mst::cut}
  Let $G = (V,E,w)$ be a connected undirected, weighted graph with
  distinct edge weights.  For any cut of~$G$, the minimum weight edge
  that crosses the cut is contained all minimum spanning trees of~$G$.
\end{lemma}

\begin{proof}

\begin{center}
  \includegraphics[width=2.2in]{./media/red-blue-fig1.jpg}
\end{center}


The proof is by contradiction.  Assume the minimum-weighted edge $e =
(u,v)$ is not in an MST.  Since the MST spans the graph, there is a
simple path~$P$ connecting~$u$ and~$v$ in the MST (i.e., consisting of
only edges in the MST).
%
Path~$P$ crosses the cut between $U$ and $V\setminus U$ at least once
since $u$ and $v$ are on opposite sides of the cut.
%

Let $e'$ be an edge in~$P$ that crosses the cut. 
%
By assumption the weight of $e'$ is larger than that of $e$. 
%
Now, insert $e$ into the MST---this gives us a cycle that includes
both $e$ and $e'$---and remove $e'$ from the MST to break the only
cycle and obtain a spanning tree again.
%This uses the facts that (1)
%  adding an edge to a spanning tree creates a cycle, and (2) removing
%  any edge from this cycle creates a tree again.  
Because the weight of $e$ is less than that of $e'$, the resulting
spanning tree has a smaller weight. 
%
This is a contradiction and thus $e$ must have been in the tree.
\end{proof}

\begin{remark}
In an MST, there can be many edges that cross a cut.
%
Therefore removing an edge does not necessarily disconnect the blocks
of the cut. 
%
It will surely disconnected the tree but not necessarily the blocks of
the cut.
%
By selecting the edge $e'$ carefully, the proof of the Light Edge
Property makes sure that the minimum edge~$e$ crosses the cut that the
removal of~$e'$ disconnects.
\end{remark}

\end{group}

\begin{example}[Cuts and Light Edges]
The figures below illustrates two cuts.  For each cut, we can find the
lightest edge that crosses that cut, which are the edges with weight
$2$ (left) and $4$ (right) respectively.
\begin{center}
\includegraphics[width=2.4in]{./media/graph-pistol-w-cut-1.jpg}
%graph-pistol-w-cut-1.pdf 
%
~~~~~~~~~~~~~~~~~
%
\includegraphics[width=2.4in]{./media/graph-pistol-w-cut-2.jpg}
\end{center}
\end{example}

\begin{gram}
An important implication of the light-edge property as proved in
LLight-Edge-Property Lemma 
%\lemref{mst::cut} 
is that any minimum-weight edge that crosses a cut
can be immediately added to the MST.  
%
In fact, the algorithms that we consider in this section all take
advantage of this implication.
%
For example, Kruskal's algorithm constructs the MST by greedily adding
the overall minimum edge.  
%
Prim's algorithm grows an MST incrementally by considering a cut
between the current MST and the rest of graph. \boruvka{}'s algorithm
constructs a tree in parallel by considering the cut defined by each
and every vertex.
%
\end{gram}


\begin{exercise}
Show that for any undirected connected graph with unique edge weights,
there exists one unique minimum spanning tree.
\end{exercise}

%% Proof: ...
%%
%%

\begin{exercise}
Consider any undirected, connected graph $G$ with unique edge weights.
%
Show that for any cycle in the graph, the heaviest edge on the cycle
is not in the MST of~$G$.
\end{exercise}
%% Proof: ...
%%
%%

\end{unit}
\end{section}

\begin{section}[Brute-Force Algorithm]
\begin{unit}

\begin{gram}
We can solve the MST problem by using a Brute-Force technique and by
appling the light-edge property.
%
The idea is to consider each and every cut of the given graph and
insert for each cut the minimum-weight edge that crosses the cut to
the MST.
%
This algorithm requires too much work to be of use but it reveals some
interesting propertief of MST's.
\end{gram}

\begin{exercise}
Prove that the Brute-Force Algorithm indeed computes a correct MST.
\end{exercise}
\end{unit}
\end{section}

\begin{section}[Prim's Algorithm]
\begin{unit}

\begin{gram}
Prim's algorithm performs a priority-first search to construct the
minimum spanning tree. 
%
To see the basic idea behind the algorithm, imagine that we have
already visited a set $X$ of vertices.
%
By the light-edge property, we know that the minimum-weight edge~$e$
with one of its endpoint in $X$ and the other in $V \setminus X$ is in
the MST, because it is a minimum edge crossing the cut defined by $X$.
%
We can therefore add~$e$ to the MST and include the other endpoint
in~$X$.  
%
\end{gram}

\begin{group}

\begin{algorithm}[Prim's Algorithm]
For a weighted undirected graph $G=(V,E,w)$ and a source $s$, Prim's
algorithm is priority-first search on $G$ starting at an arbitrary $s
\in V$ with $T = \emptyset$, using priority $\displaystyle p(v) =
\min_{x \in X} w(x,v)$ (to be minimized), and setting $T = T \cup
\cset{(u,v)}$ when visiting $v$ where $w(u,v) = p(v)$.
When the algorithm terminates, $T$ is the set of edges in the MST.

\end{algorithm}


\begin{example}
A step of Prim's algorithm.  Since the edge $(\vname{c},\vname{f})$ has minimum weight
across the cut $(X,Y)$, the algorithm will ``visit'' $\vname{f}$ adding
$(\vname{c},\vname{f})$ to $T$ and $\vname{f}$ to $X$.
\begin{center}
\includegraphics[width=3in]{./media/prims.jpg}
\end{center}
\end{example}
\end{group}

\begin{exercise}
Prove the correctness of Prim's algorithm.
\end{exercise}

\begin{gram}[Prim's and Dijkstra's]
Prim's algorithm is quite similar to Dijkstra's algorithm for shortest
paths.  
%
Both are priority-first search algorithms.
%
The only differences are 
\begin{itemize}
\item Prim's algorithms starts at an arbitrary vertex instead of at a
  source,

\item Prim's algorithm uses the priority 
%
\[
p(v) = \min_{x \in X} (x,v)
\]
instead of the priority used by Dijkstra's:
\[
\min_{x \in X} (d(x) + w(x,v))
\]

\item Prim's algorithm maintains a tree $T$ instead of a table of
  distances $d(v)$. 
\end{itemize}

Because of the similarity to implement Prim's algorithm, we can
basically use the same priority-queue implementation as in Dijkstra's
algorithm.
%
Such an implementation requires $O(m \log n)$ work and span.

\end{gram}


\begin{remark}
  Prim's algorithm was invented in 1930 by Czech mathematician Vojtech
  Jarnik and later independently in 1957 by computer scientist Robert
  Prim.  Edsger Dijkstra's rediscovered it in 1959 in the same paper
  he described his famous shortest path algorithm.
\end{remark}


%% \begin{comment}
%% \newcommand{\visitedx}{X}

%% \begin{pseudocode}[Prim's Algorithm]
%% \begin{codel}
%% \cfun~\cd{prim}(G) = \newl
%% \clet\newl
%% ~~\cfun~\cd{enqueue}~v~(Q,(u, w)) = \cd{PQ.insert}~(w, (v,u))~Q\newlspace{.1in}
%% ~~\cfun~\cd{prim'}(\visitedx,~Q,~T) =\newl
%% ~~~~\ccase~\cd{PQ.deleteMin}(Q)~\cof\newl
%% ~~~~~~~$\=$(\cnone, \cwild) \Rightarrow T~~~~~~~~~~~~~~~~~~~~~~~~\ccomment{Done}\newl
%% ~~~~~|$\>$(\csome(d, (u, v)), Q') \Rightarrow\newl
%% ~~~~~~~\cif~(v \in^? \visitedx)~\cthen~\cd{prim'}(\visitedx,~Q',~T)~~~~~~\cmark\ccomment{Already visited}\newl
%% ~~~~~~~\celse~\clet\newl[line:insert]
%% ~~~~~~~~~\cval~\visitedx' = \visitedx \cup \cset{v}\ctabc{Visit}\newl[line:add]
%% ~~~~~~~~~\cval~T' = T \cup \cset{(u,v)}\ctabc{Add edge to MST}\newl
%% ~~~~~~~~~\cval~Q'' = \citer~(\cd{enqueue}~v)~Q'~N_G(v)\ctabc{Enqueue $v$'s neighbors}\newl
%% ~~~~~~~\cin~\cd{prim'}(\visitedx',~Q'',~T')~\cend\newlspace{.1in}
%% ~~\cval~s = \ctext{an arbitrary vertex from $G$}\newl
%% ~~\cval~Q = \citera{(\cd{enqueue}~s)}{\cset{}}{N_G(s)}\ctabc{Enqueue $s$'s neighbors}\newl
%% \cin\newl
%% ~~\cd{prim'}(\cset{s},~Q,~\cset{})\newl
%% \cend
%% \end{codel}
%% \end{pseudocode}
%% \end{comment}
\end{unit}
\end{section}

\begin{section}[Kruskal's Algorithm]

\begin{unit}

\begin{gram}
As described in Kruskal's original paper, the algorithm is:
\begin{quote}
``Perform the following step as many times as possible: Among the edges
of $G$ not yet chosen, choose the shortest edge which does not form
any loops with those edges already chosen.'' [Kruskal, 1956]
\end{quote}
In more modern terminology we would replace ``shortest'' with
``lightest'' and ``loops'' with ``cycles''.  
\end{gram}

\begin{gram}
Kruskal's algorithm is correct since it maintains the invariant on
each step that the edges chosen so far are in the MST of $G$.  This is
true at the start.  Now on each step, any edge that forms a cycle with
the already chosen edges cannot be in the MST.  This is because adding
it would violate the tree property of an MST and we know, by the
invariant, that all the other edges on the cycle are in the MST.  Now
considering the edges that do not form a cycle, the minimum weight
edge must be a ``light edge'' since it is the least weight edge that
connects the connected subgraph at either endpoint to the rest of the
graph.  Finally we have to argue that all the MST edges have been
added.  Well we considered all edges, and only tossed the ones that we
could prove were not in the MST (i.e. formed cycles with MST edges).
\end{gram}

\begin{gram}
We could finish our discussion of Kruskal's algorithm here, but a few
words on how to implement the idea efficiently are warranted.  In
particular checking if an edge forms a cycle might be expensive if we
are not careful.  Indeed it was not until many years after Kruskal's
original paper that an efficient approach to the algorithm was
developed.  Note that to check if an edge $(u,v)$ forms a cycle, all
one needs to do is test if $u$ and $v$ are in the same connected
component as defined by the edges already chosen.  One way to do
this is by contracting an edge $(u,v)$ whenever it is added---i.e.,
collapse the edge and the vertices $u$ and $v$ into a single
super-vertex.  However, if we implement this as described in the last
chapter we would need to update all the other edges incident on $u$
and $v$.  This can be expensive since an edge might need to be updated
many times.  
%Furthermore the contraction can create duplicate edges
%with different weights.  We would need to allow for duplicate edges,
%or remove all but the lightest of them.
\end{gram}

\begin{gram}
To get around these problem it is possible to update the edges lazily.
What we mean by lazily is that edges incident on a contracted vertex
are not updated immediately, but rather later when the edge is
processed.  At that point the edge needs to determine what
supervertices (components) its endpoints are in.  This idea can be
implemented with a~\defn{union-find data structure}.  The ADT for a
union-find data structure consists of the following operations on a
union-find structure $U$:

\begin{itemize}
\item  $\cd{insert}~U~v$ inserts the vertex
$v$ into $U$,

\item $\cd{union}~U~(u,v)$ joins the two elements $u$ and $v$ into
a single super-vertex, 

\item $\cd{find}~U~v$ returns the super-vertex in which $v$
  belongs, possibly itself, 

\item $\cd{equals}~u~v$ returns true if $u$ and $v$ are the same
  super-vertex.  Now we can simply process the edges in increasing
  order.  
\end{itemize}
\end{gram}

\begin{algorithm}[Union-Find Kruskal]
\label{alg:mst::kruskal}
\[
\begin{array}{ll}
1 & \cd{kruskal} (G = (V,E,w)) =
\\
2 & ~~~\cd{let}
\\
3 & ~~~~~~\cd{addEdge}~((U,T),~e = (u,v)) =
\\
4 & ~~~~~~~~~\cd{let}
\\
5 & ~~~~~~~~~~~~u' = \cd{find}~(U,u)
\\
6 & ~~~~~~~~~~~~v' = \cd{find}~(U,v)
\\
7 & ~~~~~~~~\cd{in}
\\ 
8 & ~~~~~~~~~~~~\cd{if}~(u' = v')~\cd{then}
\\ 
9 & ~~~~~~~~~~~~~~~(U,T) %    @\cdc{if $u$ and $v$ are connected, skip}@
\\
10 & ~~~~~~~~~~~~\cd{else}
\\ 
%13 & ~~~~~~~~~~~ @\cdch{contract $e$ and add it to $T$}@
%\\
11 & ~~~~~~~~~~~~~~~~(\cd{union}~(U, u',v'), T \cup e)
\\
12 & ~~~~~\cd{end}
\\
13 & ~~~~~~U = \cd{iterate}~\cd{insert}~\emptyset~V%~\cdc{initialize UF}
\\
14 & ~~~~~~E' = \cd{sort}~(E,w) %~\cdc{sort the edges}@
\\
15 & ~~~\cd{in}
\\
16 & ~~~~~~\cd{iterate}~\cd{addEdge}~(U,\emptyset)~E'
\\
17 & ~~~\cd{end}
\end{array}
\]
\end{algorithm}

\begin{exercise}
Prove that Kruskal's algorithm  correctly find the MST of a
undirected graph with unique edge weights.
\end{exercise}

\begin{teachask}
What is the work and the span of Kruskal's algorithm based on union-find? 
\end{teachask}

\begin{gram}[Cost of Kruskal's]
To analyze the work and span of the algorithm we first note that there
is no parallelism, so the span equals the work.  
%
To analyze the work
we can partition it into the work required for sorting the edges and
then the work required to iterate over the edges using union and find.
%
The sort requires $O(m \log n)$ work.  
%
The union and find operations
can be implemented in $O(\log n)$ work each requiring another $O(m
\log n)$ work since they are called $O(m)$ times.  
%
The overall work is therefore $O(m \log n)$.
%
It turns out that the union and find operations can actually be
implemented with less than $O(\log n)$ amortized work, but this does
not reduce the overall work since we still have to sort.
\end{gram}
\end{unit}
\end{section}

\begin{section}[Boruvka's Algorithm]

\begin{unit}
\begin{gram}
As discussed in previous sections, Kruskal and Prim's algorithm are
sequential algorithms. In this section, we present an MST algorithm
that runs efficiently in parallel using graph contraction.  This
parallel algorithm is based on an approach by \boruvka{}.  As Kruskal's
and Prim's, \boruvka{}'s algorithm constructs the MST by inserting light
edges but unlike them, it inserts many light edges at once.
\end{gram}

\begin{teachask}
What is the most trivial cut you can think of?  What edges cross it?
\end{teachask}
%

\begin{group}
\begin{gram}[Vertex Joiners]
To see how we can select multiple light edges, recall that all light
edges that cross a cut must be in the MST.
%
Consider now a cut that is defined by a vertex $v$ and the rest of the
vertices in the graph.  
%
The edges that cross this cut are exactly the edges incident on $v$.
%
Therefore, by the light edge rule, for $v$, the minimum weight edge
between it and its neighbors is in the MST.
%
Since this argument applies to all vertices at the same time, the
minimum weight edges incident an any vertex is in the MST.  We call
such edges {\em vertex-joiners}.
\end{gram}

\begin{example}
  The vertex-joiners of the graph are highlighted.  
\begin{center}
 \includegraphics[width=2in]{./media/graph-pistol-w-min.jpg}
\end{center}
\end{example}
\end{group}

\begin{gram}
Since we know that the vertex-joiners are all in the MST, we can
insert them into it in parallel by letting each vertex pick its
vertex-joiner.
%

Sometimes just one round of picking vertex-joiners will select all the
MST edges and would generate a complete solution.  
%
In most cases, however, the minimum-weight edges on their own do not
form a spanning tree.  
%

To see how we can proceed, note that the vertex-joiners define a
partition of the graph---all the vertices are in a part.
%
Consider now the edges that remain internal to a partition but are not
vertex joiners.  
%
Such an edge cannot be in the MST, because inserting it into the MST
would create a cycle.
%
The edges that cross the partitions, however, must be considered as
they can indeed be in the MST.
%

We therefore want to eliminate the internal edges from consideration,
while keeping the cross edges is to perform a graph contraction based
on the partitioning defined by the vertex-joiners. 
%
We can do so by using graph contraction.
%
Recall that in graph contraction, we start with a partitioning of the
graph into disjoint connected subgraphs and then replace each subgraph
(partition) with a super-vertex and relabel the edges.  
%

\boruvka{}'s algorithm iterates this approach until the graph is
reduced to a single vertex.
\end{gram}

\begin{example}[One Round of \boruvka's Algorithm]
Consider the graph below and the highlighted vertex-joiners.
\begin{center}
 \includegraphics[width=2in]{./media/graph-pistol-w-min.jpg}
\end{center}

The vertices $\vname{a}$ and $\vname{b}$ both pick edge
$\cset{\vname{a},\vname{b}}$,
%
vertex $\vname{c}$ picks $\cset{\vname{c},\vname{d}}$, $\vname{d}$,
%
vertex $\vname{f}$ picks $\cset{\vname{d},\vname{f}}$, 
%
and $\vname{e}$ picks $\cset{\vname{e},\vname{b}}$.
%

The edge $(\vname{e},\vname{f})$, which is in the MST, is not selected
(neither $\vname{e}$ nor $\vname{f}$ pick it).
%

To proceed, we can take the partitions defined by vertex-joiners and
contract them by using graph contraction.
% 
The figure below illustrates such a contraction.
%
After the contraction completes, we obtain multiple edges between the
the resulting partitions.

\begin{center}
\includegraphics[width=2in]{./media/graph-pistol-w-min-contract.jpg}

\includegraphics[width=.6in]{./media/graph-pistol-w-contracted.jpg}
\end{center}
\end{example}


\begin{gram}[Redundant Edges]
When performing graph contraction, we have to be careful about
redundant edges. In our discussion of graph contraction,
% in \chref{gc},
where used unweighted graphs, we mentioned that we may treat redundant edges
differently based on the application.  
%
In unweighted graphs, the task is usually simple because we can keep
any one of the redundant edges, and it usually does not matter which
one.  
%
When the edges have weights, however, we have to decide to keep
all the edges or select some of the edges to keep.

%
For the purposes of MST, in particular, we can keep all the edges or
keep just the edge with the minimum weight, because the others, cannot
be in the MST.  In the example above, we would keep the edge with
weight $4$.
\end{gram}

\begin{gram}
What we just covered is exactly \boruvka{}'s idea.  He did not discuss
implementing the contraction in parallel.  At the time, there were not
any computers let alone parallel ones.  We are glad that he has left
us something to do.  In summary, \boruvka{}'s algorithm can be described
as follows.
\end{gram}


\begin{algorithm}[\boruvka{}]
While there are edges remaining: (1) select the minimum weight edge
out of each vertex and contract each part defined by these edges into
a vertex; (2) remove self edges, and when there are redundant edges
keep the minimum weight edge; and (3) add all selected edges to the
MST.
\end{algorithm}
\end{unit}

\begin{unit}[Boruvka's Algorithm with Tree Contraction]

\begin{gram}
We can implement Boruvka's algorithm by using tree contraction as
described in the previous section.
%
We now analyze the efficiency of this algorithm.  
\end{gram}

\begin{gram}[Cost of \boruvka{} by Using Tree Contraction]

We first focus
on the number of rounds of contraction and
then consider how to implement the contraction.
%
%% \begin{teachask}
%% Suppose that we picked $k$ vertex-joiners, how many vertices
%% will we remove?
%% \end{teachask}
%% %
Since contracting an edge removes exactly one vertex (contraction of
and edge can be viewed as folding one endpoint into the other), if $k$
edges are selected then $k$ vertices are removed. 
%
%% \begin{teachask}
%% Can we then remove all the vertices? 
%% \end{teachask}
%
Since each vertex picks a vertex joiner independently in parallel, it
is possible that $k=n$.
%
In this case, we would be able to fold all the vertices in one round.
%
In the general case, however, one edge can be chosen by two vertices
as vertex joiners.
%
%% \begin{teachask}
%% At least how many vertices can be remove?  
%% \end{teachask}
%
Therefore at least $n/2$ vertex joiners are picked and thus $n/2$
vertices will be removed.
%
Consequently, \boruvka{}'s algorithm will take at most $\log_2 n$ rounds
of selecting vertex-joiners and contracting based on the partitioning
defined by them.

%% \begin{teachask}
%%   How can be perform a round of contraction based on the partitioning
%%   defined by the vertex-joiners? Can we use edge contraction or star
%%   contraction?
%% \end{teachask}
%% %
To contract the partition defined by the vertex-joiners, we cannot use
edge or star contraction, because the parts may not correspond to an
edge or a star.  
%
% In general each part identified by selecting the
% vertex-joiners are neither single edges nor single stars.
%
%
It turns out, vertex joiners form a forest (a set of trees).
%
Therefore, each part is a tree and thus we want to contract trees. 
%
By removing all edges that are not vertex-joiners, we can contract a
part by applying star contraction to it.  
%
Furthermore, since when doing a star contraction on a tree, it remains
a tree on each step, the number of edges goes down with the number of
vertices. 
%
Therefore the total work to contract all the partitions is bounded by
$O(n)$ if using array sequences.  The span remains $O(\log^2 n)$.

After contracting each tree, we have to update the edges.  As
discussed earlier for redundant edges we want to keep the minimum
weight such edge.  
%
There are various ways to do this, including keeping the redundant
edges.
%
Keeping the edges turns out to be an effective solution, and allows
the updating the edges to be done in $O(m)$ work.  
%
Assuming redundant edges, the minimum into each component can still be
done with $O(m)$ work, as described below.  Since there are at most
$\log n$ rounds, \boruvka{}'s algorithm will run in $O(m \log n)$ work
and $O(\log^3 n)$ span.
\end{gram}


\begin{example}
  An example where minimum-weight edges give a non-star tree.  Note
  that we have in fact picked a minimum spanning tree in one round by
  picking the vertex joiners for each vertex.
\begin{center}
\includegraphics[width=2in]{./media/graph-pistol-w-min-2.jpg}
\end{center}
\end{example}
\end{unit}

\begin{unit}[Boruvka's Algorithm with Star Contraction]

\begin{gram}
We now describe how to improve the span of \boruvka{} by a logarithmic
factor by interleaving steps of star contraction with steps of finding
the vertex-joiners, instead of fully contracting the trees defined by
the vertex-joiners.  The idea is to apply randomized star contraction
on the subgraph induced by the vertex-joiners, instead of considering
the whole graph as in conventional star contraction.  Intuitively,
this is correct because we only have to care about vertex-joiners (all
other edges cannot be in the MST).
%


As we will show, on each round, we will still be able to reduce the
number of vertices by a constant factor (in expectation), leading to
logarithmic number of total rounds. Consequently, we will reduce the
overall span for finding the MST from $O(\log^3 n)$ to $O(\log^2 n)$
and maintain the same work.
\end{gram}

\begin{example}
An example of \boruvka{} with star contraction. 

\begin{center}
\includegraphics[width=3.25in]{./media/graph-pistol-w-min-2-star-contract.jpg}
\end{center}
\end{example}

\begin{gram}
For a set of vertex-joiners $\mathit{jE}$, consider the subgraph $H =
(V, \mathit{jE})$ of $G$ and apply one step of the star contraction on
$H$.  To apply star contraction, we can modify our
$\cd{starContract}$ routine so that after flipping coins, the tails
only hook across their minimum-weight edge.  \ur{Is there a difference
  between this algorithm and the description above?}  The modified
algorithm for star contraction is as follows.  In the code $w$ stands
for the weight of the edge $(u,v)$.
\end{gram}

\begin{algorithm}[Star Contraction along Vertex-Joiners]
Given a function $\cd{vertexJoiners}~(G)$ that finds the
vertex-joiners out of each vertex in~$G$, the function
$\cd{joinerStarContract}$ performs star contraction along the vertex
joiners.
\[
\begin{array}{ll}
1 & \cd{joinerStarContract}~(G=(V,E), i) =
\\
2 & ~~~\cd{let}
\\
3 & ~~~~~~\cd{jE} = \cd{vertexJoiners}~(G)
\\
4 & ~~~~~~\cd{P} = \csetf{u \mapsto (v,w) \in \cd{jE}}{\neg
  \cd{heads}(u,i) \land \cd{heads}(v,i)}
\\
5 & ~~~~~~V' = V \setminus \cd{domain}(P)
\\
6 & ~~~\cd{in}
\\ 
7 & ~~~~~~(V',P)
\\ 
8 & ~~~\cd{end}
\end{array}
\]

\end{algorithm}

\begin{gram}
Before we go into details about how we might keep track of the MST and
other information, let us try to understand what effects this change
has on the number of vertices contracted away.  If we have $n$
non-isolated vertices, the following lemma shows that the algorithm
still removes $n/4$ vertices in expectation on each step.
%
The lemma thus implies that this MST algorithm will take only $O(\log
n)$ rounds, just like our other graph contraction algorithms.
\end{gram}


\begin{group}
\begin{lemma}
  For a graph $G$ with $n$ non-isolated vertices, let $X_n$ be the random
  variable indicating the number of vertices removed by
  $\cd{joinerStarContract}~(G,r)$.  Then, $\expct{X_n} \geq n/4$.
\end{lemma}
\begin{proof}
  The proof is pretty much identical to our proof for
  $\cd{starContract}$ except here we're not working with the whole
  edge set, only a restricted one $\cd{jE}.$  Let $v \in V(G)$ be a
  non-isolated vertex.  Like before, let $H_v$ be the event that $v$
  comes up heads, $T_v$ that it comes up tails, and $R_v$ that $v \in
  \cd{domain}(P)$ (i.e, it is removed).  Since $v$ is a
  non-isolated vertex, $v$ has neighbors---and one of them has the
  minimum weight, so there exists a vertex $u$ such that $(v,u) \in
  \cd{minE}$.  Then, we have that $T_v \land H_u$ implies $R_v$
  since if $v$ is a tail and $u$ is a head, then $v$ must join $u$.
  Therefore, $\prob{R_v} \geq \prob{T_v} \prob{H_u} = 1/4$.  By the
  linearity of expectation, we have that the number of removed
  vertices is
  \begin{equation*}
    \expct{\sum_{v: v \text{ non-isolated}} \onef{R_v}} = \sum_{v: v \text{ non-isolated}} \expct{\onef{R_v}} \geq n/4
  \end{equation*}
  since we have $n$ vertices that are non-isolated.
\end{proof}
\end{group}

\begin{gram}[Tracking Edges]
There is a little bit of trickiness since, as the graph contracts, the
endpoints of each edge changes.  Therefore, if we want to return the
edges of the minimum spanning tree, they might not correspond to the
original endpoints.  To deal with this, we associate a unique label
with every edge and return the tree as a set of labels (i.e. the
labels of the edges in the spanning tree).  We also associate the
weight directly with the edge.  The type of each edge is therefore
$(\cd{vertex} \times \cd{vertex} \times \cd{weight} \times
\cd{label})$, where the two vertex endpoints can change as the
graph contracts but the weight and label stays fixed.  This leads to
the slightly-updated version of $\cd{joinerStarContract}$ that
appears in the algorithm given below.
%\algref{mst::starBoruvka}.
\end{gram}

\begin{algorithm}[\boruvka{}'s based on Star Contraction]
\label{alg:mst::starBoruvka}
The function $\cd{vertexJoiner}(G)$ in Line~\linemstminedges{}
finds the minimum edge out of each vertex $v$ and maps $v$ to the pair
consisting of the neighbor along the edge and the edge label.  By
Light-Edge-Property Lemma
%\lemref{mst::cut}, 
since all these edges are minimum out of the
vertex, they are safe to add to the MST.  Line~\linemsthooks{} then
picks from these edges the edges that go from a tail to a head, and
therefore generates a mapping from tails to heads along minimum edges,
creating stars.  Finally, Line~\linemstnewvertices{} removes all
vertices that are in this mapping to star centers.

This is ready to be used in the MST code, similar to the
$\cd{graphContract}$ code studied last time, except we return the set
of labels for the MST edges instead of the remaining vertices.   The
code is given below
% given in \algref{mst::starBoruvka}
The MST algorithm is called by running $\cd{MST}(G, \emptyset, r)$.  As an 
aside, we know that $T$ is a spanning forest on the contracted nodes. 

Finally we describe how to implement $\cd{minEdges}~(G)$, which
returns for each vertex the minimum edge incident on that vertex.
There are various ways to do this.  One way is to make a singleton
table for each edge and then merge all the tables with an appropriate
function to resolve collisions. 
%
% \algref{mst::starBoruvka} gives
The code below merges edges by taking the one with lighter edge weight.

If using sequences for the edges and vertices an even simpler way is
to presort the edges by decreasing weight and then use $\cd{inject}.$
Recall that when there are collisions at the same location
$\cd{inject}$ will always take the last value, which will be the one
with minimum weight.

\[
\begin{array}{ll}
1 & \cd{vertexJoiners}~E =
\\
2 & ~~~\cd{let}
\\
3 & ~~~~~~\mathit{ET} = \cset{(u,v,w,l)\mapsto\cset{u\mapsto(v,w,l)} : (u,v,w,l) \in E}
\\
4 & ~~~~~~\cd{joinEdges}~((v_1,w_1,l_1),(v_2,w_2,l_2)) =
\\
5 & ~~~~~~~~~\cd{if}~(w_1 \leq w_2)~\cd{then}~(v_1,w_1,l_1)~\cd{else}~(v_2,w_2,l_2) 
\\
6 & ~~~\cd{in} 
\\
7 & ~~~~~~\cd{reduce}~(\cd{union}~\cd{joinEdges})~\cset{}~\mathit{ET}
\\
8 & ~~~\cd{end} 
\\
9 & \cd{joinerStarContract}~(G=(V,E), i)
\\
10 & ~~~\cd{let}
\\
11 & ~~~~~~\mathit{minE} = \cd{vertexJoiners}~G %         @\label{line:mst::minedges}@
\\
12 & ~~~~~~P = \csetf{(u \mapsto (v,w,\ell)) \in \mathit{minE}}{\neg \cd{heads}(u,i) \land \cd{heads}(v,i)} %      @\label{line:mst::hooks}@
\\
13 & ~~~~~~V' = V \setminus \cd{domain}(P) %     @\label{line:mst::newvertices}@
\\
14 & ~~~\cd{in} 
\\
15 & ~~~~~~(V',P) 
\\
16 & ~~~\cd{end}
\\
17 & \cd{MST}~((V,E),T,i) =
\\
18 & ~~~\cd{if}~(|E| = 0)~\cd{then}~T
\\
19 & ~~~\cd{else} 
\\
20 & ~~~~~~\cd{let}
\\
21 & ~~~~~~~~~(V',PT) = \cd{joinerStarContract}~((V,E), i)
\\
22 & ~~~~~~~~~P = \cset{u \mapsto v : u \mapsto (v,w,\ell) \in PT} \cup  \cset{v \mapsto v : v \in V'}
\\
23 & ~~~~~~~~~T' = \cset{\ell : u \mapsto (v,w,\ell) \in PT}
\\
24 & ~~~~~~~~~E' = \csetf{(\cget{P}{u},\cget{P}{v},w,l) : (u,v,w,l) \in E}{\cget{P}{u} \neq \cget{P}{v}}
\\
25 & ~~~~~~\cd{in}
\\
26 & ~~~~~~~~~\cd{MST}~((V',E'),T \cup T',i+1)
\\
27 & ~~~~~~\cd{end}
\end{array}
\]
\end{algorithm}


\begin{remark}
Even though \boruvka{}'s algorithm is not the only parallel algorithm, it
was the earliest, invented in 1926, as a method for constructing an
efficient electricity network in Moravia in the Czech Republic.  It
was re-invented many times over.
\end{remark}
\end{unit}
\end{section}



\begin{section}[TSP and MST Problems]

\begin{unit}


\begin{gram}[Bounding TSP with MST]
There is an interesting connection between minimum spanning trees and
the symmetric Traveling Salesperson Problem (TSP), an NP-hard problem.
%
Recall that in TSP problem, we are given a set of $n$ cities
(vertices) and are interested in finding a tour that visits all the
vertices exactly once and returns to the origin. 
%
In the symmetric case of the problem, the edges are undirected (or
equivalently the distance is the same in each direction).  
%
For the TSP problem, we usually consider complete graphs, where there
is an edge between any two vertices.  Even if a graph is not complete,
we can typically complete it by inserting edges with large weights
that make sure that the edge never appears in a solution.  Here we
also assume the edge weights are non-negative.

%% \begin{teachask}
%% Can you think of a way to bound the solution to a TSP problem on an
%% undirected connected graph using minimum spanning trees. 
%% \end{teachask}

Since the solution to the TSP problem visits every vertex once
(returning to the origin), it spans the graph.  
%
But the solution is not a tree but a cycle in which each vertex is
visited once.
%
Dropping any edge from the solution therefore would yield a spanning
tree.  
%
Therefore, a solution to the TSP problem cannot have less total weight
than that of a minimum spanning tree. 
%
We can thus conclude that for undirected graphs with non-negative edge
weights, a minimum spanning tree can be used to obtain a lower bound
for the (symmetric) TSP problem.
\end{gram}


\begin{gram}[Approximating TSP with MST]
As we shall see in the rest of this section, minimum spanning trees
can also be used to find an approximate solutions to the TSP problem,
effectively finding an upper bound. 
%
This, however, requires one more condition on the TSP problem. 
%
In
particular in addition to requiring that weights are non-negative we
require that all distances satisfy the triangle inequality---i.e., for
any three vertices $a$, $b$, and $c$, $w(a,c) \leq w(a,b) + w(b,c)$.
%
\end{gram}


\begin{problem}[Metric Traveling Salesperson (TSP) Problem]
Given a complete undirected graph $G = (V, E)$ with edge
weights $W: E \ra \reals$ such that
\begin{itemize}
\item for all $e \in E$, $W(e) \ge 0$, and
\item for all $u, v, w \in E$, $W(u,v) + W(v,w) \ge W(u,w)$,
\end{itemize} 
find the minimum-weight cycle that visits all the vertices.
\end{problem}

\begin{gram}
We would like a way to use the MST to generate a path to take as an
approximate solution to the metric TSP problem.
%
To do this we first consider a path based on the MST that can visit a
vertex multiple times, and then take shortcuts to ensure we only visit
each vertex once.

%% \begin{teachask}
%% Given an undirected graph $G$, suppose that you compute a minimum
%% spanning tree $T$.  Can you use the tree to visit each vertex in the
%% graph from a given origin?
%% \end{teachask}

Given a minimum spanning tree $T$ we can start at any vertex $s$ and
take a path based on the depth-first search on the tree from $s$.  In
particular whenever we visit a new vertex $v$ from vertex $u$ we
traverse the edge from $u$ to $v$ and when we are done visiting
everything reachable from $v$ we then back up on this same edge,
traversing it from $v$ to $u$.  This way every edge in our path is
traversed exactly twice, and we end the path at our initial vertex.
If we view each undirected edge as two directed edges, then this path
is a so-called \defn{Euler tour} of the tree---i.e. a cycle in a graph
that visits every edge exactly once.  Since $T$ spans the graph, the
Euler tour will visit every vertex at least once, but possibly
multiple times.
\end{gram}

\begin{example}[Euler Tour]
The figure on the right shows an Euler tour of the tree on the left.
Starting at $\vname{a}$, the tour visits $\vname{a}, \vname{b},
\vname{e}, \vname{f}, \vname{e}, \vname{b}, \vname{a}, \vname{c},
\vname{d}, \vname{c}, \vname{a}$.

\begin{center}
\includegraphics[width=2in]{./media/graph-pistol-st-1.jpg}
%
~~~~~~~~~~
%
\includegraphics[width=2in]{./media/graph-pistol-st-1-traversal.jpg}
\end{center}
\end{example}


\begin{teachask}
Can you find a way to derive a non-optimal solution to TSP using the
particular approach to visiting vertices?  Let's first try to
eliminate multiple visits.
\end{teachask}

\begin{gram}[Shortcuts]
Recall that in the TSP problem, the underlying graph is complete and
thus there is an edge between every pair of vertices. 
%
Because it is possible to take an edge from any vertex to any other,
we can take shortcuts to avoid visiting vertices multiple times.  More
precisely what we can do is when about to go back to a vertex that the
tour has already visited, instead find the next vertex in the tour
that has not been visited and go directly to it.  We call this a
shortcut edge.

By the triangle inequality the shortcut edges are no longer
than the paths that they replace.  Thus by taking
shortcuts, the total distance is not increased.
%
Since the Euler tour traverses each edge in the minimum spanning tree
twice (once in each direction), the total weight of the path is
exactly twice the weight of the MST.  
%
With shortcuts, we obtain a solution to the TSP problem that is at
most the weight of the Euler tour, and hence at most twice the weight
of the MST.  
%
Because the weight of the MST is also a lower bound on the TSP, the
solution we have found is within a factor of 2 of optimal.  
%
This means
our approach is an approximation algorithm for TSP that approximates
the solution within a factor of 2.  This can be summarized as:

\[W(\mbox{MST}(G)) \leq W(\mbox{TSP}(G)) \leq 2 W(\mbox{MST}(G))~.\]

\end{gram}

\begin{example}[Shortcuts]
The figure on the right shows a solution to TSP with shortcuts, drawn
in red.  Starting at $\vname{a}$, we can visit $\vname{a}, \vname{b}, \vname{e}, \vname{f}, \vname{c}, \vname{d}, \vname{a}$.

\begin{center}
\includegraphics[width=2in]{./media/graph-pistol-st-1-traversal.jpg}
%
~~~~~~~~~~
%
\includegraphics[width=2in]{./media/graph-pistol-st-1-traversal-once.jpg}
\end{center}

\end{example}

\begin{remark}
It is possible to reduce the approximation factor to 1.5 using a well
known algorithm developed by Nicos Christofides at CMU in 1976.  The
algorithm is also based on the MST problem, but is followed by finding
a vertex matching on the vertices in the MST with odd-degree, adding
these to the tree, finding an Euler tour of the combined graph, and
again shortcutting.  Christofides algorithm was one of the first
approximation algorithms and it took over 40 years to improve on the
result, and only very slightly.
\end{remark}
\end{unit}
\end{section}
\end{chapter}
\end{book}


%% \begin{comment}
%% \section{Maximal Independent Set (MIS)}

%% In graph theory, an \emph{independent set} is a set of vertices from an
%% undirected graph that have no edges between them.  More formally, let a graph $G
%% = (V, E)$ be given. We say that a set $I \subseteq V$ is an independent set if
%% and only if $(I \times I) \cap E = \emptyset$.

%% For example, if vertices in a graph represent entities and edges represent
%% conflicts between them, an independent set is a group of non-conflicting
%% entities, which is a natural thing to want to know.  This turns out to be an
%% important substep in several parallel algorithms since it allows one to find
%% sets of things to do in parallel that don't conflict with each other.  For this
%% purpose, it is important to select a large independent set since it will allow
%% more things to run in parallel and presumably reduce the span of the algorithm.

%% Unfortunately, the problem of finding the overall largest independent
%% set---known as the Maximum Independent Set problem---is \textbf{NP}-hard. Its
%% close cousin Maximal Independent Set, however, admits efficient algorithms and
%% is a useful approximation to the harder problem.

%% More formally, the \emph{Maximal Independent Set} (MIS) problem is: given
%% an undirected graph $G = (V,E)$, find an independent set $I \subseteq V$
%% such that for all $v \in (V\setminus I)$, $I \cup \{v\}$ is not an
%% independent set. Such a set $I$ is maximal in the sense that we can't add
%% any other vertex and keep it independent, but it easily may not be a
%% maximum---i.e. largest---independent set in the graph.

%% \begin{center}
%%   \includegraphics[scale=.7]{./media/mis-example1}
%% \end{center}

%% For example, in the graph above, the set $\{a, d\}$ is an independent set,
%% but not maximal because $\{a, d, e\}$ is also an independent set. On the
%% other hand, the set $\{a, f\}$ is a maximal independent set because there's
%% no vertex that we can add without losing independence. Note that in MIS, we
%% are \emph{not} interested in computing the overall-largest independent set:
%% while maximum independent sets are maximal independent sets, maximal
%% independent set are not necessarily maximum independent sets!  Staying with
%% the example above, $\{a,f\}$ is a maximal independent set but not a maximum
%% independent set because $\{a, d, e\}$ is independent and larger.

%% \subsection{Sequential MIS}

%% Let's first think about how we would compute an MIS if we don't care about
%% parallelism. We will start by thinking about the effect of picking a vertex $v$
%% as part of our independent set $I$. By adding $v$ to $I$, we know that none of
%% $v$'s neighbors can be added to $I$. This motivates an algorithm that picks an
%% arbitrary vertex $v$ from $G$, add $v$ to $I$, and derive $G'$ from $G$ such
%% that each vertex of $G'$ is independent of $I$ and can be picked in the next
%% step. We have the following algorithm:
%% \begin{codel}
%% \cfun~\cd{seqMIS}((V,E), I) =\newl
%% \cif~|V| = 0~\cthen~I~\celse\newl
%% ~~\clet\newl
%% ~~~~\cval~v = \cd{pickAnyOne}(V)\newl
%% ~~~~\cval~V' = V\setminus (N(v) \cup \cset{v})\newl
%% ~~~~\cval~E' = E \cap (V'\times V')\newl
%% ~~\cin~\cd{seqMIS}((V',E'), I \cup \{v\})\newl
%% ~~\cend
%% \end{codel}

%% In words, the algorithm proceeds in iterations until the whole graph is
%% exhausted. Each iteration involves picking an arbitrary vertex, which is added
%% to the independent set $I$, and removing the vertices $v$, together with $v$'s
%% neighbors $N(v)$, and edges incident on these vertices.  Thus, each round picks
%% a new vertex and removes exactly the vertices that can no longer be added to
%% $I$, and nothing more.  It is not difficult to convince ourselves that this
%% algorithm indeed computes a maximal independent set of $G$.  With a proper
%% implementation (e.g., using arrays), this algorithm takes $O(m + n)$ work.


%% \subsection{Fast Parallel MIS}

%% The previous algorithm is inherently sequential, processing vertices of $G$ one
%% by one in some order.
%% %
%% We would like to obtain an algorithm that still runs in
%% $O(m + n)$ work but has much smaller span, preferably $O(\log^c n)$ span for
%% some constant $c > 0$.  The basic idea of the algorithm is to choose multiple
%% vertices per round that are guaranteed to be independent of each other and of
%% the previous vertices already added and that in expectation are going to wipe
%% out a lot of edges.  Consider the following algorithm, inspired by the famous
%% Luby's algorithm for MIS from 1986 and a new analysis suggested in 2009 by Yves
%% et al.

%% \begin{codel}
%% \cfun~\cd{MIS}(G=(V,E), I) =\newl
%% \cif~|V| = 0~\cthen~I~\celse\newl
%% ~~\clet\newl
%% ~~~~\cval~p = \cset{v \mapsto \cd{Unif}(0, 1) \;:\; v \in V}\newl[line:iset-select]
%% ~~~~\cval~I' = \cset{ v : p[v] < \min\cset{ p[u] : u \in N(v)}}\newl
%% ~~~~\cval~V' = V\setminus (N(I') \cup I')\newl
%% ~~~~\cval~E' = E \cap (V'\times V')\newl
%% ~~\cin~\cd{MIS}((V',E'), I \cup I')\newl
%% ~~\cend
%% \end{codel}

%% Again, in words, this algorithm consists of multiple rounds, where each round
%% does the following steps: First, each vertex picks a random number between $0$
%% and $1$. Then, in \linerref{mst:iset-select}, a vertex $v$ is added to $I'$
%% (which will eventually be part of the solution) if $v$ has the highest number
%% among its neighbors.  After that, we remove vertices that are adjacent to $I'$
%% and their corresponding edges because these vertices cannot subsequently be
%% added to $I$.

%% A small example might help us understand the algorithm better.  The following
%% figure shows one iteration of the parallel MIS algorithm:
%% \begin{center}
%%   \includegraphics[scale=.7]{./media/luby-onestep1}
%% \end{center}

%% In this particular example, we can see that after $2$ iterations, we will be
%% done and the algorithm \cd{MIS} produces the maximal independent set $\{a, b,
%% e\}$. But in general, why does this give an independent set?  The following
%% claim shows that each $I'$ is independent.

%% \begin{claim}
%%   In each iteration of \cd{MIS}, the set $I'$ is independent.
%% \end{claim}
%% \begin{proof}
%%   Suppose for a contradiction that $I'$ is \emph{not} independent, so there
%%   exist $u, v \in I'$ with an edge joining them.  Without loss of generality,
%%   say $p[u] < p[v]$.  Then, $v$ cannot be part of $I'$ because $p[v]$ is not the
%%   minimum number among its neighbors, a contradiction. Hence, $I'$ must be
%%   independent.
%% \end{proof}

%% With this claim, it is straightforward to show that what \cd{MIS} returns at
%% the end is indeed an MIS of $G$.  If we use Boolean STArray to represent the set
%% $I$ and an array representation of the graph, each MIS iteration can be
%% implemented in $O(|I'| + m)$ work and $O(\log n)$ span.  But to bound the
%% overall work and span, we'll need to analyze how much progress is made in each
%% iteration.


%% Like before, we might suspect that the number of vertices will drop by a
%% constant fraction in expectation.  This is not the case, however.
%% %
%% Instead, we'll prove that the number of edges goes down by a constant fraction
%% in expectation per iteration. In particular, we will show the following lemma:

%% \begin{lemma}[Edge Removal]
%%   \label{lem:mst::edge-removal}
%%   When \cd{MIS} is called on a graph with $m$ edges, the number of edges
%%   removed is at least $m/2$ in expectation.
%% \end{lemma}

%% Using this lemma and our reasoning about the cost of each iteration, we have the
%% following recurrences:
%% \begin{align*}
%%    W(m, n) \leq W(m', n) + O(m) \qquad \text{and} \qquad S(m, n) \leq S(m',n) + O(\log n)
%% \end{align*}
%% where $0 \leq m' \leq m$ and $\expct{m'} \leq m/2$.  Note that this work
%% recurrence doesn't take into account the $|I'|$ term. But the size of the final
%% MIS solution is at most $n$, so these terms sum to at most $n$, which we will
%% add to the final work bound. We know that these recurrences solve to
%% $\expct{W(m, n)} = O(m)$ and $\expct{S(m,n)} = O(\log^2 n)$.  Therefore, the
%% overall work bound is expected $O(m + n)$.

%% \newcommand{\evDom}[2]{{#1} \longrightarrow {#2}}
%% \optional{
%%   \paragraph{Proving the edge removal lemma.}  This part is optional but you're
%%   still responsible for understanding the statement of the lemma.  To prove the
%%   lemma, our first attempt might be to argue that an edge disappears with a
%%   constant probability, but this is not true.  Instead, we'll look at a more
%%   global argument. We'll define the following events that might appear
%%   counter intuitive at first. For a vertex $u$ and $v \in N(u)$, we say
%%   \[
%%   \evDom{u}{v} \quad\text{ if and only if }\quad p[u] > p[w] \text{ for all } w \in N(u)
%%   \cup N(v).
%%   \]
%%   The event $\evDom{x}{y}$ happens with probability at least $\tfrac1{\deg(x) +
%%     \deg(y)}$.  This is because $|N(x) \cup N(y)| \leq \deg(x) + \deg(y) -
%%   1$. Further, if $\evDom{x}{y}$, then
%%   \begin{enumerate}
%%   \item $x$ will be included in the MIS; and
%%   \item all edges incident on $y$ will be removed (there are $\deg(y)$ edges
%%     incident on $y$).
%%   \end{enumerate}
%%   Let $R_{x \to y}$ be a random variable defined as follows:
%%   \[
%%   R_{x \to y} = \begin{cases}
%%     \deg(y) & \text{if }\evDom{x}{y} \\
%%     0 & \text{otherwise}
%%   \end{cases}
%%   \]
%%   We'll also define
%%   \[
%%   R = \sum_{\{x, y\} \in E(G)} \Big(R_{x \to y} + R_{y \to x}\Big)
%%   \]

%%   The proof of this lemma will consist of two claims:
%% \begin{claim}
%%   The number of edges removed is at least $R/2$.
%% \end{claim}
%% \begin{proof}
%%   Consider an edge $\{u, v\} \in E(G)$. It suffices to show that $R$ counts this
%%   edge at most twice.  This edge could be counted by the events $\evDom{x}{u}$
%%   and $\evDom{y}{v}$ for some $x$ and $y$, and at most one event $\evDom{*}{u}$
%%   and at most one event $\evDom{*}{v}$ can happen.
%% \end{proof}
%% \begin{claim}
%%   $\expct{R} \geq |E|$
%% \end{claim}
%% \begin{proof}
%%   Remember that the event $\evDom{x}{y}$ happens with probability at least
%%   $\tfrac1{\deg(x) + \deg(y)}$, so
%%   \[
%%   \expct{R_{x \to y}} \geq \frac{\deg(y)}{\deg(x) + \deg(y)}
%%   \]
%%   It follows that for an edge $\{x, y\} \in E(G)$,
%%   \[
%%   \expct{R_{x \to y} + R_{y \to x}} \geq \frac{\deg(y)}{\deg(x) + \deg(y)}  +
%%   \frac{\deg(x)}{\deg(x) + \deg(y)} = 1
%%   \]
%%   By linearity of expectation, we have that $\expct{R} \geq |E|$.
%% \end{proof}

%% Together, these claims imply that the number of edges removed is at least
%% $|E|/2$, proving Lemma~\ref{lem:edge-removal}.
%% }
%% \end{comment}





