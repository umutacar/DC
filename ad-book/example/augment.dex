

\subsection{Augmenting with Reduced Values}

To compute rank-based properties of keys in a BST, we augmented the
BST so that each node stores the size of its subtree.  More generally,
we might want to associate with each node a \defn{reduced value} that
is computed by reducing over the subtree rooted at the node by a user
specified function.  In general, there is no restriction on how the
reduced values may be computed, they can be based on keys or
additional values that the tree is augmented with.
%
To compute reduced values, we simply store with every node $u$ of a
binary search tree, the reduced value of its subtree (i.e. the sum of
all the reduced values that are descendants of $u$, possibly also the
value at $u$ itself).

\begin{example}
  The following drawing shows a tree with key-value pairs on the left,
  and the augmented tree on the right, where each node additionally
  maintains the sum of it subtree.
\begin{center}
  \includegraphics[width=4in]{/media/210/bsts/augtree}
\end{center}
The sum at the root ($13$) is the sum of all values in the tree ($3 +
1 + 2 + 2 + 5$).    It is also the sum of the reduced values of its
two children ($6$ and $5$) and its own value $2$.
\end{example}



\input{/media/210/bsts/fig-treaps-rvals}



The value of each reduced value in a tree can be calculated as the sum
of its two children plus the value stored at the node.  This means
that we can maintain these reduced values by simply taking the
``sum'' of three values whenever creating a node.  We can thus change
a data structure to support reduced values by changing the way we
create nodes.  In such a data structure, if the function that we use
for reduction performs constant work, then the work and the span bound
for the data structure remains unaffected.

As an example, \figref{bst::reducedjoin} describes an extension of the
parametric implementation of Treaps to support reduced values.  The
description is parametric in the values paired with keys and the
function \cd{f} used for reduction.  
%
The type for Treaps is extended to store the value paired with the key
as well as the reduced value.  Specifically, in a \cd{Node}, the
first data entry is the value paired by the key and the second is the
reduced value.
%


To compute reduced values as the structure of the tree changes, the
implementation relies on an auxiliary function \cd{mkNode} (read
``make node'') that takes the key-value pair as well as the left and
right subtrees and computes the reduced value by applying reducer
function to the values of the left and right subtrees as well as the
value.
%
The only difference in the implementation of $\cd{split}$ and
$\cd{join}$ functions from \chref{bst} is the use of \cd{mkNode}
instead of \cd{Node}.  

\begin{example}
The following diagram shows an example of splitting an augmented tree.
\begin{center}
  \includegraphics[width=5in]{/media/210/bsts/augtree-split}
\end{center}
The tree is split by the key $c$, and the reduced values on the
internal nodes need to be updated.  This only needs to happen along
the path that created the split, which in this case is $e$, $b$, and
$d$.  The node for $d$ does not have to be updated since it is a leaf.
The \cd{makeNode} for $e$ and $b$ are what will update the reduced
values for those nodes.
\end{example}

We note that this idea can be used with any binary search tree, not
just Treaps.  We only need to replace the function for creating a node
so that as it creates the node, it also computes a reduced value for
the node by summing the reduced values of the children and the value
of the node itself.

\begin{remark}
  In an imperative implementation of binary search trees, when a child
  node is side affected, the reduced values for the nodes on the path
  from the modified node to the root must be recomputed.
\end{remark}

\input{/media/210/bsts/problems}

%% \section{Leftover material}

%% \section{Red-Black Trees}

%% \textbf{This is an early version.}
%% Another common near-balance scheme, used in many libraries, is the
%% red-black scheme.  The idea behind the scheme is to assign one of two
%% colors to each internal node of a BST.

%% \newcommand{\bh}{\overline{h}}
%% \begin{definition}[Red-Black Tree]
%% A red-black tree is a binary search tree $T$ along with a mapping from
%% every node to either \emph{black} or \emph{red} that satisfies the
%% following properties:
%% \begin{enumerate}
%% \item 
%% The root and all leaves are black.
%% \item
%% Every path from a node to a leaf contains the same number of black
%% nodes.
%% \item
%% A red node cannot have another red node as a parent.
%% \end{enumerate}
%% We define the \emph{black height} of a tree to be the number of black
%% nodes on the path to a leaf, and denote it as $\bh(t)$.
%% \end{definition}
%% The red-back tree properties guarantee that all leaves have a depth
%% that is within a factor of two of each other.      This is because
%% all paths to the leaves have the same number of black nodes, and the number
%% of red nodes in a path can be at most as many as black nodes.
%% Since all leaves are within a factor of two in depth,
%% the red-black trees have height $O(\lg |T|)$.   
%% \begin{exercise}
%% Prove that if in a binary tree $T$ all leaves have depth within a constant
%% factor of each other, then the tree has height $O(\lg |T|)$.
%% \end{exercise}

%% \begin{figure}
%% \begin{datastructure}[Implementing \adt{balancedBST} with red-black trees]~
%% \label{ds:redblack}
%% \begin{lstlisting}
%% datatype $\ccc$ = R | B@\vspace{.1in}@
%% datatype $\tttt$ = Leaf | iNode of ($\ccc$ ** $\tyint$ ** $\tttt$ ** $\kkk$ ** $\tttt$)@\vspace{.1in}@
%% val empty = Leaf@\vspace{.1in}@
%% fun singleton$(k)$ = Node(B,$1$,Leaf,$k$,Leaf)@\vspace{.1in}@
%% fun expose(Leaf) = Leaf
%%   | expose(iNode(_,_,$L,k,R$) => Node$(L,k,R)$@\vspace{.1in}@
%% fun $\bh(T)$ = case $T$ of 
%%             Leaf => 1
%%           | iNode(_,$h$,_,_,_) => $h$@\vspace{.1in}@
%% fun $c(T)$ = case $T$ of 
%%             Leaf => B
%%           | iNode($c$,_,_,_,_) => $c$@\vspace{.1in}@
%% fun blacken(iNode(R,$h,L,k,R$)) = iNode(B,$h+1,L,k,R$))
%%   | blacken($T$) = $T$@\vspace{.1in}@
%% fun joinLeft$(t_1,k,t_2)$ =      requires: $\bh(t_1) \leq \bh(t_2)$
%%   if $(\bh(t_1) = \bh(t_2)) \wedge (c(t_2) = \cd{B})$
%%   then iNode(R,$\bh(t_1),t_1,k,t_2$)
%%   else let 
%%       val iNode$(c_2,h_2,L_2,k_2,R_2)$ = $t_2$
%%       val ($T$ as iNode$(c_1,h_1,L_1,k_1,R_1))$ = joinLeft$(t_1,k,L_2)$
%%     in case $(c_1,c_2,h_1 = h_2)$ of
%%        (R,R,_)    => iNode$($B$,h_2+1,T,k_2,R_2)$
%%      | (B,B,true) => iNode$($R$,h_2,\cd{blacken}(L_1),k_1,\cd{iNode}($B$,h_2,R_1,k_2,R_2))$@\label{line:rbrotate}@
%%      |  _       => iNode$(c_2,h_2,T,k_2,R_2)$
%%     end@\vspace{.1in}@
%% fun joinMid$(t_1,k,t_2)$ =
%%   case compare$(\bh(t_1), \bh(t_2))$ of
%%      Less => blacken(joinLeft$(t_1,k,t_2)$)
%%      Equal => iNode(B,$\bh(t_1)+1,t_1,k,t_2$)
%%      Greater => blacken(joinRight$(t_1,k,t_2)$)
%% \end{lstlisting}
%% THIS CODE NOT DOUBLE CHECKED
%% \end{datastructure}
%% \end{figure}

%% \begin{comment}
%% Another version of joinLeft---much longer
%% \begin{lstlisting}
%% fun joinLeft$(t_1,k,t_2)$ =      requires: $h(t_1) \leq h(t_2)$
%%   if $h(t_1) = h(t_2)$
%%   then Node(R,$h(t_1),t_1,k,t_2$)
%%   else let 
%%       val Node$(c_2,h_2,L_2,k_2,R_2)$ = $t_2$
%%        in case $c(L_2)$ of
%%          R => (case joinLeftR$(t_1,k,L_2)$ of
%%                  Node(B,$h_1,L_1,k_1,R_1)$ =>
%%                    Node$($R$,h_2,\cd{blacken}(L_1),k_1,\cd{Node}($B$,h_2,R_1,k_2,R_2))$@\label{line:rbrotate}@
%%                | $T$ => Node$($B$,h_2,T,k_2,R_2)$)
%%         | B => NODE(B,$h_2,\cd{joinLeft}(t_1,k,L_2),k_2,R_2$)
%% and joinLeftR$(t_1,k,t_2)$ =
%%   let val Node$(c_2,h_2,L_2,k_2,R_2)$ = $t_2$
%%   in let val $T$ = joinLeft$(t_1,k,L_2)$
%%      in case $c(T)$ of
%%        R => Node$($B$,h_2+1,T,k_2,R_2)$
%%      | B => Node$($R$,h_2,T,k_2,R_2)$@\vspace{.1in}@
%% \end{lstlisting}
%% \end{comment}

%% Data Structure~\ref{ds:redblack} gives an implementation of
%% \adt{balancedBST} based on red-black trees.  As usual, all we need to
%% do is implement \cd{joinMid} so that when given two trees that
%% satisfy the red-black properties, it returns a tree that satisfy same
%% properties.    When joining trees $t_1$ and $t_2$, if they have the same
%% black height, we can just create a black node with those two trees as
%% children and $k$ as the key, and we are done.  However, if one tree is
%% higher than the other, then we have to rebalance in some way.  Let us
%% assume that $\bh(t_1) < \bh(t_2)$.  In the algorithm, the function
%% \cd{joinLeft} handles this case.  It basically searches down the
%% left spine of $t_2$ until it finds a black-rooted subtree $T'$ of the
%% same black height as $t_1$.  It then creates a red node containing
%% $t_1$, $k$, and $T'$.  This might violate the red property, which then
%% gets fixed on the way back up the recursion.

%% \begin{theorem}
%% For two red-black trees $t_1$ and $t_2$ and a key $k$ such that
%% $K(t_1) < k < K(t_2)$, \cd{joinMid}$(t_1,k,t_2)$ returns a
%% red-black tree containing $K(t_1) \cup \{k\} \cup K(t_2)$, and a black
%% root.
%% \end{theorem}
%% \begin{proof}
%% We just state the invariants for \cd{joinLeft}$(t_1,k,t_2)$.  The
%% function require the following conditions on its inputs: $\bh(t_1)
%% \leq \bh(t_2)$, $K(t_1) < k < K(t_2)$, both $t_1$ and $t_2$ have the
%% red-black properties, and the root of $t_1$ is black.  Given those
%% conditions it returns a tree $t_r$ with the red-black properties, and
%% the following additional properties: $\bh(t_2) \leq \bh(t_r) \leq
%% \bh(t_2) + 1$, and if $\bh(t_r) = \bh(t_2) + 1$ then the root of $t_r$
%% is black and its left child is red.
%% \end{proof}

%% \newcommand{\rrr}{\mathbb{R}}
%% %\newcommand{\rempty}{E}
%% %\newcommand{\rnode}{N}
%% \begin{comment}
%% datatype $\tttt$ = Leaf | Node of ($\tttt$ ** $\kkk$ ** $\tttt$)@\vspace{.1in}@
%% \end{comment}




%% \section{AVL Trees}

%% \textbf{This section currently only contains the code.}

%% \begin{figure}
%% \begin{datastructure}[Implementing \adt{balancedBST} with AVL trees]~
%% \begin{lstlisting}
%% datatype $\tttt$ = Leaf | iNode of ($\tyint$ ** $\tttt$ ** $\kkk$ ** $\tttt$)@\vspace{.1in}@
%% val empty = Leaf@\vspace{.1in}@
%% fun singleton$(k)$ = Node($1$,Leaf,$k$,Leaf)@\vspace{.1in}@
%% fun expose(Leaf) = Leaf
%%   | expose(iNode(_,$L,k,R$) => Node$(L,k,R)$@\vspace{.1in}@
%% fun $h(T)$ = case $T$ of 
%%             Leaf => 0
%%           | iNode($h$,_,_,_) => $h$@\vspace{.1in}@
%% fun newNode$(t_1,k,t_2)$ =
%%    iNode$(1+ \max(h(t_1),h(t_2)),t_1,k,t_2)$@\vspace{.1in}@
%% fun rotateRight$(t_1\cd{ as iNode}(h_1,L_1,k_1,R_1), k, t_2$) =
%%   if $h_1 > h(t_2) + 1$ 
%%   then iNode$(h_1,L_1,k_1,\cd{iNode}(h_1-1,R_1,k,t_2))$
%%   else newNode$(t_1,k,t_2)$@\vspace{.1in}@
%% fun joinLeft$(t_1,k,t_2)$ =
%%   if $h(t_1) \geq h(t_2)$ 
%%   then newNode$(t_1,k,t_2)$
%%   else let val iNode$(h_2,L_2,k_2,R_2)$ = $t_2$
%%      in rotateRight$(\cd{joinLeft}(t_1,k,L_2),k_2,t_2)$ end@\vspace{.1in}@
%% fun joinMid$(t_1,k,t_2)$ =
%%   if $h(t_1) < h(t_2) - 1$ then joinLeft$(t_1,k,t_2)$
%%   else if $h(t_2) < h(t_1) - 1$ then joinRight$(t_1,k,t_2)$
%%   else newNode$(t_1,k,t_2)$@\vspace{.1in}@
%% \end{lstlisting}
%% THIS CODE NOT DOUBLE CHECKED
%% \begin{comment}
%% Not including the following since it is redundant.
%% removeMin and join are common among implementations.
%% joinRight is by symmetry
%% \begin{lstlisting}
%% fun rotateLeft$(t_1,k,t_2 \cd{ as Node}(h_2,L_2,k_2,R_2))$ =
%%   if $h_2 > h(t_1) + 1$ 
%%   then Node$(h_2,\cd{Node}(h_2-1,t_1,k,L_2),k_2,R_2)$
%%   else newNode$(t_1,k,t_2)$@\vspace{.1in}@
%% fun joinRight$(t_1,k,t_2)$ =
%%   if $h(t_2) \geq h(t_1)$ then newNode$(t_1,k,t_2)$
%%   else let val Node$(h_1,L_1,k_1,R_1)$ = $t_1$
%%      in rotateLeft$(L_1,k_1,\cd{joinRight}(R_1,k,t_2))$ end@\vspace{.1in}@
%% fun removeMin$(T)$ =
%%   case $T$ of
%%     Node$(\_,\cd{Leaf},k,\_)$ => $(k,R)$
%%   | Node$(\_,L,k,R)$ =>
%%       let val $(m,L')$ = removeMin$(L)$
%%       in $(m,\cd{rotateLeft}(L',k,R))$ end@\vspace{.1in}@
%% fun join$(t_1,m,t_2)$ =
%%   case ($m$,$t_2$) of
%%     (Some(k),_) => joinMid$(t_1,k,t_2)$
%%   | (None,Leaf) => $t_1$
%%   | (None,_) => 
%%       let val $(k,t_2')$ = removeMin$(t_2)$
%%       in joinMid$(t_1,k,t_2')$ end
%% \end{lstlisting}
%% \end{comment}
%% \end{datastructure}
%% \end{figure}

%% \paragraph{No Balancing.}

%% \begin{figure}
%% \begin{datastructure}[Implementing \adt{balancedBST} with no balance criteria]~
%% \label{ds:bstjoin}
%% \begin{lstlisting}
%% datatype $\tttt$ = Leaf | Node of ($\tttt$ ** $\kkk$ ** $\tttt$)@
%% \vspace{.1in}@
%% val empty = Leaf@
%% \vspace{.1in}@
%% function singleton$(k)$ = Node(Leaf,$k$,Leaf)@
%% \vspace{.1in}@
%% function expose$(T)$ = $T$@
%% \vspace{.1in}@
%% function split$(T,k)$ =
%%   case expose$(T)$ of
%%     Leaf => (Leaf, False, Leaf)
%%   | Node$(L,k',R)$ =
%%       case compare$(k,k')$ of
%%         LESS => @\label{line:split-less}@
%%           let val $(L',m',R')$ = split$(L,k)$
%%           in $(L',m',\cd{Node}(R',k',R))$ end @\label{line:splitnode1}@
%%       | EQUAL => $(L,~\cd{True},R)$
%%       | GREATER => @\label{line:split-greater}@
%%           let val $(L',m',R')$ = split$(R,k)$
%%           in (Node$(L,k',L'),m',R'$) end @\label{line:splitnode2}@@
%% \vspace{.1in}@
%% function join$(t_1,m,t_2)$ =
%%   case $m$ of
%%     Some$(k)$ => Node$(t_1,k,t_2)$
%%   | None =>
%%       case $t_1$ of
%%         Leaf => $t_2$
%%       | Node$(L,k,R)$ => Node$(L,k,~\cd{join}(R,\cnone,t_2)))$@
%% \end{lstlisting}
%% \end{datastructure}
%% \end{figure}

%% \paragraph{BSTs without balance.}
%% If we do not care about balance, then the BST interface is reasonably easy
%% to implement, and is given in Data Structure~\ref{ds:bstjoin}.    
%% The $\cd{split}$ algorithm recursively traverses the tree from the
%% root to the key $k$ splitting along the path, and then when returning from
%% the recursive calls, it puts the subtrees back together.
%% \begin{example}
%% In the following tree we split on the key $c$, which does not appear
%% in the tree.  The split traverses
%% the path $\cseq{a,e,b,d}$ turning right at $a$ and $b$
%% (line~\ref{line:split-greater} of the Data Structure~\ref{ds:bstjoin})
%% and turning left at $e$ and $d$ (line~\ref{line:split-less}).  The
%% pieces are put back together into the two resulting trees on the way
%% back up the recursion.  
%% \begin{center}
%%   \includegraphics[width=5.7in]{/media/210/bsts/bstsplit}
%% \end{center}
%% %The actual way the trees will be put back together will depend on the
%% %balancing scheme.
%% \end{example}
%% The $\cd{join}$ algorithm with a key in the middle simply calls
%% \cd{Node} and therefore does constant work.  For near-balanced
%% trees, however, $\cd{join}$ needs to rebalance the tree and can take
%% work proportional to the height of the trees.

%% \begin{example}
%% Using the version of $\cd{join}$ with no rebalancing (from Data Structure~\ref{ds:bstjoin})
%% we have:
%% \[\cd{join}\left(
%% \raisebox{-.45in}{\includegraphics[scale=.6]{/media/210/bsts/bst4}}~~,
%% ~\cd{Some}(10)~,
%% ~\cd{Leaf}
%% \right) \Rightarrow 
%% \raisebox{-.7in}{\includegraphics[scale=.6]{/media/210/bsts/bst4d}}
%% \]
%% The resulting tree is clearly not balanced since we joined a tree of
%% height three with one of height zero and did no rebalancing.  The join
%% in a near-balanced scheme needs to rebalance the tree, as we will see
%% later.
%% \end{example}

%% \paragraph{Minimal Interface.}
%% We refer to a join that always takes a key in the middle
%% \cd{joinMid}$(L,k,R)$ and one that never takes a key as
%% \cd{joinPair}$(L,R)$.  It turns out that we only need one of these
%% and the other can be efficiently implemented using it.  For
%% example:
%% \begin{lstlisting}[numbers=none]
%% joinMid$(L,k,R)$ = joinPair($L$,joinPair(singleton($k$),R)
%% \end{lstlisting}
%% \begin{exercise}
%% Implement \cd{joinPair} with \cd{joinMid} and any other
%% functions in the balancedBST ADT.
%% \end{exercise}
%% It is also possible to implement $\cd{split}$ efficiently with
%% $\cd{join}$.  Indeed the algorithm for such a split is the same code
%% as in Data Structure~\ref{ds:bstjoin}, except that the
%% \cd{Node} on Lines~\ref{line:splitnode1} and~\ref{line:splitnode2}
%% replaced with \cd{joinMid}.
%% This implies that to achieve a new asymptotically efficient
%% implementation of a near balanced BST scheme, all one needs to
%% implement is one of \cd{joinMid} or \cd{joinPair}, along with
%% supplying \cd{expose} and \cd{empty} (\cd{singleton}$(k)$ is
%% easily defined as \cd{joinMid(empty,$k$,empty)}).  

%% \begin{comment}
%% \begin{exercise}
%% Write a version of \cd{insert} that takes a function $f : \data
%% \times \data$ and if the insertion key $k$ is already in the tree
%% applies $f$ to the old and new value to return the value to associate
%% with the key.
%% \end{exercise}
%% \end{comment}



\begin{comment}

Consider now a call to $\cd{union}$ with parameters
$t_1$ and $t_2$.  To simplify the analysis, we will make the following
assumptions:
\begin{enumerate}
\item $t_1$ it is perfectly balanced (i.e., \cexpose{} returns subtrees of
  size at most $|t_1|/2$), 
\item each time a key from $t_1$ splits $t_2$, it splits the tree in
  exactly in half, and
\item without loss of generality let $|t_1| \leq |t_2|$.
\end{enumerate}
Later we will relax these assumptions.  Let us define $m = |t_1|$ and
$n = |t_2|$ (recall the size of a tree is the number of keys in it).
With these assumptions and examining the algorithm we can then write
the following recurrence for the work of $\cd{union}$:
\begin{align*}
  W_{\mbox{union}}(m, n) &\leq 2W_{\mbox{union}}(m/2,n/2) + W_{\mbox{split}}(n)
+ W_{\mbox{join}}(n+m) + O(1)\\
   & \leq  2W_{\mbox{union}}(m/2, n/2) + O(\lg n)~.
\end{align*}  
The size for join is the sum of the two sizes, $m+n$, but since 
$m \leq n$, $O(\lg (n + m))$ is equivalent to $O(\lg n)$.
We also have the base case
\begin{align*}
  W_{\mbox{union}}(1, n) & \leq 2W_{\mbox{union}}(0,n/2) + W_{\mbox{split}}(n)
+ W_{\mbox{join}}(n) + O(1)\\
         & \leq O(\lg n)~.
\end{align*}
The final inequality is since $2W_{\mbox{union}}(0,n) = O(1)$.

We can draw the recursion tree showing the work performed by the
splitting of $t_2$ and by the joining of the results as follows.

%This figure should be changed so "n" is replaced by "N" except the
%bottom level should be "each costs log (1 + n/m)"

\begin{center}
  \includegraphics[scale=.65]{bsts/recurtree2}
\end{center}
%



There are several features of this tree that's worth mentioning:
First, ignoring the somewhat-peculiar cost in the base case, we know
that this tree is leaf-dominated.  Therefore, excluding the cost at
the bottom level, the cost of $\cd{union}$ is $O(\#\text{ of leaves})$
times the cost of each leaf.

\begin{teachask}
How many leaves are there?
\end{teachask}
%
To find the number of leaves, let's take a closer look at the work
recurrence.  Notice that in the recurrence, the tree bottoms out when
$m = 1$ and before that, $m$ always gets split in half (remember that
$t_1$ is perfectly balanced).  The tree $t_2$ does not affects the
shape of the recursion tree or the stopping condition.  Therefore,
this is yet another recurrence of the form $f(m) = 2f(m/2) + O(...)$,
which means that there are $m$ leaves.
%
\begin{teachask}
How deep are the leaves?
\end{teachask}
%
By the same reasoning, we can see that the leaves are $(1 + \lg m)$
deep.

Let's now determine the size of $t_2$ at the leaves.  
%
We have $m$ keys in $t_1$ to start with, and they split $t_2$ evenly
(by assumption), thus, the leaves have all the same size of
$\frac{n}{m}$.
%
Therefore, each leaf costs $O(\lg (1+\frac{n}{m}))$ (the $1+$ is
needed to deal with the case that $n = m$).  Since there are $m$
leaves, the whole bottom level costs $O(m \lg (1+ \frac{n}{m}))$.
Hence, if the trees satisfy our assumptions, we have that $\cd{union}$
runs in $ O( m\lg(1 + \frac{n}{m}))$ work.

%\paragraph{Removing An Assumption:}

Of course, in reality, our keys in $t_1$ won't split subtrees of $t_2$
in half every time.  But it turns out that any unevenness in the
splitting only helps reduce the work---i.e., the perfect split is the
worst case.  We won't go through a rigorous argument, but if we keep
the assumption that $t_1$ is perfectly balanced, then the shape of the
recursion tree stays the same.  What is now different is the cost at
each level.  Let us try to analyze the cost at level $i$.  At this
level, there are $k = 2^i$ nodes in the recursion tree. Say the sizes
of $t_2$ at these nodes are $n_1, \dots, n_k$, where $\sum_j n_j =
n$. Then, the total cost for this level is
\[
c \cdot \sum_{j=1}^k \lg (n_j) \;\;\leq\;\; c \cdot \sum_{j=1}^k \lg (n/k) =
c\cdot 2^i \cdot \lg (n/2^i),
\]
where we used the fact that the logarithm function is
concave\footnote{Technically, we're applying the so-called Jensen's
  inequality.}.  Thus, the tree remains leaf-dominated and the same
reasoning shows that the total work is $O(m \lg (1 + \frac{n}{m}))$.

Still, in reality, $t_1$ doesn't have to be perfectly balanced as we
assumed. A similar reasoning can be used to show that $t_1$ only has
to be approximately balanced. We will leave this case as an exercise.

We end by remarking that as described, the span of $\cd{union}$ is
$O(\lg^2 n)$, but this can be improved to $O(\lg n)$ by changing the
algorithm slightly.

In summary, $\cd{union}$ can be implemented in $O(m \lg (1 +
\tfrac{n}m))$ work and span $O(\lg n)$.  

Essentially the same analysis applies to the functions
$\cd{intersection}$ and $\cd{difference}$, whose structures are the same
as $\cd{union}$, except for an additional constant work and span for
the conditional (\cd{if}) expression.

\end{comment}

\flushchapter





