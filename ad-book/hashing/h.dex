\documentclass{course}
\title{Parallel and Sequential Algorithms}

% Course number must be unique in the database
\coursenumber{15210}

\semester{Spring 2018}
\picture{/210/course/air-pavilion.jpg}
\website{http://www.cs.cmu.edu/~15210}

% Provides book
% This must be provided
% The name should be relative to course number.
\providesbook{S18}

% Start counting chapters from 
% This is optional. Will start counting at 1.
\provideschapter{19}
\providessection{1}

15-210 aims to teach methods for designing, analyzing, and programming
sequential and parallel algorithms and data structures. The emphasis
is on teaching fundamental concepts applicable across a wide variety
of problem domains, and transferable across a reasonably broad set of
programming languages and computer architectures. This course also
includes a significant programming component in which students will
program concrete examples from domains such as engineering, scientific
computing, graphics, data mining, and information retrieval (web
search).

Unlike a traditional introduction to algorithms and data structures,
this course puts an emphasis on parallel thinking â€” i.e., thinking
about how algorithms can do multiple things at once instead of one at
a time. The course follows up on material learned in 15-122 and 15-150
but goes into significantly more depth on algorithmic issues. 


\begin{book}
\title{Algorithm Design: Parallel and Sequential}
\authors{Umut A. Acar and Guy Blelloch}


\begin{chapter}[Hash Functions and Hash Tables]
\label{ch:hash}


\picture{./media/lorenz-attractor.jpg}

\begin{section}[Introduction]

\begin{unit}

\begin{teachnote}
TODO:

I (umut) included a mathematical discussion of hash functions and added a
section on universal hashing.  
%
UH notes are synthesized from several books, including CLRS, KT, Jeff
Erickson's notes on hashing, Knuth book, my mathematics books on
number theory, and other research.
%

Given that hash functions are used for all sorts of reasons, it seems
unnecessary to tie their discussion to those of hash tables.  I tried
to separate the two and define them more mathematically.

I kept the discussion of hash tables intact for the most part, due to
time constraints.  
%
I would like to rework these to be more abstract.  
%
For example separate chaining does not have to use array of linked
lists.

\end{teachnote}

\begin{gram}
Hash functions and hash tables are one of the most widely used
techniques in computer science.  Even though their development have
been intertwined, hash functions today are used for many different
purposes than for implementing hash tables.  
%
In this chapter, we therefore describe hash functions as
computational structures on their own right,  and then discuss their
use in hash tables.
\end{gram}

\begin{example}

The screenshot below illustrates the deployment process of the Diderot
project on Google Cloud.
%
Deployment involves creating a virtual machine on the cloud,
installing all the needed software, and then copying the contents of
the directory from the local computer (a laptop in this case) from
which the deployment is started onto the virtual machine on the cloud.
%
The integrity of this copy operation is checked by using the ``SHA256''
hash function.
%
The hash code, a.k.a., the digest, is computed before the
transmission, transmitted along with the copied contents, and compared
against a freshly computed digest for the transmitted contents.
%

The depyloment process uses the SHA hash function to check that the
transmission operation did not corrupt the contents being copied onto
the cloud.
%

\begin{verbatim}
$ gcloud app deploy app_test.yaml --project diderot-cmu-test
Beginning deployment of service [default]
Building and pushing image for service [default]
Started cloud build [1e27052a-be74-4a38-be05-807042ca1146].
--------------- REMOTE BUILD OUTPUT ---------------
starting build "1e27052a-be74-4a38-be05-807042ca1146"
Successfully built 2abdfcbff888
Successfully tagged [...] appengine/default.20180414t103220:latest
PUSH
Pushing us.gcr.io/diderot-cmu-test/appengine/default.20180414t103220:latest
Repository: [us.gcr.io/diderot-cmu-test/appengine/default.20180414t103220]
21df82f90a72: Preparing
21df82f90a72: Waiting
67b0784928b9: Pushed
724aba9dc62d: Layer already exists
77c1da6d3730: Pushed

latest: digest: sha256:39df2d79576d3c204b8772150052610e65308706f169b72b0351c164a69c2de1 
size: 3889

DONE
-----------------------------------------------------
Updating service [default]...done.
Stopping version [diderot-cmu-test/default/20180403t134333].
Updating service [default]...done.
Updating service [default]...done.
Deployed service [default] to [https://diderot-cmu-test.appspot.com]

\end{verbatim}


\end{example}

\begin{gram}[Secure Hash Algorithm (SHA)
SHA stands for the Secure Hash Algorithm designed by the NSA.
%
Multiple generations of SHA functions have been designed and utilized.
%
The current generation SHA-2 includes a set of cryptographic hash
functions that are 224, 256, 384, and 512 bits, which are named
respectively as SHA-224, SHA-256, SHA-384, and SHA-512.


SHA functions have the characteristic that a small change to the
contents leads to a large change in the hash code,
a.k.a,. the~\defn{avalanche effect}.
\end{gram}


\begin{gram}[Applications of Hashing]
Hashing is widely employed in many applications.

\begin{enumerate}

\item We describe how hashing can be used in treaps.  In particular we
  suggested using a hash function to hash the keys to generate the
  ``random'' priorities.  Here what was important is that the ordering
  of the priorities is somehow random with respect to the keys.  Our
  analysis assumed the priorities were truly random, but it can be
  shown that a limited form of randomness that arise out of relatively
  simple hash functions is sufficient.

\item
  In cryptography hash functions can be used to hide information.  One
  such application is in digital signatures where a so-called secure
  hash is used to describe a large document with a small number of
  bits.

\item A one-way hash function is used to hide a string, for example
  for password protection.  Instead of storing passwords in plain
  text, only the hash of the password is stored.  To verify whether a
  password entered is correct, the hash of the password is compared to
  the stored value.  These signatures can be used to
  \emph{authenticate} the source of the document, ensure the
  \emph{integrity} of the document as any change to the document
  invalidates the signature, and prevent \emph{repudiation} where the
  sender denies signing the document.

\item String commitment protocols use hash functions to hide to what
  string a sender has committed so that the receiver gets no
  information.  Later, the sender sends a key that allows the receiver
  to reveal the string.  In this way, the sender cannot change the
  string once it is committed, and the receiver can verify that the
  revealed string is the committed string.  Such protocols might be
  used to flip a coin across the Internet: The sender flips a coin and
  commits the result. In the mean time the receiver calls heads or
  tails, and the sender then sends the key so the receiver can reveal
  the coin flip.

\item
 Hashing can be used to approximately match documents, or even parts
 of documents. \emph{Fuzzy matching} hashes overlapping parts of a
 document and if enough of the hashes match, then it concludes that
 two documents are approximately the same. Big search engines look for
 similar documents so that on search result pages they don't show the
 many slight variations of the same document (e.g., in different
 formats). It is also used in spam detection, as spammers make slight
 changes to email to bypass spam filters or to push up a document's
 content rank on a search results page. When looking for malware,
 fuzzy hashing can quickly check if code is similar to known malware.
 Geneticists use it to compare sequences of genes fragments with a
 known sequence of a related organism as a way to assemble the
 fragments into a reasonably accurate genome.

\item Hashing is used to implement hash tables.  In hash tables one is
  given a set of keys $K \subset \alpha$ and needs to map them to a
  range of integers so they can stored at those locations in an array.
  The goal is to spread the keys out across the integers as well as
  possible to minimize the number of keys that collide in the array.
  You should not confuse the terms hash function and hash table.  They
  are two separate ideas, and the latter uses the former.
\end{enumerate}

\end{gram}
\end{unit}

\end{section}

\begin{section}[Hash Functions and Universality]
\begin{unit}[Hash Functions]


\begin{definition}[Hash Function]
A \defn{hash fuction} $h$ is a function from a domain $\kuni{}$,
typically called the \defn{universe} to range $\natspre{m} = \{0, 1,
\ldots, m-1 \}$, i.e.,
\[
h : \kuni{} \ra \natspre{m}.
\]
%
The size of the range $m$ is usually significantly smaller than the
size of the universe.
%
An element of the universe is typically called a \defn{key}.
%
An element of the range is  called a \defn{hash value}, \defn{hash
  code}, or sometimes \defn{digest}.
%
For example, for a key $x \in \kuni{}$, $h(x)$ is the hash value, hash
code, or the digest of $x$.
%
\end{definition}

\begin{definition}[Collisions]
If for distinct $x, y \in \kuni{}$, if $h(x) = h(y)$, 
%
then we say  that $h(x)$ and $h(y)$ \defn{collide}.
\end{definition}

\begin{gram}[Properties of Good Hash Functions]

Hash functions should have at least the following qualities.

\begin{itemize}
\item \textbf{Cost:} the hash of a key is not too difficult to
  compute, e.g., linear in the size of the key, using constant space
  (memory).

\item \textbf{Compactness:} the hash function should not require more than
  constant space (memory) to store and compute.

\item \textbf{Coverage:} The image of $h$ and its range match, i.e.,
  for any $0 \le i < n$, there exists $x \in \kuni{}$ such that $h(x)
  = i$.  In other words, the hash function should be surjective.

\item \textbf{Mixing:} Given
%
a small set of keys $\{x_0, x_1, \ldots,  x_{k-1} \} \subset \kuni{}$
%
and 
%
their hash codes $h(x_0) \cdots h(x_{k-1})$,
%
it should be difficult to predict the hash code $h(x_k)$ of any other
key $x_k$.

\item \textbf{Collision Avoidance:} The function should minimize the
  number of collision as much as possible.
\end{itemize}
\end{gram}


\begin{gram}[Intuition for Hash Functions]
A hash function can be viewed as a way of naming entities from a
universe. 
%
In many applications, naming entities is useful because
\begin{itemize}
\item names allows us to compare entities quickly by comparing their names,
\item names can be chosen as natural numbers, which allows enumerate
  entities and also index them, which enables fast random access.
\end{itemize}

In real life as well as in computing, names can be much fewer than the
actual items that they identify.
%
For example, there are more than 300 million people in the United
States of America but there are approximately 150,000 distinct last
names.
%
In China, the number of distinct last names is less than 10000.
%

Given the relatively few number of names, you would expect there to
lots of collisions, but in many cases, we usually compare a tiny
fraction of the population
%
For example, in your class only less than 10\% of you share a name
with somebody else.  
%
To resorve collisions, we can use additional data, e.g., the city
somebody is from. 

\end{gram}

\begin{example}
Let $\kuni{}$ be set of all natural numbers and consider the following
hash function 
\[
h(x) = x \bmod 4.
\]

This is not a good has function because, it only consider the least
significant two bits of the key.  Just a few different applications of
the hash could reveal the behavior of $h$, making it easy to predict
the hash of any key in the universe.

More generally any hash function of the form 
\[
h(x) = x \bmod a^b,
\]
where $a, b \in \nats{}$, is not a good hash function for a similar
reason: the function treats the input key as a number base $a$ and
takes the least significiant $b$ digits.
\end{example}

\begin{example}
Let $\kuni{}$ be set of all natural numbers and let $p$ be a
prime number. Consider the hash function
\[
h(x) = x \bmod p.
\]

This is not a good hash function because it is relatively easy to
predict by for example trying out some arguments of the form $x,
x+1, x+2, x+2^2, \ldots$.
%
Thus by observing the behavior of function on logarithmically many
values, we can make a good guess for any value $y \in \kuni{}$.
%
More generally, the hash function mays any two keys $x$ and $x + cp$
for any $c$ to the same value.
%
In other words, the function does not mix well.
\end{example}


\begin{example}[Random Hash Function]
Consider a universe $\kuni{}$ and a range $\natspre{m}$.
%
We can construct a hash function for the universe by for picking, for
each key, a uniformly random natural number less than $m$.
%
Because the function is random, it is difficult to predict the value
of any key from a small number of observations.
%
The function thus mixes its input keys quite well.
%

The function also minimizes the number of collision:
for any $x, y \in \kuni{}$, if $x \not= y$ then the probability that
$h(x)$ and $h(y)$ collide is $1/m$.

The problem with this hash function is that it is not compact: for
each key in the universe, we have to remember the hash value that it
maps to, which can require a large amounts of space (memory).
\end{example}

\begin{remark}
Even though uniformly random hash functions are not compact, they are
commonly used in the design of algorithms, because they offer a clean
theoretical model.  This is sometimes referred to as the simple
uniform hashing assumption.
\end{remark}

\begin{definition}[Simple Uniform Hashing Assumption]
The~\defn{simple uniform hashing assumption} postulates that for any
universe and any range there is a hash function that ensures that each
key has equal probability of being mapped to any element of the range
set independent of what the other elements are mapped to.
\end{definition}


\begin{gram}[Collision Avoidance]
The key challenge is designing hash functions is avoiding collisions.
%
As it turns out, collisions are difficult to avoid completely even if
we have relatively large range that our hash function maps to.
%

To see this, let's recall a fun fact: the \defn{birthday paradox}: we
only need 23 people in a room to have a 50\% chance that at least two
people have the same birthday.
%
If we have 60 people, then we have a 99\% chance that two people have
the same birthday.
%

We can generalize the birthday paradox to show that when hashing to a
range size $m$, we expect a collision to occur with only
$\sqrt{\frac12 \pi\;m}$ keys.
%

A related question is how many key-value pairs do we need until every
hash-code in the range is taken (mapped to). 
%
By using the \defn{coupon-collector's problem}, we can show that if we
have $\Theta(m \log m)$ distinct keys than we expect every hash-code
will be taken.
\end{gram}

\begin{exercise}
Given the universe $\kuni{}$ and the range $\natspre{m}$.
%
Let $h$ be a random hash function that is constructed by selecting for
each key in the universe a hash-code in the range.
%
Prove that the hash function $h$ satisfies that
%
\begin{itemize}
\item 
for all $x \in \kuni{}$, and 
%
for all $i, 0  \le i < m$, 
\[
\probover{h \in \kallhash{}}{h(x) = i} = 1/m.
\]

\item
for all $x, y \in \kuni{}$ such that $x \not= y$
\[
\probover{h \in \kallhash{}}{h(x) = h(y)} = 1/m.
\]
\end{itemize}
\end{exercise}

\begin{exercise}
Given the universe $\kuni{}$ and the range $\natspre{m}$,
%
consider the set of all functions $\kallhash{}$ and let $h \in
\kallhash{}$ be a function that is uniformly randomly chosen from
$\kallhash{}$.


\begin{itemize}
\item 
for all $x \in \kuni{}$, and 
%
for all $i, 0  \le i < m$, 
\[
\probover{h \in \kallhash{}}{h(x) = i} = 1/m.
\]

\item
for all $x, y \in \kuni{}$ such that $x \not= y$
\[
\probover{h \in \kallhash{}}{h(x) = h(y)} = 1/m.
\]
\end{itemize}

As we will see, we refer to classes (sets) of hash functions for
which the second property hold as ``universal.''

\end{exercise}

\begin{group}
\begin{exercise}
Given the universe $\kuni{}$ and the range $\natspre{m}$,
%
construct a set of hash functions $\kallhash{}$ 
such that 
%
for all $x \in \kuni{}$, and 
%
for all $i, 0  \le i < m$, 
\[
\probover{h \in \kallhash{}}{h(x) = i} = 1/m
\]
but the following does not hold:
for all $x, y \in \kuni{}$ such that $x \not= y$
\[
\probover{h \in \kallhash{}}{h(x) = h(y)} = 1/m.
\]
\end{exercise}
\begin{solution}
Let $\kallhash{}$ be the set of all distinct constant functions, each
of which maps all the elements in the universe to a single hash code
in the range.
%
The first property holds because for a uniformly randomly hash
function, each hash code is equally likely to be selected.
%
The second property does not hold, because each hash function is a
constant function and thus  the releveant probability is $1.0$.
\end{solution}

\end{group}

\end{unit}

\begin{unit}[Universal Hashing]


\begin{gram}
We have seen in the previous section that if we construct a random
hash function by selecting for each key a uniformly random hash code,
then the function minimizes the number of collision:
%
the probability that any two keys collide is $1/m$ for the range
$\natspre{m}$.
%
We have also seen, however, that such a random hash function is not
compact, because we have to remember the mapping of keys to hash
values explicitly.
%
A natural question is whether it is possible to construct a hash
function that is compact and cheap to compute but has the same
guarantee over collisions.
%
In this section, we shall see that this is indeed possible.
\end{gram}

\begin{definition}[Universal Class of Hash Functions]
Let $\kallhash{}$ be a class (set) of hash function from a universe
$\kuni{}$ to the range $\natspre{m}$ for some $m$.
%
We say that $\kallhash{}$ is \defn{universal} if the probability that
two distinct keys of the universe collide under a uniformly randomly
chosen hash function is $1/m$, i.e.,
\[
\text{for all}~x, y \in \kuni{}~\text{such that}~x \not= y, \probover{h \in \kallhash{}}{h(x) = h(y)} = 1/m.
\] 
\end{definition}

\begin{gram}
There are at least several different ways to obtain classes of  universal
hash functions. 
%
We will state here, without proof, a few methods that are probably
some of the most commonly used methods.
\end{gram}

\begin{theorem}[Multiplicative Hashing (I)]
Consider a finite universe $\kuni{} \subset \nats{}$ and any range
$\natspre{m}$.
%
Let $p$ be a prime number that is greater than the size of the
universe, i.e., $p > |\kuni{}|$.
%
For any natural number $a$, $0 < a < p$,  let
\[
h_a(x) = (a x \bmod p) \bmod m.
\] 

The class of hash functions \kallhash{} defined as
\[
\kallhash{} = \{ h_a(x) ~|~ 0 < a < p \}
\]
is universal.
\end{theorem}

\begin{theorem}[Multiplicative Hashing with Offset)]
Consider a finite universe $\kuni{} \subset \nats{}$ and any range
$\natspre{m}$.
%
Let $p$ be a prime number that is greater than the size of the
universe, i.e., $p > |\kuni{}|$.
%
For integers $a$ and $b$,  $0 < a, b < p$,  let
\[
h_{a,b}(x) = (a x + b \bmod p) \bmod m.
\] 

The class of hash functions \kallhash{} defined as
\[
\kallhash{} = \{ h_{a,b}(x) ~|~ 0 < a, b < p \}
\] 
is universal.
\end{theorem}


\begin{gram}
As evidenced by these theorems, it is relatively easy to construct a
class of hash functions that are universal that are also compact and
easily computable.
%
One somewhat concerning assumption could be that we need a prime that
is larger than the size of the universe.
%
The next theorem eliminates this assumption by allowing us to work
with essentially any sufficiently large prime number.
\end{gram}

\begin{theorem}[Generalized Multiplicative Hashing]
Let $p$ be a prime number and $r$ be a positive integer.
%
Consider the universe $\kuni{} = \natspre{p^r}$ and
the range $\natspre{p}$.
%
For any natural number $a$, $0 < a < p^r$,  let
\[
h_a(x) =  \left( \sum_{i = 0}^{r-1}{a_i \cdot x_i} \right) \bmod p,
\] 
where $a_i$ and $x_i$ denote the $i^{th}$ digit of $a_i$ and $x_i$ in
base $p$

The class of hash functions \kallhash{} defined as
\[
\kallhash{} = \{ h_a(x), ~|~ 0 < a < p^r \}
\]
is universal.
\end{theorem}

\begin{gram}[Intuition]
The idea behind the theorem is to read the keys of the universe as
numbers in base $p$, which is a prime number.
%
We select $r$ to be big enough such that all keys are natural numbers
less than $p^r$. 
%
Given some $0 \le a < p^r$, we then define the hash function $h(x)$ to
the sum of the products of digits of $a$ and the key $x$ modulo
$p$.
%
\end{gram}

\begin{remark}
The theorem fixes the hash codes to be numbers congruent to a prime
$p$, i.e., the integers between $0$ and $p-1$.
%
For the theorem to be effective, we would need to select $p$ to be
close to the number of distinct hash codes that we are interested in.
%
\end{remark}

\begin{example}[Universal Hashing for Strings]
Generalized multiplicative hashing yields a natural hash function for
strings.
%
Let $r$ be the maximum length of the strings and interpret each
character of the string as a natural number.
%
Select a prime $p$ to bound the value of each character. 
%

For any length-$r$ string $a$, define
\[
h_a(x) = \left( \sum_{i = 0}^{r-1}{a_i \cdot x_i} \right) \bmod p,
\] 
where $a_i$ and $x_i$ denote the $i^{th}$ character of $a_i$ and $x_i$.

The class of hash functions $\kallhash{}$ defined as
\[
\kallhash{} = \{ h_a(x) ~|~ a~\text{is a strings of length}~r \}
\]
is universal.
\end{example}


\begin{exercise}
Consider a universal class of hash function $\kallhash{}$ for some
universe $\kuni{}$ and $T \subseteq U$ be any subset of $U$.
%
Prove that for any key $x \in U$, the number of keys whose hash value
collide with $x$ under a uniformly randomly chose hash function $h \in
\kallhash{}$ is constant. 
\end{exercise}

\begin{teachnote}
There is something unsatisfactory with the prime choice, we should be
able to scale it back to mod m for any m.  I think jeff erickson's
notes do this but not i a direct way perhaps.
\end{teachnote}

\end{unit}



\end{section}

\begin{section}[Hash Tables]
\begin{unit}[Preliminaries]


\begin{gram}[Interface]
A hash table is an abstract data type that supports the following
operations on a set of keys drawn from a universe (e.g., integers,
strings, records) and values drawn from possibly another universe.
%
As suggested by the definition of a universe being a set, we assume
that there exists an equality function on keys.

\begin{itemize}
\item The $\cd{createTable}$ function takes as argument a hash function
  from keys to natural numbers, an equality function on keys, and an
  initial size and creates an empty hash table of that given size.

\item The $\cd{insert}$ function takes as argument a hash table and a
  key-value pair and inserts the pair into the table.

\item The $\cd{find}$ function takes a hash table and a key and returns
  the value for the key stored in the hash table if any, or indicates
  that the key is not found.

\item The $\cd{resize}$ function take a hash table and a new size,
  usually double or half the current size, and returns a new hash
  table that contains the same key-value pairs as in the original
  paper, nothing less and nothing more. 
\end{itemize}

As suggested by the operations, hash tables enable us to maintain a
dynamically changing mapping from keys to values. 
%
In this sense, they are similar to other data structures such as
binary search trees but they don't require the keys to be totally
ordered, neither do they demand a comparison function on them. 
%
Instead, they require an equality function and a hash function on keys.

\end{gram}

\begin{gram}[Collisions]
The main challenge in designing hash tables is dealing collisions,
where two keys hash to the same location.  If the universe is known in
advance, it is possible to design a hash function that avoids all
collisions---this is called \defn{perfect hashing}.  
%
But in many applications, the universe is large, making perfect
hashing infeasible. 
%
As we shall see, we can resolve collisions relatively quickly by using
a bit more ``room'' so that we can spreads the key-value pairs in the
table.
%
There are several well-studied collision resolution strategies.
\begin{itemize}
\item \textbf{Nested tables:} use an outer table to map each key to an
  inner table that contains the entries that map to that key.  The
  inner table can be represented in many different ways, including as
  a lists or as another hash table.  If the inner table is a linked
  list, the technique is called \defn{separate chaining}.   


\item \textbf{Flat Tables or Open Addressing}: Use a single, flat
  table mapping keys to entries.
%
Leave some ``room'' in the table to accommodate collisions.
%

\item \textbf{Perfect hashing}: If the universe is small and known in
  advance, then it is possible to avoid collisions entirely, by using
  a family of hash function and a nested table of two-levels deeps.

\item \textbf{Multiple-choice hashing hashing}: use multiple,
  independent hash functions to resolve conflicts.
\end{itemize}
\end{gram}

\begin{example}
Different types of hash tables.  The grey indicates the location is
already full with another key.
\begin{center}\hspace*{-.25in}
\begin{tabular}{cccc}
\includegraphics[height=2in]{./media/separate-chaining.jpg} &
\includegraphics[height=2in]{./media/open-addressing.jpg} &
\includegraphics[height=2in]{./media/perfect-hashing.jpg} &
\includegraphics[height=2in]{./media/cuckoo-hashing.jpg} \\
separate chaining & open addressing & perfect hashing & cuckoo hashing
\end{tabular}
\end{center}
\end{example}

\end{unit}


\begin{unit}[Nested Tables]
\begin{gram}[A Parametric Nested Table Implementation]
The basic structure of a nested table implementation is very simple:
keep an outer table that maps each key to an inner table, which can be
used to resolve collisions. 
%
Given a key, we  use an outer hash function to determine the inner
table that the key maps to.
%
We then use the inner table to resolve the collisions.
%
Because the outer hash function maps keys to a prefix of the natural
numbers, the domain of the inner table is natural numbers less than
the current size $m$. 
%

We can thus use an array to represent the outer table and locate the
inner table efficiently, doing constant work.
%
We leave the implementation of the inner table to the application to decide.
\end{gram}

\begin{example}
Consider the following key value pairs

\[
\begin{alignat}{2}
\{
 & (\cstr{aa}, \cstr{a}), (\cstr{bb}, \cstr{b}), (\cstr{cc}, \cstr{b}), (\cstr{dd}, \cstr{d}), (\cstr{ee}, \cstr{e}),
\\
 & (\cstr{ff}, \cstr{f}), (\cstr{gg}, \cstr{g}), (\cstr{hh}, \cstr{h}), (\cstr{ii}, \cstr{i}), (\cstr{jj}, \cstr{j})
\}.
\end{alignat}
\]

Let $h(x) = \left(\sum{\mathit{pos}(x[i])}\right) \bmod m$ be a hash function that
maps each string to a hash code by summing up the positions of its
characters in the alphabet (counting from zero) modulo the table size
$m = 5$.

The nested hash table for the set of key-value pairs is

\[
\begin{alignat}{1}
\{
& 0 \mapsto \{ \cstr{aa} \mapsto \cstr{a}, \cstr{ff} \mapsto \cstr{f} \},
\\
& 1 \mapsto \{ \cstr{dd} \mapsto \cstr{d}, \cstr{ii} \mapsto \cstr{i} \},
\\
& 2 \mapsto \{ \cstr{bb} \mapsto \cstr{b}, \cstr{gg} \mapsto \cstr{g} \},  
\\
& 3 \mapsto \{ \cstr{ee} \mapsto \cstr{e}, \cstr{jj} \mapsto \cstr{j} \},  
\\
& 4 \mapsto \{ \cstr{cc} \mapsto \cstr{c}, \cstr{hh} \mapsto \cstr{h} \},  
\\
\} & .
\end{alignat}
\]
\end{example}

\begin{exercise}
Describe how you can implement the hash table interface specified
above by using nested tables.
%
For the inner tables use the Table ADT that you have learned about
earlier but leave out the implementation and thus the costs
unspecified.
\end{exercise}


\begin{exercise}
Analyze now the cost of your implementation by assuming that each
inner table is no larger than some value $c$.
\end{exercise}


\begin{gram}[Bounding the Inner Tables]
The key quantity of interest in understanding the efficiency of nested
tables is the size of an inner table.
%
Since any inner table stores the key-value pairs that conflict with
each other, it suffices to bound the number of conflicts between keys.
%

Define $\xconfp{x}{y}$ to be  the indicator random variable such that 
\[
\xconfp{x}{y} = 
\left\{ 
\begin{array}{ll}
1 & \mbox{if}~h(x) = h(y)
\\
0 & \mbox{otherwise}
\end{array}
\right.
\]

Because $\xconfp{x}{y}$ is an indicator random variable its expectation
is the same as the probability that it is $1$.
%
Assuming simply uniform hashing, or more weakly universal hashing, we
know that for any $x \not= y$
\[
\expct{\xconfp{x}{y}} = \prob{\xconfp{x}{y} = 1} = 1/m.
\]

Let's now consider the total number of collisions that any key is
involved in.
%
Define the random variable $\xconf{x}$ to be total number of keys
other than $x$ that collide with $x$. 
\[
\begin{array}{lcl}
\xconf{x} = \sum_{y, y \not= x}{\xconf{x}{y}}
\\
\expct{\xconf{x}} = \sum_{y, y \not= x}{\expct{\xconfp{x}{y}}}
\\
\expct{\xconf{x}} = \frac{n-1}{m}.
\end{array}
\]

Thus we can conclude that if $x$ is in the hash table, then the
expected size of its inner table is $1 + \frac{n-1}{m} \le 1 +
\kloadfac{}$
\end{gram}

\begin{teachnote}
It is an interesting exercise to try to do the same analysis by trying
to bound the length of an inner table at position i.  It should fail
because you need a max over expectaitons but then you need
independence.

Note also that the analysis does not bound the expected maximum length
of the table, and thus cannot be interpreted as expected worst-case time.
\end{teachnote}

\begin{gram}[Load Factor]
The analysis of the expected number of collisions involving each key
shows that the key quantity to limit in a hash table is the load
factor, which is the ratio of $n$ the number of elements in the table
to the size of the outer table $m$.
%

Because $m$ is fixed and $n$ changes, the load factor can increase as we
insert items into the table.
%
To keep the load factor from growing, we can resize the table, by for
example doubling it every time the load factor exceeds $0.5$ or some
other value.
%
The cost of the resize operation can be amortized by the fact doubling
ensures that the new keys pay for the old ones.
\end{gram}

\begin{exercise}
Does it make sense to reduce the size of the hash table?  If so, then
under what conditions and how?
\end{exercise}


\begin{gram}[Separate Chaining]
\defn{Separate chaining} uses a list implementation for the inner
tables.
\end{gram}

\begin{exercise}[Cost Analysis]
Analyze the cost of hash table operations using separate chaining.
\end{exercise}
\end{unit}


\begin{unit}[Open Addressing]
\end{unit}
\end{section}
\end{chapter}
\end{book}
