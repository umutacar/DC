%%
%% TODO: 1) complete the chapter.
%% 2) Add some brick questions (leaves dominated etc)
%%
\documentclass{course}
\title{Parallel and Sequential Algorithms}

% Course number must be unique in the database
\coursenumber{15210}

\semester{Spring 2018}
\picture{/210/course/air-pavilion.jpg}
\website{http://www.cs.cmu.edu/~15210}

% Provides book
% This must be provided
% The name should be relative to course number.
\providesbook{S18}

% Start counting chapters from 
% This is optional. Will start counting at 1.
\provideschapter{4}


15-210 aims to teach methods for designing, analyzing, and programming
sequential and parallel algorithms and data structures. The emphasis
is on teaching fundamental concepts applicable across a wide variety
of problem domains, and transferable across a reasonably broad set of
programming languages and computer architectures. This course also
includes a significant programming component in which students will
program concrete examples from domains such as engineering, scientific
computing, graphics, data mining, and information retrieval (web
search).

Unlike a traditional introduction to algorithms and data structures,
this course puts an emphasis on parallel thinking â€” i.e., thinking
about how algorithms can do multiple things at once instead of one at
a time. The course follows up on material learned in 15-122 and 15-150
but goes into significantly more depth on algorithmic issues. 


\begin{book}
\title{Algorithm Design: Parallel and Sequential}
\label{15210-2016}
\unique{15210S18}
\authors{Umut A. Acar and Guy Blelloch}


\begin{chapter}[Analysis of Algorithms]
%\unique{2}
\picture{./media/nafea-faa-ipoipo.jpg}

This chapter describes asymptotic notation and cost models that we use
for analyzing the resource consumption of algorithms.


\begin{section}[Introduction]

This section describes the basic framework for algorithm analysis.

\begin{unit}[Abstraction and Analysis]

\begin{gram}[Algorithm Analysis]
The term~\defn{algorithm analysis} refers to mathematical analysis of
algorithms for the purposes of determining their consumption of
resources such as the amount of total work they perform, the energy
they consume, the time to execute, and the memory or storage space
that they require.
%
When analyzing algorithms, it is important to be precise so that we
can compare different algorithms to assess their suitability for our
purposes or to select the better one.
%
It is also equally important to be abstract enough so that we don't
have to worry about details of compilers and computer architectures,
and our analysis remains valid even as these change over time.

To find the right balance between precision and abstraction, we rely
on two levels of abstraction: asymptotic analysis and cost models.
%
Asymptotic analysis enables abstracting from small factors such as the
exact time a particular operation may require. 
%
Cost models  specify the cost of operations available in a
computational model, usually only up to the precision of the
asymptotic analysis. 
%

%% Of the two forms of cost models, machine-based models and
%% language-based models, we use, in this book, a language-based cost
%% model.
%% %
%% Perhaps the most important reason for this is the increase in the
%% complexity of specification and the analysis of algorithms based on
%% machine-based cost models.  Such models usually require reasoning
%% about the mapping of a parallel algorithm to actual parallel hardware,
%% which involves for example reasoning about the effects of scheduling.
\end{gram}

\begin{checkpoint}
\begin{questionfr}
\points 10
\prompt
Why do we need the two levels of abstraction?
\answer 
See the discussion above.
\end{questionfr}
\end{checkpoint}
\end{unit}
\end{section}

\begin{section}[Asymptotic Complexity]
\label{sec:asymp}

This section reviews the basic terminology on asymptotic complexity. 

\begin{unit}[Asymptotic Dominance]

\begin{gram}
When analyzing algorithms, we are usually interested in
properties such as the total work, the running time, or space usage. 
%
In such analysis, we typically characterize the behavior of an
algorithm with a~\defn{numeric function} from the domain of natural
numbers to the domain of real numbers.
%
The parameters of such numeric functions, natural numbers, typically
denote the size of the problem-instance or the input.
%
\end{gram}

\begin{example}[Numeric Functions]
By analyzing the work of the algorithm~$A$ for problem~$P$ in terms of
its input size $n$, we may obtain the numeric function
%
$$W_A(n) = 2n\log{n} + 3n + 4\log{n} + 5.$$  
%
By applying the analysis method to another algorithm, algorithm~$B$,
we may derive the numeric function
%
$$ W_B(n) = 6n + 7\log^2{n} + 8\log{n} + 9.$$
%
Both of these functions are numeric because their domain is the natural
numbers.
\end{example}

\begin{gram}
When given numeric functions, how should we interpret them?
%
Perhaps more importantly given two algorithms and their work cost as
represented by two numeric functions, how should we compare them?
%
One option would be to calculate the two functions for varying values
of~$n$ and pick the algorithm that does the least amount of work for
the values of $n$ that we are interested in.

In computer science, we typically care about the cost of an algorithm
for large inputs.
%
We are therefore usually interested in the~\defn{growth} or
the~\defn{growth rate} of the functions.
%
Asymptotic analysis offers a technique for comparing algorithms by
comparing the growth rate of their cost functions.
%
\end{gram}


\begin{example}[Asymptotics]
Consider two algorithms~$A$ and $B$ for a problem~$P$ and suppose that
their work costs, in terms of the input size $n$, are 
%
$$W_A(n) = 2n\log{n} + 3n + 4\log{n} + 5, \mbox{and}$$
%
$$W_B(n) = 6n + 7\log^2{n} + 8\log{n}+ 9.$$
%
Via asymptotic analysis, we derive 
%
$$W_A(n) = \Theta(n\log{n}),~\mbox{and}$$
%
$$W_B(n) = \Theta(n).$$
%
Since ~$n\log{n}$ grows faster that~$n$, we would usually prefer the
second algorithm, because it performs better for sufficiently large
inputs.

%
The difference between the exact work expressions and the ``asymptotic
bounds'' written in terms of the ``Theta'' functions is that the
latter ignores so called~\defn{constant factors}, which are the
constants in front of the variables, and~\defn{lower-order terms},
which are the terms such as $3n$ and $4\log{n}$ that diminish in
growth with respect to $n\log{n}$ as $n$ increases.
\end{example}


\begin{remark}
In addition to enabling us to compare algorithms, asymptotic analysis
also allows us to ignore certain details such as the exact time an
operation may require to complete on a particular architecture.
%
This is important because it makes it possible to apply our analysis
to different architectures, where such constant may differ.
%
Furthermore, it also enables us to create more abstract cost models:
in designing cost models, we assign most operations unit costs
regardless of the exact time they might take on hardware.
%
\end{remark}


\begin{teachask}
Do you know of an algorithm that compared to other algorithms for the
same problem, performs asymptotically better at large inputs but poorly
at smaller inputs.
\end{teachask}

\begin{group}
\begin{exercise}
Compared to other algorithms solving the same problem, some algorithm
may perform better on larger inputs than on smaller ones.  Can you
give an example? 
\end{exercise}

\begin{solution}
There are many such algorithms.  
%
A classic example is the merge-sort algorithm that performs
$\Theta(n\log{n})$ work but performs worse on smaller inputs than the
asymptotically inefficient $\Theta(n^2)$-work insertion-sort
algorithm.
%
Asymptotic notation does not help in comparing the efficiency of
insertion-sort and merge sort at small input sizes.
%
For this, we need to compare their actual work functions which include
the constant factors and lower-order terms that asymptotic notation
ignores.
\end{solution}

\end{group}

%

\begin{gram}
In computer science, we use a variety of asymptotic functions to
reason about algorithms. Example such functions include the
``Big-Oh'', ``Omega'' and ``Theta'' functions.
%
All of these asymptotic functions are defined based on the notion of
asymptotic dominance.
%
\end{gram}


\begin{definition}[Asymptotic dominance]
Let $f(\cdot)$ and $g(\cdot)$ be two numeric functions, we say that
$f(\cdot)$~\defn{asymptotically dominates} $g(\cdot)$ or that
  $g(\cdot)$ is asymptotically dominated by $f(\cdot)$ if there exists
  constants~$c > 0$ and~$n_0 > 0$ such that for all for all $n \ge
  n_0 > 0$,
\[
|g(n)| \le c \cdot f(n).
\]

When a function $f(\cdot)$ asymptotically dominates another
$g(\cdot)$, we say that $f(\cdot)$ \defn{grows faster} than
$g(\cdot)$: the absolute value of $g(n)$ does not exceed a
constant multiple of $f(n)$ for sufficiently large $n$.
\end{definition}

\begin{example}
In the following example, the function $f(\cdot)$ asymptotically
dominates and thus grows faster than the function $g(\cdot)$.
%
\begin{itemize}
\item $f(n) = 2n$, $g(n) = n$.
\item $f(n) = 2n$, $g(n) = 4n$.
\item $f(n) = n\lg{n}$, $g(n) = 8n$.
\item $f(n) = n\lg{n}$, $g(n) = 8n\lg{n} + 16n$.
\item $f(n) = n\sqrt{n}$, $g(n) = n\lg{n} + 2n$.
\item $f(n) = n\sqrt{n}$, $g(n) = n\lg^8{n} + 16n$.
\item $f(n) = n^2$, $g(n) = n\lg^2{n} + 4n$.
\item $f(n) = n^2$, $g(n) = n\lg^2{n} + 4n\lg{n} + n$.
\end{itemize}
\end{example}

\begin{checkpoint}
\begin{questionfr}
\points 10
\prompt
Prove that $n^2$ grows faster than $2n^2 + 10n$.
\answer

\end{questionfr}

\begin{questionfr}
\points 10
\prompt
Prove that $n^2$ grows faster than $2n\lg{n} + 10\lg^4{n}$.
\answer

\end{questionfr}

\begin{questionfr}
\points 10
\prompt
Prove that $n\lg{n}$ grows faster than $5n\lg{n} + 6n$.
\answer

\end{questionfr}


\begin{questionfr}
\points 10
\prompt
Prove or disprove:  $O(n\lg^3{n})$ grows faster than $5n\lg^2{n} + 6n\lg{n}$.
\answer

\end{questionfr}


\begin{questionfr}
\points 10
\prompt
Prove or disprove: $O(\sqrt{n})$ grows faster than $7\lg^2{n} + 6\lg{n}$. 
\answer

\end{questionfr}

\begin{questionfr}
\points 10
\prompt
Prove or disprove:
for any  two numeric functions, $f(\cdot)$ and $g(\cdot)$, 
$f(n) + g(n)$ grows faster than $\max{}(f(n), g(n)).$
\]
\answer

\end{questionfr}

\begin{questionfr}
\points 10
\prompt
Prove or disprove:
for any  two numeric functions, $f(\cdot)$ and $g(\cdot)$, such that $f(n) >
g(n)$ for any $n$, 
$f(n) - g(n)$ grows faster than $\minof{}(f(n), g(n))$.
\]
\answer

\end{questionfr}

\end{checkpoint}
\end{unit}

\begin{unit}[Upper bounds and Big-Oh: $O(\cdot)$]
%[Upper bounds and the Big-Oh: $O(\cdot)$]

\begin{gram}
Given a function $f(n)$, the asymptotic notation $O(f(n))$ denotes the
set of all functions that are asymptotically dominated by the function
$f(n)$.
%
This means that the set consists of the functions that grow at slower
rate than $f(n)$.  We write $g(n) \in O(f(n))$ to refer to a function
$g(n)$ that is in the set $O(f(n))$.
%
We often think of $f(n)$ being an~\defn{upper bound} for $g(n)$.
\end{gram}

\begin{definition}[The $O(\cdot)$ Notation]
For a function $g(n)$, we say that $g(n) \in O(f(n))$
if there exist  constants $n_0 > 0$ and $c > 0$ such that for all $n
\geq n_0 > 0$, we have $g(n) \leq c \cdot f(n)$.
%

For convenience, we usually replace the set membership and write $g(n)
= O(f(n))$ instead of $g(n) \in O(f(n))$.
\end{definition}

\begin{example}
The following upper bounds hold for the function $f(\cdot)$.
%
\begin{itemize}
\item $f(n) = 2n = O(n)$
\item $f(n) = 4n\lg{n} = O(n\lg{n})$
\item $f(n) = n = O(n\lg{n})$
\item $f(n) = n\lg{n} + 2n = O(n\sqrt{n})$
\item $f(n) = n\lg^8{n} + 16n = O(n\sqrt{n})$
\item $f(n) = n\lg^2{n} + 4n = O(n^2)$
\item $f(n) = n\lg^2{n} + 4n\lg{n} + n = O(n^2)$
\end{itemize}

\end{example}

\begin{group}
\begin{exercise}
Prove or disprove the following statement:
%
if $g(n) = O(f(n))$ and $g(n)$ is a finite function ($g(n)$ is finite
for all $n$), then it follows that there exist constants $k_1$ and
$k_2$ such that for all $n \geq 1$,
\[
g(n) \;\leq\; k_1\cdot f(n) + k_2.
\]
\end{exercise}


\begin{solution}
The statement is correct. 
%
For example, we can take $k_1 = c$ and $k_2 = \sum_{i=1}^{n_0} |g(i)|$.
\end{solution}

\end{group}



\begin{exercise}[Graphical Illustration of Upper bounds]
Can you illustrate graphically when $g(n) \in O(f(n))$?  Show
different cases by considering different functions.
\end{exercise}

\begin{checkpoint}
\begin{questionfr}
\points 10
\prompt
Prove that $2n^2 + 10n \in O(n^2)$.
\answer

\end{questionfr}

\begin{questionfr}
\points 10
\prompt
Prove that $2n\lg{n} + 10\lg^4{n} \in O(n^2)$.
\answer

\end{questionfr}

\begin{questionfr}
\points 10
\prompt
Prove that $5n\lg{n} + 6n \in O(n\lg{n})$.
\answer

\end{questionfr}


\begin{questionfr}
\points 10
\prompt
Prove or disprove $5n\lg^2{n} + 6n\lg{n} \in O(n\lg^3{n})$.
\answer

\end{questionfr}


\begin{questionfr}
\points 10
\prompt
Prove or disprove $7\lg^2{n} + 6\lg{n} \in O(\sqrt{n})$.
\answer

\end{questionfr}

\begin{questionfr}
\points 10
\prompt
Prove or disprove:
for any  two numeric functions, $f(\cdot)$ and $g(\cdot)$, 
\[
\max{}(f(n), g(n)) = O(f(n) + g(n)).
\]
\answer

\end{questionfr}

\begin{questionfr}
\points 10
\prompt
Prove or disprove:
for any  two numeric functions, $f(\cdot)$ and $g(\cdot)$, such that $f(n) >
g(n)$ for any $n$ 
\[
\minof{}(f(n), g(n)) = O(f(n) - g(n)).
\]
\answer

\end{questionfr}

\begin{questionfr}
\points 10
\prompt
Prove or disprove:
for any  two numeric functions, $f(\cdot)$ and $g(\cdot)$, 
\[
f(n) * g(n) = O(f(n)) * O(g(n)).
\]
\answer

\end{questionfr}



\end{checkpoint}

\end{unit}

\begin{unit}[Lower bounds and Omega: $\Omega(\cdot)$]

\begin{gram}
Given a function $f(n)$, the asymptotic notation $\Omega(f(n))$
denotes the set of all functions that asymptotically dominate the
function $f(n)$.
%
This means that the set consists of the functions that grow faster
than~$f(n)$.
%
We write $g(n) \in \Omega(f(n))$ to refer to a function $g(n)$ that is
in the set $\Omega(f(n))$.  
%
We think of $f(n)$ being a \defn{lower bound} for $g(n)$.
\end{gram}

%%% TODO: CHECK THIS DEFINITION
\begin{definition}[The $\Omega$ Notation]
For a function $g(n)$, we say that $g(n) \in \Omega(f(n))$
if there exist constants $n_0 > 0$ and $c > 0$ such that for all $n
\geq n_0 > 0$, we have $c \cdot f(n) \leq |g(n)|$.

For convenience, we usually replace the set membership and write $g(n)
= \Omega(f(n))$ instead of $g(n) \in \Omega(f(n))$.
\end{definition}

\begin{example}
The following lower bounds hold for the function $f(\cdot)$.
%
\begin{itemize}
\item $f(n) = 2n = \Omega(n)$
\item $f(n) = 4n\lg{n} = \Omega(n\lg{n})$
\item $f(n) = n\lg{n} \Omega(n)$
\item $f(n) = n\sqrt{n} = \Omega(n\lg{n} + 2n)$
\item $f(n) = n\sqrt{n} = \Omega(n\lg^8{n})$
\item $f(n) = n^2 = \Omega(n\lg^2{n})$
\item $f(n) = n^2 = \Omega(n\lg^16{n})$
\end{itemize}
\end{example}

\begin{checkpoint}
\begin{questionfr}
\points 10
\prompt
Prove that $2n^2 + 10n \in \Omega(n^2)$.
\answer

\end{questionfr}

\begin{questionfr}
\points 10
\prompt
Prove or disprove $2n\lg{n} + 10\lg^4{n} \in \Omega(n^2)$.
\answer

\end{questionfr}

\begin{questionfr}
\points 10
\prompt
Prove or disprove: $5n\lg{n} + 6n \in \Omega(n\lg{n})$.
\answer

\end{questionfr}


\begin{questionfr}
\points 10
\prompt
Prove or disprove $5n\lg^2{n} + 6n\lg{n} \in \Omega(n\lg{n})$.
\answer

\end{questionfr}


\begin{questionfr}
\points 10
\prompt
Prove or disprove $7\lg^2{n} + 6\lg{n} \in O(\lg{n})$.
\answer

\end{questionfr}


\begin{questionfr}
\points 10
\prompt
Prove or disprove:
for any  two numeric functions, $f(\cdot)$ and $g(\cdot)$, 
\[
\maxof{}(f(n), g(n)) = \Omega(f(n) + g(n)).
\]
\answer

\end{questionfr}

\begin{questionfr}
\points 10
\prompt
Prove or disprove:
for any  two numeric functions, $f(\cdot)$ and $g(\cdot)$, 
\[
\max{}(f(n), g(n)) = \Omega(\minof{}(f(n), g(n))).
\]
\answer

\end{questionfr}

\begin{questionfr}
\points 10
\prompt
Prove or disprove:
for any  two numeric functions, $f(\cdot)$ and $g(\cdot)$, such that $f(n) >
g(n)$ for any $n$ 
\[
\minof{}(f(n), g(n)) = \Omega(f(n) - g(n)).
\]
\answer

\end{questionfr}

\begin{questionfr}
\points 10
\prompt
Prove or disprove:
for any  two numeric functions, $f(\cdot)$ and $g(\cdot)$, 
\[
f(n) * g(n) = \Omega(f(n)) * O(g(n)).
\]
\answer

\end{questionfr}


\end{checkpoint}

\end{unit}

\begin{unit}[Tight bounds and Theta: $\Theta(\cdot)$]

\begin{gram}
The asymptotic expression $\Theta(f(n))$ is the set of all functions
that grow at the same rate as $f(n)$.  In other words, the set
$\Theta(f(n))$ is the set of functions that are both in $O(f(n))$ and
$\Omega(f(n))$.

We write $g(n) \in \Theta(f(n))$ to refer to a function $g(n)$ that is
in the set $\Theta(f(n))$.  We often think of $f(n)$ being a
\emph{tight bound} for $g(n)$.
\end{gram}

\begin{group}

\begin{definition}[The $\Theta$ Notation]
For a function $g(n)$, we say that $g(n) \in \Theta(f(n))$ if there
exist positive constants $n_0$, $c_1$, and $c_2$ such that for all $n
\geq n_0$, we have $c_1 \cdot f(n) \leq |g(n)| \leq c_2\cdot f(n)$.

For convenience, we usually replace the set membership and write $g(n)
= \Theta(f(n))$ instead of $g(n) \in \Theta(f(n))$.
\end{definition}

\begin{example}
The following lower bounds hold for the function $f(\cdot)$.
%
\begin{itemize}
\item $f(n) = 2n = \Theta(n)$
\item $f(n) = 4n\lg{n} = \Theta(n\lg{n})$
\item $f(n) = n\sqrt{n} + n\lg^2{n} = \Theta(n\sqrt{n})$
\item $f(n) = n\sqrt{n} + n\lg^3{n} = \Theta(n\sqrt{n})$
\item $f(n) = n^2 + n\lg^4{n} = \Theta(n^2)$
\end{itemize}
\end{example}

\end{group}

\begin{checkpoint}
\begin{questionfr}
\points 10
\prompt
Prove that $2n^2 + 10n \in \Theta(n^2)$.
\answer

\end{questionfr}

\begin{questionfr}
\points 10
\prompt
Prove or disprove $2n\lg{n} + 10\lg^4{n} \in \Theta(n^2)$.
\answer

\end{questionfr}

\begin{questionfr}
\points 10
\prompt
Prove or disprove: $5n\lg{n} + 6n \in \Theta(n\lg{n})$.
\answer

\end{questionfr}


\begin{questionfr}
\points 10
\prompt
Prove or disprove $4n\lg^4{n} + 6n\lg{n} \in \Theta(n\lg^4{n})$.
\answer

\end{questionfr}


\begin{questionfr}
\points 10
\prompt
Prove or disprove $7\lg^2{n} + 6\lg{n} \in \Theta(\lg^2{n})$.
\answer

\end{questionfr}


\begin{questionfr}
\points 10
\prompt
Prove or disprove:
for any  two numeric functions, $f(\cdot)$ and $g(\cdot)$, 
\[
\max{}(f(n), g(n)) = \Theta(f(n) + g(n)).
\]
\answer

\end{questionfr}

\begin{questionfr}
\points 10
\prompt
Prove or disprove:
for any  two numeric functions, $f(\cdot)$ and $g(\cdot)$, 
\[
\max{}(f(n), g(n)) = \Theta(\min{}(f(n), g(n))).
\]
\answer

\end{questionfr}

\begin{questionfr}
\points 10
\prompt
Prove or disprove:
for any  two numeric functions, $f(\cdot)$ and $g(\cdot)$, 
\[
f(n) * g(n) = \Theta(f(n)) * O(g(n)).
\]
\answer

\end{questionfr}

\end{checkpoint}
\end{unit}

\begin{unit}[Important conventions]

\begin{gram}
When using asymyptotic notations, we follow some standard conventions
of convenience.
%

\begin{itemize}
\item  We use the equality relation instead of set membership to state
that a function belongs to an asymptotic class, e.g., $g(n) = O(f(n))$
instead of $g(n) \in O(f(n))$. 
%
Although we use the equality, these equalities should not be thought
as symmetric: we never write $O(f(n)) = g(n)$.
%

%% TODO: is this needed
\item 
We use the convention that the right hand side of the equation is more
abstract or less precise than the left hand side.
%
This convention is consistent with the view that the equality 
simply stands in for set membership.


\item 
We treat expressions that involve asymptotic notation as sets. 
%
For example, in $4W(n/2) + O(n)$, the $O(n)$ refers to some function
$g(n) \in O(n)$ that we care not to specify.  We can think of the
expression as the set of all expression that we would obtain by
plugging each possibility for $O(n)$.
%

\item
If we have an equality where the asymptotic notation is used both on
the left and the right hand side, then we interpret the left  hand
side as being a subset of the right hand side.
%
For example, consider the equality $4W(n/2) + O(n) = \Theta(n^2)$.
This equation says that the set on the left hand side is contained in
the set on the right hand side.  
%
In other words, for any function $g(n) = O(n)$, there is some function
$h(n) = \Theta(n^2)$ that satisfy the actual equality.
%
Note that the ``subset'' interpretation of equality is a
generalization of its set-membership interpretation.
\end{itemize}
\end{gram}
\end{unit}
\end{section}


\begin{section}[Cost Models]
\label{sec:analysis::models}

Any algorithmic analysis must assume a \defn{cost model} that specifies
the resource cost of the operations that can be performed by an
algorithm.
%
There are two broadly accepted ways of defining cost models:
machine-based and language-based cost models.
%
In this book,  we use a language-based cost model.
%
Perhaps the most important reason for this is the increase in the
complexity of specification and the analysis of algorithms based on
machine-based cost models.  Such models usually require reasoning
about the mapping of an algorithm to a lower level machine model,
which can be difficult, especially for parallel algorithms.


\begin{unit}[Machine versus Language Models]


\begin{gram}
There are two broadly accepted ways of defining cost models:
machine-based and language-based cost models.

A \defn{machine-based (cost) model} takes a machine model as the
starting point and defines the cost of each instruction that can be
executed by the machine.
%
When using a machine-based model for analyzing an algorithm, we
consider the instructions executed by the algorithm on the machine and
calculate their cost.
%

A \defn{language-based model} takes a programming language as the
starting point and defines cost as a function mapping the expressions
of the language to their cost.
%
Such a cost function is usually defined as a recursive function over
the different forms of expressions in the language.  
%
When using a language-based model for analyzing an algorithm, we apply
the cost function to the expression describing the algorithm in the
language.
\end{gram}



\begin{gram}
There are certain advantages and disadvantages to both models.

The advantage to machine-based models is that they can better
approximate the actual cost of an algorithm, i.e., the cost observed
when the algorithm is executed on actual hardware.
%
The disadvantage is the complexity of analysis and the limited
expressiveness of the languages that can be used for specifying the
algorithms.
%
When using a machine model, we have to reason about how the algorithm
compiles and runs on that machine.  For example, if we express our
algorithm in a low-level language such as C, cost analysis based on a
machine model that represents a von Neumann machine is straightforward
because there is an almost one-to-one mapping of statements in C to
the instructions of such a machine.  For higher-level languages, this
becomes difficult.  There may be uncertainties, for example, about the
cost of automatic memory management, or the cost of dispatching in an
object-oriented language.
%
%
For parallel programs, cost analysis based on machine-based models
even more difficult, because we have to reason about how parallel
tasks of the algorithm are scheduled on the processors of the machine,
a.k.a., scheduling.
%
Due to this gap between the level at which algorithms are analyzed
(machine level) and the level they are usually implemented
(programming-language level), there can be difficulties in
implementing an algorithm in a high-level language in such a way that
matches the bound given by the analysis.

The advantage to language-based models is that they simplify analysis of
algorithms. 
%
When analyzing algorithms in a language-based model we don't need to
care about how the language compiles or runs on the machine.  Costs
are defined directly in the language, specifically its syntax and its
dynamic semantics, which specifies the evaluation of expressions.  We
thus simply consider the algorithm as expressed and analyze the cost
by applying the cost function provided by the model.
%
The disadvantage of language-based models is that the predicted cost
bounds may not precisely reflect the cost observed when the algorithm
is executed on actual hardware.  This imprecision of the language
model can be minimized and even eliminated by defining the model to be
consistent with the machine model and the programming-language
environment assumed such as the compiler and the run-time system.
\end{gram}


\begin{gram}
In the sequential algorithms literature, much work is based on machine
models rather than language-based model, partly because the mapping
from language constructs to machine cost (time or number of
instructions) can be made simple in low-level languages, and partly
because much work on algorithm predates or coincides with the
development of higher-level languages.
%
For parallel algorithms, however, many years of experience shows that
machine based models are difficult to use, especially when considering
higher-level languages that are commonly used in practice today.
%
For this reason, in this book we use a language-based cost model.
%
Our language-based model allows us to use abstract cost measures, work
and span, which have no direct meaning on a physical machine.
\end{gram}

\begin{remark}
We note that both machine models and language-based models usually
abstract over existing architectures and programming languages
respectively.  This is necessary because we wish our cost analysis to
have broader relevance than just a specific architecture or
programming language.
%
For example, machine models are usually defined
to be valid over many different architectures such as an Intel Nehalem
or AMD Phenom. 
%
Similarly, language-based models are defined to be
applicable to a range of languages.  
%
To this end, we usually simplify our cost functions by ignoring
``constant factors'' that depend on the specifics of the actual
practical hardware our algorithms may execute on.
%
Similarly, we may abstract over differences in language constructs,
e.g, a call to a ``plus'' operation, or call to an abstract
function may both take unit time.
%
In this book, we use an abstract language that is essentially lambda
calculus with some syntactic sugar. As you may know the lambda
calculus can be used to model many languages.
\end{remark}


\end{unit}


\begin{unit}[Machine-based cost models]

\begin{definition}[Machine-Based Cost Model]
A~\defn{machine-based (cost) model} takes a machine model and defines the
cost of each instruction that can be executed by the machine.
%
When using a machine-based model for analyzing an algorithm, we
translate the algorithm so that it can be executed on the machine and
then analyze the cost of the machine instructions.

\end{definition}


\begin{gram}[RAM Model]
%
Traditionally, algorithms have been analyzed in the  \def{Random Access
Machine}  or \defn{RAM}. 
%
%%???????
%%(This is not to be confused with Random Access Memory---RAM---model.)  
%
In this model, a machine consists of a single processor that can
access unbounded memory; the memory is indexed by the natural numbers.
%
The processor interprets sequences of machine instructions (code) that
are stored in the memory.  Instructions include basic arithmetic and
logical operations (e.g. $\cd{+}$, $\cd{-}$, $\cd{*}$, and, or,
$\cd{not}$), reads from and writes to arbitrary memory locations, and
conditional and unconditional jumps to other locations in the code.
%
Each instruction takes unit time. 
%
The execution-time, or simply~\defn{time} of a computation is measured
in terms of the number of instructions executed by the machine.

The RAM model is reasonably adequate for analyzing the asymptotic
runtime of sequential algorithms; most work on sequential algorithms
to date has used this model. 
%% Commented out, student advice
%It is therefore important to understand
% the model, or at least know what it is.
%
One reason for the RAM's success is that it is relatively easy to
reason about the cost of algorithms because algorithmic pseudo code
and sequential languages such as C and C++ can easily be mapped
(translated) to the model. 
%
When using higher level languages, however, such as functional
languages and languages with automatic memory management, the mapping
and thus the analysis can be more difficult.
%
The model is suitable for deriving asymptotic bounds (i.e., using
big-O, big-Theta and big-Omega) but not for predicting exact
runtimes. The reason for this is that on a real machine not all
instructions take the same time, and furthermore not all machines have
the same instructions.
\end{gram}

% The model we use in this course also does not directly account for the
% variance in memory costs.  Towards the end of the course, if time
% permits, we will discuss how it can be extended to capture memory
% costs.

\begin{gram}[PRAM Model]
RAM model is sequential but can be extended to use multiple processors
which share the same memory.  The extended model is called the
\defn{Parallel Random Access Machine} or \defn{PRAM}.
%
In PRAM model, each processor is assigned a unique index, starting
from $0$, called a~\defn{processor id}, which the processor has access
to.
%
Processors in the PRAM model operate under the control of a common
clock. For this reason, the PRAM model is considered to be a {\em
  synchronous} model.
%
In PRAM, all processors execute the same program.  This sometimes
leads to programs where each processor executes the same instruction
but possibly on different data.  Some PRAM algorithms therefore fit
into~\defn{single instruction multiple data} or~\defn{SIMD}
programming model.
%
Not all PRAM programs have to be of SIMD type. In fact, since a
processor has access to its processor id, it is technically allowed to
make local decisions independently of other processors and thus can
execute a different instruction that others.
%
Nevertheless, algorithms for the PRAM model are usually specified by
defining a single function for each processor to execute.
%
For example we can specify a PRAM algorithm for adding one to each
element of a integer array with $p$ elements using $p$ processors by
the following program, which is parameterized by the processor id $i$.

\[
\begin{array}{l}
\cd{(* Input: integer array A. *)}
\\
\cd{addone} =  A[i] \la A[i]+1
\end{array}
\]
\end{gram} 


%%% Umut: expanded on this. 
% In the model all of $p$ processors run the same instruction on each
% step, although typically on different data.  
\begin{gram}
Since it is synchronous and it requires the algorithm designer to map
computation to processors (i.e., to perform manual scheduling), the
PRAM model can be awkward to work with.
%
For simple parallel loops over $n$ elements we could imagine dividing
up the elements evenly among the processors---about $n/P$ each,
although some rounding might be required when $n$ is not a
multiple of $P$.
%
If the cost of each iteration of the loop is different then we would
further have to add some load balancing.  In particular simply giving
$n/P$ to each processor might be the wrong choice---one processor
could get stuck with all the expensive iterations.  
%
For computations with nested parallelism, such as divide-and-conquer
algorithms the mapping becomes more complicated.

\end{gram}

\begin{remark}
Even though we don't use the PRAM model in this book, most of the
ideas presented in this course also work with the PRAM, and many of
them were originally developed in the context of the PRAM.
\end{remark}


\begin{remark}
One problem with the RAM model is that it assumes that accessing all
memory locations has the same cost.  On real machines this is not the
case.  In fact, there can be a factor of 100 difference between the
time for accessing a word of memory from the first level cache and
accessing it from main memory.  Various extensions to the RAM model
have been developed to account for this cost.  For example one variant
is to assume that the cost for accessing the $i^{th}$ memory location
is $f(i)$ for some function $f$, e.g. $f(i) = \log(i)$.  Fortunately,
however, most of the algorithms that turn out to be good in these more
detailed models are also good in the RAM.  Therefore analyzing
algorithms in the simpler RAM model is often a reasonable
approximation to analyzing in the more refined models.  Hence the RAM
has served quite well despite not fully accounting for the variance in
memory costs.  The model we use in this course also does not account
for the variance in memory costs, but as with the RAM the costs can be
refined.
\end{remark}


%% \begin{quiz}
%% \end{quiz}
\end{unit}

\begin{unit}[The Work-Span Model]

\begin{gram}
In this book, we use a language-based cost model to analyze parallel
algorithms. 
%
Our language-based cost model is based on two cost metrics: work and
span.  Roughly speaking, the~\defn{work} of computation corresponds to
the total number of operations it performs, and~\defn{span} correponds
to the longest chain of dependencies in the computation.
\end{gram}
%%  As discussed in \secref{analysis::models}, we have to be
%% careful about the cost model to make sure that it can be mapped to
%% real hardware by implementing the necessary compilation and run-time
%% system support.  Indeed, for the cost-model that we describe here,
%% this is the case (see \secref{analysis::scheduling} for more details).

%% To formally define a cost model in terms of programming constructs
%% requires a so-called ``operational semantics''.  We won't define a
%% complete semantics, but will give a partial semantics to give a sense
%% of how it works.  

\begin{example}
The notation
\[
W(7 + 3)
\]
denotes the work of adding $7$ and $3$.

The notation 
\[
S(\cd{fib}(11))
\]
denotes the span for calculating the 11$^{th}$ Fibonacci number.

The notation 
\[
W(\cd{mySort}(S))
\]
denotes the work for $\cd{mySort}$ applied to the sequence~$S$.
\end{example}


\begin{gram}
Note that in the third example the sequence $S$ is not defined within
the expression.  Therefore we cannot say in general what the work is
as a fixed value.  However, we might be able to use asymptotic
analysis to write a cost in terms of the length of $S$, and in
particular if $\cd{mySort}$ is a good sorting algorithm we would have:
\[
W(\cd{mySort}(S)) = O(|S| \log |S|).
\]
Often instead of writing  $|S|$ to indicate the size of the input, we
use $n$ or $m$ as shorthand.  Also if the cost is for a particular
algorithm we use a subscript to indicate the algorithm. This leads to
the following notation
\[
W_{\cd{mySort}}(n) = O(n \log n).
\]
where $n$ is the size of the input of $\cd{mysort}$.  When obvious
from the context (e.g. when in a section on analyzing $\cd{mySort}$)
we sometimes drop the subscript, giving $W(n) = O(n \log n)$.
\end{gram}


%% \begin{gram}
%% The definition below specifies the work and span of expressions in \PML,
%% our language for describing algorithms.
%% %
%% In the definition and throughout this beek, we write~$W(e)$ for the
%% work of the expression and~$S(e)$ for its span. 
%% %
%% Both work and span are cost functions that map an expression to a cost
%% measured as in units of time. 
%% %
%% As common in language-based models, the definition follows the
%% definition expressions for \PML (\chref{language}). 
%% %
%% We make one simplifying assumption in the presentation: instead of
%% considering general bindings, we only consider the case where a single
%% variable is bound to the value of the expression.
%% \end{gram}

\begin{definition}[\PML Cost Model]
\label{def:analysis::pml-cost-model}
The work and span of \PML expressionsare defined below.
%
In the definition and throughout this book, we write~$W(e)$ for the
work of the expression and~$S(e)$ for its span. 
%
Both work and span are cost functions that map an expression to a cost
measured as in units of time. 
%
As common in language-based models, the definition follows the
definition expressions for \PML.
%
% (\chref{language}). 
%
We make one simplifying assumption in the presentation: instead of
considering general bindings, we only consider the case where a single
variable is bound to the value of the expression.

In the definition, the notation $\eval{e}$ evaluates the expression
$e$ and returns the result, and the notation $[v/x]~e$ indicates that
all free (unbound) occurrences of the variable $x$ in the expression
$e$ are replaced with the value $v$.

\[
\begin{array}{lcl}
W(v) & = & 1 
\\
W(\cfn{p}{e})  & = & 1 
\\
W(e_1~e_2) & =  & W(e_1) + W(e_2) 
\\
          &     & + W([\eval{e_2}/x]~e_3) + 1
\\
          &     & \mbox{where}~\eval{e_1} = \cfn{x}{e_3}
\\
W(e_1~\cd{op}~e_2) & = &  W(e_1) + W(e_2) + 1
\\
W(e_1~,~e_2) & = & W(e_1) + W(e_2) + 1
\\
W(e_1~||~e_2) &=& W(e_1) + W(e_2) + 1
\\
W\left(
\begin{array}{l}
\cif~e_1
\\
\cthen~e_2
\\
\celse~e_3
\end{array}
\right)  
& = & 
\left\{
\begin{array}{ll}
W(e_1) + W(e_2) + 1 & \mbox{if}~\eval{e_1} = \cd{true}
\\
W(e_1) + W(e_3) + 1 & \mbox{otherwise}
\end{array}\right.
\\[2ex]
W\left(
\begin{array}{l}
\clet~x=e_1~
\\
\cin~e_2~\cend
\end{array}
\right) 
& = & W(e_1) + W([\eval{e_1}/x]~e_2) + 1
\\[1ex]
W( ( e ) ) & = & W(e)
\end{array}
\]
%
\medskip
%
\[
\begin{array}{lcl}
S(v) & = & 1 
\\[1ex]
S(\cfn{p}{e})  & = & 1 
\\[1ex]
S(e_1~e_2) & =  & S(e_1) + S(e_2) + 1
\\[1ex]
S(e_1~\mbox{op}~e_2) & = &  S(e_1) + S(e_2) + 1
\\[1ex]
S(e_1~,~e_2) & = & S(e_1) + S(e_2) + 1
\\[1ex]
S(e_1~||~e_2) &=& \max{}{(S(e_1),S(e_2))} + 1
\\[1ex]
S\left(
\begin{array}{l}
\cif~e_1
\\
\cthen~e_2
\\
\celse~e_3
\end{array}
\right)
& = & 
\left\{
\begin{array}{ll}
S(e_1) + S(e_2) + 1 & \eval{e_1} = \cd{true}\\
S(e_1) + S(e_3) + 1 & \mbox{otherwise}
\end{array}\right.
\\[2ex]
S\left(
\begin{array}{l}
\clet~x=e_1~
\\
\cin~e_2~\cend
\end{array}
\right) 
& = & S(e_1) + S([\eval{e_1}/x]~e_2) + 1
\\[1ex]
S( ( e ) ) & = & S(e)
\\[1ex]
\end{array}
\]

%   Notice that all the rules for span
% are the same as for work except for parallel application indicated by
% $(e_1\ ||\ e_2)$ and the parallel map indicated by
% $\{f(x) : x \in A\}$.}
%  The expression $e$ inside $W(e)$ and $S(e)$ have to
%be closed.  Note, however, that in the rules such as for \texttt{let}
%we replace all the free occurrences of $x$ in the expression $e_2$ with
%their values before applying $W$.}
\end{definition}


\begin{example}
Consider the expression $e_1 + e_2$ where $e_1$ and $e_2$ are
themselves other expressions (e.g., function application).  Note that
this is an instance of the rule $e_1~\mbox{op}~e_2$, where
\mbox{op} is a plus operation.
%
In \PML, we evaluate this expressions by first evaluating $e_1$ and
then $e_2$ and then computing the sum.  The work of the expressions is
therefore
\[
W(e_1 + e_2) = W(e_1) + W(e_2) + 1
.
\]
The additional $1$ accounts for computation of the sum.   
\end{example}


\begin{group}
\begin{gram}
For the $\cd{let}$ expression, we first evaluate $e_1$ and assign it
to $x$ before we can evaluate $e_2$.  Hence the fact that the span is
composed sequentially, i.e., by adding the spans.
\end{gram}

\begin{example}
In \pml, $\cd{let}$ expressions compose sequentially.
\[
\begin{array}{l}
W(\cd{let}~y = f(x)~\cd{in}~g(y)~\cd{end}) & = & 1 + W(f(x)) + W(g(y))
\\
S(\cd{let}~y = f(x)~\cd{in}~g(y)~\cd{end}) & = & 1 + S(f(x)) + S(g(y))
\end{array}
\]
\end{example}
\end{group}


\begin{teachask}
In \PML, when are expressions evaluated in parallel?
\end{teachask}

\begin{gram}
In \PML, we use the notation $(e_1\ ||\ e_2)$ to mean that the two
expressions are evaluated in parallel.  The result is a pair of values
containing the two results. 
%
As a result, the work and span for all expressions except for the
parallel construct~$||$ are defined in the same way.
%
As we will see later, in addition to the $||$ construct, we assume the
set-like notation such as $\{f(x) : x \in A\}$ to be evaluated in
parallel, i.e., all calls to $f(x)$ run in parallel.
\end{gram}

\begin{group}
\begin{example}
The expression $(\cd{fib}(6)\ ||\ \cd{fib}(7))$ runs the two
calls to {\tt fib} in parallel and returns the pair $(8,13)$.   It
does work 
\[
1 + W(\cd{fib}(6)) + W(\cd{fib}(7))
\] 
and span
\[
1 + \max{}(S(\cd{fib}(6)), S(\cd{fib}(7))).
\]   
If we know that the
span of $\cd{fib}$ grows with the input size, then the span can
be simplified to $1 + S(\cd{fib}(7))$.
\end{example}
\end{group}

\begin{remark}
Since in this book we are assuming purely functional programs, it is
always safe to run things in parallel if there is no explicit sequencing.
Since in \PML, we evaluate~$e_1$ and~$e_2$ sequentially, the span of
the expression is calculated in the same way:
\[
S(e_1 + e_2) = S(e_1) + S(e_2) + 1.
\]
Note that this does not mean that the span and the work of the
expressions are the same!

Since \PML is purely functional language, we could have in fact
evaluated $e_1$ and $e_2$ in parallel, wait for the to complete and
perform the summation.  In this case the span of would have been
\[
S(e_1 + e_2) = \max{}(S(e_1), S(e_2)) + 1
.
\]
Note that since we have to wait for both of the expressions to
complete, we take the maximum of their span. Since we can perform the
final summation serially after they both return, we add the $1$ to the
final span.

In this book, we do not evaluate expressions in parallel unless it is
explicitly indicated so by the syntax.
\end{remark}


\begin{teachnote}
As there is no $||$
construct in the ML, in your assignments you will need to specify in comments when two calls run
in parallel.  We will also supply an ML function $\cd{par (f1,f2)}$ with
type 
%
\[
(\cd{unit} \ra \alpha) \times (\cd{unit} \ra \beta) \ra \alpha
\times \beta.
\]
This function executes the two functions that are passed in as
arguments in parallel and returns their results as a pair.  For
example:

$\cd{par (fn => fib(6), fn => fib(7))}$

returns the pair $(8,13)$.  We need to wrap the expressions in
functions in ML so that we can make the actual implementation run them
in parallel.  If they were not wrapped both arguments would be
evaluated sequentially before they are passed to the function
$\cd{par}$.  

Also in the ML code you do not have the set
notation $\{f(x) : x \in A\}$, but as mentioned before, it is
basically equivalent to a
$\cd{map}$.   Therefore, for ML code you can use the rules:

  \[ W(\mbox{\tt map } f\ \cseq{s_0, \ldots, s_{n-1}}) = 1 + \sum_{i=0}^{n-1} W(f(s_i)) \]
   \[S(\mbox{\tt map } f\ \cseq{s_0, \ldots, s_{n-1}}) = 1 + \max{}_{i=0}^{n-1} S(f(s_i)) \]

\end{teachnote}
\end{unit}

\begin{unit}[Parallelism]

\begin{definition}[Average Parallelism]
Parallelism, sometimes called \defn{average parallelism}, is
defined as the work over the span:
\[ 
\paral = \frac{W}{S}.
\]
%
Parallelism informs us approximately how many processors we can use
efficiently.
\end{definition}

\begin{example}
For a mergesort with work $\Theta(n \log n)$ and span
$\Theta(\log^2 n)$ the parallelism would be $\Theta(n/\log
n)$.
\end{example}

\begin{example}
Consider an algorithm with work $W(n) = \Theta(n^3)$ and span 
$S(n) = \Theta(n \log n)$.
%
For $n = 10,000$, $\paral(n) \approx 10^7$, which is a lot of
parallelism.  
%
But, if $W(n) = \Theta(n^2) \approx 10^8$ then
$\paral(n) \approx 10^3$, which is much less parallelism. 
%
Note that the decrease in parallelism is not because of an increase in
span but because of a reduction in work.
\end{example}

\begin{teachask}
What are ways in which we can increase parallelism?
\end{teachask}

%% \begin{gram}
%% We can increase parallelism by decreasing span and/or increasing work.
%% UnnessIncreasing work, however, is not desirable because it leads to an
%% inefficient algorithm.
%% \end{gram}


\begin{gram}[Designing Parallel Algorithms]

In parallel-algorithm design, we aim to keep parallelism as high
as possible.
%
Since parallelism is defined as the amount of work per unit of span, 
we can do this by decreasing span.
%
We can increase parallelism by increasing work also, but this is
usually not desirable.
%
In designing parallel algorithms our goals are: 

\begin{enumerate}
\item   to keep work as low as possible, and
\item   to keep span as low as possible.
\end{enumerate}

Except in cases of extreme parallelism, where for example, we may have
thousands or more processors available to use, the first priority is
usually to keep work low, even if it comes at the cost of increasing
span.

\end{gram}

\begin{group}
\begin{definition}[Work efficiency]
 We say that a parallel algorithm is {\em
  work efficient} if it perform asymptotically the same work as the best known sequential algorithm for that problem. 
\end{definition}


\begin{example}
A (comparison-based) parallel sorting algorithm with
$\Theta(n\log{n})$ work is work efficient; one with $\Theta(n^2)$ is
not, because we can sort sequentially with $\Theta(n\log{n})$ work.
\end{example}
\end{group}

\begin{note}
In this book, we will mostly cover work-efficient algorithms where
the work is the same or close to the same as the best sequential time.
Indeed this will be our goal.
%
Among the algorithm that have the same work as the best sequential
time we will try to achieve the greatest parallelism.
\end{note}

\end{unit}
\end{section}

\begin{section}[Scheduling]
\label{sec:scheduling}

Scheduling involves executing a parallel program by mapping the
computation over the existing to processors in such a way to minimize
the completion time and possibly, the use of other resources such as
space and energy.
%
There are many forms of scheduling.
%
This section describes the scheduling problem and briefly reviews one
particular technique called greedy scheduling.


\begin{unit}[Scheduling Problem]

\begin{group}
\begin{gram}
An important advantage of the work-depth model is that it allows us to
design parallel algorithms without having to worry about the details
of how they are executed on an actual parallel machine. 
%
In other words, we never have to worry about mapping of the parallel
computation to processors, i.e., scheduling. 

Scheduling can be challenging, because a parallel algorithm generates
independently executable \defn{tasks} on the fly as it runs, and it
can generate a large number of them, typically many more than the
number of processors.
%
\end{gram}

\begin{example}
A parallel algorithm with $\Theta(n/\lg{n})$ parallelism can easily
generate millions of parallel subcomptutations or task at the same time,
even when running on a multicore computer with $10$ cores.
%
For example, for $n = 10^8$, the algorithm may generate millions of
independent tasks.
\end{example}
\end{group}

\begin{definition}[Scheduler]
A~\defn{scheduling algorithm} or a~\defn{scheduler} is an algorithm
for mapping parallel tasks to available processor.
%
%
% so that each
%processor remains busy as much as possible is the task of a scheduler.
The scheduler works by taking all parallel tasks, which are generated
dynamically as the algorithm evaluates, and assigning them to
processors.  
%
If only one processor is available, for example, then all
tasks will run on that one processor.  If two processor are available,
the task will be divided between the two.

Schedulers are typically designed to minimize the execution time of a
parallel computation
%
but 
%
minimizing space usage is also important.
\end{definition}

\begin{teachask}
Can you think of a scheduling algorithm?
\end{teachask}
\end{unit}

\begin{unit}[Greedy Scheduling]

\begin{definition}[Greedy Scheduler]
We say that a scheduler is~\defn{greedy} if whenever there is a
processor available and a task ready to execute, then it assigns the
task to the processor and starts running it immediately.  Greedy
schedulers have an important property that is summarized by the greedy
scheduling principle.
\end{definition}

\begin{definition}[Greedy Scheduling Principle]
The \defn{greedy scheduling principle} postulates that if a
computation is run on $P$ processors using a greedy scheduler, then
the total time (clock cycles) for running the computation is bounded
by
\[
\begin{array}{lll}
T_P & < & \frac{W}{P} + S
\label{eqn:greedy}
\end{array}
\]
where $W$ is the work of the computation, and $S$ is the span of the
computation (both measured in units of clock cycles).
\end{definition}


\begin{gram}[Optimality of Greedy Schedulers]
This simple statement is powerful. 

First, the time to execute the computation cannot be less than
$\frac{W}{P}$ clock cycles since we have a total of $W$ clock cycles
of work to do and the best we can possibly do is divide it evenly
among the processors.
%

Second, the time to execute the computation cannot be any better than
$S$ clock cycles, because $S$ represents the longest chain of
sequential dependencies.  Therefore we have

\[
T_p \geq \max{}\left(\frac{W}{P},S\right).
\]

We therefore see that a greedy scheduler does reasonably close to the
best possible.  In particular $\frac{W}{P} + S$ is never more than
twice $\max{}(\frac{W}{P},S)$.


Furthermore, greedy scheduling is particularly good for algorithms
with abundant parallellism.  To see this, let's rewrite the inequality
of Greedy Principle
%
%~\ref{eqn:greedy} above
%
in terms of the parallelism $\paral = W/S$:
\[
\begin{array}{lll}
T_P & < & \frac{W}{P} + S \\
    & = &  \frac{W}{P} + \frac{W}{\paral{}}\\
    & = &  \frac{W}{P}\left(1 + \frac{P}{\paral{}}\right).
\end{array}
\]
Therefore, if $\paral{} \gg P$, i.e., the parallelism is much greater
than the number of processors, then the parallel time $T_P$ is close
to $W/P$, the best possible.  In this sense, we can view parallelism
as a measure of the number of processors that can be used effectively.
\end{gram}

\begin{definition}[Speedup]
The \defn{speedup} $S_P$ of a $P$-processor parallel execution over a
sequential one is defined as
\[
S_P = T_s / T_P, 
\]
where $T_S$ denotes the sequential time.
%
We use the term \defn{perfect speedup} to refer to a speedup that is
equal to $P$.

When assessing speedups, it is important to select the best sequential
algorithm that solves the same problem (as the parallel one).  
\end{definition}


\begin{exercise}
Describe the conditions under which a parallel algorithm would obtain
near perfect speedups.
\end{exercise}
%%%% SOLUTION (incomplete)
%%  obtain near perfect
%% speedup. (Speedup is $W/T_P$ and perfect speedup would be
%% $P$). Therefore $\paral{}$ gives us a rough upper bound on the number
%% of processors we can effectively use.


\begin{example}
\label{ex:analysis::mergesortcost}
As an example, consider the mergeSort algorithm for sorting a
sequence of length $n$.  The work is the same as the sequential time,
which you might know is \[W(n) = O(n \log n)\; .\] We will show that
the span for mergeSort is \[S(n) = O(\log^2 n)\; .\] The parallelism
is therefore 
\[\paral{} = O\left(\frac{n \log n}{\log^2 n}\right)
= O\left(\frac{n}{\log n}\right)\; .\] 

This means that for sorting a million keys, we
can effectively make use of quite a few processors: $10^6/(\log_2
10^6) \approx 50,000$. 
\end{example}


\begin{remark}
Note that the Greedy Scheduling Principle does not account for the
time it requires to compute the (greedy) schedule, assuming instead
that such a schedule can be created instantaneously and at no cost.
%
This is of course unrealistic and indeed there has been much work on
algorithms that attempt to match the Greedy Scheduling Principle.
%
No real schedulers can match it exactly, however, because there is
work and thus overhead in scheduling.
%
We refer to such overheads as~\defn{scheduling friction}.
%
For example, there will surely be some delay from when a task becomes
ready for execution and when it actually starts executing.
%
In practice, therefore, the efficiency of a scheduler is quite
important to achieving good efficiency.  
%
Because of this, the greedy scheduling principle should only be viewed
as an asymptotic cost estimate in much the same way that the RAM model
or any other computational model should be just viewed as an
asymptotic estimate of real time.
%
We also note that the greedy scheduling principle does not account for
cost of memory operations.  By moving a job we might have to move data
along with it; the cost of such data movement could be large.
\end{remark}

\end{unit}
\end{section}

\end{chapter}
\end{book}
