\begin{chapter}[Algorithm Design Techniques]
\label{ch:design}


One of the most difficult and probably most important tasks for any
practitioner of computer-science is designing their own algorithms.
%
Beginners often feel that they do not even know where to start when
designing an algorithm.
%
In this chapter, we describe some of the key techniques used for
designing algorithms by considering a variety of examples.
%
These techniques are usually a good starting point when designing an
algorithm.  Even if a particular technique does not lead to an
algorithm with the desired properties, it can contribute to the
understanding of the problem, and which can then lead to a better
algorithm.
%

\begin{section}[Reduction]
\begin{subsection}[Reduction]
\begin{gram}
Sometimes the easiest approach to solving a problem is to reduce it to
another problem for which known algorithms exist. More precisely, to
reduce a problem $A$ to a problem $B$, we use some preprocessing to
convert the input of problem $A$ to an input for problem $B$, then
solve problem $B$ on that input, and finally convert the result of
problem $B$ back to the result of problem $A$.

\begin{center}
\includegraphics[width=5in]{./media/reduction.jpg}
\end{center}


When reducing one problem to another it is important to include the
cost of converting the input and converting the result.  Indeed to
calculate the total work for reducing a problem $A$ to $B$ we can just
add the work of the two conversions to the cost of an algorithm for
solving $B$. 
%
\end{gram}

\begin{definition}[Efficient Reduction]
We say a reduction of $A$ to $B$ is~\defn{ (asymptotically) efficient}
if the conversion takes asymptotically no more work or span than $B$
on the same input size as $A$.
%
Thus an efficient reduction of problem $A$ to problem $B$ tells us
that problem $A$ is asymptotically as easy as problem $B$.
\end{definition}

\begin{example}
  We can reduce the problem of finding the maximal element in a
  sequence to the problem of sorting.  
%
The idea is to sort the sequence in ascending order and then pick the
last element, which is one the largest elements in the input sequence.
Assuming accessing the final element requires less work than sorting,
which it usually does, this reduction is efficient.


 The resulting algorithm is not a work-efficient algorithm, because it
 would require $O(n\lg{n})$ work even though we can find the maximum
 in $O(n)$ work.

\end{example}

\begin{example}

Consider the problem of finding the maximal element of a sequence of
integers.
%
We can reduce the problem of finding the minimum of a sequence of
integers to this problem by inverting the sign of all the elements of
the input, finding the maximal value of the converted input, and then
inverting the sign of the result to obtain the minimum.
%
This reduction requires asympytotically linear work in the size of the
input sequence.  It is therefore efficient if the solution to
maximal-element problem requires same or more work, but is inefficient
otherwise.
\end{example}

\begin{remark}
Reduction is a powerful technique and can allow us to solve a problem
by reducing it to another problem, which might appear very different
or even unrelated.
%
For example, we might be able to reduce a problem on strings such as
that of finding the shortest superstring of a set of strings to a
graph problem (e.g, \chref{genome}.
%

Reduction technique can also be used ``in the other direction'' to
show that some problem is at least as hard as another or to establish
a lower bound.
%
In particular, if we know (or conjecture) that problem $A$ is hard
(e.g., requires exponential work), and we can reduce it to problem $B$
(e.g., using polynomial work), then we know that $B$ must also be
hard.  Indeed the theory of NP-complete problems is based on this
idea.
\end{remark}
\end{subsection}
\end{section}


\begin{section}[Brute Force] 
\begin{subsection}[Brute Force] 

\begin{gram}
The brute-force technique involves enumerating all possible solutions,
a.k.a.,~\defn{candidate solutions} to a problem, and checking whether
each candidate solution is valid.
%
Depending on the problem, a solution may be returned when found, or
the search may continue until a better solution is found.
\end{gram}

\begin{example}
We are asked to sort a set of keys $K$.
%
To achieve this, we can try all permutations of $K$ and test each
permutation to see if it is sorted.  We are guaranteed to find at
least one that is sorted.
%% %
%% \begin{question}
%% When might we find more than one valid solution (permutation) for the
%% sorting problem?  In this case do we care which one?
%% \end{question}
%% %
But, because there are $n!$ permutations and $n!$ is very large even
for small $n$, this algorithm is inefficient.
%
For example, using Sterling's approximation, 
we know that  $100! \approx \frac{100}{e}^{100} \ge 10^{100}$.
%
There are only about $10^{80}$ atoms in the universe so there is
probably no feasible way we could apply the brute force method
directly.
%
We thus conclude that this approach to sorting is
not~\defn{tractable}, because would require a lot of time for many
inputs, except for tiny ones.
\end{example}

%
\begin{remark}
The total number of atoms in the (known) universe, which is less than
$10^{100}$ is a good number to remember.
%
If your solution requires this much work or time, it is probably
intractable.
\end{remark}
%

\begin{important}
Brute-force algorithms are usually naturally parallel.
%
Enumerating all candidates (e.g., all permutations, all elements in a
sequence) is typically easy to do in parallel and so is testing that a
candidate solution is indeed a valid solution.
%

Brute-force algorithms are usually not work efficient.
%
This is important, because, in a algorithm design, our first priority
is to minimize the total work and only then the span.
\end{important}

\begin{teachask}
  Can you suggest a brute-force algorithm for finding the largest
  element in a sequence.  WHat is the complexity?
\end{teachask}
%
\begin{example}
Suppose that we are given a sequences of natural numbers and we wish
to find the maximal number in the sequence.
%
The brute-force technique advises uses to try all candidate solutions.
%
We can do so by picking each element and testing that it is greater or
equal to all the other elements in the sequence.  When we find one
such element, we know that we have found the maximal element.
%
Such an algorithm requires $O(n^2)$ comparisons and can be effective
for small inputs.
\end{example}

%% \begin{question}
%% Does the brute-force technique parallelize well?
%% \end{question}
%

%% \begin{question}
%% Given that the brute-force algorithms are often inefficient, why do we
%% care about them?
%% \end{question}


\begin{example}
  Given two strings $a$ and $b$, let's define the
  (maximum)~\defn{overlap} between $a$ and $b$ as the largest suffix
  of $a$ that is also a prefix of $b$. For example, the overlap
  between ``15210'' and ``2105'' is ``210'', and the overlap between
  ``15210'' and ``1021'' is ``10''.
%
  We can find the overlap between $a$ and $b$ using the brute force
  technique as follows: consider each suffix of $a$ starting from the
  beginning of $a$ to the end, and check if it is a prefix of
  $b$. Select the longest such suffix that is also a prefix.
%

  The work of this algorithm is $|a| \cdot |b|$, i.e., the product of
  the lengths of the strings.  Since we can try all positions in
  parallel, the span is that of checking that a particular suffix is
  $O(\lg{|b|})$.  Selecting the maximum is $O(\lg{a})$.  Thus the span
  is $O(\lg{a + b})$.

\end{example}


\begin{example}
Consider the following problem: we are given a graph where each edge
is assigned distance (e.g., a nonnegative real number), and asked to find
the shortest path between two given vertices in the graph.

We can solve this problem by enumerating all possible paths between
two nodes and selecting the one with the smallest distance, which can
be computed by summing up the distances of all edges on the path.
\end{example}


\begin{example}
Consider the following variant of the shortest path problem: we are
given a graph where each edge is assigned distance (e.g., a nonnegative
real number), and asked to find the shortest distance between two
given vertices in the graph.  This problem differs from the one above,
because it asks for the shortest distance rather than the path.
%

If we apply the brute-force design technique to this problem, then we
could decide to solve this problem by enumerating all candidate
solutions, which are real numbers starting from zero, and selecting
the smallest one.
%
But, this is impossible, because real numbers are not countable, and
cannot be enumerated.

We can still apply the brute-force technique by first reducing the
problem to the shortest-path version of the problem described above.
%
The shortest path problem returns the shortest path between the given
vertices. To solve our problem, we compute the distance of the
returned path.

\end{example}


\begin{example}
Consider the ``decision problem'' variant of the shortest path
problem: we are given a graph where each edge is assigned a distance
(e.g., a nonnegative real number), and we are given a \defn{budget},
which is a distance value.
%
We are asked to check whether there is a path between two given
vertices in the graph that is shorter than the given budget.  
%
This problem differs from the shortest path problem, because it asks
us to return a ``Yes'' or ``No'' answer. Such problems are
called~\defn{decision problems}.

To apply the brute-force design technique to this problem, we could
decide to enumerate all candidate solutions, which are either ``Yes''
or ``No'', and test whether each one is indeed a valid solution.
%
In this case, this does not help, because we are back to our original
problem that we started with.


We can solve this problem by first reducing it to the original
shortest-path problem as described above, and then checking whether
the distance of the resulting path is larger than the our budget.
%
\end{example}

\begin{remark}
Experienced algorithms designers usually perform reductions between
different variants of the problem as shown in the last two examples
``quietly''.
%
That is, they don't explicitly state the reduction but simply apply
the brute-force technique to a slightly different set of candidate
solutions.
%
For beginners and even for experienced designers, however, recognizing
such reductions can be instructive.
\end{remark}

\begin{remark}
Even though brute-force algorithms are usually inefficient, they can
be useful.
%
\begin{itemize}
\item A brute-force algorithm can help in testing the correctness of
  a more efficient algorithm by offering a solution that is easy to
  implement correctly.
%

\item Brute-force algorithms are usually easy to design and are a good
  starting point toward a more efficient algorithm.  They can help in
  understanding the structure of the problem, and thus can help guide
  the design of a more efficient algorithm.
\end{itemize}
\end{remark}

%% \begin{checkpoint}

%% \begin{questionfr}
%% \points 10

%% \prompt
%% Describe a brute-force algorithm for deciding whether an integer $n$
%% is composite (not prime).   

%% \answer

%% \end{questionfr}
%% \end{checkpoint}
\end{subsection}
\end{section}


\begin{section}[Divide and Conquer]

This sections describes the divide-and-conquer technique for algorithm
design and applies it to several problems.
%
We also discuss a closely related design technique, called
strengthening, that allows us to apply divide-and-conquer to a wider
variety of problems.


\begin{subsection}[Divide and Conquer]

\begin{gram}
Divide and conquer is one of the most important algorithm-design
techniques that can be used to solve a variety of computational
problems.  
%
A divide-and-conquer algorithm has a base case to handle sufficiently
small instances and inductive step that has three distinct phases:~\defn{divide},~\defn{recur}, and~\defn{combine} that respectively
divides the problem instance into smaller instances, solves them, and
combines the results for the smaller instance to construct the
solution to the larger instance.
%
This structure can be summarized as follows.
%
\begin{description}
\item[Base Case:] When the instance $I$ of the problem $P$ is
  sufficiently small, compute the solution $P(I)$ perhaps by using a
  different algorithm.

\item[Inductive Step:]\mbox{}
\begin{enumerate}
\item {\bf Divide} $I$ into some number of 
smaller instances of the same problem $P$.
\item {\bf Recur} on each of the smaller instances and compute
  their solutions.
\item {\bf Combine} the solutions to obtain the solution to the
  original instance $I$.
\end{enumerate}
\end{description}
\end{gram}

%% \begin{comment}
%% Note that CLRS when defining DandC refers to the instance as a
%% ``problem'' and sub instances as ``subproblems''.  This is
%% inconsistent with their previous definition of ``problem'' meaning the
%% general problem, and ``instance'' meaning a particular instance.  Lets
%% avoid falling into the same trap.
%% \end{comment}

\begin{example}
The drawing below illustrates the structure of a divide-and-conquer
algorithm that divides the problem instance into three independent
subinstances.

\begin{center}
\includegraphics[width=5in]{./media/divide-and-conquer.jpg}
\end{center}
\end{example}

\begin{gram}[Properties of Divide-and-Conquer Algorithms]
Divide-and-Conquer has several nice properties.  
%

\begin{itemize}
\item  It follows the structure of an inductive proof, and therefore
usually leads to relatively simple proofs of correctness. 
%
To prove a divide-and-conquer algorithm correct, we first prove the
base case is correct.  Then, we we assume by strong (or structural)
induction that the recursive solutions are correct, and show that,
given correct solutions to smaller instances, the combined solution is
a correct answer.  
%

\item Divide-and-conquer algorithms can be work efficient.
%
To ensure efficiency, we need to make sure that the divide and combine
steps are efficient, and that they do not create too many
sub-instances.
%

\item The work and span for a divide-and-conquer algorithm can be
  expressed as a mathematical equation called recurrence. Such
  recurrences can be usually be solved without too much difficulty,
  enabling the analysis of divide-and-conquer algorithms.
%

\item Divide-and-conquer algorithms are naturally parallel, because
  the sub-instances can be solved in parallel.  This can lead to
  significant amount of parallelism, because each inductive step can
  create more independent instances. For example, even if the
  algorithm divides the problem instance into two subinstances, each
  of those subinstances could themselves generate two more
  subinstances, leading to a geometric progression, which can quickly
  produce abundant parallelism.
\end{itemize}
\end{gram}

\begin{gram}[Analysis of Divide-and-Conquer Algorithms]
Consider an algorithm that divides a problem instance of size $n$ into
$k > 1$ independent subinstances of sizes $n_1, n_2, \ldots n_k$,
recursively solves the instances, and combine the solutions to
construct the solution to the original instance.

We can write the work of such an algorithm using the recurrence then
\begin{align*}
  W(n) \;&=\; W_{\textrm{divide}}(n) \;+\; \sum_{i=1}^k W(n_i) \;+\; W_{\textrm{combine}}(n) + 1.
\end{align*}
%
The work recurrence simply adds up the work across all phases of the
algorithm (divide, recur, and combine).

To analyze the span, note that after the instance is divided into
subinstance, the subinstances can be solved in parallel (because they
are independent), and the results can be combined.  The span can thus
be written as the recurrence:
%
\begin{align*}
S(n) &= S_{\textrm{divide}}(n) \;+\; \max_{i=1}^k S(n_i) \;+\; S_{\textrm{combine}}(n) + 1.\\
\end{align*}
\end{gram}

\begin{note}
The work and span recurrences for a divide-and-conquer algorithm
usually follow the recursive structure of the algorithm, but is a
function of size of the arguments instead of the actual values.
\end{note}


\begin{example}[Maximal Element]
We can find the maximal element in a sequence using divide and conquer
as follows.
%
If the sequence has only one element, we return that element,
otherwise, we divide the sequence into two equal halves and
recursively and in parallel compute the maximal element in each half.
%
We then return the maximal of the results from the two recursive
calls.
%
For a sequence of length $n$, we can write the work and span for this
algorithm as recurrences as follows:
\[
W(n) = \left\{
\begin{array}{lll}
\Theta(1) & \mbox{if} & n \le 1
\\
2W(n/2) + \Theta(1) &  \mbox{otherwise}
\end{array}
\right.
\]
%
\[
S(n) = \left\{
\begin{array}{lll}
\Theta(1) & \mbox{if} & n \le 1
\\
S(n/2) + \Theta(1) &  \mbox{otherwise}.
\end{array}
\right.
\]
%
This recurrences yield 
\begin{align*}
W(n) & = \Theta(n)~\mbox{and}
\\
S(n) & = \Theta(\lg{n}).
\end{align*}
\end{example}


\begin{algorithm}[Reduce with Divide and Conquer]
\label{alg:dandc::mcssSimp}
Generalizing over the algorithm for computing the maximal element,
leads us to an important parallel-algorithm primitive called
reduce.
%
The~\defn{reduce} primitive performs a computation that involves
applying an associative binary operation $op$ to the elements of a
sequence to obtain (reduce the sequence to) a final value.
%
For example, reducing the sequence $\cseq{0,1,2,3,4}$ with the $+$
operation gives us $0 + 1 + 2 + 3 + 4 = 10$.
%
If the operation requires constant work (and thus span), then the work
and span of a reduction is $\Theta(n)$ and $\Theta(\lg{n})$ respectively.

We can write the code for the reduce primitive on sequences as
follows.

\[
\begin{array}{l}
\cd{reduceDac}~f~\cd{id}~a =
\\
~~\cd{if}~\cd{isEmpty}~ a~\cd{then}
\\
~~~~\cd{id}
\\
~~\cd{else}~\cd{if}~\cd{isSingleton}~a~\cd{then}
\\
~~~~a[0]
\\
~~\cd{else}
\\ 
~~~~\cd{let}
\\ 
~~~~~~(l, r) = \cd{splitMid}~a
\\
~~~~~~(a, b) = (\cd{reduceDac}~f~\cd{id}~l~\cpar{}~\cd{reduceDac}~f~\cd{id}~r)
\\
~~~~\cd{in} 
\\
~~~~~~f(a,b)
\\ 
\cd{end}        
\end{array} 
\]
\end{algorithm}
\end{subsection}

\begin{subsection}[Merge Sort]

\begin{gram}
In this section, we consider the comparison sorting problem and the
merge-sort algorithm, which offers a divide-and-conquer algorithm for
it.
\end{gram}

\begin{problem}[The Comparison-Sort Problem]
  Given a sequence $a$ of elements from a universe $U$, with a total
  ordering given by $<$, return the same elements in a sequence $r$
  in sorted order, i.e. $r[i] \leq r[i+1], 0 < i \leq |a|-1$.
\end{problem}

\begin{algorithm}[Merge Sort]
\label{alg:dandc::mergesort}
Given an input sequence, merge sort divides it into two sequences that
are approximately half the length, sorts them recursively, and merges
the sorted sequences.
%
Mergesort can be written as follows.

\[
\begin{array}{l}
\cd{mergeSort}~a =
\\ 
~~\cd{if}~|a| \leq 1~\cd{then}
\\ 
~~~~a
\\
~~\cd{else}
\\ 
~~~~\cd{let}
\\
~~~~~~(l,r) = \cd{splitMid}~a
\\ 
~~~~~~(l',r') = (\cd{mergeSort}~l \cpar{} \cd{mergeSort}~r)
\\
~~~~\cd{in}
\\ 
~~~~~~\cd{merge} (l',r')
\\
~~~~\cd{end}
\end{array} 
\]
\end{algorithm}


\begin{note}
In the merge sort algorithm given above the base case is when the
sequence is empty or contains a single element.  In practice, however,
instead of using a single element or empty sequence as the base case,
some implementations use a larger base case consisting of perhaps ten
to twenty keys.
%
\end{note}

\begin{gram}[Correctness and Cost]

To prove correctness we first note that the base case is correct.
Then by induction, we note that by induction $l'$ and $r'$ are sorted
versions of $l$ and $r$.
%
Because $l$ and $r$ together contain exactly the same elements as $a$,
we conclude that $\cd{merge}~(l',r')$ returns a sorted version of $a$.


For the work and span analysis, we assume a linear-work,
logarithmic-span algorithm for merging sorted sequences.


Assuming that merging can be done in $\Theta(n)$ work and
$\Theta(\lg{n})$ span, where $n$ is the sum of the lengths of the two
sequences, we can write the work and span for this sorting algorithm
as recurrences as follows
\[
W(n) = \left\{
\begin{array}{lll}
\Theta(1) & \mbox{if} & n \le 1
\\
2W(n/2) + \Theta(n) &  \mbox{otherwise}
\end{array}
\right.
\]
%
\[
S(n) = \left\{
\begin{array}{lll}
\Theta(1) & \mbox{if} & n \le 1
\\
S(n/2) + \Theta(\lg{n}) &  \mbox{otherwise}.
\end{array}
\right.
\]

The recurrences solve to
\begin{align*}
W(n) & = \Theta(n\lg{n})
\\
S(n) & = \Theta(\lg^2{n}).
\end{align*}
\end{gram}
%

\begin{remark}[Quick  Sort]
Another divide-and-conquer algorithm for sorting is the quick-sort
algorithm.
% 
Both merge sort and quick sort require $\Theta(n \log n)$ work, which
is optimal for the comparison sorting problem. 
%
While merge sort has a trivial divide step and interesting combine
step, quick sort has an interesting divide step but trivial combine
step.
%
We will study quick sort in greater detail.
%
%\ref{}
\end{remark}
\end{subsection}

\begin{subsection}[Sequence Scan]

\begin{gram}[Intuition for Scan with Divide and Conquer]
To develop some intuition on how to we can design a divide-and-conquer
algorithm for the sequence scan problem, let's start by trying out a
classic technique.
%
We can dividie the sequence in two halves, solve each half, and then
put the results together.  Putting the results together is the tricky
part.
%


Consider the sequence $\cseq{2,1,3,2,2,5,4,1}$,
%
if we divide in the middle and scan over the two resulting sequences
we obtain $(b,b')$ and $(c,c')$, such that 
\begin{align*}
((b,b') & = (\cseq{0,2,3,6},8),~\mbox{and}
\\
(c,c') & = (\cseq{0,2,7,11},12).
\end{align*}
%

Note now that $b$ already gives us the first half of the solution.
%
To compute the second half, observe that in calculating $c$ in the
second half, we started with the identity instead of the sum of the
first half, $b'$.  
%
Therefore, if we add the sum of the first half, $b'$, to each element
of $c$, we would obtain the desired result.  
%
\end{gram}


\begin{algorithm}[Scan with Divide and Conquer]
By refining the intuitive description above, we can obtain a
divide-and-conquer algorithm for sequences scan, which is given below.

\[
\begin{array}{l}
\cd{scanDac}~f~\cd{id}~a =
\\
~~\cd{if}~|a| = 0~\cd{then}
\\
~~~~(\cseq{}, \cd{id})
\\
~~\cd{else if}~|a| = 1~\cd{then}
\\ 
~~~~(\cseq{\cd{id}},a[0])
\\
~~\cd{else}
\\ 
~~~~\cd{let}
\\ 
~~~~~~(b, c) = \cd{splitMid}~a
\\
~~~~~~((l,b'),(r,c')) = (\cd{scanDac}~f~\cd{id}~b \cpar{}~\cd{scanDac}~f~\cd{id}~c)
\\
~~~~~~r' = \cseq{f (\cd{b'},x) : x \in r}
\\
~~~~\cd{in}
\\
~~~~~~(\cd{append}~(l,r'), f(b',c'))
\\
~~~~\cd{end}
\end{array}
\]
\end{algorithm}
%

\begin{remark}
Observe that this algorithm takes advantage of the fact that $\cd{id}$ is
really the identity for $f$, i.e. $f(id,x) = x$.
\end{remark}

\begin{gram}[Cost Analysis]
We consider the work and span for the algorithm.  Note that the
combine step requires a map to add $b'$ to each element of $c$, and
then an append.  Both these take $O(n)$ work and $O(1)$ span, where $n
= |a|$.
%
This leads to the following recurrences for the whole
algorithm:
\[
\begin{array}{lllll}
W(n) & = & 2W(n/2) + O(n) & \in &  O(n \log n)
\\
S(n) & = & S(n/2) + O(1) & \in & O(\log n).
\end{array}
\]
Although this is much better than $O(n^2)$ work, we can do better by
using another design technique called contraction.
\end{gram}
\end{subsection}

\begin{subsection}[Euclidean Traveling Salesperson Problem]

\begin{gram}
We now turn to another example of divide and conquer. In this example,
we will apply it to devise a heuristic method for an
$\textbf{NP}$-hard problem.  The problem we are concerned with is a
variant of the well-known Traveling Salesperson Problem (TSP).

%
This variant is known as the Euclidean Traveling Salesperson Problem
(eTSP) because in this problem, the points (a.k.a., cities, nodes, and
vertices) lie in a Euclidean space and the distance measure is the
Euclidean measure.  More specifically, we're interested in the planar
version of the eTSP problem, defined as follows:
\end{gram}

\begin{definition}[The Planar Euclidean Traveling Salesperson Problem]
  Given a set of points $P$ in the $2$-d plane, the~\defn{planar Euclidean
    traveling salesperson} (eTSP) problem is to find a tour of minimum total
  distance that visits all points in $P$ exactly once, where the distance
  between points is the Euclidean (i.e. $\ell_2$) distance.
\end{definition}

\begin{example}
Assuming that we could go from one place to another using your
personal aeroplane, this is the problem we would want to solve to find
a minimum length route visiting your favorite places in Pittsburgh.
\end{example}

\begin{gram}
%
As with the TSP, it is \textbf{NP}-hard, but this problem is easier to
approximate.
%
Unlike the TSP problem, which only has constant approximations, it is
known how to approximate this problem to an arbitrary but fixed
constant accuracy $\varepsilon$ in polynomial time (the exponent of
$n$ has $1/\varepsilon$ dependency).
%
That is, such an algorithm is capable of producing a solution that has
length at most $(1+\varepsilon)$ times the length of the best tour.
%

\end{gram}

\begin{note}
In \chref{genome}, we cover another approximation algorithm for a
metric variant of TSP that is based on Minimum Spanning Trees (MST).
That approximation algorithm gives a constant-approximation guarantee.
\end{note}

\begin{gram}[Intuition for a Divide and Conquer Algorithm for eTSP]
%
%This divide-and-conquer algorithm is more interesting than the ones we have done
%so far because it does work both before and after the recursive
%calls.  Also, as we will see, the recurrence it generates is root dominated.
%

We can solve an instance of the eTPS problem by splitting the points
by a cut in the plane, solving the eTSP instances on the two parts,
and then merging the solutions in some way to construct a solution for
the original problem.  

For the cut, we can pick a cut that is orthogonal to the coordinate
lines. We could for example find the dimension along which the points
have a larger spread, and then cut just below the median point along
that dimension.  


This division operation gives us two smaller instances of eTSP, which
can then be solved independently in parallel, yielding two cycles.
%
To construct the solution for the original problem, we can merge the
solutions.
%
To merge the solution in the best possible way, we can consider
bridging any two each pair of edges.
%
For each such pair, there are two possible ways that we can bridge
them, because when we are on the one side, we can jump to any one of
the endpoints of the two bridges. 
%
To construct, the best solution, we can try out which one of these
yields the best solution and take that one. 

\begin{center}
  \includegraphics[width=6in]{./media/etsp-merge.jpg}
\end{center}
%\newcommand{\norm}[1]{\ensuremath{\left\Vert #1 \right\Vert}}
To choose which swap to make, we consider all pairs of edges of
the recursive solutions consisting of one edge $e_\ell = (u_\ell,v_\ell)$ from
the left and one edge $e_r = (u_r,v_r)$ from the right and determine which
pair minimizes the increase in the following
cost: 
\[
\cd{swapCost}((u_\ell,v_\ell), (u_r,v_r)) =
\norm{u_\ell-v_r} + \norm{u_r-v_\ell} - \norm{u_\ell-v_\ell} -
\norm{u_r-v_r}
\] where $\norm{u-v}$ is the Euclidean distance between
points $u$ and $v$.

\end{gram}



\begin{algorithm}[Divide-and-Conquer eTSP]

By refining the intuition describe above, we arrive at a
divide-and-conquer algorithm for solving eTPS, whose pseudo-code is
shown below.

\[
\begin{array}{l}
\cd{eTSP}~(P) =
\\
~~\cd{case}(|P|) 
\\
~~|~ 0,1 \dra \cd{raise TooSmall}
\\
~~|~2 \dra \cseq{(P[0],P[1]),(P[1],P[0])}
\\
~~|~n \dra
\\
~~~~\cd{let}
\\
~~~~~~(P_\ell, P_r) = \cd{split}~P~\cd{along the longest dimension}
\\
~~~~~~(L, R) = (\cd{eTSP}~P_\ell) \cpar{} (\cd{eTSP}~P_r)
\\
~~~~~~(c,(e,e')) = \cd{minVal}_{\cd{first}} \cset{(\cd{swapCost}(e,e'),(e,e')) : e \in L, e' \in R}
\\
~~~~\cd{in}
\\
~~~~~~~~\cd{swapEdges}~(\cd{append}~(L,R),e,e')
\\
~~~~\cd{end}
\end{array}
\]

The function $\cd{minVal}_{\cd{first}}$ uses the first value of
the pairs to find the minimum, and returns the (first) pair with that minimum. The function
$\cd{swapEdges}(E,e,e')$ finds the edges $e$ and $e'$ in $E$ and
swaps the endpoints. As there are two ways to swap, it picks the
cheaper one.
\end{algorithm}

\begin{remark} 
This heuristic divide-and-conquer algorithm is known to work well in
practice.
\end{remark}


\begin{group}
\begin{theorem}
Let $\vareps > 0$ be a
constant and consider  the recurrence
\begin{align*}
  W(n) & = 2W(n/2) + k\cdot n^{1+\vareps}.
\end{align*}

  %
  If $W(n) \leq 2 W(n/2) + k \cdot n^{1+\vareps}$ for $n > 1$ and $W(1) \leq k$ for $n \leq
  1$, then for some constant $\kappa$, \[ W(n) \;\leq\;
  \kappa \cdot n^{1+\vareps}. \]
\end{theorem}

\begin{proof}
  Let $\kappa = \frac1{1-1/2^{\vareps}} \cdot k$. The base case is easy: $W(1) =
  k \leq \kappa_1$ as $\frac1{1 - 1/2^{\vareps}} \geq 1$.  For the inductive
  step, we substitute the inductive hypothesis into the recurrence and obtain
  \begin{eqnarray*}
    W(n) &\leq& 2W(n/2) + k \cdot n^{1+\vareps}\\
    &\leq& 2 \kappa\left(\frac{n}2 \right)^{1+\vareps} + k \cdot n^{1+\vareps}\\
    &=& \kappa \cdot n^{1+\vareps} + \left(2 \kappa\left(\frac{n}2 \right)^{1+\vareps} +
      k \cdot n^{1+\vareps} - \kappa \cdot n^{1+\vareps}\right)\\
    &\leq& \kappa \cdot n^{1+\vareps},
  \end{eqnarray*}
  where in the final step, we use the fact  that for any $\delta > 1$:

  \begin{eqnarray*}
    2 \kappa\left(\frac{n}2 \right)^{\delta} +
    k \cdot n^{\delta} - \kappa \cdot n^{\delta}
    &=& \kappa \cdot 2^{-\vareps} \cdot n^{\delta}  +
    k \cdot n^{\delta} - \kappa \cdot n^{\delta} \\
    &=& \kappa \cdot 2^{-\vareps} \cdot n^{\delta}  +
    (1 - 2^{-\vareps})\kappa\cdot n^{\delta} - \kappa \cdot n^{\delta} \\
    &\leq& 0.
  \end{eqnarray*}

%% Case I: Equally heavy
%% Case II: Leaf heavy
%%

An alternative way to prove the same theorem is to use the tree method
and evaluate the sum directly. The recursion tree here has depth $\log
n$ and at level $i$ (again, the root is at level $0$), we have $2^i$
nodes, each costing $k\cdot (n/2^i)^{1+\vareps}$.  Thus, the total
cost is
  \begin{eqnarray*}
    \sum_{i=0}^{\log n} k\cdot 2^i \cdot \pparen{\frac{n}{2^i}}^{1+\vareps}
%    &=&  k \sum_{i=0}^{\log n}  2^{i - i - \vareps \cdot i} \cdot n^{1+\vareps} \\
    &=&  k\cdot n^{1+\vareps} \cdot \sum_{i=0}^{\log n} 2^{-i\cdot\vareps} \\
    &\leq& k\cdot n^{1+\vareps} \cdot \sum_{i=0}^{\infty} 2^{-i\cdot\vareps}.
  \end{eqnarray*}
  But the infinite sum $\sum_{i=0}^{\infty} 2^{-i\cdot\vareps}$ is at most
  $\frac1{1 - 1/2^{\vareps}}$. Hence, we conclude $W(n) \in O(n^{1+\vareps})$.

\end{proof}
\end{group}

\begin{gram}[Cost Analysis]

Let's analyze the cost of this algorithm in terms of work and
span.  
%
We have
\begin{align*}
  W(n) & = 2W(n/2) + O(n^2)\\
  S(n) & =  S(n/2) + O(\log n)
\end{align*}
%

We have already seen the recurrence $S(n) = S(n/2) + O(\log n)$, which solves
to $O(\log^2 n).$  Here we'll focus on solving the work recurrence.  % Let's try
% the tree method first.


% TODO \ref{} the theorem should be moved to recurrences.
To solve the recurrence, we can apply the theorem above and obtain 
\[
W(n) = O(n^2).
\]

We can also solve this recurrences using the tree method by evaluating
the sum directly or apply the theorem above to conclude that

\[
W(n) \in O(n^{1+\vareps}).
\]
\end{gram}

\begin{gram}[Strengthening]

In most divide-and-conquer algorithms you have encountered so far, the
subproblems are occurrences of the problem you are solving.  For
example, in sorting the subproblems are smaller sorting instances.
This is not always the case.  Often, you will need more information
from the subproblems to properly combine the results.  In this case,
you'll need to~\defn{strengthen} the problem definition.  If you
have seen the approach of strengthening an inductive hypothesis in a
proof by induction, it is very much an analogous idea.  Strengthening
involves defining a problem that solves more than what you ultimately
need, but makes it easier or even possible to use solutions of
subproblems to solve the larger problem.
\end{gram}

\begin{teachnote}
\begin{question}
You have recently seen an instance of strengthening when solving
a problem with divide and conquer. Can you think of the problem and
how you used strengthening?
\end{question}

In the recitation you looked at how to solve the Parenthesis Matching
problem by defining a version of the problem that returns the number
of unmatched parentheses on the right and left.  This is a stronger
problem than the original, since the original is the special case when
both these values are zero (no unmatched right or left parentheses).
This modification was necessary to make divide-and-conquer work---if
the problem is not strengthened, it is not possible to combine the
results from the two recursive calls (which tell you only whether the
two halves are matched or not) to conclude that the full string is
matched.  This is because there can be an unmatched open parenthesis
on one side that matches a close parenthesis on the other.
\end{teachnote}

\end{subsection}

\begin{subsection}[Divide and Conquer with Reduce]

\begin{gram}
Many divide-and-conquer algorithms have the following structure, where
$\cd{emptyVal}$, $\cd{base}$, and $\cd{myCombine}$ span for algorithm
specific values.

\[
\begin{array}{l}
\cd{myDC}~a =
\\ 
~~\cd{if}~|a| = 0 ~\cd{then}
\\
~~~~\cd{emptyVal}
\\
~~~~\cd{else if}~|a| = 1 ~\cd{then}
\\
~~~~~~\cd{base}(a[0])
\\
~~~~\cd{else}
\\
~~~~~~\cd{let}~(l,r) = \cd{splitMid}~a~\cd{in}
\\
~~~~~~~~(l',r') = (\cd{myDC}~l~\cpar{}~\cd{myDC}~r)
\\
~~~~~~\cd{in}
\\
~~~~~~~~\cd{myCombine}~(l', r')
\\
~~~~~~\cd{end}
\end{array}
\]

Algorithms that fit this pattern can be implemented in one line using
the sequence $\cd{reduce}$ function.
%
Turning a divide-and-conquer algorithm into a reduce-based solution is
as simple as invoking $\cd{reduce}$ with the following parameters
\[
\cd{reduce}~\cd{myCombine}~\cd{emptyVal}~(\cd{map}~\cd{base}~a).
\]
\end{gram}

\begin{important}
This pattern does not work in general for divide-and-conquer
algorithms.  In particular, it does not work for algorithms that do
more than a simple split that partitions their input in two parts in
the middle.  
%
For example, it cannot be used for implementing the quick-sort
algorithm, because the divide step partitions the data with respect to
a pivot.  
%
This step requires picking a pivot, and then filtering the
data into elements less than, equal, and greater than the pivot.  
%
It also does not work for divide-and-conquer algorithms that split
more than two ways, or make more than two recursive calls.
\end{important}

\end{subsection}
\end{section}



\begin{section}[Contraction]

This sections describes the contraction technique for algorithm design
and applies it to several problems.
%


\begin{subsection}[Contraction]


\begin{gram}

Contraction, an inductive technique for designing parallel algorithms,
is probably one of the most important algorithm-design techniques.
Like divide-and-conquer algorithms, contraction algorithms involve
solving a smaller instance of the same problem, but unlike in
divide-and-conquer, there is only one subproblem to solve at a time.
%
A contraction algorithm for problem $P$ has the following structure.
%
\begin{description}[topsep=1pt,itemsep=1pt]

\item[Base Case:] If the problem instance is sufficiently small, then
  compute and return the solution, possibly using another algorithm.


\item[Inductive Step:] If the problem instance is sufficiently large,
  then apply the following three steps (possibly multiple times).

\begin{enumerate}[topsep=1pt,itemsep=1pt]
\item \defn{Contract:} 
``contract'', i.e., map the instance of the problem $P$ to a smaller
  instance of $P$.

\item \defn{Solve:}
solve the smaller instance recursively.

\item \defn{Expand:}
use the solution to solve the original instance.
\end{enumerate}
\end{description}
%

\end{gram}

\begin{remark}
Contraction differs from divide and conquer in that it allows there to
be only one independent subproblem to be solved at a time, though
there could be multiple dependent subproblems to be solved one after
another (sequentially).
\end{remark}

\begin{gram}[Properties of Contraction]

Contraction algorithms have several important properties.  

\begin{itemize}
\item Due to their inductive structure, we can establish the
  correctness of a contraction algorithm using principles of
  induction: we first prove correctness for the base case, and then
  prove the general (inductive) case by using strong induction, which
  allows us to assume that the recursive call is correct.

\item 
The work and span of a contraction algorithm can be expressed as a
mathematical recurrence that reflects the structure of the algorithm
itself. Such recurrences can then unually be solved using
well-understood techniques, and without significant difficulty.

%
\item 
Contraction algorithms can be work efficient, if they can reduce the
problem size geometrically (by a constant factor greater than $1$) at
each contraction step and if the contraction and the expansions steps
are themselven efficient.
%

\item Contraction algorithms can have a low span (high parallelism),
  if size of the problem instance decreases geometrically, and if
  contraction and expansion steps have low spans.

\end{itemize}


%
\end{gram}


\begin{example}[Maximal Element]
We can find the maximal element in a sequence $a$ using contraction as
follows.
%
If the sequence has only one element, we return that element,
%
otherwise,
%
we can map the sequence $a$ into a sequence $b$ which is half the
length by comparing the elements of $a$ at consecutive even-odd
positions and writing the larger into $b$.
%
We then find the largest in $b$ and return this as the result.

For example, we map the sequence
%
$\cseq{1,2,4,3,6,5}$ 
%
to 
%
$\cseq{2,4,6}$.
% 
The largest element of this sequence, $6$ is then the largest element
in the input sequence.
%

%
For a sequence of length $n$, we can write the work and span for this
algorithm as recurrences as follows
\[
W(n) = \left\{
\begin{array}{lll}
\Theta(1) & \mbox{if} & n \le 1
\\
W(n/2) + \Theta(n) &  \mbox{otherwise}
\end{array}
\right.
\]
%
\[
S(n) = \left\{
\begin{array}{lll}
\Theta(1) & \mbox{if} & n \le 1
\\
S(n/2) + \Theta(1) &  \mbox{otherwise}.
\end{array}
\right.
\]
%
Using the techniques discussed at the end of this chapter, we can solve
the recurrences to obtain $WCn) = \Theta(n)$ and $S(n) =
\Theta(\lg{n})$.
\end{example}

\begin{algorithm}[Reduce with Contraction]
\label{alg:contract::reduce}

Generalizing the algorithm for computing the maximal element leads us
to an implementation of an important parallelism primitive called $\cd{reduce}$.


The~\defn{reduce} primitive performs a computation that involves
applying an associative binary operation $op$ to the elements of a
sequence to obtain (reduce the sequence to) a final value.
%
For example, reducing the sequence $\cseq{0,1,2,3,4}$ with the $+$
operation gives us $0 + 1 + 2 + 3 + 4 = 10$.
%
Recall that the type signature for~$\cd{reduce}$ is as follows.
%
\[
\cd{reduce}~(f: \alpha * \alpha \ra \alpha)~(\cdvar{id}: \alpha)~(a: \sseq{\alpha}): \alpha,
\]
where $f$ is a binary function, $a$ is the sequence, and $\cdvar{id}$
is the identity element of $f$.
%

%In section \ref{seq}
Even though we define $\cd{reduce}$ broadly for both associative and
non-associative functions, throughout this section, we assume that the
function~$f$ is associative.

The crux in using the contraction technique is to design an algorithm
for reducing an instance of the problem to a geometrically smaller
instance by performing a parallel contraction step.
%
To see how this can be done, consider instead applying the
function~$f$ to consecutive pairs of the input.
%

For example if we wish to compute the sum of the input sequence 
\[
\cseq{ 2,1,3,2,2,5,4,1}
\]
by using the addition function, we can contract the sequence to
\[
\cseq{ 3, 5, 7, 5 }.
\]
%
Note that the contraction step can be performed in parallel, because
each pair can be considered independently in parallel.

By using this contraction step, we have reduced the input size by a
factor of two.
%
We next solve the resulting problem by invoking the same algorithm and
apply expansion to construct the final result.
%
We note now that by solving the smaller problem, we obtain a solution
to the original problem, because the sum of the sequence remains the
same as that of the original.  Thus, the expansion step requires no
additional work.
%

We can thus express our algorithm as follows; for simplicity, we assume
that the input size is a power of two.
%

\[
\begin{array}{l}
\cd{(* Assumption:} |a| \cd{is a power of 2 *)}
\\
\cd{reduceContract}~f~\cdvar{id}~a =
\\ 
~~\cd{if}~\cd{isSingleton}~a~\cd{then}
\\
~~~~a[0]
\\
~~\cd{else}
\\ 
~~~~\cd{let}
\\
~~~~~~b = \cseq{f(a[2i],a[2i+1]) : 0 \le i < \lfloor |a|/2
  \rfloor}
\\ 
~~~~\cd{in}
\\
~~~~~~\cd{reduceContract}~f~\cdvar{id}~b
\\
~~~~\cd{end}
\end{array}
\]
\end{algorithm}
%

\begin{gram}[Cost of Reduce with Contraction]
Assuming that the function being reduced over performs constant work, 
parallel tabulate in the contraction step requires linear work, 
we can thus write the work of this algorithm as 
\[
W(n) = W(n/2) + n.
\]
This recurrence solves to $O(n)$.
%


Assuming that the function being reduced over performs constant span, 
parallel tabulate in the contraction step requires constant span;
we can thus write the work of this algorithm as
%
\[
S(n) = S(n/2) + 1.
\]
This recurrence solves tot $O(\log{n})$.
\end{gram}
\end{subsection}

\begin{subsection}[Scan with Contraction]

\begin{gram}
We describe how to implement the $\cd{scan}$ sequence primitive
efficiently by using contraction.
%
Recall that 
the $\cd{scan}$ function has the type signature
\[
\cd{scan}~(f: \alpha * \alpha \ra \alpha)~(\cdvar{id}: \alpha)~(a: \sseq{\alpha})~:~(\sseq{\alpha} * \alpha)
\]
where $f$ is an associative function, $a$ is a sequence, and
$\cdvar{id}$ is the identity element of $f$.
%
When evaluated with a function and a sequence, $\cd{scan}$ can be
viewed as applying a reduction to every prefix of the sequence and
returning the results of such reductions as a sequence.
%
\end{gram}

\begin{example}
Applying $\cd{scan~`+`}$, i.e., ``plus scan'' on the sequence
$\cseq{2,1,3,2,2,5,4,1}$ returns
\[
\left(\cseq{0, 2, 3, 6, 8, 10, 15, 19}, 20\right).
\]
We will use this as a running example.
\end{example}

\begin{gram}
Based on its specification, a direct algorithm for $\cd{scan}$ is to
apply a reduce to all prefixes of the input sequence.  
%
Unfortunately, this easily requires quadratic work in the size of the
input sequence.
%

We can see that this algorithm is inefficient by noting that it
performs lots of redundant computations.  In fact, two consecutive
prefixes overlap significantly but the algorithm does not take
advantage of such overlaps at all, computing the result for each
overlap independently. 
%

By taking advantage of the fact that any two consecutive prefixes
differ by just one element, it is not difficult to give a linear work
algorithm (modulo the cost of the application of the argument
function) by using iteration.  

Such an algorithm may be expressed as
follows
\[
\cd{scan}~f~\cdvar{id}~{a} = h \left( \cd{iterate}~g~(\cseq{},id)~a \right), 
\]
where
\[ 
g((b, y), x) = ((\cd{append} \cseq{y} b),~f(y,x))~
\]
\\
and 
\\
\[
h (b,y) = ((\cd{reverse b}), y)
\]
where $\cd{reverse}$ reverses a sequence.
%

This algorithm is correct but it almost entirely sequential, leaving
no room for parallelism.

\end{gram}


\begin{gram}[Scan via Contraction, the Intuition]

Considering the fact that it has to compute some value for each
prefix, it may seem difficult to give a parallel algorithm for
$\cd{scan}.$  
%
We might be inclined to believe that any efficient algorithms will
have to keep a cumulative ``sum,'' computing each output value by
relying on the ``sum'' of the all values before it.  

It is this apparent dependency that makes $\cd{scan}$ so
powerful. Indeed, we often use $\cd{scan}$ when we seem toq need a
function that depends on the results of other elements in the
sequence.

We can implement $\cd{scan}$ efficiently using contraction.  To this
end, we need to reduce a given problem instance to a geometrically
smaller instance by applying a contraction step.
%
As a starting point, let's apply the same idea as we used for
implementing reduce with contraction and see how far that takes use.
%

Applying the contraction step from the $\cd{reduce}$ algorithm
described above, we would reduce the input sequence
\[
\cseq{2,1,3,2,2,5,4,1}
\]
to the sequence
\[
\cseq{3,5,7,5},
\]
which if recursively used as input would give us the result
\[
(\cseq{ 0, 3, 8, 15 }, 20).
\]
Notice that in this sequence, the elements in the first, third, etc.,
positions are actually consistent with the result expected:
\[
(\cseq{0, 2, 3, 6, 8, 10, 15, 19}, 20).
\]

The reason for why half of the elements is correct is because the
contraction step which pairs up the elements and reduces them, does
not affect, by associativity of the function being used, the result at
the position that do not fall in between a pair.  
%

Thus, we can use an expansion step to compute the missing items.
%
It is actually quite simple: all we have to do is to compute the
missing elements by applying the function element-wise to the elements
of the input at odd positions in the input sequence and the results of
the recursive call to $\cd{scan}.$


To illustrate, the diagram below shows how to produce the final output
sequence from the original sequence and the result of the recursive
call:
\begin{center}
  \includegraphics[width=4in]{./media/prefix-eval.jpg}
\end{center}
\end{gram}


\begin{algorithm}[Scan Using Contraction, for powers of 2]
\label{alg:contract::scan}

Based on the intuitive description above, we can write the pseudo-code
for $\cd{scan}$ as  follows.
%
For simplicity, we assume that $n$ is a power of two.
%

\[
\begin{array}{l}
\cd{(* Assumption:}~|a|~\cd{is a power of two. *)}
\\
\cd{scan}~f~\cdvar{id}~a =
\\
~~\cd{case}~|a| 
\\
~~|~0 \dra \left(\cseq{}, id\right)
\\
~~|~1 \dra \left(\cseq{id}, a[0]\right)
\\
~~|~n \dra
\\ 
~~~~\cd{let}
\\
~~~~~~a' = \cseq{ f(a[2i],a[2i+1]) : 0 \leq i < n/2}
\\
~~~~~~(r,t) = \cd{scan}~f~\cdvar{id}~ a'
\\
~~~~\cd{in}
\\ 
~~~~~~(\cseq{ p_i : 0 \leq i < n }, t),~\com{where}~p_i = 
%
\begin{cases}
     r[i/2]  & \cd{even}(i) \\
     f(r[i/2], a[i-1]) & \cd{otherwise}
\end{cases}
%
\\
~~~~\cd{end}
\end{array}
\]
\end{algorithm}


\begin{gram}[Cost of Scan with Contraction]
Let's assume for simplicity that the function being applied has
constant work and constant span. We can write out the work and span
for the algorithm as a recursive relation as 
\begin{align*}
W(n) &= W(n/2) + n,~\mbox{and}
\\
S(n) &= S(n/2) + 1,
\end{align*}
\]
because 1) the contraction step which tabulates the smaller instance
of the problem performs linear work in constant span, and 2) the
expansion step that constructs the output by tabulating based on the
result of the recursive call also performs linear work in constant
span.

These recursive relations should look familiar.  They are the same as
those that we ended up with when we analyzed the work and span of our
contraction-based implementation of $\cd{reduce}$ and  yield 
\begin{align*}
W(n) &= O(n)
\\
S(n) &=  O(\log{n}).
\end{align*}
\end{gram}

%% \begin{checkpoint}

%% \begin{questionfr}[Ranking sequences]
%% \points 10
%% \prompt

%%   Given a sequence of $n$ natural numbers and and a rank $r < n$, you
%%   want to find the element of the sequence with the given rank. For
%%   example if the input is $\cseq{1,3,2,4}$ and you are asked to find
%%   the element with rank $0$ then the answer is $1$ because $1$ is the
%%   element with rank $0$, i.e., the smallest element.

%% \begin{itemize}
%% \item Using the contraction technique, design an algorithm for finding
%%   the element of the sequence with a given rank.

%% \item Analyze the worst-case work and span of your algorithm.

%% \item Analyze the best-case work and span of your algorithm.
%% \end{itemize}
%% \answer
%% \end{questionfr}

%% \end{checkpoint}

\end{subsection}
\end{section}

\begin{section}[Dynamic Programming]
\begin{subsection}[Dynamic Programming]
\begin{gram}[Dynamic Programming]
 Like divide and conquer, dynamic programming divides the problem into
 smaller subproblems, solves the subproblems, and combines the
 solutions to the subproblems.  The difference, though, is that the
 solutions to subproblems are used multiple times.  It therefore
 becomes important to store the solutions in a data structure that
 facilitates quick re-use as needed. 
\end{gram}
\end{subsection}

\end{section}

\begin{section}[Other Techniques]

\begin{subsection}[Greedy]
\begin{gram}[Greedy]
 For a problem on size $n$ use
  some approach to find the ``best'' element by some greedy metric,
  remove this element, and solve the problem on the remaining $n-1$
  elements.  
\end{gram}

% \begin{definition}[The Greedy \textbf{Technique}]
%    Give a set of elements, on each step remove at least one element by making
%    a locally optimal decision based on some criteria.
% \end{definition}
\end{subsection}


\begin{subsection}[Randomization]


\begin{gram}
Randomization is a powerful algorithm design technique that can be
applied along with the aforementioned techniques.  It sometimes leads
to simpler algorithms.
\end{gram}


\begin{example}
We can sort a sequence using divide and conquer and randomization as
follows.
%
If the sequence has only one element, we return the sequence unchanged
because it is sorted.
%
Otherwise, we select one of the elements as a~\defn{pivot} and divide
the sequence into two sequences consisting of the keys less than, and
greater than the pivot.
%
We then sort each recursively and the concatenate the results in
order.
%
Assuming that splitting based on the pivot can be done in $\Theta(n)$
work and %$\Theta(\lg{n})$ span, where $n$ is length of the sequence,
%
and assuming that the pivot divides the sequence into two equal
halves,
we can write the work and span for this sorting algorithm as
recurrences as follows
\[
W(n) = \left\{
\begin{array}{lll}
\Theta(1) & \mbox{if} & n \le 1
\\
2W(n/2) + \Theta(1) &  \mbox{otherwise}
\end{array}
\right.
\]
%
\[
S(n) = \left\{
\begin{array}{lll}
\Theta(1) & \mbox{if} & n \le 1
\\
S(n/2) + \Theta(\lg{n}) &  \mbox{otherwise}.
\end{array}
\right.
\]
We can solve the recurrences to obtain $W(n) = \Theta(n\lg{n})$ and
$S(n) = \Theta(\lg^2{n})$.
%

The assumption we made about pivot dividing the sequence into two
equal halves in unrealistic. 
%
We can analyze the
``expected work'' and ``expected span'' of randomized algorithm more
precisely by using probabilistic analysis techniques later in
\chref{randomization}. 
%
\end{example}

\begin{gram}[Symmetry Breaking]
Randomization plays a crucial raole in parallel algorithm design
because it helps ``break'' the symmetry in a problem without global
communication.
%
%% \begin{question}
%% Have you heard of the term ``symmetry breaking''?  Can you think of a
%% somewhat amusing real-life phenomena, where we engage in randomized
%% symmetry breaking?
%% \end{question}
%
We will cover several examples of randomized algorithms that use
symmetry breaking in this book. 
%
Formal cost analysis for randomized algorithms requires knowledge of
probability theory. 
%
%In \chref{randomized}, 
%
In this course, we cover the probability
theory required by the material covered in the book.
\end{gram}

\begin{example}[Symmetry breaking]
We often perform randomized symmetry breaking when walking on a
crowded sidewalk with many people coming in our direction.  With each
person we encounter, we may pick a random direction to turn and the
other person responds, or the other way.  If we recognize each other
late, we may end up in a situation where the randomness becomes more
apparent, as we attempt to get past each other but make the same
(really opposite) decisions.  Since we make essentially random
decisions, the symmetry is eventually broken---or we run into each
other.
\end{example}
\end{subsection}
\end{section}
\end{chapter}



%% \clearpage
%% \section{Problems}


%% \begin{probl}{Closed form solutions}

%% Find the closed from for the following recursions.  First use the tree
%% method by writing out the sum and solving it.  Then apply the brick
%% method and compare your answers. 

%% \begin{itemize}
%% \item $W(n) = 3W(n/2) + n$

%% \item $S(n) = 2S(n/4) + n$

%% \item $W(n) = 2W(n/4) + n^2$

%% \item $W(n) = 2W(n/2) + n\log{n}$

%% \item $W(n) = 2W(n/2) + n\log{n}$

%% \item $S(n) = 2S(\sqrt{n}) + \log{n}$

%% \item $W(n) = W(n/3) + W(n/4) + n$

%% \item $S(n) = \max{(S(n/3),S(n/4))} + \log{n}$

%% \item $W(n,m) = 2W(n/2,m/4) + nm$

%% \item $W(n,m) = W(n/2,m/4) + n^2m$

%% \item $W(n,m) = W(n/2,m/4) + \log{nm}$
%% \end{itemize}
%% \end{probl}

%% \begin{probl}{Substitution Method}
%% Show that $W(n) = 2W(n/2) + \log{n} \in O(n)$ by using tree of
%% substitution method.
%% \end{probl}

%% \begin{probl}{Bricks and trees}
%% Derive the brick method from the tree method.
%% \end{probl}


%% \begin{probl}{2-Optimality of Brent's Theorem}
%%   Prove that the bound given by Brent's theorem is within a factor two
%%   of optimal.
%% \end{probl}
%% %
%% \begin{answer}
%% Answer: We know that W/P and S are both lower bounds.
%% Thus $\max{\{W/P,.S\}}$ is a lower bound.
%% $W/P + S \le \max{\{W/P,.S\}} + \max{\{W/P,.S\}} = 2 \max{\{W/P,.S\}}$.
%% \end{answer}



%% \begin{probl}{Order statistics by reduction to sorting}
%% Suppose that you have an algorithm that can, for given a comparison
%%  function, comparison sort a sequence of numbers in
%%  $\Theta(n\log{n})$ work and
%%  $\Theta(\log^2{n})$ span.  Using the problem reduction technique:

%% \begin{itemize}
%% \item Give an algorithm that finds the minimum element in a given sequence of
%% numbers in the same work and time.

%% \item Give an algorithm that finds the maximum element in a given sequence
%% of numbers in the same work and time.

%% \item Give an algorithm that finds the median element in a given sequence
%% of numbers in the same work and time.

%% \item Give an algorithm that finds the element with any given rank in a
%% given sequence of numbers in the same work and time.

%% \end{itemize}
%% \end{probl}



%% \begin{probl}{Repeated strings}
%% Describe a brute force algorithm for finding the length of the longest
%% repeated string in a string of characters.  For example for the string
%% ``\cd{axabaxcabaxa}'' the longest repeated string is \cd{abax} and has
%% length 4.  You can assume you are given a routine \cd{find}$(S,w)$
%% that returns how many times the string $w$ appears in the string $S$.
%% Your algorithm should generate about $n^2$ candidate solutions, where
%% $n$ is the length of the input string.
%% \end{probl}


%% \begin{probl}{Graph connectivity}
%% Suppose that you know how to solve the problem of determining whether
%% there is a path from any two vertices in a given undirected graph as
%% long as all the vertices in the graph have degree $3$ or less.
%% %
%% Using reduction, solve the problem of determining whether any two
%% vertices are connected in an arbitrary degree graph.
%% %
%% Is your reduction efficient?
%% \end{probl}


%% \begin{probl}{Multiplication by addition}
%%   Suppose that you have an algorithm that can sum up the floating
%%   points numbers in a given sequence in $\Theta(n)$ work and
%%   $\Theta(\log{n})$ span.  Using the problem reduction technique:
%%   give an algorithm that finds the product of the numbers in a given
%%   sequence of numbers in the same work and time.  Explain the
%%   assumptions that you need to make sure that the result is correct.
%% \end{probl}

%% \begin{answer}
%% Take the log of the numbers and then add them up.  Then take exponent
%% of the sum in the same base.  These operations must not lead to any
%% floating point errors for this to be correct.
%% \end{answer}


%% \begin{probl}{Sorting via reduction to convex hulls}
%% Given a sequence of points in two dimensions $P$, the planar
%% convex hull problem requires finding the convex hull of the points,
%% which is the smallest polygon  containing $P$.  The
%% polygon is defined by the sequence $Q$, consisting of
%% a subset of the points in  $P$ ordered clockwise, such
%% that no three points are collinear (on the same line).

%% \begin{itemize}
%% \item Design a sorting algorithm by reduction to the convex hull problem. 

%% \item State the work and the span of your algorithms in terms of the work
%% and the span of a convex hull algorithm.
%% \end{itemize}
%% \end{probl}

%% \begin{answer}
%% Something like this should work out: map each point (x) to (x,
%% convex-function of x) and find the convex hull.  All points will be on
%% the hull.  And they will be sorted.
%% \end{answer}


%% \begin{probl}{Smallest enclosing rectangle}
%% You are given a set of points in two dimensions and asked to find the
%% smallest axis-aligned rectangle that encloses the points as
%% illustrated in the drawing below.  An axis-aligned rectangle is one
%% whose sides are parallel to the ``x'' and ``y'' axes.  Design a
%% divide-and-conquer algorithm for finding rectangle.

%% \begin{center}
%% \includegraphics[width=2.5in]{introduction/points-and-square}
%% \end{center}

%% \end{probl}

%% \begin{probl}{Academic life}
%% Every morning, a professor wakes up to perform a collection of
%% $n$ tasks $t_1 \ldots t_n$, where each task has a known
%% length $l_i$.  Each day, the professor completes the tasks in
%% some order, performing one task at a time, and thus assigning a finish
%% time $f_i$ to each.  Over time, the professor has developed a
%% strategy of minimizing the average completion time of these tasks,
%% that is $\frac{\sum_{i=1}^{n}{f_i}}{n}$.  Exactly why this
%% strategy works continues to be an (unfunded) research project.

%% \begin{itemize}
%% \item Design a brute-force algorithm that minimizes the average
%% completion time.

%% \item What is the work and span of your brute-force algorithm.

%% \item Design a reduction-based algorithm by sorting that minimizes the
%% average completion time.

%% \item What is the work and span of your reduction-based algorithm.

%% \item Prove that your reduction-based algorithm  minimizes the
%% average completion time.
%% \end{itemize}
%% \end{probl}

%% \begin{probl}{Greedy dining}
%% It is lunch time and you are very hungry today. But you don't want to
%% spend any more than your usual budget, \$5, on lunch.  Describe a
%% greedy algorithm for having a big lunch without exceeding your budget.
%% \end{probl}
%% \begin{answer}
%% Food might run out.
%% \end{answer}

