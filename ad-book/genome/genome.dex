\documentclass{course}
\title{Parallel and Sequential Algorithms}

% Course number must be unique in the database
\coursenumber{15210}

\semester{Spring 2018}
\picture{/210/course/air-pavilion.jpg}
\website{http://www.cs.cmu.edu/~15210}

% Provides book
% This must be provided
% The name should be relative to course number.
\providesbook{S18}

% Start counting chapters from 
% This is optional. Will start counting at 1.
\provideschapter{7}
\providessection{1}

15-210 aims to teach methods for designing, analyzing, and programming
sequential and parallel algorithms and data structures. The emphasis
is on teaching fundamental conceopts applicable across a wide variety
of problem domains, and transferable across a reasonably broad set of
programming languages and computer architectures. This course also
includes a significant programming component in which students will
program concrete examples from domains such as engineering, scientific
computing, graphics, data mining, and information retrieval (web
search).

Unlike a traditional introduction to algorithms and data structures,
this course puts an emphasis on parallel thinking â€” i.e., thinking
about how algorithms can do multiple things at once instead of one at
a time. The course follows up on material learned in 15-122 and 15-150
but goes into significantly more depth on algorithmic issues. 


\begin{book}
\title{Algorithm Design: Parallel and Sequential}
\authors{Umut A. Acar and Guy Blelloch}

\begin{chapter}[Genome Sequencing]
\label{ch:genome}

\picture{./media/protein.jpg}

Sequencing of a complete human genome represents one of the greatest
scientific achievements of the century.
%
When the human genome project was first proposed in mid 1980's, the
technology available could only sequence couple of hundred bases at a
time.
%
After a few decades, the efforts led to the several major landmark
accomplishments.
%
In 1996, the DNA of the first living species (fruit fly) was sequenced. 
%
This was followed, in 2001, by the draft sequence of the human genome.
%
Finally in 2007, the full human genome diploid was sequenced.


%% \begin{flushleft}
%%   \quad
%%   \begin{tabular}{r l}
%%     % \rule
%%     1996 & sequencing of first living species, \\
%%     2001 & draft sequence of the human Genome, \\
%%     2007 & full human Genome diploid sequence.\\
%%     % \bottomrule
%%   \end{tabular}
%% \end{flushleft}

Efficient parallel algorithms played a crucial role in all these
achievements.  In this chapter, we review some of the algorithms
behind the results---and the power of problem abstraction which will
reveal surprising connections between seemingly unrelated problems.
%
As with many ``real world'' applications, defining precisely the
problem that models our application is interesting in itself.  We
therefore start with the vague and not well-defined problem of
``sequencing the human genome'' and convert it into a precise
algorithmic problem.  Our starting point is the ``shotgun method'' for
genome sequencing, which was the method used for sequencing the human
genome.

\begin{section}[Background]

This section reviews some basic terminology behind genomes and the
genome sequencing problem.

\begin{unit}[Basic Definitions]

\begin{definition}[Nucleotide]
A nucleotide is the basic building block of nucleic acid polymers such
as DNA and RNA. 
%
It is comprised of a number of components, which bind together to form
the double-helix.
%
The components include 
\begin{itemize}
\item a~\defn{nitrogenous base}, one of Adenine,
Cytosine, Guanine, Thymine (Uracil), 

\item a 5-carbon sugar molecule, and
\item one or more phosphate groups.
\end{itemize}
%
We distinguish between four kinds of nucleotides based on their
nitgogenous base and label them as \str{A} (Adenine), \str{C}
(Cytosine), \str{G} (Guanine), or \str{T} (Thymine).
\end{definition}

\begin{definition}[Nucleic Acid]
Nucleic acids are large molecules structured as linear chains of
nucleotides.  DNA and RNA are two important examples of nucleic acids.
\end{definition}

\begin{definition}[Human Genome]

The~\defn{human genome} is the full nucleic acid sequence for humans
consisting of nucleotide bases of A (Adenine), Cytosine (C),
Guanine (G), or Thymine (T).
%
It contains over $3$ billion~\defn{base pairs}, each of which consist
of two nucleotide bases bound by hydrogen bonds.
%
It can be written as a sequence or a string of bases consisting of the
letters, ``A'', ``C'', ``G'', and ``T''.
%
\end{definition}

\begin{remark}
The human-genome sequence, if printed as a book, would be
about as tall as the Washington Monument.
%
The human genome is present in each cell of the human body. 
%
It appears to have all the information needed by the cells in our
bodies and its deeper understanding will likely lead to insights into
the nature of life.
\end{remark}

\begin{checkpoint}
\begin{questionfr}
\points 10
\prompt What is the difference between a nucleoite and a nucleic acid?
\answer 
% A nucleic acid consists of many necleoutides.
\end{questionfr}

\begin{questionfr}
\points 10
\prompt How long is the human genome?
\answer Human genome consist of over 3 Billion base pairs.
\explain
\end{questionfr}


\end{checkpoint}

\end{unit}

\begin{unit}[Sequencing Methods]


\begin{teachask}
What makes genome sequencing hard?
\end{teachask}
%

\begin{gram}
The challenge in sequencing the genome is that there is currently
no way to read long strands with accuracy. 
%
Current DNA ``reading'' techniques are only capable of efficiently
reading relatively short strands, e.g., 10-1000 base pairs, compared
to the over three billion contained in the whole human genome.
%
Scientists therefore cut strands into shorter fragments, sequence
them, and then reassemble the sequenced fragments.
%
\end{gram}


\begin{gram}[Primer Walking]  
A technique called~\defn{primer walking} can be used to sequence
nucleic acid molecules upto 10,000 bases.  
%
The basic idea behind primer walking is to sequence a nucleic acid
from each end using special molecules called~\defn{primers} that can
be used to read the first 1000 bases or so.
%
The process is then repeated to ``walk'' the rest of the nucleic acid
by using most recently read part as a primer for the next part.
%
Since the process requires constructing primers and since it is
sequential, it is only effective for sequencing short molecules.
\end{gram}


%
%% \begin{teachask}
%% Can you think of a way to parallelize primer walking? 
%% \end{teachask}

\begin{gram}[Fragments]
To sequence a larger molecule, we can cut it into~\defn{fragments},
which can be achieved in a lab, and then use primer walking to sequence
each fragment.
%
Since each fragment can be sequenced independently in parallel, this
would allow us to parallelize the hard part of sequencing.
%
The problem though is that we don't know how to assemble them
together, because the cutting process destroys the order of the
fragments.
%
%% \begin{question}
%% The problem of putting together the pieces is a bit like solving a
%% jigsaw puzzle. But it is harder. Can you see why?  Can you think of a
%% way of turning this into a jigsaw puzzle that we can solve?
%% \end{question}
%
\end{gram}

\begin{note}
The approach of dividing the genome and sequencing each piece
independently is somewhat analogous to solving a jigsaw puzzle but
without knowing the complete picture.
%
It can be difficult and perhaps even impossible to solve such a
puzzle.
\end{note}

\begin{example}
When cut, the strand \texttt{cattaggagtat} might turn into,
\texttt{ag}, \texttt{gag}, \texttt{catt}, \texttt{tat}, destroying the
original ordering.
\end{example}

\begin{gram}[The Shotgun Method]
When we cut a genome into fragments we lose all the information
about how the fragments should be assembled.
%
If we had some additional information about how to assemble them, then
we could imagine solving this problem.
%
%\begin{question}
%Can you think of a way to relate different pieces? 
%\end{question}
One way to get additional information on assembling the fragments is
to make multiple copies of the original sequence and generate many
fragments that overlap.
%
Overlaps between fragments can then help relate and join them.
%
This is the idea behind the shotgun (sequencing) method, which was the
primary method used in sequencing the human genome for the first time.
\end{gram}

\begin{example}
For the sequence
%
\texttt{cattaggagtat}, 
%
we produce three copies:

\begin{quote}
\texttt{cattaggagtat}\\
\texttt{cattaggagtat}\\
\texttt{cattaggagtat}
\end{quote}

We then divide each into fragments
%
\begin{quote}
\texttt{catt}~~|~~\texttt{ag}~~|~~\texttt{gagtat}
\\
\texttt{cat}~~|~~\texttt{tagg}~~|~~\texttt{ag}~~|~~\texttt{tat}
\\
\texttt{ca}~~|~~\texttt{tta}~~|~~\texttt{gga}~~|~~\texttt{gtat}
\end{quote}
 
Note how each cut is ``covered'' by an overlapping fragment telling us how to reverse the cut.

\end{example}

\begin{definition}[Shotgun Method]

The~\defn{shotgun method} works as follows.
\begin{enumerate}
\item Take a DNA sequence and make multiple copies.  

\item Randomly cut the sequences using a ``shotgun'' (in reality,
  using radiation or chemicals) into short fragments.

\item Sequence each fragments (possibly in parallel).

\item Reconstruct the original genome from the fragments.  
\end{enumerate}
\end{definition}

\begin{remark}
Steps 1--3 of the Shotgun Method are done in a wet lab, while step~4
is the algorithmically interesting component.
%
%\begin{question}
%In step 4, is it always possible to reconstruct the sequence?
%\end{question}
%
Unfortunately it is not always possible to reconstruct the exact
original genome in step~4. 
%
For example, we might get unlucky and cut all sequences in the same
location.  
%
Even if we cut them in different locations, there can be many DNA
sequences that lead to the same collection of fragments. 
%
A particularly challenging problem is repetition: there is no easy way
to know if repeated fragments are actual repetitions in the sequence
or if they are a product of the method itself.
%
\end{remark}
\begin{checkpoint}

\begin{questionfr}
\points 10
\prompt
 How large are the nucleic acids that can be ``read'' easily in
a lab today?
\answer
It is possible to read molecules up to 1000 nucleotides.
\end{questionfr}


\begin{questionfr}
\points 10
\prompt
 How large are the nucleic acids that can be ``read'' easily in
a lab today?
\answer
It is possible to read molecules up to 1000 nucleotides.
\end{questionfr}

\begin{questionfr}
\points 10
\prompt How does the primer walking technique work? 
\answer It works by sequencing a molecule in small chunks by starting
with a primer molecule that can be used to start reading the chunk. To
start the process, primers can be constructed.  Once the process
starts it can be iterated because the last chunk serves as a primer
for the next.
\explain
\end{questionfr}

\begin{questionfr}
\points 10
\prompt Is primer walking parallel?
\answer The technique is sequential because it requires a primer.
\explain
\end{questionfr}

\begin{questionfr}
\points 10
\prompt How does the shotgun method differ from primer walking?
\answer The difference is that in the shotgun method, we make copies
of the original molecule.  This allows us to assemble the results from
independent readings together.
\explain
\end{questionfr}


\end{checkpoint}
\end{unit}
\end{section}

\begin{section}[Genome Sequencing Problem]

This section formulates the genome sequencing problem as an
algorithmic problem.

\begin{unit}[Defining the Problem]

\begin{gram}
Recall that the using the Shotgun Method, we can construct and sequence
short fragments made from copies of the original genome. 
%
Our goal is to use algorithms to reconstruct the original genome
sequence from the many fragments by somehow assembling back the
sequenced fragments.
%
But there are many ways that such fragments can be assembled and we
have no idea how the original genome looked like.  So what can we do?
%
In some sense we want to come up with the ``best solution'' that we
can, given the information that we have (the fragment sequences).
%
\end{gram}

\begin{gram}[Basic Terminology on Strings]
From an algorithmic perspective, we can treat a genome sequence just
as a sequence made up of the four different characters representing
nucleotides.
%
To study the problem algorithmically, let's review some basic
terminology on strings.
\end{gram}

%% TODO: Cleanup these definitions
%% You need substring first and then superstring.
\begin{group}
\begin{definition}[Superstring]
A string $r$ is a~\defn{superstring} of another string $s$ if $s$
occurs in $r$ as a contiguous block, i.e., $s$ is a substring of $r$.
\end{definition}

\begin{example}
\begin{itemize}
\item
\texttt{gata} is a superstring of \texttt{aa} and \textt{gt} but not
of \texttt{tg}.

\item
\texttt{gatag} is a superstring of \texttt{gtg} and \textt{tag} but not
of \texttt{tac}.
 
\end{itemize}

\end{example}

\end{group}

\begin{group}
\begin{definition}[Substring, Prefix, Suffix]
A string $s$ is a~\defn{substring} of another string $r$, if $s$ occurs
in $r$ as a contiguous block.
%
Prefix and suffix are substrings that occur at specific positions in
the string.
%
A string, $s$ is a~\defn{prefix} of another string $r$, if $s$ is
occurs at the beginning of $r$.
% 
A string, $s$ is a~\defn{suffix} of another string $r$, if $s$ is
occurs at the end of $r$.
\end{definition}

\begin{example}
\begin{itemize}
\item
\texttt{ag} is a substring of \texttt{ggag}, and is also a suffix. 
\item
\texttt{gga} is a substring of \texttt{ggag}, and is also a prefix.
\item
\texttt{ag} is not a substring of \texttt{attg}.
\end{itemize}
\end{example}
\end{group}

\begin{group}
\begin{definition}[Kleene Operators]
For any set $\Sigma$, its~\defn{Kleene star} $\Sigma^*$ is the set of
all possible strings consisting of characters $\Sigma$, including the
empty string.

For any set $\Sigma$, its~\defn{Kleene plus} $\Sigma^+$ is the set of
all possible strings consisting of characters $\Sigma$, excluding the
empty string.
\end{definition}

\begin{example}
Given $\Sigma = \{\texttt{a},\texttt{b}\}$,
\[
\begin{array}{rl}
\Sigma^* 
%
= 
%
\{
&
\str{}, 
%
\\
&
\str{\texttt{a}}, \str{\texttt{b}}, 
%
\\
&
\str{\texttt{aa}}, \str{\texttt{ab}}, 
\str{\texttt{ba}}, \str{\texttt{bb}}, 
\\
%
&
\str{\texttt{aaa}}, \str{\texttt{aab}}, \str{\texttt{aba}},
\str{\texttt{abb}},
\\
& \str{\texttt{baa}}, \str{\texttt{bab}}, \str{\texttt{bba}}, \str{\texttt{bbb}},
\\
%
&
\ldots
\\
\} &
\end{array}
\]
%
and
%
\[
\begin{array}{rl}
\Sigma^+
%
= 
%
\{
&
\str{\texttt{a}}, \str{\texttt{b}}, 
\\
%
&
\str{\texttt{aa}}, \str{\texttt{ab}}, 
\str{\texttt{ba}}, \str{\texttt{bb}}, 
\\
%
&
\str{\texttt{aaa}}, \str{\texttt{aab}}, \str{\texttt{aba}}, \str{\texttt{abb}}, 
\\
&
\str{\texttt{baa}}, \str{\texttt{bab}}, \str{\texttt{bba}}, 
\str{\texttt{bbb}},
\\
%
& \ldots
\\
\} & 
\\
\end{array}
\]
\end{example}
\end{group}

\begin{definition}[Shortest Superstring (SS) Problem]
  Given an alphabet set $\Sigma$ and a set of finite-length strings
  $A \subseteq \Sigma^*$, return a shortest string $r$ that contains every $x
  \in A$ as a substring of $r$.

%% In this definition the notation $\Sigma^*$, the ``Kleene star'', means
%% the set of all possible non-empty strings consisting of characters
%% $\Sigma$.
\end{definition}

\begin{gram}[Genome Sequencing as a String Problem] 
We can now try to understand the properties of the genome-sequencing
problem and the solution that we are looking for.
\end{gram}

\begin{exercise}[Properties of the Solution]
What is a property that the result sequence needs to have in relation
to the fragments sequenced in the Shotgun Method?
\end{exercise}

\begin{solution}
Because the fragments all come from the original genome, the result
should contain all of them.  In other words, the result is
a~\defn{superstring} of the fragments.
%
\end{solution}

\begin{exercise}
There can be multiple superstrings for any given set of fragments.
%
Which superstrings are more likely to be the actual genome?
%
\end{exercise}

\begin{solution}
One possibly good solution is the shortest superstring. 
%
Because the Shotgun Method starts by making copies of the original
sequence, by insisting on the shortest superstring, we would make sure
that the duplicates would be eliminated.
%
More specificially, if the original sequence had no duplicated
fragments, then this approach would eliminate all the duplicates
created by the copies that we made in the beginning.
%
%
\end{solution}

\begin{teachnote}
The claim above could probably be made more precise, perhaps with an
example.
\end{teachnote}



\begin{gram}
We can abstract the genome-sequencing problem as an instance of the
shortest superstring problem where $\Sigma = \{a, c, g, t\}$.
%
We have thus converted a vague problem, sequencing the genome, into a
concrete mathematical problem, the SS problem. 
%
\end{gram}

\begin{remark}
As we shall discuss more later, the SS problem is not exactly the
right abstraction for the application of sequencing the genome,
because it ignores several important practical factors.
%
One issue is that there can be errors in sequencing the genome, which can
make it difficult to assemble the result by recoginizing overlaps.
%
Another issue is that genome can contain repeated sections that are
much longer than the fragments.
%
The proposed appproach will not take such repeated sections into
account.
%
Fortunately, the basic approach described here can be generalized to
deal with these issues.
\end{remark}
\begin{checkpoint}


\begin{questionfr}
\points 10

\prompt Let $s$ be a string of size $n$ in $\Sigma^+$, where $\Sigma =
\{a, c, g, t \}.$  Imagine cutting $s$ into fragments and constructing
the shortest superstring $r$ of these fragments.  How many possible
shortest superstrings are there?  
\answer
The shortest string is no longer that $n$... So we have  a sum...

\end{questionfr}

\begin{questionfr}
\points 10

\prompt
Let $s$ be a large string and consider the string $r = ss$, i.e., $r$~is
obtained by concatenating $s$ with itself.  Suppose that we apply the
Shotgun Method to $r$, can we construct it back?  

\answer
Seems unlikely. 
\explain

\end{questionfr}


\end{checkpoint}

\end{unit}
\end{section}
\begin{section}[Algorithms]

This section discusses several algorithms for solving the Genome
Sequencing Problem.
%
The algorithms are derived by using consider three algorithm-design
techniques: brute-force, ...


\begin{unit}[Brute Force]

\begin{gram}
Designing algorithms may appear to be an intimidating task, because it
may seem as though we would need brilliant ideas that come out of
nowhere. 
%
In reality, we design algorithms by starting with simple ideas based
on several well-known techniques and refine them until  the
desired result is reached.
%
Perhaps the simplest algorithm-design technique (and usually the least
effective) is brute-force.
\end{gram}

\begin{gram}[Brute Force Algorithm 1]
As applied to the genome-sequencing problem, the brute-force technique
involves trying all candidate superstrings of the fragments and
selecting the shortest one.
%
Concretely, we can consider all strings $x \in \Sigma^*$, and for each
$x$ to check if every fragment is a substring.
%
Although we won't describe how here, such a check can be performed
efficiently.
%
We then pick the shortest $x$ that is indeed a superstring of
all fragments.
%

The problem with the above approach is that there are an infinite
number of strings in $\Sigma^*$ so we cannot check them all.
%
But, we don't need to: we only need to consider strings up to
the total length of the fragments~$m$, since we can easily construct a
superstring by concatenating all fragments. 
%
Since the length of such a string is $m$, the shortest superstring has
length at most $m$.


Unfortunately, there are still $|\Sigma|^m$ strings of length $m$;
this number is not infinite but still very large.  For the sequencing
the genome $\Sigma = 4$ and $m$ is in the order of billions, giving
something like $4^{1,000,000,000}$.  
%
There are only about $10^{80}
\approx 4^{130}$ atoms in the universe so there is no feasible way we
could apply the brute force method directly.  
%
In fact we can't even apply it to two strings each of length $100$.

\end{gram}
\end{unit}

\begin{unit}[Understanding the Structure of the Problem]

\begin{gram}
To improve over the brute force-algorithm described above, let's take
a closer look at the problem to make some observations.
\end{gram}

\begin{gram}[Observation 1]
We can ignore a fragment that is a substring of another fragment,
because it doesn't contribute to the solution.
%
For example, if we have \texttt{gagtat}, \texttt{ag}, and \texttt{gt},
we can throw out \texttt{ag} and \texttt{gt}. 
\end{gram}

\begin{definition}[Snippets]
In genome sequencing problem, we refer to the fragments that are not
substrings of others as~\defn{snippets}.
\end{definition}

\begin{gram}[Observation 2: Ordering of the snippets]
Because no snippet is a substring of another, in the result
superstring, snippets cannot start at the same position.
%
Furthermore, if one starts after another, it must finish after the
other.  
%
This leads to our second observation: in any superstring, the start
positions of the snippets is a strict (total) order, which is the same
order as their finish positions.
%

This observation means that to compute the shortest superstring of the
snippets, it suffices to consider the permutations of snippets and
compute for each permutation the corresponding superstring.
\end{gram}


\begin{example}
In our example, we had the following fragments.

\begin{quote}
\texttt{catt}~~|~~\texttt{ag}~~|~~\texttt{gagtat}
\\
\texttt{cat}~~|~~\texttt{tagg}~~|~~\texttt{ag}~~|~~\texttt{tat}
\\
\texttt{ca}~~|~~\texttt{tta}~~|~~\texttt{gga}~~|~~\texttt{gtat}
\end{quote}

The snippets are now:
$S = \{$
%
\texttt{catt}, \texttt{gagtat}, \texttt{tagg},  \texttt{tta}, \texttt{gga}
%
$\}$

%\begin{equation*}
%  S = \Big\{\texttt{catt}, \texttt{gagtat}, \texttt{tagg},  \texttt{tta}, \text%tt{gga}\Big\}.
%\end{equation*}

The other fragments
%
$\{$
%
\texttt{cat}, \texttt{ag}, \texttt{tat},
\texttt{ca}, \texttt{gtat}
% 
$ \}$ are all contained within the snippets.

Consider a superstring such as \texttt{cattaggagtat}.  The starting
points for the snippets are: 
%
$0$ for \texttt{catt}, 
%
$2$ for \texttt{tta},
%
$3$ for \texttt{tagg},
%
$5$ for \texttt{gga}.

\end{example}

\begin{gram}[Shortest Superstring of a Permutation]
Our second observation says that a superstring corresponds to a
permutation.  This leads to the natural question of how to find the
shortest superstring for a given permutation.
%
The following theorem tells us how.
\end{gram}

\begin{group}
\begin{theorem}[Shortest Superstring by Overlap Removal]   
Given any start ordering of the snippets $s_1,s_2, \ldots, s_n$,
removing the maximum overlap between each adjacent pair of snippets
$(s_i,s_{i+1})$ gives the shortest superstring of the snippets for
that permutation.
\end{theorem}
\begin{proof}
The theorem can be proven by induction.  
%
The base case is true since it is clearly true for a single snippet.
%
Inductively, we assume it is true for the first $i$ snippets, i.e.,
that removing the maximum overlap between adjacent snippets among
these $i$ snippets yields the shortest superstring of $s_1, \ldots,
s_i$ starting in that order.  
%
We refer to this superstring as $r_i$.
%
We now prove that the theorem it is true for $i$ then it is true for
${i+1}$.
%
Consider adding the snippet $s_{i+1}$ after $r_i$, we know that
$s_{i+1}$ does not fully overlap with the previous snippet ($s_{i}$)
by the definition of snippets.
%
Therefore when we add it on using the maximum
overlap, the resulting string $r_{i+1}$ will be $r_i$ with some new characters
added to the end.  
%
The string $r_{i+1}$ is a superstring of $s_0, \ldots, s_{i+1}$
because it includes $r_i$, which by induction is a superstring of
$s_0, \ldots, s_{i}$, and because it includes $s_{i+1}$.
%
It is also be the shortest since $r_i$ is the shortest for $s_1,
\ldots s_i$ and a shorter string would not include $s_{i+1}$, because
we have already eliminated the maximum overlap between $s_{i+1}$ and
$r_i$.
\end{proof}


\end{group}

\begin{example}
In our running example, consider the following permutation
\begin{quote}
  \texttt{catt}\, \texttt{tta}\, \texttt{tagg}\,
  \texttt{gga}\, \texttt{gagtat}
\end{quote}
When the maximum overlaps are removed we obtain \texttt{cattaggagtat},
which is indeed the shortest superstring for the given permutation.
In this case, it is also the overall shortest superstring.
\end{example}


\begin{checkpoint}
\begin{questionfr}[Start positions of snippets]
\points 10
\prompt
Consider the set of snippets for a genome sequence, and let
$s$ be a superstring of the snippets, such that each
snippet is contained in the superstring as substring.  Prove than no
two snippets can start at the same position in $s$.  
\answer
\end{questionfr}

\begin{questionfr}[End positions of snippets]
\points 10
\prompt
Consider the set of snippets for a genome sequence, and let
$s$ be a superstring of the snippets, such that each
snippet is contained in the superstring as substring.  Prove than no
two snippets can end at the same position in $s$.  
\answer
\end{questionfr}

\begin{questionfr}[Substring Check]
\points 10
\prompt
Given a string $s$ of length $n$, and a string $t$ of
length $m$: design an algorithm that checks whether $s$ is a
substring of $t$, and analyze the work and span of your
algorithms.
\answer

%%
%% Answer: Start at every position in $t$ and check for overlap. 
%%  Work: $O(nm)$ Span: $log(m) + log(n)$. 
%%


\end{questionfr}


\end{checkpoint}
\end{unit}

\begin{unit}[Brute Force Reloaded]

\begin{gram}
We will now design better algorithms by applying the observations from
the previous unit. 
%
Specifically, we now know that we can compute the shortest superstring
by finding the permutation that gives us the shortest superstring.
%
Furthermore, we can find the shortest supersting for a permutation by
removing overlaps.
%
Let's first design an algorithm for finding and removing overlaps.
\end{gram}


\begin{algorithm}[Finding Overlap]
Consider two strings~$s$ and~$t$ in that order.
%
To find the overlap between $s$ and $t$, we can use the brute
force-technique: consider each suffix of~$s$ and check if it is a
prefix of~$t$ and select the longest suffix that is also a prefix.
%

The work of this algorithm is $|s| \cdot |t|$, i.e., the product of
the lengths of the strings.  
%
For the span, note that we can check if a particular suffix is a
prefix in $O(\lg{t})$ span by checking each position in~$O(1)$ spand
and checking that all positions match in~$O(\lg{t})$ span.
%
Since we can try all suffices in parallel, the span remains
$O(\lg{|t|})$.
%
The span for selecting the maximum over all matches is
$O(\lg{s})$.  
%
The total span is thus $O(\lg{s} + \lg{t}) = O(\lg{(s+t)})$.
\end{algorithm}


%
%\begin{question}
%How can we compute the overlap between two strings?
%\end{question}
%
%
\begin{algorithm}[Finding the Shortest Permutation]
Using the above algorithm for finding overlaps, we can compute the
shortest superstring of a permutation by eliminating the ovelaps
between successive strings in the permutation.
%
We can then find the permutation with the shortest superstring by
considering each permutation and selecting the permutation with the
shortest superstring.
%
Since we can consider each permutation independently of the others,
this algorithm reveals parallelism.

We can improve this algorithm.
%
Notice that this algorithm repeatedly computes the overlap between the
same snippets.
%
Since we only remove the overlap between successive snippets, there
are only $O(n^2)$ pairs that we have to consider.
%
We can thus~\defn{stage} the algorithm:

\begin{itemize}
\item First,  compute the overlaps between each pair of snippets and
  store them in a dictionary for quick lookup.
\item Second, try all permutations and compute the shortest
  superstring by removing overlaps as defined by the dictionary.
\end{itemize}
%
\end{algorithm}

\begin{algorithm}[Finding the Shortest Permutation (Improved)]
We can improve our algorithm for finding the shortest permutation by
using a technique known as~\defn{staging}.
%
Notice that our algorithm repeatedly computes the overlap between the
same snippets, because the permutations all belong to the same set of
snippets.
%
Since we only remove the overlap between successive snippets in a
permutation, there are only $O(n^2)$ pairs to consider.
%
We can thus stage work of the algorithm more carefully:

\begin{itemize}
\item First,  compute the overlaps between each pair of snippets and
  store them in a dictionary for quick lookup.
\item Second, try all permutations and compute the shortest
  superstring by removing overlaps as defined by the dictionary.
\end{itemize}
%
\end{algorithm}

\begin{note}
Staging technique is inherently sequential: it creates a sequential
dependency between the different stages of work that must be done.  
%
If, however, the number of stages is small (in our example, we have
two), then this is harmless. 
\end{note}

\begin{gram}[Cost Analysis]
%
Let's analyze the work and the span of our staged algorithm.
%
For the analysis, let $W_1$ and $S_1$ be the work and span for the
first phase of the algorithm, i.e., for calculating all pairs of
overlaps in our set of input snippets $s = \{s_1, \ldots, s_n \}$.
%
Let $m = \sum_{x \in S} |x|$.
%
Using our algorithm, $\cd{overlap}(x,y)$ for finding the maximum overlap
between two strings $x$ and $y$, we have
\[
\begin{array}{lll}
W_{1} & \leq & \sum_{i=1}^{n}\sum_{j=1}^{n} W(\cd{overlap}(s_i,s_j)))\\
  & = & \sum_{i=1}^{n}\sum_{j=1}^{n} O(|s_i| |s_j|)\\
  & \leq & \sum_{i=1}^{n}\sum_{j=1}^{n} (c_1 + c_2 |s_i| |s_j|)\\
  & =    & c_1 n^2 + c_2 \sum_{i=1}^{n}\sum_{j=1}^{n} (|s_i| |s_j|)\\
  & =    & c_1 n^2 + c_2 \sum_{i=1}^{n} \left(|s_i| \sum_{j=1}^{n} |s_j|\right)\\
  & =    & c_1 n^2 + c_2 \sum_{i=1}^{n} (|s_i| m) \\
  & =    & c_1 n^2 + c_2 m \sum_{i=1}^{n} |s_i| \\
  & =    & c_1 n^2 + c_2 m^2\\
  & \in  & O(m^2)  \mbox{~~~~~~~~~~~since } m \geq n.
\end{array}
\]

Since all pairs can be considered in parallel, we have for span
\[
\begin{array}{lll}
S_{1} & \leq & \max_{i=1}^{n}\max_{j=1}^{n} S(\cd{overlap}(s_i,s_j)))
\\
S_{1} & \leq & \max_{i=1}^{n}\max_{j=1}^{n} O(\lg{(|s_i| + |s_j|)})
\\
     & \in  & O(\lg m).
\end{array}
\]
%
We therefore conclude that the first stage of the algorithm requires
$O(m^2)$ work and $O(\lg{m})$ span.

Moving onto the second stage, we want to compute the shortest
superstring for each permutation.
%
Given a permutation, we know that all we have to do is remove the
overlaps.
%
Since there are $n$ overlaps to be considered and summed, this
requires $O(n)$ work, assuming that we can lookup the overlaps in
constant time.
%
Since we can lookup the overlaps in parallel, the span is constant for
finding the overlaps and $O(\lg{n})$ for summing them. 
%
Therefore the cost for handling each permutation is $O(n)$ work and
$O(\lg{n})$ span.
%


Unfortunately, there are $n!$ permutations to consider.
%
Even though we have designed reasonably efficient algorithms for
computing the overlaps and the shortest superstring for each
permutation, there are too many permutations for this algorithm to be
efficient.
%
For $n = 10$ strings the algorithm is probably feasible, which is
better than our previous brute-force algorithm, which did not even
work for $n = 2$.
%
However for $n = 100$, we'll need to consider
$100!  \approx 10^{158}$ combinations, which is still more than the
number of atoms in the universe. 
%
Thus, the algorithm is still not feasible if the number of snippets is
more than a couple dozen.
\end{gram}


\begin{teachnote}
Since all pairs can be computed in parallel seem to require pfor.
\end{teachnote}

\begin{note}
Recall Sterling's approximation:
\[
n! \sim {\sqrt {2\pi n}}\left({\frac {n}{e}}\right)^{n}.
\]
You can think of this as 
\[
n! \sim \left( {\frac {n}{2.56}}\right)^{n} \sim \left( {\frac {n}{2}}\right)^{n}\]

The intuition is this: you have $n/2$ pairs multiplied all of which
add up to $n+1$.  The largest of such pairs occur in the middle,
i.e. when they are the same, so if all pairs were to be maximum, we
would have $((n/2)^2)^{n/2} = (n/2)^n$.  Now they are not all equal to
the maximum but many are close, because many of the numbers are close
to middle $n/2,$ thus we get something close no $(n/2)^n$ but a bit
less.
\end{note}

%
\begin{remark}
The technique of staging used above is a key technique in algorithm
design and engineering.
%
The basic idea is to identify a computation that is repeated many
times and pre-compute it, storing it in a data structure for fast
retrival.
%
Later instances of that computation can then be recalled via lookup
instead of re-computing every time it is needed. 
\end{remark}

\begin{checkpoint}

\begin{questionfr}[String concatenation]
\points 10
\prompt
  You are given a set of strings $s_1, \ldots, s_n$ and a target
  string $t$.

Use the brute-force technique to come up with an algorithm that
gives you a subset of the strings and the specific order in which they
can be concatenated to obtain the target string.
%
Analyze the work and spane of your algorithm.

\answer
\explain
\end{questionfr}


\begin{questionfr}[Compact String concatenation]
\points 10
\prompt
You are given a set of strings $s_1, \ldots, s_n$ and
another target string $t$.  
%
Use the brute-force technique to design an algorithm that gives you a
subset of the strings and the specific order in which they can be
concatenated while removing overlaps to obtain the target string.  For
example if you concatenate "parallel" and "elision", you will get
"parallelision".  Analyze the work and the span of your algorithm.

\answer
\explain
\end{questionfr}

\begin{questionfr}[Improved Overlap Finding]
\points 10
\prompt
Recall that we have used a brute-force algorithm for finding the
overlap between two strings. Can you improve on this algorithm? 

\answer
%% It should be possible.  Have not thought about it carefully.
\explain
\end{questionfr}


\end{checkpoint}
\end{unit}


\begin{unit}[Shortest Superstrings by Algorithmic Reduction]

\begin{gram}
Another fundamental technique in algorithm design is to reduce one
algorithms problem to another that has a known solution.
%
It is sometimes quite surprising that we can reduce a problem to
another, seemingly very different one.
%
Here, we will reduce the shortest superstring problem to the Traveling
Salesman Problem  (TSP), which might appear to be quite different.
\end{gram}

\begin{gram}[Traveling Salesperson]
The TSP is a canonical NP-hard problem dating back to the
1930s and has been extensively studied.
%
The two major variants of the problem are~\defn{symmetric} TSP
and~\defn{asymmetric} TSP, depending on whether the graph has
undirected or directed edges, respectively.
%

Asymmetric TSP requires finding a Hamiltonian cycle of the
graph such that the sum of the arc (directed edge) weights along the
cycle is the minimum of all such cycles.
%
Recall that a cycle is a path in a graph that starts and ends at the
same vertex and that a~\defn{Hamiltonian cycle} is a cycle that visits
every vertex exactly once.
%

The symmetric version of the problem can be viewed as considering only
graphs where for each arc $(u,v)$, there is also a reverse arc $(v,u)$
with the same weight.  
\end{gram}

\begin{gram}
A poster from a contest run by Proctor and Gamble in 1962 is
reproduced below.  The goal was to solve a 33 city instance of the
TSP.  Gerald Thompson, a Carnegie Mellon professor, was one of the
winners.

\begin{center}
\includegraphics[width=4.0in]{./media/car54small.jpg}
\end{center}
\end{gram}

\begin{problem}[The Asymmetric Traveling Salesperson Problem  (aTSP)]
  Given a weighted directed graph, find the shortest cycle that starts
  at some vertex and visits all vertices exactly once before returning
  to the starting vertex.
\end{problem}

\begin{note}
  Note that the version of the problem that requires starting at a
  particular vertex is also NP-hard because otherwise we can solve the
  general problem by trying each vertex.
\end{note}

%% \begin{question}
%% Can you reduce the SS problem to the TSP problem.  Hint: First try to
%% set up the problem so that each permutation corresponds to cycle.
%% \end{question}

\begin{gram}[Reduction from SS to TSP]
We can reduce the Shortest Superstring problem to TSP by using again
our second Observation, which we also used in the brute-force
algorithm: the shortest superstring problem can be solved by trying
all permutations.
%
In particular we will make TSP try all the permutations for us.
%

For the reduction, we set up a graph so that each valid Hamiltonian cycle
corresponds to a permutation.  The graph is~\defn{complete}: it
contains an arc between any two vertices, guaranteeing the existence
of a Hamiltonian cycle.

To specify the reduction, let $\cd{overlap}(s_i, s_j)$ denote the
maximum overlap for $s_i$ followed by $s_j$.
%
For example, for \texttt{tagg} and \texttt{gga}, we have
$\cd{overlap}($\texttt{tagg}$, $\texttt{gga}$) = 2$.
%
Now we build a graph $D = (V, A)$.
\begin{itemize}
\item The vertex set $V$ has one vertex per snippet and a special
  ``source'' vertex~$u$ where the cycle starts and ends.

\item The arc (directed edge) from $s_i $ to $s_j$ has weight $w_{i,j}
  = |s_j| - \cd{overlap}(s_i, s_j)$.  This quantity represents the increase
  in the string's length if $s_i$ is followed by $s_j$. 
%
  For example, if we have \texttt{tagg} followed by
  \texttt{gga}, then we can generate \texttt{tagga} which only
  adds 1 character giving a weight of 1---indeed,
  $|$ \texttt{gga}$|$ -
  $\cd{overlap}$ $($ \texttt{tagg}, \texttt{gga} $) = 3 -
  2 = 1$.

\item The weights for arcs incident to source $u$ are set as follows:
  $(u, s_i) = |s_i|$ and $(s_i, u) = 0$.  That is, if
  $s_i$ is the first string in the permutation, then the arc
  $(u, s_i)$ pays for the whole length $s_i$.  If $s_i$ is the
  last string we have already paid for it, so the arc $(s_i, u)$
  is free.
\end{itemize}
\end{gram}

\begin{example}
To see this reduction in action, the snippets in our running example,
%
\{\texttt{catt}, \texttt{gagtat}, \texttt{tagg},
  \texttt{tta}, \texttt{gga} \} 
%
results in the graph, a subgraph of which is shown below (not all arcs
are shown).

\begin{center}
\includegraphics[width=3.0in]{./media/tsp-reduction.jpg}
\end{center}


%% \begin{question}
%%   What does a Hamiltonian cycle in the graph starting at the source
%%   correspond to? What about the total weight of the arcs on a cycle?
%% \end{question}

As intended, in this graph, a Hamiltonian cycle corresponds to
a permutation in the brute force method: we start from the source and
follow the cycle to produce the permutation.
%
Furthermore, the sum of the arc weights in that cycle is equal to the
length of the superstring produced by the permutation.
%
Since the TSP finds the minimum weight cycle, it also finds the
permutation that leads to the shortest superstring.
%
Therefore, if we could solve the TSP problem, we can solve the
shortest superstring problem.

We have thus reduced the shortest-superstring problem, which is
NP-hard, to another NP-hard problem: TSP.
%
We constructed the reduction by using an insight from a brute-force
algorithm: that we can solve the problem by trying out all
permutations. 
%
The advantage of this reduction is that we know a lot about TSP, which
can help, because for any algorithm that solves or approximates TSP,
we obtain an algorithm for the shortest-superstring problem, and thus
for sequencing the genome.
%
\end{example}


\begin{remark}[Hardness of Shortest Superstring]
In addition to designing algorithms, reductions can be used to prove
that a problem is NP-hard or NP-complete.  For example, if we reduce
an NP-hard (NP-complete) problem $A$ to another problem $B$ by
performing polynomial work, then $B$ must be NP-hard (NP-complete).

We can establish that the Shortest Superstring problem is NP-hard by
using this technique and the same idea as in the reduction described
above.
%
To see this, note that for any cycle that starts at the source
vertex, we have a corresponding permutation.
%
The total weight of the arcs on such a cycle is exactly the length of
the shortest superstring with overlaps removed. 
%
Thus, if we have an algorithm for finding the shortest superstring,
then we can solve TSP.  Since constructing the graph only requires
polynomial work this reduction is efficient.
%
But we know that TSP is NP hard.
%
We thus conclude that the shortest superstring problem is also NP
hard.
\end{remark}

\begin{teachnote}
The reduction above has to try all sources.  It is actually reducing
to the TSP with start problem.
\end{teachnote}

\begin{checkpoint}
\begin{questionfr}[Hamiltonian cycle with start]
\points 10
\prompt

Prove using the reduction technique that if the TSP problem in
NP-hard, then so is the problem of finding the shortest Hamiltonian
cycle that starts and ends at a specified vertex.

\answer
\explain
\end{questionfr}

\end{checkpoint}

\end{unit}

\begin{unit}[Greedy Algorithm]
%\label{sec:genome::greedy}

\begin{gram}
We have thus far developed a brute-force algorithm for solving the
Shortest Supersting problem that requires exponential time and reduced
the problem to the Traveling Salesperson Problem (TSP), which is
NP-hard.
%
We also remarked that the Shortest Superstring problem in NP-hard by
reducing TSP to it.
%
Thus, we are still far away from a polynomial-work solution to tho
problem and we are unlikely to find one.
%

When a problem is NP hard, it means that there are {\em instances} of
the problem that are difficult to solve.  
%
NP-hardness doesn't rule out the possibility of algorithms that
quickly compute approximate or even near optimal answers or algorithms
that perform well on real world instances.
%
For example the type-checking problem for strongly typed languages
(e.g., the ML family of languages) is NP-hard but we use them all the
time, even on large programs.
\end{gram}

\begin{gram}
One interesting approach to overcoming difficult NP-hard problems is
to use approximation.
%
For this particular problem, we know efficient approximation
algorithms that are theoretically good: they guarantee that the length
of the superstring is within a constant factor of the optimal answer.
%
Furthermore, these algorithms tend to perform even better in practice
than the theoretical bounds suggest.
%
In the rest of this unit, we discuss such an algorithm.
\end{gram}

\begin{gram}[Greedy Algorithms]
To design an approximation algorithm we will use an iterative design
technique based on a \defn{greedy heuristic}.
%
When applying this design technique, we consider the current solution
at hand and make a \defn{greedy}, i.e., locally optimal decision to
reduce the size of the problem.
%
We then repeat the same process of making a locally optimal decision,
hoping that eventually these decisions lead us to a global optimum.
%
For example, a greedy algorithm for the TSP can visit the closest
unvisited city (the locally optimal decision), removing thus one city
from the problem.
%

Because greedy algorithms rely on a heuristic they may or may not
return an optimal solution.
%
%
Nevertheless, greedy algorithms are popular partly because they tend
to be simple and partly because they can perform quite well.
%
We note that, for a given problem there might be several greedy
algorithms that depend on what is considered to be locally optimal.
%
\end{gram}


\begin{gram}
The key step in designing a greedy algorithm is to decide the locally
optimal decision.
%
In the case of the SS problem, observe that we can minimize the length
of the superstring by maximizing the overlap among the snippets.
%
Thus, at each step of the algorithm, we can greedily pick a pair of
snippets with the largest overlap and join them by placing one
immediately after the other and removing the overlap.  
%
This can then be repeated until there is only one string left.
%
This is the basic idea behind the greedy algorithm below.
\end{gram}


\begin{algorithm}[Greedy Approximate SS]
\label{lst:genome::greedySS}
\[
\begin{array}{l}
\cd{greedyApproxSS}~S = 
\\
~~\cd{if}~|S| = 1~\cd{then}
\\ 
~~~~\cd{return}~x \in S
\\
~~\cd{else}
\\
~~~~\cd{let}
\\
~~~~~~T = \cset{\cd{overlap}(x, y), x, y) : x \in S, y \in S, x \neq
  y}
\\
~~~~~~(o_{xy},x,y) = \cd{argmax}_{(o,\_,\_) \in T} o
\\
~~~~~~z = \cd{join}(x,y)
\\
~~~~~~S' = S \cup \{z\} \setminus \{x,y\}
\\
~~~~\cd{in}
\\
~~~~~~\cd{greedyApproxSS}~S'
\\
~~~~\cd{end}
\end{array}
\] 


The pseudocode for our algorithm is shown above.
%
The algorithm relies on a function $\cd{overlap}(x, y)$ to compute the
overlap of the strings $x$ and $y$, and
%
the function $\cd{join}(x, y)$, which places $x$ after $y$ and
removes the maximum overlap.
%
For example, 
%
$\cd{join}($\texttt{tagg},\texttt{gga} $)$ $ = $ \texttt{tagga}.
%
Given a set of strings $S$, the $\cd{greedyApproxSS}$ algorithm
checks if the set has only 1 element, and if so returns that element.
%
Otherwise it finds the pair of distinct strings $x$ and $y$ in $S$
that have the maximum overlap.  It does this by first calculating the
overlap for all pairs and then picking the one of these that has the
maximum overlap.
%
Note that $T$ is a set of triples each corresponding to an overlap and
the two strings that overlap.  The notation $\cd{argmax}_{(o,\_,\_) \in T}
o$ is mathematical notation for selecting the element of $T$ that
maximizes the first element of the triple, which is the overlap.
%
After finding the pair $(x,y)$ with the maximum overlap, the
algorithm then replaces $x$ and $y$ with $z = \cd{join}(x,y)$
in $S$ to obtain the new set of snippets $S'$.

The new set $S'$ contains one element less than $S$.
%
The algorithm recursively repeats this process on this new set of
strings until there is only a single string left.  It thus terminates
after $|S|$ recursive calls.
\end{algorithm}

\begin{group}
\begin{exercise}
Why is the algorithm greedy?
\end{exercise}

\begin{solution}
The algorithm is greedy because at every step it takes the pair of
strings that when joined will remove the greatest overlap, a locally
optimal decision.  Upon termination, the algorithm returns a single
string that contains all strings in the original set $S$.  However,
the superstring returned is not necessarily the shortest superstring.
\end{solution}
\end{group}

%% \begin{todo}
%% 	In the picture there should be a directed arrow from "gga" to "gagtat" with weight 2?
%% \end{todo}

\begin{example}
Consider the snippets in our running example, \\
%
\Big\{
%
\texttt{catt}, \texttt{gagtat}, \texttt{tagg}, \texttt{tta},
\texttt{gga}
%
\Big\}.  
%
\\
The graph below illustrates the overlaps between different snippets.
An arc from vertex $u$ to $v$ is labeled with the size of the overlap
when $u$ is followed by $v$.
%
All arcs with weight $0$ are omitted for simplicity.
%
\begin{center}
\includegraphics[width=3.0in]{./media/overlaps.jpg}
\end{center}

Given these overlaps, the greedy algorithm could proceed as follows:
\begin{itemize}
\item join \texttt{tagg} and \texttt{gga} to obtain \texttt{tagga},
\item join \texttt{catt} and \texttt{tta} to obtain \texttt{catta},
\item join \texttt{gagtat} and \texttt{tagga}  to obtain
  \texttt{gagtatagga}, and
\item join \texttt{gagtatagga} and \texttt{catta} to obtain \texttt{gagtataggacatta}.
\end{itemize} 

\end{example}

\begin{note}
Although the greedy algorithm merges pairs of strings one by one, we
note there is still significant parallelism in the algorithm, at least
as described.  In particular we can calculate all the overlaps in
parallel, and the largest overlap in parallel using a reduction. 
\end{note}

\begin{gram}[Cost Analysis]
From the analysis of our brute-force algorithm, we know that we can
find the overlaps between the strings in $O(m^2)$ work and $O(\lg{m})$
span.
%
Thus the $\cd{argmax}$ for finding the maximum overlap can be computed in
$O(m^2)$ work and $O(\lg{m})$ span using a simple reduce.  The other
steps have less work and span.  Therefore, not including the recursive
call each call to $\cd{greedyApproxSS}$ costs is $O(m^2)$ work and
$O(\lg{m})$ span.
%%%%%
Observe now that each call to $\cd{greedyApproxSS}$ reduces the
number of snippets: $S'$ contains one fewer element than $S$, so
there are at most $n$ calls to $\cd{greedyApproxSS}$.  
%
These calls are sequential because one call must complete before the
next call can take place.  
%
Hence, the total cost for the algorithm is $O(n m^2)$ work and $O(n
\lg m)$ span.  
%
The algorithm is therefore highly parallel.
%
There are ways to make the algorithm more efficient, but leave that as
an exercise to the reader.
\end{gram}

\begin{gram}[Approximation Quality]
Since the $\cd{greedyApproxSS}$ algorithm does only polynomial work,
and since the TSP problem is NP hard, we cannot expect it to give an
exact answer on all inputs---that would imply \textbf{P} $=$
\textbf{NP}, which is unlikely.
%
Although $\cd{greedyApproxSS}$ does not return the shortest
superstring, it returns a good approximation of the shortest
superstring.
%
In particular, it is known that it returns a string that is within a
factor of 3.5 of the shortest; it is conjectured that the algorithm
returns a string that is within a factor of 2.  
%
In practice, the greedy algorithm typically performs better than the
bounds suggest.  The algorithm also generalizes to other similar
problems.
%
Algorithms such as $\cd{greedyApproxSS}$ that solve an NP-hard problem
to within a constant factor of optimal, are
called~\defn{constant-factor approximation algorithms}.
%
\end{gram}

\begin{checkpoint}
\begin{questionfr}[Containted Strings]
\points 10
\prompt
  In the greedy algorithm \cd{greedyApproxSS}
%  (\algref{genome::greedySS}), 
we remove ${x,y}$ from the set of strings but do not remove any
strings from $s$ that are contained within $xy = \cd{join}(x,y)$.
Prove that there cannot be any such strings.  
\answer 
\explain
\end{questionfr}


\begin{questionfr}[Correctness of the Greedy Algorithm]
\points 10
\prompt
Prove that algorithm $\cd{greedyApproxSS}$
% (\algref{genome::greedySS})
returns a string that is a superstring of all original strings.
\answer
\explain
\end{questionfr}

\begin{questionfr}[Exactness of Greedy Approximation]
\points 10
\prompt
Give an example input for which $\cd{greedyApproxSS}$
%(\algref{genome::greedySS}) 
does not return the shortest superstring.
\answer
\explain
\end{questionfr}


\begin{questionfr}[Improved Greedy Algorithm]
\points 10
\prompt
Improve the greedy algorithm's cost bounds by presenting a more
efficient implementation.
\answer
\explain
\end{questionfr}

\begin{questionfr}[Generous Grandmother]
\points 10
\prompt
Your rich grandmother enjoys collecting precious items, such as
jewelry and gold-plated souvenirs.  
%
When you visit her for Spring break (instead of going to Cancun), she
is very pleased and decides to reward you. She gives you a bag and a
scale and instructs you to take anything you want as long as the bag
does not hold any more than 10 pounds.  Delighted by the surprising
(based on your parents' stories of their childhood) generosity of your
grandmother, you also realize that your grandmother forgot to take the
price tags off the items, which gives you an idea about the value of
these items.


First, design a brute force algorithm for selecting the items to take
away with you. Is your algorithm optimal? What is the work and span of
your algorithm?

Next, design a greedy algorithm for selecting the items to take away
with you.  Why is your algorithm greedy?  Is your greedy algorithm
optimal? What is the work and span of your greedy algorithm?

\answer
\explain
\end{questionfr}

\end{checkpoint}

\end{unit}

\end{section}

\begin{section}[Concluding Remarks]

The~\defn{Human Genome Project} was an international scientific
research project that was one of the largest collaborative projects in
the human history. 
%
The project was formally launched in 1990, after several years of
preperations and pronounced complete in 2000.
%
It costed approximately three billion dollars (in the currency of
Fiscal Year 1991).

Abstracting a real-world problem such as the sequencing of the human
genome, which is one of the most challenging problems that has been
tackled in science, is significantly more complex than the relatively
simple abstractions that we used in this chapter.  
%
This section discusses some of the complexities of genome sequencing
that we have not addressed.

\begin{unit}[Abstraction versus Reality]

\begin{gram}
Often when abstracting a problem we can abstract away some key aspects
of the underlying application that we want to solve.  Indeed this is
the case when using the Shortest Superstring (SS) problem for
sequencing genomes.  In actual genome sequencing there are two
shortcomings with using the SS problem.

The first is that when reading the base pairs using a DNA sequencer
there can be errors.  This means the overlaps on the strings that are
supposed to overlap perfectly might not.  This can be dealt with by
generalizing the Shortest Superstring problem to deal with approximate
matching.  Describing such a generalization is beyond the scope of
this course, but basically one can give a score to every overlap and
then pick the best one for each pair of fragments.  The nice feature
of this change is that the same algorithmic techniques we discussed
for the SS problem still work for this generalization, only the
``overlap'' scores will be different.

The second shortcoming of using the SS problem by itself is that real
genomes have long repeated sections, possibly much longer than the
length of the fragments that are sequenced.  The SS problem does not
deal well with such repeats.  In fact when the SS problem is applied
to the fragments of an initial string with longer repeats than the
fragment sizes, the repeats or parts of them are removed.  One method
that researchers have used to deal with this problem is the so-called
\emph{double-barrel shotgun method}.  In this method strands of DNA
are cut randomly into lengths that are long enough to span the
repeated sections.  After cutting it up one can read just the two ends
of such a strand and also determine its length (approximately).  By
using the two ends and knowing how far apart they are it is possible
to build a ``scaffolding'' and recognize repeats.  This method can be
used in conjunction with the generalization of the SS discussed in the
previous paragraph.  In particular the SS method allowing for errors
can be used to generate strings up to the length of the repeats, and
the double barreled method can put them together.
\end{gram}
\end{unit}
\end{section}

\end{chapter}

\end{book}
