%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilCourse}
\begin{dilFieldTitle}
Parallel and Sequential Algorithms
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Parallel and Sequential Algorithms
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210
\end{dilFieldUnique}
\begin{dilLabel}
15210
\end{dilLabel}
\begin{dilNo}
15210
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldCourseNumber}
15210
\end{dilFieldCourseNumber}
\begin{dilFieldPicture}
/210/course/air-pavilion.jpg
\end{dilFieldPicture}
\begin{dilFieldSemester}
Spring 2018
\end{dilFieldSemester}
\begin{dilFieldWebsite}
http://www.cs.cmu.edu/~15210
\end{dilFieldWebsite}
\begin{dilFieldProvidesBook}
S18
\end{dilFieldProvidesBook}
\begin{dilFieldProvidesChapter}
11
\end{dilFieldProvidesChapter}
\begin{dilFieldProvidesSection}
1
\end{dilFieldProvidesSection}
\begin{dilFieldProvidesUnit}
1
\end{dilFieldProvidesUnit}
\begin{dilFieldProvidesAssignment}
1
\end{dilFieldProvidesAssignment}
\begin{dilIntro}
<p>15-210 aims to teach methods for designing, analyzing, and programming sequential and parallel algorithms and data structures. The emphasis is on teaching fundamental concepts applicable across a wide variety of problem domains, and transferable across a reasonably broad set of programming languages and computer architectures. This course also includes a significant programming component in which students will program concrete examples from domains such as engineering, scientific computing, graphics, data mining, and information retrieval (web search).</p>
<p>Unlike a traditional introduction to algorithms and data structures, this course puts an emphasis on parallel thinking — i.e., thinking about how algorithms can do multiple things at once instead of one at a time. The course follows up on material learned in 15-122 and 15-150 but goes into significantly more depth on algorithmic issues.</p>

\end{dilIntro}
\begin{dilIntroDex}
15-210 aims to teach methods for designing, analyzing, and programming
sequential and parallel algorithms and data structures. The emphasis
is on teaching fundamental concepts applicable across a wide variety
of problem domains, and transferable across a reasonably broad set of
programming languages and computer architectures. This course also
includes a significant programming component in which students will
program concrete examples from domains such as engineering, scientific
computing, graphics, data mining, and information retrieval (web
search).

Unlike a traditional introduction to algorithms and data structures,
this course puts an emphasis on parallel thinking — i.e., thinking
about how algorithms can do multiple things at once instead of one at
a time. The course follows up on material learned in 15-122 and 15-150
but goes into significantly more depth on algorithmic issues.
\end{dilIntroDex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilBook}
\begin{dilFieldTitle}
Algorithm Design: Parallel and Sequential
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Algorithm Design: Parallel and Sequential
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18
\end{dilFieldUnique}
\begin{dilLabel}
book:15210:S18
\end{dilLabel}
\begin{dilNo}
0
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldAuthors}
Umut A. Acar and Guy Blelloch
\end{dilFieldAuthors}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilChapter}
\begin{dilFieldTitle}
Randomized Algorithms
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Randomized Algorithms
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11
\end{dilFieldUnique}
\begin{dilLabel}
chapter:15210:S18:CH11:ch:randomized
\end{dilLabel}
\begin{dilNo}
11
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldPicture}
./media/turing-flow-jonathan-mccabe.jpg
\end{dilFieldPicture}
\begin{dilIntro}
<p>This chapter presents an introduction to randomized algorithms and their analysis.</p>

\end{dilIntro}
\begin{dilIntroDex}




%% \begin{teachnote}

%% A cleanup is needed to eliminate various redundancies in maxtwo and quicksort.

%% Use rank instead of sorted sequence in analysis of quicksort.
%% Use $r_i$ instead of $t_i$ 

%% Terminology confusion $a_i$ versus $a[i]$.

%% \end{teachnote}

This chapter presents an introduction to randomized algorithms and
their analysis.
\end{dilIntroDex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilSection}
\begin{dilFieldTitle}
Randomized Algorithms
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Randomized Algorithms
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC1
\end{dilFieldUnique}
\begin{dilLabel}
section:15210:S18:CH11:SEC1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilIntro}
<p>...NO.INTRO...</p>

\end{dilIntro}
\begin{dilIntroDex}


...NO.INTRO...
\end{dilIntroDex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilUnit}
\begin{dilFieldTitle}
Basic Concepts
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Basic Concepts
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC1:UN1
\end{dilFieldUnique}
\begin{dilLabel}
unit:15210:S18:CH11:SEC1:UN1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC1:UN1:GR1
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC1:UN1:GR1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomDefinition}
\begin{dilFieldTitle}
Randomized Algorithm
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Randomized Algorithm
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC1:UN1:GR1:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC1:UN1:GR1:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>We say that an algorithm is <span style="color: black"><span><strong><em>randomized</em></strong></span></span> if it makes random choices. Algorithms typically make their random choices by consulting a  <span style="color: black"><span><strong><em>source of randomness</em></strong></span></span> such as a pseudo-random number generator.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
We say that an algorithm is \defn{randomized} if it makes random
choices.
%
Algorithms typically make their random choices by consulting
a~\defn{source of randomness} such as a pseudo-random number
generator.

\end{dilFieldBodyDex}
\end{dilAtomDefinition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomExample}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC1:UN1:GR1:AT2
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC1:UN1:GR1:AT2
\end{dilLabel}
\begin{dilNo}
2
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>A classic randomized algorithm is the quick-sort algorithm, which selects a random element, called the pivot, and partitions the input into two by comparing each element to the pivot. To determine a randomly chosen pivot, the algorithm needs <span class="math inline">\(\log{n}\)</span> bits of random information, usually drawn from a pseudo-random number generator.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
A classic randomized algorithm is the quick-sort algorithm, which
selects a random element, called the pivot, and partitions the input into
two by comparing each element to the pivot.
%
To determine a randomly chosen pivot, the algorithm needs $\log{n}$
bits of random information, usually drawn from a pseudo-random number
generator.

\end{dilFieldBodyDex}
\end{dilAtomExample}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC1:UN1:GR2
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC1:UN1:GR2
\end{dilLabel}
\begin{dilNo}
2
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
Why Use Randomness?
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Why Use Randomness?
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC1:UN1:GR2:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC1:UN1:GR2:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>Randomization has several advantages and is therefore used quite frequently in algorithm design.</p>
<ul>
<li><p>Randomization can simplify the design of algorithms, sometimes dramatically.</p></li>
<li><p>Randomization is particularly useful in designing parallel algorithms, because it facilitates symmetry breaking without relying on communication and coordination.</p></li>
<li><p>Randomization can be used to eliminate inherent bias.</p></li>
</ul>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
Randomization has several advantages and is therefore used quite
frequently in algorithm design.

\begin{itemize}
\item Randomization can simplify the design of algorithms, sometimes
  dramatically.

\item Randomization is particularly useful in designing parallel
  algorithms, because it facilitates symmetry breaking without relying
  on communication and coordination.

\item Randomization can be used to eliminate inherent bias.
\end{itemize}

\end{dilFieldBodyDex}
\end{dilAtomGram}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomExample}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC1:UN1:GR2:AT2
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC1:UN1:GR2:AT2
\end{dilLabel}
\begin{dilNo}
2
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>A classic example where randomization simplifies algorithm design is primality testing. The problem of <span style="color: black"><span><strong><em>primality testing</em></strong></span></span> requires determining whether a given integer is prime. In the late 70s Miller and Rabin developed a famous and simple randomized algorithm for the problem that only requires polynomial work. For over 20 years it was not known whether the problem could be solved in polynomial work without randomization. Eventually a polynomial time algorithm was developed, but it is more complex and computationally more costly than the randomized version. Hence in practice everyone still uses the randomized version.</p>
<p>Symmetry breaking is commonly required in parallel algorithms when the algorithm performs a typically “local” computation on any one of elements of a larger structure, such as nodes in a tree or vertices in a graph. If a parallel algorithm operates by making local decisions, without necessarily knowing the global structure, it might have to be break the “symmetry” and perform a computation for some elements but not others. In such cases, instead of running a deterministic and possibly expensive coordination protocol the algorithm could make local decisions by using randomness. We shall see many examples of such symmetry breaking. For example, in a parallel graph algorithm, the algorithm might “flip a coin” for each vertex in the graph and determine the computation to perform based on the outcome of the coin.</p>
<p>As an example for eliminating bias, consider a problem might require choosing one of two actions, none of which is preferable to other. In such a case, choosing one action deterministically would introduce a bias, that especially when repeated could create heavily biased outcomes. Choosing one of two choice randomly with say 50% probability would eliminate the bias.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
A classic example where randomization simplifies algorithm design is
primality testing.
%
The problem of \defn{primality testing} requires determining whether a
given integer is prime.  
%
In the late 70s Miller and Rabin developed a famous and simple
randomized algorithm for the problem that only requires polynomial
work.  
%
For over 20 years it was not known whether the problem could be
solved in polynomial work without randomization.  
%
Eventually a polynomial time algorithm was developed, but it is more
complex and computationally more costly than the randomized version.
Hence in practice everyone still uses the randomized version.

Symmetry breaking is commonly required in parallel algorithms when the
algorithm performs a typically ``local'' computation on any one of
elements of a larger structure, such as nodes in a tree or vertices in
a graph.
%
If a parallel algorithm operates by making local decisions, without
necessarily knowing the global structure, it might have to be break
the ``symmetry'' and perform a computation for some elements but not
others.
%
In such cases, instead of running a deterministic and possibly
expensive coordination protocol the algorithm could make local
decisions by using randomness.
%
We shall see many examples of such symmetry breaking.  For example, in
a parallel graph algorithm, the algorithm might ``flip a coin'' for
each vertex in the graph and determine the computation to perform
based on the outcome of the coin.

As an example for eliminating bias, consider a problem might require
choosing one of two actions, none of which is preferable to other.
%
In such a case, choosing one action deterministically would introduce
a bias, that especially when repeated could create heavily biased
outcomes.
%
Choosing one of two choice randomly with say 50\% probability would
eliminate the bias.

\end{dilFieldBodyDex}
\end{dilAtomExample}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC1:UN1:GR3
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC1:UN1:GR3
\end{dilLabel}
\begin{dilNo}
3
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
Disadvantages of Randomization
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Disadvantages of Randomization
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC1:UN1:GR3:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC1:UN1:GR3:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>One disadvantage of randomization is that it can complicate the analysis of an algorithm, even though it simplifies the algorithm itself. Because we only have to analyze an algorithm once, but usually execute it many times, this trade-off is usually well worth it.</p>
<p>Another disadvantage is that uncertainty due to randomization. For example, an randomized algorithm could get unlucky and take long to compute the answer. In some applications, such as real-time systems, this uncertainty may be unacceptable.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
One disadvantage of randomization is that it can complicate the
analysis of an algorithm, even though it simplifies the algorithm
itself.
%
Because we only have to analyze an algorithm once, but usually execute
it many times, this trade-off is usually well worth it.
%

Another disadvantage is that uncertainty due to randomization.  
%
For example, an randomized algorithm could get unlucky and take long
to compute the answer.  In some applications, such as real-time
systems, this uncertainty may be unacceptable.

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC1:UN1:GR4
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC1:UN1:GR4
\end{dilLabel}
\begin{dilNo}
4
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomDefinition}
\begin{dilFieldTitle}
Las Vegas and Monte Carlo Algorithms
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Las Vegas and Monte Carlo Algorithms
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC1:UN1:GR4:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC1:UN1:GR4:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>There are two distinct uses of randomization in algorithms.</p>
<p>The first method, which is more common, is to use randomization to weaken the cost guarantees, such as the work and span, of the algorithm. That is, randomization is used to organize the computation in such a way that the impact is on the cost but not on the correctness. Such algorithms are called <span style="color: black"><span><strong><em>Las Vegas algorithms</em></strong></span></span>.</p>
<p>Another approach is to use randomization to weaken the correctness guarantees of the computation: an execution of the algorithm might or might not return a correct answer. Such algorithms are called <span style="color: black"><span><strong><em>Monte Carlo algorithms</em></strong></span></span>.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
There are two distinct uses of randomization in algorithms.
%

The first method, which is more common, is to use randomization to
weaken the cost guarantees, such as the work and span, of the
algorithm.  That is, randomization is used to organize the computation
in such a way that the impact is on the cost but not on the
correctness.
%
Such algorithms are called \defn{Las Vegas algorithms}.
%

Another approach is to use randomization to weaken the correctness
guarantees of the computation: an execution of the algorithm might or
might not return a correct answer.
%  
Such algorithms are called \defn{Monte Carlo algorithms}.

\end{dilFieldBodyDex}
\end{dilAtomDefinition}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC1:UN1:GR5
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC1:UN1:GR5
\end{dilLabel}
\begin{dilNo}
5
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomNote}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC1:UN1:GR5:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC1:UN1:GR5:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>In this book, we only use Las Vegas algorithms. Our algorithm thus always return the correct answer, but their costs (work and span) will depend on random choices.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
In this book, we only use Las Vegas algorithms.
%
Our algorithm thus always return the correct answer, but their costs
(work and span) will depend on random choices.
%

\end{dilFieldBodyDex}
\end{dilAtomNote}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC1:UN1:GR6
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC1:UN1:GR6
\end{dilLabel}
\begin{dilNo}
6
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
Random Distance Run
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Random Distance Run
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC1:UN1:GR6:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC1:UN1:GR6:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>Every year around the middle of April the Computer Science Department at Carnegie Mellon University holds an event called the “Random Distance Run”. It is a running event around the track, where the official die tosser rolls a die immediately before the race is started. The die indicates how many initial laps everyone has to run. When the first person is about to complete the laps, the die is rolled again to determine the additional laps to be run. Rumor has it that Carnegie Mellon scientists have successfully used their knowledge of probabilities to train for the race and to adjust their pace during the race (e.g., how fast to run at the start).</p>
<p>Thanks to Carnegie Mellon CSD PhD Tom Murphy for the design of the 2007 T-shirt.</p>
<p><img src="./media/rdr.jpg" alt="image" style="width:3in" /></p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
Every year around the middle of April the Computer Science Department
at Carnegie Mellon University holds an event called the ``Random
Distance Run''.  It is a running event around the track, where the
official die tosser rolls a die immediately before the race is
started.  The die indicates how many initial laps everyone has to run.
When the first person is about to complete the laps, the die is rolled
again to determine the additional laps to be run.
%
Rumor has it that Carnegie Mellon scientists have successfully used
their knowledge of probabilities to train for the race and to adjust
their pace during the race (e.g., how fast to run at the start).

Thanks to Carnegie Mellon CSD PhD Tom Murphy for the design of the
2007 T-shirt.

\includegraphics[width=3in]{./media/rdr.jpg}

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}

\end{dilUnit}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilUnit}
\begin{dilFieldTitle}
Analysis of Randomized Algorithms
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Analysis of Randomized Algorithms
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC1:UN2
\end{dilFieldUnique}
\begin{dilLabel}
unit:15210:S18:CH11:SEC1:UN2:sec:randomized::probability
\end{dilLabel}
\begin{dilNo}
2
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC1:UN2:GR1
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC1:UN2:GR1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomTeachNote}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC1:UN2:GR1:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC1:UN2:GR1:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>Develop an example here. I think what you need is to have n tasks each of which require 1 time with probability (m-1)/m and m time with probablity 1/m. you can show that the span is almost alway m by choosing n to be large.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
Develop an example here.  I think what you need is to have n tasks
each of which require 1 time with probability (m-1)/m and m time with
probablity 1/m.  you can show that the span is almost alway m by
choosing n to be large.

\end{dilFieldBodyDex}
\end{dilAtomTeachNote}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC1:UN2:GR2
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC1:UN2:GR2
\end{dilLabel}
\begin{dilNo}
2
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomDefinition}
\begin{dilFieldTitle}
Expected and High-Probability Bounds
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Expected and High-Probability Bounds
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC1:UN2:GR2:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC1:UN2:GR2:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>In analyzing costs for a randomized algorithms there are two types of bounds that are useful: expected bounds, and high-probability bounds.</p>
<ul>
<li><p><span style="color: black"><span><strong><em>Expected bounds</em></strong></span></span> inform us about the average cost across all random choices made by the algorithm.</p></li>
<li><p><span style="color: black"><span><strong><em>High-probability</em></strong></span></span> bounds inform us that it is very unlikely that the cost will be above some bound. For an algorithm, we say that some property is true with <span style="color: black"><span><strong><em>high probability</em></strong></span></span> if it is true with probability <span class="math inline">\(p(n)\)</span> such that <span class="math inline">\(\lim_{n \rightarrow\infty}(p(n)) = 1\)</span>, where <span class="math inline">\(n\)</span> is an algorithm specific parameter, which is usually the instance size. In other words, the term “high probability” refers to an property that is almost certain to hold, especially as the size of the problem instance increases.</p></li>
</ul>
<p>As the terms suggest, expected bounds characterize average-case behavior whereas high-probability bounds characterize the common-case behavior of an algorithm.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
In analyzing costs for a randomized algorithms there are two types of
bounds that are useful: expected bounds, and high-probability bounds.

\begin{itemize}
\item
\defn{Expected bounds} inform us about the average cost across all
random choices made by the algorithm.  
%

\item
\defn{High-probability} bounds inform us that it is very unlikely that
the cost will be above some bound.
%
For an algorithm, we say that some property is true with \defn{high
  probability} if it is true with probability $p(n)$ such that
$\lim_{n \ra \infty}(p(n)) = 1$, where $n$ is an algorithm specific
parameter, which is usually the instance size.
%
In other words, the term ``high probability'' refers to an property
that is almost certain to hold, especially as the size of the problem
instance increases.
\end{itemize}

As the terms suggest, expected bounds characterize average-case
behavior whereas high-probability bounds characterize the common-case
behavior of an algorithm.

\end{dilFieldBodyDex}
\end{dilAtomDefinition}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC1:UN2:GR3
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC1:UN2:GR3
\end{dilLabel}
\begin{dilNo}
3
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomRemark}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC1:UN2:GR3:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC1:UN2:GR3:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>In computer science, the function <span class="math inline">\(p(n)\)</span> in the definition of high probability is usually of the form <span class="math inline">\(1 - \frac{1}{n^k}\)</span> where <span class="math inline">\(n\)</span> is the instance size or a similar measure and <span class="math inline">\(k\)</span> is some constant such that <span class="math inline">\(k \ge 1\)</span>.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
In computer science, the function $p(n)$ in the definition of high
probability is usually of the form $1 - \frac{1}{n^k}$ where $n$ is
the instance size or a similar measure and $k$ is some constant such
that $k \ge 1$.

\end{dilFieldBodyDex}
\end{dilAtomRemark}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC1:UN2:GR4
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC1:UN2:GR4
\end{dilLabel}
\begin{dilNo}
4
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomExample}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC1:UN2:GR4:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC1:UN2:GR4:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>If an algorithm has <span class="math inline">\(\Theta(n)\)</span> expected work, it means that when averaged over all random choices it makes in all runs, the algorithm performs <span class="math inline">\(\Theta(n)\)</span> work. Because expected bounds are averaged over all random choices in all possible runs, there can be runs that require more or less work. For example once in every <span class="math inline">\(1/n\)</span> tries the algorithm might require <span class="math inline">\(\Theta(n^2)\)</span> work, and (or) once in every <span class="math inline">\(\sqrt{n}\)</span> tries the algorithm might require <span class="math inline">\(\Theta(n^{3/2})\)</span> work.</p>
<p>As an example of a high-probability bound, suppose that we have <span class="math inline">\(n\)</span> experiments where the probability that work exceeds <span class="math inline">\(O(n\lg{n})\)</span> is <span class="math inline">\(1/n^k\)</span>. We can use the union bound to argue that the total probability that the work exceeds <span class="math inline">\(O(n\lg{n})\)</span> is at most <span class="math inline">\(n \cdot 1/n^{k} = 
1/n^{k-1}\)</span>. This means that the work is <span class="math inline">\(O(n\lg{n})\)</span> is at least <span class="math inline">\(1 - 1/n^{k-1}\)</span>. If <span class="math inline">\(k &gt; 2\)</span>, then we have a high probability bound of <span class="math inline">\(O(n\lg{n})\)</span> work.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
If an algorithm has $\Theta(n)$ expected work, it means that when
averaged over all random choices it makes in all runs, the algorithm
performs $\Theta(n)$ work.
%
Because expected bounds are averaged over all random choices in all
possible runs, there can be runs that require more or less work.
%
For example
%
once in every $1/n$ tries the algorithm
might require $\Theta(n^2)$ work, 
%
and (or)
%
once  in every $\sqrt{n}$ tries the algorithm might require
$\Theta(n^{3/2})$ work.


As an example of a high-probability bound, suppose that we have $n$
experiments where the probability that work exceeds $O(n\lg{n})$ is
$1/n^k$.
%
We can use the union bound to argue that the total probability that
the work exceeds $O(n\lg{n})$ is at most $n \cdot 1/n^{k} =
1/n^{k-1}$.
%
This means that the work is $O(n\lg{n})$ is at least $1 - 1/n^{k-1}$.
%
If $k > 2$, then we have a high probability bound of $O(n\lg{n})$ work.

\end{dilFieldBodyDex}
\end{dilAtomExample}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC1:UN2:GR5
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC1:UN2:GR5
\end{dilLabel}
\begin{dilNo}
5
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
Analyzing Expected Work
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Analyzing Expected Work
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC1:UN2:GR5:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC1:UN2:GR5:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>Expected bounds are quite convenient when analyzing work (or running time in traditional sequential algorithms). This is because the linearity of expectations allows adding expectations across the components of an algorithm to get the overall expected work. For example, if the algorithm performs <span class="math inline">\(n\)</span> tasks each of which take on average <span class="math inline">\(2\)</span> units of work, then the total work on average across all tasks will be <span class="math inline">\(n \times 2 = 2n\)</span> units.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
Expected bounds are quite convenient when analyzing work (or
running time in traditional sequential algorithms).  
%
This is because the linearity of expectations
%
% (\chref{probability})
%
allows adding expectations across the components of an algorithm to
get the overall expected work.
%
For example, if the algorithm performs $n$ tasks each of which take on
average $2$ units of work, then the total work on average across all tasks will
be $n \times 2 = 2n$ units.
%

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC1:UN2:GR6
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC1:UN2:GR6
\end{dilLabel}
\begin{dilNo}
6
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
Analyzing Expected Span
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Analyzing Expected Span
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC1:UN2:GR6:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC1:UN2:GR6:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>When analyzing span, expectations are less helpful, because bounding span requires taking the maximum of random variables, rather than their sum and the expectation of the maximum of two randow variables is not equal to the maximum of expectations of the random variables.</p>
<p>For example, if we had <span class="math inline">\(n\)</span> tasks each of which has expected span of <span class="math inline">\(2\)</span> units of time, we cannot say that the expected span across all tasks is <span class="math inline">\(2\)</span> units. It could be that most of the time each task has a span of <span class="math inline">\(2\)</span> units, but that once with probability <span class="math inline">\(1/n\)</span>, the task requires <span class="math inline">\(n\)</span> units. The expected span for each task is still close to <span class="math inline">\(2\)</span> units but if we have <span class="math inline">\(n\)</span> tasks chances are high that one task will take <span class="math inline">\(n\)</span> units and the expected maximum will be close to <span class="math inline">\(n\)</span> rather than <span class="math inline">\(2\)</span>. We therefore cannot compose the expected span from each task by taking a maximum.</p>
<p>High-probability bounds can help us bound the span of a computation. Going back to our example, let’s say that every task finishes in <span class="math inline">\(2\)</span> units of time with probability <span class="math inline">\(1 - 1/n^5\)</span>, or equivalently that each task takes more than <span class="math inline">\(2\)</span> units of time with probability <span class="math inline">\(1/n^5\)</span> and takes at most <span class="math inline">\(n\)</span> units of time otherwise. Now with <span class="math inline">\(n\)</span> tasks the probability that there will be at least one that requires more than <span class="math inline">\(2\)</span> units of time is at most <span class="math inline">\(1/n^4\)</span> by union bound. Furthermore, when it does, the contribution to the expectation is <span class="math inline">\(1/n^3\)</span>.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
When analyzing span, expectations are less helpful, because bounding
span requires taking the maximum of random variables, rather than
their sum and the expectation of the maximum of two randow variables
is not equal to the maximum of expectations of the random variables.

% This kind of composition does not work when analyzing the span of an
% algorithm, because bounding span requires taking the maximum of random
% variables, rather than their sum.
%

For example, if we had $n$ tasks each of which has expected span of
$2$ units of time, we cannot say that the expected span across all
tasks is $2$ units.
%
It could be that most of the time each task has a span of $2$ units,
but that once with probability $1/n$,  the task requires $n$ units.
%
The expected span for each task is still close to $2$ units but if we
have $n$ tasks chances are high that one task will take $n$ units and
the expected maximum will be close to $n$ rather than $2$.
%
We therefore cannot compose the expected span from each task by taking
a maximum.
%

High-probability bounds can help us bound the span of a computation.
%
Going back to our example, let's say that every task finishes in $2$
units of time with probability $1 - 1/n^5$, or equivalently that each
task takes more than $2$ units of time with probability $1/n^5$ and
takes at most $n$ units of time otherwise.
%
Now with $n$ tasks the probability that there will be at least one
that requires more than $2$ units of time is at most $1/n^4$ by union
bound.
%
Furthermore, when it does, the contribution to the expectation is
$1/n^3$.
%

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC1:UN2:GR7
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC1:UN2:GR7
\end{dilLabel}
\begin{dilNo}
7
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomRemark}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC1:UN2:GR7:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC1:UN2:GR7:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>Because of these properties of summing versus taking a maximum, we usually analyze work using expectation, but analyze span using high probability.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
Because of these properties of summing versus taking a maximum, we
usually analyze work using expectation, but analyze span using high
probability.

\end{dilFieldBodyDex}
\end{dilAtomRemark}
\end{dilGroup}

\end{dilUnit}
\end{dilSection}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilSection}
\begin{dilFieldTitle}
Finding The Two Largest
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Finding The Two Largest
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC2
\end{dilFieldUnique}
\begin{dilLabel}
section:15210:S18:CH11:SEC2
\end{dilLabel}
\begin{dilNo}
2
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilIntro}
<p>...NO.INTRO...</p>

\end{dilIntro}
\begin{dilIntroDex}


...NO.INTRO...
\end{dilIntroDex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilUnit}
\begin{dilFieldTitle}
The Problem
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
The Problem
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC2:UN3
\end{dilFieldUnique}
\begin{dilLabel}
unit:15210:S18:CH11:SEC2:UN3
\end{dilLabel}
\begin{dilNo}
3
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC2:UN3:GR1
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC2:UN3:GR1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomProblem}
\begin{dilFieldTitle}
Max-Two Problem
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Max-Two Problem
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC2:UN3:GR1:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC2:UN3:GR1:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>The <span style="color: black"><span><strong><em>max-two problem</em></strong></span></span> requires finding the two largest elements from a sequence of <span class="math inline">\(n\)</span> unique numbers.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
The \defn{max-two problem} requires finding the two largest elements
from a sequence of $n$ unique numbers.

\end{dilFieldBodyDex}
\end{dilAtomProblem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomExample}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC2:UN3:GR1:AT2
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC2:UN3:GR1:AT2
\end{dilLabel}
\begin{dilNo}
2
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>The max-two of the sequence <span class="math inline">\(\left\langle\, 9, 3, 2, 5, 4, 7, 8, 6 \,\right\rangle\)</span> is <span class="math inline">\((9, 
8)\)</span>.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
The max-two of the sequence $\cseq{9, 3, 2, 5, 4, 7, 8, 6}$ is $(9,
8)$.

\end{dilFieldBodyDex}
\end{dilAtomExample}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC2:UN3:GR2
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC2:UN3:GR2
\end{dilLabel}
\begin{dilNo}
2
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomAlgorithm}
\begin{dilFieldTitle}
Iterative Max-Two
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Iterative Max-Two
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC2:UN3:GR2:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC2:UN3:GR2:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>The following is a simple iterative algorithm for solving the max-two problem.</p>
<p><span class="math display">\[\begin{array}{ll} 
1 &amp; \texttt{max2}~a = 
\\  
2 &amp; ~~~\texttt{let} 
\\ 
3 &amp; ~~~~~~\texttt{update}~((m_1,m_2),v) = 
\\ 
4 &amp; ~~~~~~~~~\texttt{if}~v \leq m_2~\texttt{then} 
\\ 
5 &amp; ~~~~~~~~~~~~(m_1,m_2) 
\\ 
6 &amp; ~~~~~~~~~\texttt{else if}~v \leq m_1~\texttt{then} 
\\ 
7 &amp; ~~~~~~~~~~~~(m_1, v) 
\\ 
8 &amp; ~~~~~~~~~\texttt{else} 
\\ 
9 &amp; ~~~~~~~~~~~~(v, m_1) 
\\ 
10 &amp;~~~~~~ \texttt{init} = \texttt{if}~a[0] \leq a[1]~\texttt{then} 
\\ 
11 &amp; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~(a[1], a[0]) 
\\ 
12 &amp; ~~~~~~~~~~~~~~~~~~~~~~~~~\texttt{else}  
\\ 
13 &amp; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~(a[0], a[1]) 
\\ 
14 &amp; ~~~\texttt{in} 
\\  
15 &amp; ~~~~~~\texttt{iterate}~\texttt{update}~\texttt{init}~a[2 \cdots |a|-1] 
\\ 
16  &amp; ~~~\texttt{end} 
\end{array}\]</span></p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
The following is a simple iterative algorithm for solving the max-two
problem.

\[
\begin{array}{ll}
1 & \cd{max2}~a =
\\ 
2 & ~~~\cd{let}
\\
3 & ~~~~~~\cd{update}~((m_1,m_2),v) =
\\
4 & ~~~~~~~~~\cd{if}~v \leq m_2~\cd{then}
\\
5 & ~~~~~~~~~~~~(m_1,m_2)
\\
6 & ~~~~~~~~~\cd{else if}~v \leq m_1~\cd{then}
\\
7 & ~~~~~~~~~~~~(m_1, v)
\\
8 & ~~~~~~~~~\cd{else}
\\
9 & ~~~~~~~~~~~~(v, m_1)
\\
10 &~~~~~~ \cd{init} = \cd{if}~a[0] \leq a[1]~\cd{then}
\\
11 & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~(a[1], a[0])
\\
12 & ~~~~~~~~~~~~~~~~~~~~~~~~~\cd{else} 
\\
13 & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~(a[0], a[1])
\\
14 & ~~~\cd{in}
\\ 
15 & ~~~~~~\cd{iterate}~\cd{update}~\cd{init}~a\cirange{2}{|a|-1}
\\
16  & ~~~\cd{end}
\end{array}
\]

\end{dilFieldBodyDex}
\end{dilAtomAlgorithm}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC2:UN3:GR3
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC2:UN3:GR3
\end{dilLabel}
\begin{dilNo}
3
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
Worst-Case Cost Analysis
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Worst-Case Cost Analysis
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC2:UN3:GR3:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC2:UN3:GR3:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>We can analyze the work of this algorithm by bounding the number of comparisons performed by the algorithm. To this end, we note that the algorithm performs one comparison to initialize and then it performs at most two comparisons per element. Thus, in the worst case the algorithm performs <span class="math inline">\(1 + 2(n-2) = 2n - 3\)</span> comparisons.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
We can analyze the work of this algorithm by bounding the number of
comparisons performed by the algorithm.
%
To this end, we note that the algorithm performs one comparison to
initialize and then it performs at most two comparisons per element.
%
Thus, in the worst case the algorithm performs $ 1 + 2(n-2) = 2n - 3$
comparisons.
%

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC2:UN3:GR4
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC2:UN3:GR4
\end{dilLabel}
\begin{dilNo}
4
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomExercise}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC2:UN3:GR4:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC2:UN3:GR4:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>Give the input that leads to worst-case number of comparisons.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
Give the input that leads to worst-case number of comparisons.

\end{dilFieldBodyDex}
\end{dilAtomExercise}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomSolution}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC2:UN3:GR4:AT2
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC2:UN3:GR4:AT2
\end{dilLabel}
\begin{dilNo}
2
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>For any <span class="math inline">\(n &gt; 0\)</span>, the sequence <span class="math inline">\(\left\langle\, 0, 1, 2, \ldots, n \,\right\rangle\)</span> leads to worst-case number of comparisons.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
For any $n > 0$, the sequence $\cseq{0, 1, 2, \ldots, n}$ leads to
worst-case number of comparisons.

\end{dilFieldBodyDex}
\end{dilAtomSolution}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC2:UN3:GR5
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC2:UN3:GR5
\end{dilLabel}
\begin{dilNo}
5
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC2:UN3:GR5:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC2:UN3:GR5:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>This bound may seem like the best we can do. But, in reality, many inputs will be much “nicer” and will lead to fewer numbers of comparisons because intuitively the next element in the sequence is unlikely to be greater than the second largest computed thus far. Using the techniques that we have learned so far, we have no way of giving an analysis that takes advantage of this intuition. We therefore analyzed the algorithm by considering the worst case.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
This bound may seem like the best we can do. 
%
But, in reality, many inputs will be much ``nicer'' and will lead to
fewer numbers of comparisons because intuitively the next element in
the sequence is unlikely to be greater than the second largest computed thus far.
%
Using the techniques that we have learned so far, we have no way of
giving an analysis that takes advantage of this intuition.  We
therefore analyzed the algorithm by considering the worst case.

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC2:UN3:GR6
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC2:UN3:GR6
\end{dilLabel}
\begin{dilNo}
6
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomTeachNote}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC2:UN3:GR6:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC2:UN3:GR6:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>Surprisingly, there is a divide-and-conquer algorithm that uses only about <span class="math inline">\(3n/2\)</span> comparisons (exercise to the reader). More surprisingly still is the fact that it can be done in <span class="math inline">\(n + O(\log n)\)</span> comparisons. But how?</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
Surprisingly, there is a
divide-and-conquer algorithm that uses only about $3n/2$ comparisons
(exercise to the reader).  More surprisingly still is the fact that it
can be done in $n + O(\log n)$ comparisons. But how?
%

\end{dilFieldBodyDex}
\end{dilAtomTeachNote}
\end{dilGroup}

\end{dilUnit}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilUnit}
\begin{dilFieldTitle}
A Randomized Algorithm for Max-Two
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
A Randomized Algorithm for Max-Two
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC2:UN4
\end{dilFieldUnique}
\begin{dilLabel}
unit:15210:S18:CH11:SEC2:UN4
\end{dilLabel}
\begin{dilNo}
4
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC2:UN4:GR1
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC2:UN4:GR1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomAlgorithm}
\begin{dilFieldTitle}
Randomized Max-Two
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Randomized Max-Two
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC2:UN4:GR1:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC2:UN4:GR1:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>We give a randomized algorithm for solving the max-two problem. The algorithm takes an input sequence of <span class="math inline">\(n\)</span> elements and then permutes it uniformly randomly. It then runs the iterative algorithm <span class="math inline">\(\texttt{max2}\)</span> given above.</p>
<p>Given input sequence <span class="math inline">\(b\)</span>,</p>
<ol>
<li><p>let <span class="math inline">\(a = \texttt{permute}(b, \pi)\)</span>, where <span class="math inline">\(\pi\)</span> is a uniformly random permutation (i.e., we choose one of the <span class="math inline">\(n!\)</span> permutations with equal probability), and</p></li>
<li><p>run algorithm <span class="math inline">\(\texttt{max2}\)</span> on <span class="math inline">\(a\)</span>.</p></li>
</ol>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
We give a randomized algorithm for solving the max-two problem.
%
The algorithm takes an input sequence of $n$ elements and then
permutes it uniformly randomly.
%
It then runs the iterative algorithm $\cd{max2}$ given above.
 
Given input sequence $b$,
\begin{enumerate}
\item let $a = \cd{permute}(b, \pi)$, where $\pi$ is a uniformly
  random permutation (i.e., we choose one of the $n!$
  permutations with equal probability), and 

\item run algorithm $\cd{max2}$ on $a$.
\end{enumerate}
%

\end{dilFieldBodyDex}
\end{dilAtomAlgorithm}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC2:UN4:GR2
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC2:UN4:GR2
\end{dilLabel}
\begin{dilNo}
2
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomRemark}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC2:UN4:GR2:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC2:UN4:GR2:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>The randomized algorithm does not need to explicitly construct the sequence <span class="math inline">\(a\)</span> but can simulate doing so by, at each step, picking a random element that hasn’t been considered thus far and running <span class="math inline">\(\texttt{update}\)</span> with that element.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
The randomized algorithm does not need to explicitly construct the
sequence $a$ but can simulate doing so by, at each step, picking a
random element that hasn't been considered thus far and running
$\cd{update}$ with that element.

\end{dilFieldBodyDex}
\end{dilAtomRemark}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC2:UN4:GR3
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC2:UN4:GR3
\end{dilLabel}
\begin{dilNo}
3
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
Analysis of Randomized Max-Two
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Analysis of Randomized Max-Two
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC2:UN4:GR3:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC2:UN4:GR3:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>As noted above, the algorithm need not permute the input explicitly but we assume so, because it streamlines the analysis.</p>
<p>After applying the random permutation, our sample space <span class="math inline">\(\Omega\)</span> corresponds to each permutation of the input <span class="math inline">\(b\)</span>. Because there are <span class="math inline">\(n!\)</span> permutations on a sequence of length <span class="math inline">\(n\)</span> and each has equal probability, we have <span class="math display">\[\begin{aligned}
|\Omega| &amp; = n! &amp;  \text{and} &amp; 
\\ 
\forall x, x \in \Omega. \mathbf{P}\left[{x}\right] &amp; = 1/n!.  &amp; &amp;   \end{aligned}\]</span></p>
<p>Let <span class="math inline">\(i\)</span> be the position in <span class="math inline">\(a\)</span> (indexed from <span class="math inline">\(0\)</span> to <span class="math inline">\(n-1\)</span>) and define <span class="math inline">\(X_i\)</span> as an indicator random variable denoting whether Line 4 and hence the corresponding comparison gets executed for the value at <span class="math inline">\(a[i]\)</span>.</p>
<p>Let’s calculate the probability that <span class="math inline">\(X_i = 1\)</span>. By definition of indicator random variables, we know that <span class="math inline">\(X_i = 1\)</span> if the comparison in Line 4 occurs, which is the case if <span class="math inline">\(a[i] &gt; m_2\)</span>. A moment’s thought shows that the condition <span class="math inline">\(a[i] &gt; m_2\)</span> holds exactly when <span class="math inline">\(a[i]\)</span> is either the largest element or the second largest element in <span class="math inline">\(\{a_0, \dots, a[i]\}\)</span>. So ultimately we want to calculute the probability that <span class="math inline">\(a[i]\)</span> is the largest or the second largest element in randomly-permuted sequence of length <span class="math inline">\(i+1\)</span>?</p>
<p>To calculate this probability, note that each element in the sequence is equally likely to be anywhere in the permuted sequence, because we chose a random permutation. In particular, if we look at the <span class="math inline">\(k\)</span>-th largest element, it has <span class="math inline">\(1/i\)</span> chance of being at <span class="math inline">\(a[i]\)</span>. Therefore, the probability that <span class="math inline">\(a[i]\)</span> is the largest or the second largest element in <span class="math inline">\(\{a_0, \dots, a[i]\}\)</span> is <span class="math inline">\(\frac{1}{i+1} + \frac{1}{i+1} = 
\frac{2}{i+1}\)</span>.</p>
<p>To bound the total number of comparisons, define another random variable (function) <span class="math inline">\(Y\)</span> that for any permutation denotes the total number of comparisons the algorithm takes on that permutation. We can write <span class="math inline">\(Y\)</span> as <span class="math display">\[Y = \underbrace{\;\;1\;\;}_{\text{Line~10}} \,\,+\,\, 
\underbrace{\;\;n - 2\;\;}_{\text{Line~4}} \,\, + \,\, 
\underbrace{\sum_{i=3}^n X_i}_{\text{Line~6}} 
.\]</span></p>
<p>To bound the number of comparisons that the algorithm does in the expected case, we calculate the expected value of <span class="math inline">\(Y\)</span>. By linearity of expectation, we have <span class="math display">\[\begin{aligned}
  \mathbf{E}\left[{Y}\right] &amp; = \mathbf{E}\left[{1 + (n-2) + \sum_{i=2}^{n-1} X_i}\right] 
\\ 
            &amp; =  1 + (n-2) + \sum_{i=2}^{n-1} \mathbf{E}\left[{X_i}\right]. \end{aligned}\]</span> Since <span class="math inline">\(X_i\)</span> is an indicator random variable, we know that <span class="math display">\[\mathbf{E}\left[{X_i}\right] = 1\cdot \frac{2}{i+1} = \frac{2}{i+1}\]</span> Plugging this into the expression for <span class="math inline">\(\mathbf{E}\left[{Y}\right]\)</span>, we obtain <span class="math display">\[\begin{aligned}
  \mathbf{E}\left[{Y}\right] &amp;= 1 + (n-2) + \sum_{i=3}^{n-1} \mathbf{E}\left[{X_i}\right] \\ 
  &amp;= 1 + (n-2) + \sum_{i=2}^{n-1} \frac{2}{i+1} \\ 
  &amp;= 1 + (n-2) + 2\Big(\frac{1}{3} + \frac{1}{4} + \dots \frac{1}{n}\Big)\\ 
  &amp;= n - 4 + 2\Big(1 + \frac{1}{2} + \frac{1}{3} + \frac{1}{4} + \dots \frac{1}{n}\Big)\\ 
  &amp;= n - 4 + 2H_n, \end{aligned}\]</span> where <span class="math inline">\(H_n\)</span> is the <span class="math inline">\(n\)</span>-th Harmonic number. But we know that <span class="math inline">\(H_n \leq 1 + 
\lg n\)</span>, so we obtain <span class="math inline">\(\mathbf{E}\left[{Y}\right] \leq n - 2 + 2\lg n\)</span>. We can also use the following bound on Harmonic sums: <span class="math display">\[H(n) = O(\lg{n} + 1),\]</span> or more precisely <span class="math display">\[\begin{aligned}
    H_n = 1 + \frac12 + \dots + \frac1n = \ln n + \gamma + \varepsilon_n, \end{aligned}\]</span> where <span class="math inline">\(\gamma\)</span> is the Euler-Mascheroni constant, which is approximately <span class="math inline">\(0.57721\cdots\)</span>, and <span class="math inline">\(\varepsilon_n \sim \frac1{2n}\)</span>, which tends to <span class="math inline">\(0\)</span> as <span class="math inline">\(n\)</span> approaches <span class="math inline">\(\infty\)</span>. This shows that the summation and integral of <span class="math inline">\(1/i\)</span> are almost identical (up to a small adative constant and a low-order vanishing term).</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
As noted above, the algorithm need not permute the input explicitly
but we assume so, because it streamlines the analysis.
%

%

After applying the random permutation, our sample space $\Omega$
corresponds to each permutation of the input $b$. 
%
Because there are $n!$ permutations on a sequence of length $n$ and
each has equal probability, we have 
%
\begin{align*}
|\Omega| & = n! &  \text{and} &
\\
\forall x, x \in \Omega. \prob{x} & = 1/n!.  & &  
\end{align*}
%

%%%% This is cryptic.
%  However, as we will see, we do not really need
%to know this, all we need to know is what fraction of the sample space
%obeys some property.

Let $i$ be the position in $a$ (indexed from $0$ to $n-1$) and define
$X_i$ as an indicator random variable denoting whether Line
\randomizedmaxtwocmpone{} and hence the corresponding comparison
gets executed for the value at $a[i]$.
%%%% Unnecessary for the notes
%% (i.e., Recall that an indicator random variable is actually a
%% function that maps each primitive event (each permutation in our
%% case) to 0 or 1.  In particular given a permutation, it returns 1
%% iff for that permutation the comparison on Line~\ref{code:if2} gets
%% executed on iteration $i$.  
%%
%%


Let's calculate the probability that $X_i = 1$.
%
By definition of indicator random variables, we know that $X_i = 1$ if
the comparison in Line~\randomizedmaxtwocmpone{} occurs, which is the
case if $a[i] > m_2$.
%
A moment's thought shows that the condition $a[i] > m_2$ holds exactly
when $a[i]$ is either the largest element or the second largest element
in $\{a_0, \dots, a[i]\}$. 
%
So ultimately we want to calculute the probability that $a[i]$ is the
largest or the second largest element in randomly-permuted sequence of
length $i+1$?

To calculate this probability, note that each element in the sequence
is equally likely to be anywhere in the permuted sequence, because we
chose a random permutation.  
%
In particular, if we look at the $k$-th largest element, it has $1/i$
chance of being at $a[i]$. 
%
% (You should also try to work it out using a counting argument.)  
%
Therefore, the probability that $a[i]$ is the largest or the second
largest element in $\{a_0, \dots, a[i]\}$ is $\frac{1}{i+1} + \frac{1}{i+1} =
\frac{2}{i+1}$.


To bound the total number of comparisons, define another random
variable (function) $Y$ that for any permutation denotes the total
number of comparisons the algorithm takes on that permutation. We can
write $Y$ as 
\[
Y = \underbrace{\;\;1\;\;}_{\text{Line~\randomizedmaxtwostart}} \,\,+\,\,
\underbrace{\;\;n - 2\;\;}_{\text{Line~\randomizedmaxtwocmpone}} \,\, + \,\,
\underbrace{\sum_{i=3}^n X_i}_{\text{Line~\randomizedmaxtwocmptwo}}
.
\]

To bound the number of comparisons that the algorithm does in the
expected case, we calculate the expected value of $Y$.
%
By linearity of
expectation, we have
%
\begin{align*}
  \expct{Y} & = \expct{1 + (n-2) + \sum_{i=2}^{n-1} X_i}
\\
            & =  1 + (n-2) + \sum_{i=2}^{n-1} \expct{X_i}.
\end{align*}
%
Since $X_i$ is an indicator random variable, we know that 
\[
  \expct{X_i} = 1\cdot \frac{2}{i+1} = \frac{2}{i+1}
\]
Plugging this into the expression for $\expct{Y}$, we obtain
\begin{align*}
  \expct{Y} &= 1 + (n-2) + \sum_{i=3}^{n-1} \expct{X_i} \\
  &= 1 + (n-2) + \sum_{i=2}^{n-1} \frac{2}{i+1} \\
  &= 1 + (n-2) + 2\Big(\frac{1}{3} + \frac{1}{4} + \dots \frac{1}{n}\Big)\\
  &= n - 4 + 2\Big(1 + \frac{1}{2} + \frac{1}{3} + \frac{1}{4} + \dots \frac{1}{n}\Big)\\
  &= n - 4 + 2H_n,
\end{align*}
where $H_n$ is the $n$-th Harmonic number.  But we know that $H_n \leq 1 +
\lg n$, so we obtain $\expct{Y} \leq n - 2 + 2\lg n$.
%
We can also use the following bound on  Harmonic sums: 
\[
H(n) = O(\lg{n} + 1),
\]
or more precisely
\begin{align*}
    H_n = 1 + \frac12 + \dots + \frac1n = \ln n + \gamma + \vareps_n,
\end{align*}
where $\gamma$ is the Euler-Mascheroni constant, which is
approximately $0.57721\cdots$, and $\vareps_n \sim \frac1{2n}$,
 which tends to $0$ as $n$ approaches $\infty$. This shows that the
summation and integral of $1/i$ are almost identical (up to
a small adative constant and a low-order vanishing term).  
%

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC2:UN4:GR4
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC2:UN4:GR4
\end{dilLabel}
\begin{dilNo}
4
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomRemark}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC2:UN4:GR4:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC2:UN4:GR4:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>In this section, we analyzed a deterministic algorithm and its randomized variant. The analysis of the randomized algorithm follows a typical analysis strategy that involves declaration of random variables and calculation of their expectation.</p>
<p>Although the analysis shows that the randomized algorithm performs in expectation approximately a factor of two fewer comparisons than the deterministic algorithm would in the worst case, we would not expect such a reduction to lead to efficiency improvements in practice, because for example, the deterministic algorithm has better data locality.</p>
<p>What is perhaps most interesting about the analysis is that it hints at why on a typical “real-world” input, the deterministic algorithm performs much better than the worst-case analysis suggests. This is because real-world instances are usually not adversarial, especially when there are few adversarial instances.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
In this section, we analyzed a deterministic algorithm and its
randomized variant.
%
The analysis of the randomized algorithm follows a typical analysis
strategy that involves declaration of random variables and calculation
of their expectation.
%

Although the analysis shows that the randomized
algorithm performs in expectation approximately a factor of two fewer
comparisons than the deterministic algorithm would in the worst case,
we would not expect such a reduction to lead to efficiency
improvements in practice, because for example, the deterministic
algorithm has better data locality.
% 

What is perhaps most interesting about the analysis is that it hints
at why on a typical ``real-world'' input, the deterministic algorithm
performs much better than the worst-case analysis suggests.
%
This is because real-world instances are usually not adversarial,
especially when there are few adversarial instances.

\end{dilFieldBodyDex}
\end{dilAtomRemark}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilCheckpoint}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC2:UN4:GR5:CP5
\end{dilFieldUnique}
\begin{dilLabel}
checkpoint:15210:S18:CH11:SEC2:UN4:GR5:CP5
\end{dilLabel}
\begin{dilNo}
5
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilQuestionFR}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC2:UN4:GR5:CP5:Q1
\end{dilFieldUnique}
\begin{dilLabel}
question_fr:15210:S18:CH11:SEC2:UN4:GR5:CP5:Q1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldPoints}
 10
\end{dilFieldPoints}
\begin{dilFieldPrompt}
<p>Give a divide-and-conquer algorithm for the max-two problem that performs approximately <span class="math inline">\(3n/2\)</span> comparisons in the worst case.</p>

\end{dilFieldPrompt}
\begin{dilFieldPromptDex}

Give a divide-and-conquer algorithm for the max-two problem
that performs approximately $3n/2$ comparisons in the worst case.
\end{dilFieldPromptDex}
\begin{dilFieldHint}
<p>...NO.HINT...</p>

\end{dilFieldHint}
\begin{dilFieldHintDex}

...NO.HINT...
\end{dilFieldHintDex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAnswer}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC2:UN4:GR5:CP5:Q1:a1
\end{dilFieldUnique}
\begin{dilLabel}
answer:15210:S18:CH11:SEC2:UN4:GR5:CP5:Q1:a1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldPoints}
0
\end{dilFieldPoints}
\begin{dilFieldBody}
<p>...NO.ANSWER...</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}


...NO.ANSWER...
\end{dilFieldBodyDex}
\begin{dilFieldExplain}
<p>...NO.EXPLANATION...</p>

\end{dilFieldExplain}
\begin{dilFieldExplainDex}
...NO.EXPLANATION...

\end{dilFieldExplainDex}
\end{dilAnswer}
\end{dilQuestionFR}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilQuestionFR}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC2:UN4:GR5:CP5:Q2
\end{dilFieldUnique}
\begin{dilLabel}
question_fr:15210:S18:CH11:SEC2:UN4:GR5:CP5:Q2
\end{dilLabel}
\begin{dilNo}
2
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldPoints}
 10
\end{dilFieldPoints}
\begin{dilFieldPrompt}
<p>Prove that the probability of the random variable <span class="math inline">\(X_i\)</span> defined in the analysis of the randomized algorithm is <span class="math inline">\(\frac{1}{i+1}\)</span> by using a counting argument.</p>

\end{dilFieldPrompt}
\begin{dilFieldPromptDex}

Prove that the probability of the random variable $X_i$
defined in the analysis of the randomized algorithm is $\frac{1}{i+1}$
by using a counting argument.
\end{dilFieldPromptDex}
\begin{dilFieldHint}
<p>...NO.HINT...</p>

\end{dilFieldHint}
\begin{dilFieldHintDex}

...NO.HINT...
\end{dilFieldHintDex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAnswer}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC2:UN4:GR5:CP5:Q2:a1
\end{dilFieldUnique}
\begin{dilLabel}
answer:15210:S18:CH11:SEC2:UN4:GR5:CP5:Q2:a1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldPoints}
0
\end{dilFieldPoints}
\begin{dilFieldBody}
<p>...NO.ANSWER...</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}


...NO.ANSWER...
\end{dilFieldBodyDex}
\begin{dilFieldExplain}
<p>...NO.EXPLANATION...</p>

\end{dilFieldExplain}
\begin{dilFieldExplainDex}
...NO.EXPLANATION...

\end{dilFieldExplainDex}
\end{dilAnswer}
\end{dilQuestionFR}
\end{dilCheckpoint}
\end{dilUnit}
\end{dilSection}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilSection}
\begin{dilFieldTitle}
Order Statistics
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Order Statistics
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3
\end{dilFieldUnique}
\begin{dilLabel}
section:15210:S18:CH11:SEC3:sec:randomized::select
\end{dilLabel}
\begin{dilNo}
3
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilIntro}
<p>...NO.INTRO...</p>

\end{dilIntro}
\begin{dilIntroDex}


...NO.INTRO...
\end{dilIntroDex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilUnit}
\begin{dilFieldTitle}
The Order-Statistics Problem
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
The Order-Statistics Problem
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN5
\end{dilFieldUnique}
\begin{dilLabel}
unit:15210:S18:CH11:SEC3:UN5
\end{dilLabel}
\begin{dilNo}
5
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN5:GR1
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC3:UN5:GR1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomProblem}
\begin{dilFieldTitle}
Order Statistics
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Order Statistics
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN5:GR1:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC3:UN5:GR1:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>Given an <span class="math inline">\(a\)</span> sequence and an integer <span class="math inline">\(k\)</span> where <span class="math inline">\(0 \leq k &lt; |a|\)</span>, and a comparison <span class="math inline">\(&lt;\)</span> defining a total ordering over the elements of the sequence, find the <span class="math inline">\(k^{th}\)</span>  <span style="color: black"><span><strong><em>order statistics</em></strong></span></span>, i.e., <span class="math inline">\(k^{th}\)</span> smallest element, in the sequences.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
Given an $a$ sequence and an integer $k$ where $0 \leq k < |a|$, and a
  comparison $<$ defining a total ordering over the elements of the
  sequence, find the $k^{th}$~\defn{order statistics}, i.e.,  $k^{th}$ smallest element, in the sequences.

\end{dilFieldBodyDex}
\end{dilAtomProblem}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN5:GR2
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC3:UN5:GR2
\end{dilLabel}
\begin{dilNo}
2
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
Reduction to Sorting
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Reduction to Sorting
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN5:GR2:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC3:UN5:GR2:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>We can solve this problem by sorting first and selecting the <span class="math inline">\(k^{th}\)</span> element but this would require <span class="math inline">\(O(n \log n)\)</span> work, assuming that comparisons require constant work. We wish to do better; in particular we would like to achieve linear work and still achieve <span class="math inline">\(O(\log^2 n)\)</span> span.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
We can solve this problem by sorting first and selecting the $k^{th}$
element but this would require $O(n \log n)$ work, assuming that
comparisons require constant work.
%
We wish to do better; in particular we would like to
achieve linear work and still achieve $O(\log^2 n)$ span.  
%

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}

\end{dilUnit}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilUnit}
\begin{dilFieldTitle}
Randomized Algorithm for Order Statistics
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Randomized Algorithm for Order Statistics
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN6
\end{dilFieldUnique}
\begin{dilLabel}
unit:15210:S18:CH11:SEC3:UN6
\end{dilLabel}
\begin{dilNo}
6
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN6:GR1
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC3:UN6:GR1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN6:GR1:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC3:UN6:GR1:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>We present a randomized algorithm for computing order statistics that uses the contraction technique: it solves a given problem instance by reducing it a problem instance whose size is expected to be smaller.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
We present a randomized algorithm for computing order statistics that
uses the contraction technique: it solves a given problem instance by
reducing it a problem instance whose size is expected to be smaller.

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN6:GR2
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC3:UN6:GR2
\end{dilLabel}
\begin{dilNo}
2
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomAlgorithm}
\begin{dilFieldTitle}
Contraction-Based Select
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Contraction-Based Select
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN6:GR2:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC3:UN6:GR2:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>For the purposes of simplicity, let’s assume that sequences consist of unique elements and consider the following simple algorithm. Based on the contraction design technique, the algorithm uses randomization to contract the problem to a smaller instance.</p>
<p><span class="math display">\[\begin{array}{ll} 
1 &amp; \texttt{select}~a~k =  
\\ 
2 &amp; \texttt{let} 
\\ 
3 &amp; ~~~p = a[0] 
\\ 
4 &amp; ~~~\ell = \left\langle\, x \in a \;|\; x &lt; p \,\right\rangle 
\\ 
5 &amp; ~~~r = \left\langle\, x \in b \;|\; x &gt; p \,\right\rangle 
\\ 
6 &amp; \texttt{in} 
\\ 
7 &amp; ~~~\texttt{if}~(k &lt; |\ell|)~\texttt{then}~\texttt{select}~\ell~k 
\\ 
8 &amp; ~~~\texttt{else if}~(k = |\ell|)~\texttt{then}~p 
\\ 
9 &amp; ~~~\texttt{else}~\texttt{select}~r~(k - (|\ell|)) 
\\ 
10 &amp; \texttt{end} 
\end{array} 
%% \\ 
%% 7 &amp; ~~~\cd{if}~(k &lt; |\ell|)~\cd{then}~\cd{select}~\ell~k 
%% \\ 
%% 8 &amp; ~~~\cd{else if}~(k &lt; |a| - |r|)~\cd{then}~p 
%% \\ 
%% 9 &amp; ~~~\cd{else}~\cd{select}~r~(k - (|a| - |r|)) 
%% \\ 
%% 10 &amp; \cd{end}\]</span></p>
<p>The algorithm divides the input into left and right sequences, <span class="math inline">\(\ell\)</span> and <span class="math inline">\(r\)</span>, and figures out the side <span class="math inline">\(k^{th}\)</span> smallest must be in, and explores that side recursively. When exploring the right side, <span class="math inline">\(r\)</span>, the algorithm adjusts the parameter <span class="math inline">\(k\)</span> because the elements less or equal to the pivot <span class="math inline">\(p\)</span> are being thrown out (there are <span class="math inline">\(|a|-|r|\)</span> such elements).</p>
<p>As written the algorithm picks as pivot the first key in the sequence instead of a random key. As with the max-two problem we considered earlier in the chapter, we can randomize the algorithm by uniformly randomly permuting the input sequence and then applying <span class="math inline">\(\texttt{select}\)</span> on the permuted sequences. This is equivalent to randomly picking a pivot at each step of contraction.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
For the purposes of simplicity, let's assume that sequences consist of
unique elements and consider the following simple algorithm.  Based on
the contraction design technique, the algorithm uses randomization to
contract the problem to a smaller instance.
%

\[
\begin{array}{ll}
1 & \cd{select}~a~k = 
\\
2 & \cd{let}
\\
3 & ~~~p = a[0]
\\
4 & ~~~\ell = \cseqf{x \in a}{x < p}
\\
5 & ~~~r = \cseqf{x \in b}{x > p}
\\
6 & \cd{in}
\\
7 & ~~~\cd{if}~(k < |\ell|)~\cd{then}~\cd{select}~\ell~k
\\
8 & ~~~\cd{else if}~(k = |\ell|)~\cd{then}~p
\\
9 & ~~~\cd{else}~\cd{select}~r~(k - (|\ell|))
\\
10 & \cd{end}
\end{array}
%% \\
%% 7 & ~~~\cd{if}~(k < |\ell|)~\cd{then}~\cd{select}~\ell~k
%% \\
%% 8 & ~~~\cd{else if}~(k < |a| - |r|)~\cd{then}~p
%% \\
%% 9 & ~~~\cd{else}~\cd{select}~r~(k - (|a| - |r|))
%% \\
%% 10 & \cd{end}
\]

%
% This algorithm is similar to \qsort{} but instead of recursing on both
% sides, it only recurs on one side.  
%
The algorithm divides the input into left and right sequences, $\ell$
and $r$, and figures out the side $k^{th}$ smallest must be in, and
explores that side recursively.  
%
When exploring the right side, $r$, the algorithm adjusts the
parameter $k$ because the elements less or equal to the pivot $p$ are
being thrown out (there are $|a|-|r|$ such elements).
%


As written the algorithm picks as pivot the first key in the sequence
instead of a random key.
%
As with the max-two problem we considered earlier in the chapter, we
can randomize the algorithm by uniformly randomly permuting the input sequence
and then applying $\ksmall$ on the permuted sequences.
%
This is equivalent to randomly picking a pivot at each step of
contraction.

\end{dilFieldBodyDex}
\end{dilAtomAlgorithm}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN6:GR3
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC3:UN6:GR3
\end{dilLabel}
\begin{dilNo}
3
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomExample}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN6:GR3:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC3:UN6:GR3:AT1:ex:randomized::select
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>Example runs of <span class="math inline">\(\texttt{select}{}\)</span> illustrated by a  <span style="color: black"><span><strong><em>pivot tree.</em></strong></span></span> For illustrative purposes, we show all possible recursive calls being explored down to singleton sequences. In reality, the algorithm explores only one path.</p>
<ul>
<li><p>The path highlighted with red is the path of recursive calls taken by <span class="math inline">\(\texttt{select}\)</span> when searching for the first-order statistics, <span class="math inline">\(k = 0\)</span>.</p></li>
<li><p>The path highlighted with brown is the path of recursive calls taken by <span class="math inline">\(\texttt{select}\)</span> when searching for the fifth-order statistics, <span class="math inline">\(k = 4\)</span>.</p></li>
<li><p>The path highlighted with green is the path of recursive calls taken by <span class="math inline">\(\texttt{select}\)</span> when searching for the eight-order statistics, <span class="math inline">\(k = 7\)</span>.</p></li>
</ul>
<p><img src="./media/select-example.jpg" alt="image" style="width:5in" /></p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
Example runs of $\ksmall{}$ illustrated by a~\defn{pivot tree.}  For
illustrative purposes, we show all possible recursive calls being
explored down to singleton sequences.
%
In reality, the algorithm explores only one path. 
%

\begin{itemize}
\item
The path highlighted with red is the path of recursive calls taken by
$\ksmall$ when searching for the first-order statistics, $k = 0$.
%

\item
The path highlighted with brown is the path of recursive calls taken by
$\ksmall$ when searching for the fifth-order statistics, $k = 4$.
%

\item
The path highlighted with green is the path of recursive calls taken by
$\ksmall$ when searching for the eight-order statistics, $k = 7$.
\end{itemize}

\begin{center}
\includegraphics[width=5in]{./media/select-example.jpg}
\end{center}

\end{dilFieldBodyDex}
\end{dilAtomExample}
\end{dilGroup}

\end{dilUnit}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilUnit}
\begin{dilFieldTitle}
Analysis
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Analysis
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN7
\end{dilFieldUnique}
\begin{dilLabel}
unit:15210:S18:CH11:SEC3:UN7
\end{dilLabel}
\begin{dilNo}
7
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN7:GR1
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC3:UN7:GR1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN7:GR1:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC3:UN7:GR1:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>Let’s analyze the work and span of the randomized algorithm that picks pivots uniformly randomly. We first present the intuition behind the proof and then present a more mathematical argument.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
Let's analyze the work and span of the randomized algorithm that picks
pivots uniformly randomly.
%
We first present the intuition behind the proof and then present a
more mathematical argument.

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN7:GR2
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC3:UN7:GR2
\end{dilLabel}
\begin{dilNo}
2
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
Intuition
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Intuition
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN7:GR2:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC3:UN7:GR2:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>Before we cover the more precise analysis, let’s develop some intuition by considering the probability that we split the input sequence more or less evenly.</p>
<p>Recall that rank of an element in a sequence is the position of the element in the corresponding sorted sequence and consider the rank of the pivot selected at a step. If we select a pivot whose rank is greater <span class="math inline">\(n/4\)</span> and less than <span class="math inline">\(3n/4\)</span> then the size of the input to the recursive call is at most <span class="math inline">\(3n/4\)</span>. Since all elements are equally likely to be selected as a pivot this probability is <span class="math display">\[\frac{3n/4 - n/4}{n} = 1/2.\]</span> The figure below illustrates this.</p>

<p><img src="./media/qsort-span-intuition.jpg" alt="image" style="width:4in" /></p>
<p>This observations implies that at each recursive call, the size of the input sequence sent to the recursive call decreases by a constant fraction of at least <span class="math inline">\(3/4\)</span> with probability <span class="math inline">\(1/2\)</span>. Thus if we are lucky, we successfully decrease the input size by a constant fraction. But what if we are unlucky?</p>
<p>Consider two successive recursive calls, the probability that the input size decreases by <span class="math inline">\(3/4\)</span> after two calls is the probability that it decreases at either step, which is at least <span class="math inline">\(1-\frac{1}{2} \cdot 
\frac{1}{2} = \frac{3}{4}\)</span>. More generally, after <span class="math inline">\(c &gt; 1\)</span> such successive calls, the probability that the input size decreases by a factor of <span class="math inline">\(\frac{3}{4}\)</span> is <span class="math inline">\(1 - 
\frac{1}{2^c}\)</span>, which quickly approaches <span class="math inline">\(1\)</span> as <span class="math inline">\(c\)</span> increases. For example if <span class="math inline">\(c = 10\)</span> then this probability is <span class="math inline">\(0.999\)</span>. In other words, chances of getting unlucky at any given step is reasonably high (<span class="math inline">\(1/4\)</span>) but chances of getting unlucky over and over again is low and we only need to get lucky once.</p>
<p>This means that with quite high probability, a constant number <span class="math inline">\(c\)</span> of recursive calls is almost guaranteed to decrease the input size by a constant fraction. We thus informally expect <span class="math inline">\(\texttt{select}\)</span> to complete after <span class="math inline">\(c\log{n}\)</span> levels for some constant <span class="math inline">\(c\)</span>.</p>
<p>By inspecting the algorithm, we see that each recursive call performs linear work and logarithmic span to filter the input sequence and then recurs. Therefore, we can write the total work is a geometrically decreasing sum totaling up to <span class="math inline">\(O(n)\)</span> and span is <span class="math inline">\(O(\log^2{n})\)</span>.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
Before we cover the more precise analysis, let's develop some
intuition by considering the probability that we split the input
sequence more or less evenly.
%
%\begin{question}
%What is the probability that $X(n)$ is at most $3n/4$? 
%\end{question}
%

Recall that rank of an element in a sequence is the position of the
element in the corresponding sorted sequence and consider the rank of
the pivot selected at a step.
%
% 
If we select a pivot whose rank is greater $n/4$ and less than $3n/4$
then the size of the input to the recursive call is at most $3n/4$.
%
Since all elements are equally likely to be selected as a pivot this
probability is 
\[
\frac{3n/4 - n/4}{n} = 1/2.
\]
The figure below illustrates this.

\begin{center}
\centering
\includegraphics[width=4in]{./media/qsort-span-intuition.jpg}
\end{center}

%% \begin{question}
%% What does this say about what the span of $\cd{quicksort}$ may be? 
%% \end{question}

This observations implies that at each recursive call, the size of the
input sequence sent to the recursive call decreases by a constant
fraction of at least $3/4$ with probability $1/2$.
%
Thus if we are lucky, we successfully decrease the input size by a
constant fraction.  But what if we are unlucky? 

Consider two successive recursive calls, the probability that the
input size decreases by $3/4$ after two calls is the probability that
it decreases at either step, which is at least $1-\frac{1}{2} \cdot
\frac{1}{2} = \frac{3}{4}$.
%
More generally, after $c > 1$ such successive calls, the probability
that the input size decreases by a factor of $\frac{3}{4}$ is $1 -
\frac{1}{2^c}$, which quickly approaches $1$ as $c$ increases.  For
example if $c = 10$ then this probability is $0.999$.
%
In other words, chances of getting unlucky at any given step is
reasonably high ($1/4$) but chances of getting unlucky over and over
again is low and we only need to get lucky once.

This means that with quite high probability, a constant number $c$ of
recursive calls is almost guaranteed to decrease the input size by a
constant fraction.
%
We thus informally expect $\cd{select}$ to complete after $c\log{n}$
levels for some constant $c$.
%

By inspecting the algorithm, we see that each recursive call performs
linear work and logarithmic span to filter the input sequence and then
recurs.
%
Therefore, we can write the total work is a geometrically decreasing
sum totaling up to $O(n)$ and span is $O(\log^2{n})$.

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN7:GR3
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC3:UN7:GR3
\end{dilLabel}
\begin{dilNo}
3
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
The Recurrences
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
The Recurrences
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN7:GR3:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC3:UN7:GR3:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>Let <span class="math inline">\(n = |a|\)</span> and consider the partition of <span class="math inline">\(a\)</span> into <span class="math inline">\(\ell\)</span> and <span class="math inline">\(r\)</span>. Define <span class="math inline">\(X(n)= \max\{|\ell|, |r|\}/|a|\)</span> as the fractional size of the larger side.</p>
<p>Notice that <span class="math inline">\(X(n)\)</span> is an upper bound on the fractional size of the side the algorithm actually recurs into. Because lines 4 and are calls to <span class="math inline">\(\texttt{filter}\)</span>, we have the following recurrences: <span class="math display">\[\begin{aligned}
W(n) &amp;\leq  W(X(n) \cdot n) + O(n) 
\\ 
S(n) &amp;\leq  S(X(n) \cdot n) + O(\log n) \end{aligned}\]</span></p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
Let $n = |a|$ and consider the partition of $a$ into $\ell$ and $r$.
%
Define $X(n)= \max\{|\ell|, |r|\}/|a|$ as the fractional size of the
larger side.  

Notice that $X(n)$ is an upper bound on the fractional size of the
side the algorithm actually recurs into.
%
Because lines \randomizedosfilterone{} and \randomizedosfiltertwo{}
are calls to $\cd{filter}$, we have the following recurrences:
\begin{align*}
W(n) &\leq  W(X(n) \cdot n) + O(n)
\\
S(n) &\leq  S(X(n) \cdot n) + O(\log n)
\end{align*}

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN7:GR4
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC3:UN7:GR4
\end{dilLabel}
\begin{dilNo}
4
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
Bounding the Expected Fraction
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Bounding the Expected Fraction
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN7:GR4:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC3:UN7:GR4:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>Let’s first bound <span class="math inline">\(\mathbf{E}\left[{X(n)}\right]\)</span>, i.e., the expected fraction of the size of the input instance for the recursize call with respect to the input of size <span class="math inline">\(n\)</span>.</p>
<p>Note that all pivots are equally likely and thus we can draw the following plot of the size of <span class="math inline">\(\ell\)</span> and size of <span class="math inline">\(r\)</span> as a function of to the <span style="color: black"><span><strong><em>rank</em></strong></span></span> of the pivot, i.e., its position in the sorted order of <span class="math inline">\(a\)</span>.</p>
<p><img src="./media/max-random.jpg" alt="image" style="width:3.5in" /></p>
<p>If the pivot is the minumum element then <span class="math inline">\(\ell\)</span> is empty and <span class="math inline">\(|r| 
=|a|-1\)</span>, and if the pivot is the maximum element then <span class="math inline">\(r\)</span> is empty and <span class="math inline">\(|\ell|=|a|-1\)</span>. Since the probability that we choose a pivot on any point along the <span class="math inline">\(x\)</span> axis is <span class="math inline">\(1/n\)</span>, we can write the expectation for <span class="math inline">\(X(n)\)</span> as <span class="math display">\[\begin{aligned}
  \mathbf{E}\left[{X(n)}\right] = \frac{1}{n} \sum_{i=0}^{n-1} \frac{\max\{i, n-i-1\}}{n} \leq \frac{1}{n} \sum_{j=n/2}^{n-1} \frac{2}{n}\cdot j \leq \frac{3}{4} \end{aligned}\]</span> (Recall that <span class="math inline">\(\sum_{i=x}^y i = \frac12(x+y)(y - x + 1)\)</span>.)</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
Let's first bound $\expct{X(n)}$, i.e., the expected fraction of the
size of the input instance for the recursize call with respect to the
input of size $n$.
%

Note that all pivots are equally likely and thus we can draw the
following plot of the size of $\ell$ and size of $r$ as a function of
to the \defn{rank} of the pivot, i.e., its position in the sorted
order of $a$.
\begin{center}
  \includegraphics[width=3.5in]{./media/max-random.jpg}
\end{center}
%
If the pivot is the minumum element then $\ell$ is empty and $|r|
=|a|-1$, and if the pivot is the maximum element then $r$ is empty and
$|\ell|=|a|-1$.
%
Since the probability that we choose a pivot on any point along the $x$
axis is $1/n$, we can write the expectation for $X(n)$ as
\begin{align*}
  \expct{X(n)} = \frac{1}{n} \sum_{i=0}^{n-1} \frac{\max\{i, n-i-1\}}{n} \leq \frac{1}{n} \sum_{j=n/2}^{n-1} \frac{2}{n}\cdot j \leq \frac{3}{4}
\end{align*}
(Recall that $\sum_{i=x}^y i = \frac12(x+y)(y - x + 1)$.)

%\myp{Aside:} This is a counterexample showing that $\expct{\max\{X,
%  Y\}} \neq \max\{\expct{X},\expct{Y}\}$.

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN7:GR5
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC3:UN7:GR5
\end{dilLabel}
\begin{dilNo}
5
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomImportant}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN7:GR5:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC3:UN7:GR5:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>Note that expectation bound hold for all input sizes <span class="math inline">\(n\)</span>.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
Note that expectation bound hold for all input sizes $n$.

\end{dilFieldBodyDex}
\end{dilAtomImportant}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN7:GR6
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC3:UN7:GR6
\end{dilLabel}
\begin{dilNo}
6
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN7:GR6:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC3:UN7:GR6:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>The calculation of <span class="math inline">\(\mathbf{E}\left[{X(n)}\right]\)</span> tells us that in expectation, <span class="math inline">\(X(n)\)</span> is a smaller than <span class="math inline">\(1\)</span>. Thus when bounding the work we should have a nice geometrically decreasing sum that adds up to <span class="math inline">\(O(n)\)</span>. But it is not quite so simple, because the constant fraction is only in expectation. For example, we could get unlucky for a few contraction steps and leading to little or no reduction in the size of the input sequence.</p>
<p>We next show that that even if we are unlucky on some steps, the expected size will indeed go down geometrically. Together with the linearity of expectations this will allow us to bound the work.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
The calculation of $\expct{X(n)}$ tells us that in expectation, $X(n)$
is a smaller than $1$. 
%
Thus when bounding the work we should have a nice geometrically
decreasing sum that adds up to $O(n)$.
%
But it is not quite so simple, because the constant fraction is only
in expectation.
%
For example, we could get unlucky for a few contraction steps and
leading to little or no reduction in the size of the input sequence.
%

We next show that that even if we are unlucky on some steps, the
expected size will indeed go down geometrically.  Together with the
linearity of expectations this will allow us to bound the work.
%

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN7:GR7
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC3:UN7:GR7
\end{dilLabel}
\begin{dilNo}
7
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomTheorem}
\begin{dilFieldTitle}
Expected Size of Input
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Expected Size of Input
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN7:GR7:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC3:UN7:GR7:AT1:thm:random::contract
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>Starting with size <span class="math inline">\(n\)</span>, the expected size of <span class="math inline">\(a\)</span> in algorithm <code>select</code> after <span class="math inline">\(d\)</span> recursive calls is <span class="math inline">\(\left(\frac{3}{4}\right)^d n\)</span>.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
Starting with size $n$, the expected size of $a$ in algorithm
\ksmall{} after
$d$ recursive calls is $\left(\frac{3}{4}\right)^d n$.

\end{dilFieldBodyDex}
\end{dilAtomTheorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomProof}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN7:GR7:AT2
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC3:UN7:GR7:AT2
\end{dilLabel}
\begin{dilNo}
2
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>The proof is by induction on the depth of the recursion <span class="math inline">\(d\)</span>. In the base case, <span class="math inline">\(d = 0\)</span> and the lemma holds trivially. For the inductive case assume that the lemma holds for some <span class="math inline">\(d  \ge 0\)</span>. Consider now the <span class="math inline">\((d+1)^{th}\)</span> recursive call. Let <span class="math inline">\(Y_d\)</span> be the random variable denoting the size of the input to the <span class="math inline">\(d^{th}\)</span> recursive call and let <span class="math inline">\(Z\)</span> the pivot chosen at the <span class="math inline">\(d^{th}\)</span> call. For any value of <span class="math inline">\(y\)</span> and <span class="math inline">\(z\)</span>, let <span class="math inline">\(f(y,z)\)</span> be the fraction of the input reduced by the choice of the pivot at position <span class="math inline">\(z\)</span> for an input of size <span class="math inline">\(y\)</span>. We can write the expectation for the input size at <span class="math inline">\((d+1)^{st}\)</span> call as <span class="math display">\[\begin{array}{lll} 
E[Y_{d+1}]  
&amp; = &amp; \sum_{y,z}{y f(y,z) \mathbf{P}_{Y,Z}(y,z)}  
\\ 
&amp; = &amp; \sum_{y}{\sum_{z}{y f(y,z) \mathbf{P}_{Y_d}(y) \mathbf{P}_{Z{\:\mid\:}Y_d}(z {\:\mid\:}y)}}  
\\ 
&amp; = &amp; \sum_{y}{y \mathbf{P}_{Y_d}(y) \sum_{z}{f(y,z) \mathbf{P}_{Z{\:\mid\:}Y_d}(z {\:\mid\:}y)}}  
\\ 
&amp; \le &amp; \sum_{y}{y \mathbf{P}_{Y_d}(y) \mathbf{E}\left[{X(y)}\right]}. 
\\ 
&amp; \le &amp; \frac{3}{4} \sum_{y}{y \mathbf{P}_{Y}(y)}. 
\\ 
&amp; \le &amp; \frac{3}{4} \mathbf{E}\left[{Y_d}\right]. 
\end{array}\]</span></p>
<p>Note that we have used the bound <span class="math display">\[\mathbf{E}\left[{X(y)}\right] = \sum_{z}{f(y,z) \mathbf{P}_{Z{\:\mid\:}Y_d}(z {\:\mid\:}y)} \le \frac{3}{4},\]</span> which we established above.</p>
<p>We thus conclude that <span class="math inline">\(\mathbf{E}\left[{Y_{d+1}}\right] = \frac34 \mathbf{E}\left[{Y_d}\right]\)</span>, which this trivially solves to the bound given in the theorem, since at <span class="math inline">\(d=0\)</span> the input size is <span class="math inline">\(n\)</span>.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
The proof is by induction on the depth of the recursion~$d$. 
%
In the base case, $d = 0$ and the lemma holds trivially.
%
For the inductive case assume that the lemma holds for some $d  \ge 0$.
%
Consider now the $(d+1)^{th}$ recursive call.
%
Let $Y_d$ be the random variable denoting the size of the input to the
$d^{th}$ recursive call and let $Z$ the pivot chosen at the
$d^{th}$ call.
%
For any value of $y$ and $z$, let $f(y,z)$ be the fraction of the
input reduced by the choice of the pivot at position $z$ for an input of size
$y$. 
%
We can write the expectation for the input size at $(d+1)^{st}$ call
as 
\[
\begin{array}{lll}
E[Y_{d+1}] 
& = & \sum_{y,z}{y f(y,z) \pmf{Y,Z}(y,z)} 
\\
& = & \sum_{y}{\sum_{z}{y f(y,z) \pmf{Y_d}(y) \pmf{Z \given Y_d}(z \given y)}} 
\\
& = & \sum_{y}{y \pmf{Y_d}(y) \sum_{z}{f(y,z) \pmf{Z \given Y_d}(z \given y)}} 
\\
& \le & \sum_{y}{y \pmf{Y_d}(y) \expct{X(y)}}.
\\
& \le & \frac{3}{4} \sum_{y}{y \pmf{Y}(y)}.
\\
& \le & \frac{3}{4} \expct{Y_d}.
\end{array}
\]  

Note that we have used the bound 
\[
\expct{X(y)} = \sum_{z}{f(y,z) \pmf{Z \given Y_d}(z \given y)} \le \frac{3}{4},
\]
which we established above.


We thus conclude that $\expct{Y_{d+1}} = \frac34 \expct{Y_d}$, which
this trivially solves to the bound given in the theorem, since at
$d=0$ the input size is $n$.

\end{dilFieldBodyDex}
\end{dilAtomProof}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN7:GR8
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC3:UN7:GR8
\end{dilLabel}
\begin{dilNo}
8
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomRemark}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN7:GR8:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC3:UN7:GR8:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>Note that the proof of this theorem would have been relatively easy if the successive choices made by the algorithm were independent but they are not, because the size to the algorithm at each recursive call depends on prior choices of pivots.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
Note that the proof of this theorem would have been relatively easy if
the successive choices made by the algorithm were independent but they
are not, because the size to the algorithm at each recursive call
depends on prior choices of pivots.

\end{dilFieldBodyDex}
\end{dilAtomRemark}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN7:GR9
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC3:UN7:GR9
\end{dilLabel}
\begin{dilNo}
9
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
Completing the Work Analysis
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Completing the Work Analysis
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN7:GR9:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC3:UN7:GR9:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>We now have all the ingredients to complete the analysis.</p>
<p>The work at each level of the recursive calls is linear in the size of the input and thus can be written as <span class="math inline">\(W_{\texttt{select}}(n) \leq k_1n + k_2\)</span>, where <span class="math inline">\(n\)</span> is the input size. Because at least one element, the pivot, is taken out of the input for the recursive call at each level, there are at most <span class="math inline">\(n\)</span> levels of recursion. Thus, by using the theorem below, we can bound the expected work as <span class="math display">\[\begin{aligned}
\mathbf{E}\left[{W_{\texttt{select}}(n)}\right] 
&amp; \leq  \sum_{i=0}^n (k_1 \mathbf{E}\left[{Y_i}\right] + k_2) \\ 
\mathbf{E}\left[{W_{\texttt{select}}(n)}\right] 
&amp; \leq  \sum_{i=0}^n (k_1 n \left(\frac{3}{4}\right)^i + k_2) \\ 
&amp; \leq  k_1 n \left(\sum_{i=0}^n \left(\frac{3}{4}\right)^i\right) + k_2 n \\ 
&amp; \leq  4 k_1 n + k_2 n\\ 
&amp; \in  O(n). \end{aligned}\]</span></p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
We now have all the ingredients to complete the analysis.
%

The work at each level of the recursive calls is linear in the size of
the input and thus can be written as $W_{\cd{select}}(n) \leq k_1n + k_2$, where $n$ is the input size.
%
Because at least one element, the pivot, is taken out of the input for
the recursive call at each level, there are at most $n$ levels of
recursion. Thus, by using the theorem below, we can bound the expected
work as
\begin{align*}
\expct{W_{\ksmall}(n)}
& \leq  \sum_{i=0}^n (k_1 \expct{Y_i} + k_2) \\
\expct{W_{\ksmall}(n)}
& \leq  \sum_{i=0}^n (k_1 n \left(\frac{3}{4}\right)^i + k_2) \\
& \leq  k_1 n \left(\sum_{i=0}^n \left(\frac{3}{4}\right)^i\right) + k_2 n \\
& \leq  4 k_1 n + k_2 n\\
& \in  O(n).
\end{align*}

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN7:GR10
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC3:UN7:GR10
\end{dilLabel}
\begin{dilNo}
10
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomNote}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN7:GR10:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC3:UN7:GR10:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>Many algorithms, such as graph algorithm, have the same property that the size of the problem instance goes down by an expected constant factor on each contraction step.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
Many algorithms, such as graph algorithm, have the same property that
the size of the problem instance goes down by an expected constant
factor on each contraction step.
%

\end{dilFieldBodyDex}
\end{dilAtomNote}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN7:GR11
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC3:UN7:GR11
\end{dilLabel}
\begin{dilNo}
11
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
Span Analysis
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Span Analysis
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN7:GR11:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC3:UN7:GR11:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>We can bound the span of the algorithm by <span class="math inline">\(O(n\lg{n})\)</span> trivially in the worst case, but we expect the average span to be a lot better because chances of picking a poor pivot over and over again, which would be required for the linear span is unlikely. To bound the span in the expected case, we shall use Expected-Size-of-Input Theorem established above to bound the number of levels taken by <span class="math inline">\(\texttt{select}{}\)</span> more tightly using a high probability bound.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
We can bound the span of the algorithm by $O(n\lg{n})$ trivially
in the worst case, but we expect the average span to be a lot better
because chances of picking a poor pivot over and over again, which
would be required for the linear span is unlikely.
%
To bound the span in the expected case, we shall use
Expected-Size-of-Input Theorem established above
%
%\thmref{random::contract}
%
to bound the number of levels taken by
$\ksmall{}$ more tightly using a high probability bound.
%

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN7:GR12
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC3:UN7:GR12
\end{dilLabel}
\begin{dilNo}
12
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN7:GR12:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC3:UN7:GR12:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>Consider depth <span class="math inline">\(d = 10 \lg n\)</span>. At this depth, the expected size upper bounded by <span class="math display">\[n\left(\frac{3}{4}\right)^{10 \lg n}.\]</span> With a little math this is equal to <span class="math inline">\(n \times n^{-10 \lg (4/3)} 
\approx n^{-3.15}\)</span>. Now, by Markov’s inequality, if the expected size is at most <span class="math inline">\(n^{-3.15}\)</span> then the probability of having size at least <span class="math inline">\(1\)</span> is bounded by <span class="math display">\[\mathbf{P}\left[{Y_{10\lg n} \geq 1}\right] \leq \frac{E[Y_{10\lg n}]}{1} = n^{-3.15}.\]</span></p>
<p>In applying Markov’s inequality, we choose <span class="math inline">\(1\)</span>, because we know that the algorithm terminates for that input size. By increasing the constant factor from <span class="math inline">\(10\)</span> to <span class="math inline">\(20\)</span> would decrease the probability to <span class="math inline">\(n^{-7.15}\)</span>, which is extremely unlikely: for <span class="math inline">\(n = 
10^6\)</span> this is <span class="math inline">\(10^{-42}\)</span>.</p>
<p>We have therefore shown that the number of steps is <span class="math inline">\(O(\log n)\)</span> with high probability. Each step has span <span class="math inline">\(O(\log n)\)</span> so the overall span is <span class="math inline">\(O(\log^2 n)\)</span> with high probability.</p>
<p>Using the high probability bound, we can bound the expected span by using the Total Expectations theorem.</p>
<p>For brevity let the random variable <span class="math inline">\(Y\)</span> be defined as <span class="math inline">\(Y = Y_{10\lg n}\)</span>, <span class="math display">\[\begin{array}{lll} 
\mathbf{E}\left[{S}\right] &amp; = &amp; \sum_{y}\mathbf{P}_{Y}(y) \mathbf{E}\left[{S{\:\mid\:}Y = y}\right]. 
\\ 
&amp; = &amp;  
\sum_{y \le 1}{\mathbf{P}_{Y}(y) \mathbf{E}\left[{S{\:\mid\:}Y = y}\right]} 
 +  
\sum_{y &gt;1}{\mathbf{P}_{Y}(y) \mathbf{E}\left[{S{\:\mid\:}Y = y}\right]} 
\\ 
&amp; \le &amp;  
(1 - n^{-3.5}) O(\lg^2{n})  
 +  
n^{-3.5} O(n) 
\\ 
&amp; = &amp; 
O(\lg^2{n}).  
\end{array}\]</span></p>
<p>The expected bound follows by the fact that with high probability the depth of the recursive calls is <span class="math inline">\(O(\lg{n})\)</span> and that each recursive call has <span class="math inline">\(O(\lg{n})\)</span> span, because it requires a sequences <span class="math inline">\(\texttt{filter}\)</span>. The span for the case when the span is not greater that <span class="math inline">\(10\lg{n}\)</span> contributes only a constant value to the expectation as long as it is a polynomial that is less that <span class="math inline">\(n^{3.5}\)</span>.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
Consider depth $d = 10 \lg n$.
%
At this depth, the expected size upper bounded by
\[
n\left(\frac{3}{4}\right)^{10 \lg n}.
\]
%
With a little math this is equal to $n \times n^{-10 \lg (4/3)}
\approx n^{-3.15}$.
%
Now, by Markov's inequality, if the expected size is at most
$n^{-3.15}$ then the probability of having size at least $1$ is
bounded by
\[ 
\prob{Y_{10\lg n} \geq 1} \leq \frac{E[Y_{10\lg n}]}{1} = n^{-3.15}. 
\]

In applying Markov's inequality, we choose $1$, because we know that
the algorithm terminates for that input size.
%
By increasing the constant factor from $10$ to $20$ would decrease the
probability to $n^{-7.15}$, which is extremely unlikely: for $n =
10^6$ this is $10^{-42}$.  
%

We have therefore shown that the number of steps is $O(\log n)$ with
high probability.  
%
Each step has span $O(\log n)$ so the overall span is $O(\log^2 n)$
with high probability.

Using the high probability bound, we can bound the expected span by
using the Total Expectations theorem.
%

For brevity let the random variable $Y$ be defined as $Y = Y_{10\lg n}$,
%
\[
\begin{array}{lll}
\expct{S} & = & \sum_{y}\pmf{Y}(y) \expct{S \given Y = y}.
\\
& = & 
\sum_{y \le 1}{\pmf{Y}(y) \expct{S \given Y = y}}
 + 
\sum_{y >1}{\pmf{Y}(y) \expct{S \given Y = y}}
\\
& \le & 
(1 - n^{-3.5}) O(\lg^2{n}) 
 + 
n^{-3.5} O(n)
\\
& = &
O(\lg^2{n}). 
\end{array}
\]

The expected bound follows by the fact that with high probability the
depth of the recursive calls is $O(\lg{n})$ and that each
recursive call has $O(\lg{n})$ span, because it requires a
sequences $\cd{filter}$.
%
The span for the case when the span is not greater that $10\lg{n}$
contributes only a constant value to the expectation as long as it is
a polynomial that is less that $n^{3.5}$.

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN7:GR13
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC3:UN7:GR13
\end{dilLabel}
\begin{dilNo}
13
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
Summary
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Summary
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN7:GR13:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC3:UN7:GR13:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>In summary, we have shown than the <span class="math inline">\(\texttt{select}{}\)</span> algorithm on input of size <span class="math inline">\(n\)</span> does <span class="math inline">\(O(n)\)</span> work in expectation and has <span class="math inline">\(O(\log^2 n)\)</span> span with high probability.</p>
<p>For reasons explained at the start of the chapter, we typically analyze work using expectation and span using high probability.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
In summary, we have shown than the $\ksmall{}$ algorithm on input of
size $n$ does $O(n)$ work in expectation and has $O(\log^2 n)$ span
with high probability.  
%

For reasons explained at the start of the chapter, we typically
analyze work using expectation and span using high probability.

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN7:GR14
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC3:UN7:GR14
\end{dilLabel}
\begin{dilNo}
14
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomTeachNote}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC3:UN7:GR14:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC3:UN7:GR14:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<h4 id="alternative-analysis.">Alternative Analysis.</h4>
<p>Let <span class="math inline">\(Z_n\)</span> denote the random variable denoting the size of the maximum of the two subsequences computed by splitting inpet sequence of size <span class="math inline">\(n\)</span> by the pivot. We can see that <span class="math inline">\(\mathbf{P}\left[{Z(n) \le \frac 34 n}\right] \ge \frac12\)</span>, because choosing any one of the elements between rank <span class="math inline">\(\frac{n}{4}\)</span> and <span class="math inline">\(\frac{3n}{4}\)</span> yield such a fairly balanced partition.</p>
<p>Using total expectation theorem, we can write the expected work of the algorithm by conditioning on the random variable <span class="math inline">\(Z\)</span>. <span class="math display">\[\mathbf{E}\left[{W(n)}\right] = \sum_{i = z}^{n}{\mathbf{P}\left[{Z(n) = z}\right] \cdot \mathbf{E}\left[{W(n}\right]  {\:\mid\:}n = z} + O(n).\]</span> We can bound this sum as follows <span class="math display">\[\begin{array}{lll} 
\mathbf{E}\left[{W(n)}\right]  
&amp; = &amp;  
\sum_{i = z}^{n}{\mathbf{P}\left[{Z(n) = z}\right] \cdot \mathbf{E}\left[{W(n}\right]  {\:\mid\:}n = z} + O(n). 
\\ 
&amp; = &amp;  
\sum_{i = z}^{n}{\mathbf{P}\left[{Z(n) = z}\right] \cdot \mathbf{E}\left[{W(z}\right]} + O(n). 
\\ 
&amp; \le &amp;  
\mathbf{P}\left[{Z(n) \le \frac 34 n}\right] \cdot \mathbf{E}\left[{W(\frac 34 n}\right]  
+  
\mathbf{P}\left[{Z(n) &gt; \frac 34 n}\right] \cdot \mathbf{E}\left[{W(n)}\right] 
+ 
O(n) 
\\ 
&amp; \le &amp; \frac12 \mathbf{E}\left[{W(\frac 34 n)}\right] +  \frac12 \mathbf{E}\left[{W(n)}\right] + O(n). 
\end{array}\]</span></p>
<p>We now have the following recurrence <span class="math display">\[\mathbf{E}\left[{W(n)}\right]  
\le  
\frac12 \mathbf{E}\left[{W(\frac 34 n)}\right] +  \frac12 \mathbf{E}\left[{W(n)}\right] + O(n),\]</span> which can be simplified as <span class="math display">\[\mathbf{E}\left[{W(n)}\right]  
\le  
\mathbf{E}\left[{W(\frac 34 n)}\right] + O(n).\]</span></p>
<p>This solves to <span class="math inline">\(O(n)\)</span>. Exactly the same approach can be followed to obtain the following recurrence for span <span class="math display">\[\mathbf{E}\left[{W(n)}\right]  
\le  
\mathbf{E}\left[{W(\frac 34 n)}\right] + O(\lg{n}).\]</span> This solves to <span class="math inline">\(O(\lg^2{n})\)</span>.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
\paragraph{Alternative Analysis.}

Let $Z_n$ denote the random variable denoting the size of the maximum
of the two subsequences computed by splitting inpet sequence of size
$n$ by the pivot.
%
We can see that $\prob{Z(n) \le \frac34 n} \ge \frac12$, because
choosing any one of the elements between rank $\frac{n}{4}$ and
$\frac{3n}{4}$ yield such a fairly balanced partition.
%

Using total expectation theorem, we can write the expected work of the
algorithm by conditioning on the random variable $Z$.
\[
\expct{W(n)} = \sum_{i = z}^{n}{\prob{Z(n) = z} \cdot \expct{W(n}  \given n = z} + O(n).
\]
We can bound this sum as follows
\[
\begin{array}{lll}
\expct{W(n)} 
& = & 
\sum_{i = z}^{n}{\prob{Z(n) = z} \cdot \expct{W(n}  \given n = z} + O(n).
\\
& = & 
\sum_{i = z}^{n}{\prob{Z(n) = z} \cdot \expct{W(z}} + O(n).
\\
& \le & 
\prob{Z(n) \le \frac34 n} \cdot \expct{W(\frac34 n} 
+ 
\prob{Z(n) > \frac34 n} \cdot \expct{W(n)}
+
O(n)
\\
& \le & \frac12 \expct{W(\frac34 n)} +  \frac12 \expct{W(n)} + O(n).
\end{array}
\]

We now have the following recurrence
\[
\expct{W(n)} 
\le 
\frac12 \expct{W(\frac34 n)} +  \frac12 \expct{W(n)} + O(n),
\]
which can be simplified as 
\[
\expct{W(n)} 
\le 
\expct{W(\frac34 n)} + O(n).
\]

This solves to $O(n)$.  Exactly the same approach can be followed to
obtain the following recurrence for span
\[
\expct{W(n)} 
\le 
\expct{W(\frac34 n)} + O(\lg{n}).
\]
This solves to $O(\lg^2{n})$.

\end{dilFieldBodyDex}
\end{dilAtomTeachNote}
\end{dilGroup}

\end{dilUnit}
\end{dilSection}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilSection}
\begin{dilFieldTitle}
The Quick Sort Algorithm and Its Analysis
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
The Quick Sort Algorithm and Its Analysis
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4
\end{dilFieldUnique}
\begin{dilLabel}
section:15210:S18:CH11:SEC4
\end{dilLabel}
\begin{dilNo}
4
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilIntro}
<p>In this section, we consider the randomized quick sort algorithm and analyze its work and span.</p>

\end{dilIntro}
\begin{dilIntroDex}


In this section, we consider the randomized quick sort algorithm and
analyze its work and span.
\end{dilIntroDex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilUnit}
\begin{dilFieldTitle}
The Quick Sort Algorithm
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
The Quick Sort Algorithm
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN8
\end{dilFieldUnique}
\begin{dilLabel}
unit:15210:S18:CH11:SEC4:UN8
\end{dilLabel}
\begin{dilNo}
8
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN8:GR1
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC4:UN8:GR1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomAlgorithm}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN8:GR1:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC4:UN8:GR1:AT1:alg:randomized::qsort
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>Consider the quick sort algorithm specified below, where we intentionally leave the pivot-choosing step unspecified.</p>
<p><span class="math display">\[\begin{array}{ll} 
1 &amp; \texttt{quicksort}~a = 
\\ 
2 &amp; ~~~\texttt{if}~|a| = 0~\texttt{then}~a 
\\ 
3 &amp; ~~~\texttt{else}  
\\ 
4 &amp; ~~~~~~\texttt{let} 
\\ 
5 &amp; ~~~~~~~~~p = \texttt{pick a pivot from}~a 
\\ 
6 &amp; ~~~~~~~~~    a_1 = \left\langle\, x \in a \;|\; x &lt; p \,\right\rangle 
\\ 
7 &amp; ~~~~~~~~~    a_2 = \left\langle\, x \in a \;|\; x = p \,\right\rangle 
\\ 
8 &amp; ~~~~~~~~~    a_3 = \left\langle\, x \in a \;|\; x &gt; p \,\right\rangle 
\\ 
9 &amp; ~~~~~~~~~    (s_1,s_3) = (\texttt{quicksort}~a_1)~\mid\mid{}~(\texttt{quicksort}~a_3) 
\\ 
10 &amp; ~~~~~~   \texttt{in} 
\\ 
11 &amp; ~~~~~~~~~    s_1 \texttt{++}{} a_2 \texttt{++}{} s_3 
\\ 
12 &amp; ~~~~~~  \texttt{end} 
\end{array}\]</span></p>
<p>There is plenty of parallelism in this version quick sort. There is both parallelism due to the two recursive calls and in the fact that the filters for selecting elements greater, equal, and less than the pivot can be parallel.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
Consider the quick sort algorithm specified below, where we
intentionally leave the pivot-choosing step unspecified.

% @\label{line:randomized::qsort-rk}@

\[
\begin{array}{ll}
1 & \cd{quicksort}~a =
\\
2 & ~~~\cd{if}~|a| = 0~\cd{then}~a
\\
3 & ~~~\cd{else} 
\\
4 & ~~~~~~\cd{let}
\\
5 & ~~~~~~~~~p = \cd{pick a pivot from}~a
\\
6 & ~~~~~~~~~    a_1 = \cseqf{x \in a}{x < p}
\\
7 & ~~~~~~~~~    a_2 = \cseqf{x \in a}{x = p}
\\
8 & ~~~~~~~~~    a_3 = \cseqf{x \in a}{x > p}
\\
9 & ~~~~~~~~~    (s_1,s_3) = (\cd{quicksort}~a_1)~\cpar{}~(\cd{quicksort}~a_3)
\\
10 & ~~~~~~   \cd{in}
\\
11 & ~~~~~~~~~    s_1 \cappend{} a_2 \cappend{} s_3
\\
12 & ~~~~~~  \cd{end}
\end{array}
\]


There is plenty of parallelism in this version quick sort.
%
There is both parallelism due to the two recursive calls and in the
fact that the filters for selecting elements greater, equal, and less
than the pivot can be parallel.

\end{dilFieldBodyDex}
\end{dilAtomAlgorithm}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN8:GR2
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC4:UN8:GR2
\end{dilLabel}
\begin{dilNo}
2
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
Pivot Tree
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Pivot Tree
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN8:GR2:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC4:UN8:GR2:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>Note that each call to <span class="math inline">\(\texttt{quicksort}\)</span> either makes no recursive calls (the base case) or two recursive calls. We can therefore represent an execution of <span class="math inline">\(\texttt{quicksort}\)</span> as a binary tree. More specifically, it is convenient to map the run of a <span class="math inline">\(\texttt{quicksort}\)</span> to a binary-search tree (BST) representing the recursive calls along with the pivots chosen. We refer to this tree as  <span style="color: black"><span><strong><em>pivot tree</em></strong></span></span>.</p>
<p>We use the pivot-tree representation to reason about the properties of <span class="math inline">\(\texttt{quicksort}\)</span>, e.g., the comparisons performed, its span.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
Note that each call to $\cd{quicksort}$ either makes no recursive calls (the
base case) or two recursive calls.  
%
We can therefore represent an execution of $\cd{quicksort}$ as a binary tree.
%
More specifically, it is convenient to map the run of a $\cd{quicksort}$ to a
binary-search tree (BST) representing the recursive calls along with
the pivots chosen.
%
We refer to this tree as~\defn{pivot tree}.
%

We use the pivot-tree representation to reason about the
properties of $\cd{quicksort}$, e.g., the comparisons performed, its span.

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN8:GR3
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC4:UN8:GR3
\end{dilLabel}
\begin{dilNo}
3
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomExample}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN8:GR3:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC4:UN8:GR3:AT1:ex:randomized::qsort
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>An example run of <span class="math inline">\(\texttt{quicksort}\)</span> along with its pivot tree.</p>
<p><img src="./media/qsort-bst-example.jpg" alt="image" style="width:4in" /></p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
An example run of $\cd{quicksort}$ along with its pivot tree.
\begin{center}
\includegraphics[width=4in]{./media/qsort-bst-example.jpg}
\end{center}

\end{dilFieldBodyDex}
\end{dilAtomExample}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN8:GR4
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC4:UN8:GR4
\end{dilLabel}
\begin{dilNo}
4
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
Pivot Selection
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Pivot Selection
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN8:GR4:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC4:UN8:GR4:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>Let’s consider some strategies for picking a pivot.</p>
<ul>
<li><p><strong>Always pick the first element:</strong> If the sequence is sorted in increasing order, then picking the first element is the same as picking the smallest element. We end up with a lopsided recursion tree of depth <span class="math inline">\(n\)</span>. The total work is <span class="math inline">\(O(n^2)\)</span> since <span class="math inline">\(n-i\)</span> keys will remain at level <span class="math inline">\(i\)</span> and hence we will do <span class="math inline">\(n-i-1\)</span> comparisons at that level for a total of <span class="math inline">\(\sum_{i=0}^{n-1} (n-i-1)\)</span>. Similarly, if the sequence is sorted in decreasing order, we will end up with a recursion tree that is lopsided in the other direction. In practice, it is not uncommon for a sort function input to be a sequence that is already sorted or nearly sorted.</p></li>
<li><p><strong>Pick the median of three elements:</strong> Another strategy is to take the first, middle, and the last elements and pick the median of them. For sorted lists the split is even, so each side contains half of the original size and the depth of the tree is <span class="math inline">\(O(\log n)\)</span>. Although this strategy avoids the pitfall with sorted sequences, it is still possible to be unlucky, and in the worst-case the costs and tree depth are the same as the first strategy. This is the strategy used by many library implementations of <span class="math inline">\(\texttt{quicksort}\)</span>. Can you think of a way to slow down a <span class="math inline">\(\texttt{quicksort}\)</span> implementation that uses this strategy by picking an adversarial input?</p></li>
<li><p><strong>Pick an element randomly:</strong> It is not immediately clear what the depth of this is, but intuitively, when we choose a random pivot, the size of each side is not far from <span class="math inline">\(n/2\)</span> in expectation. This doesn’t give us a proof but it gives us hope that this strategy will result in a tree of depth <span class="math inline">\(O(\log n)\)</span> in expectation or with high probability. Indeed, picking a random pivot gives us expected <span class="math inline">\(O(n \log n)\)</span> work and <span class="math inline">\(O(\log^2 n)\)</span> span for <span class="math inline">\(\texttt{quicksort}\)</span> and an expected <span class="math inline">\(O(\log n)\)</span>-depth tree, as we will show.</p></li>
</ul>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
Let's consider some strategies for picking a pivot.   

\begin{itemize}
\item \textbf{Always pick the first element:} If the sequence is sorted
  in increasing order, then picking the first element is the same as
  picking the smallest element.  We end up with a lopsided recursion
  tree of depth $n$.  The total work is $O(n^2)$ since $n-i$ keys will
  remain at level $i$ and hence we will do $n-i-1$ comparisons at that
  level for a total of $\sum_{i=0}^{n-1} (n-i-1)$.  Similarly, if the
  sequence is sorted in decreasing order, we will end up with a
  recursion tree that is lopsided in the other direction.  In
  practice, it is not uncommon for a sort function input to be a
  sequence that is already sorted or nearly sorted.

\item \textbf{Pick the median of three elements:} Another strategy is to
  take the first, middle, and the last elements and pick the median of
  them.  For sorted lists the split is even, so each side contains
  half of the original size and the depth of the tree is $O(\log n)$.
  Although this strategy avoids the pitfall with sorted sequences, it
  is still possible to be unlucky, and in the worst-case the costs and
  tree depth are the same as the first strategy.  This is the strategy
  used by many library implementations of $\cd{quicksort}$.  Can you think of
  a way to slow down a $\cd{quicksort}$ implementation that uses this
  strategy by picking an adversarial input?

\item \textbf{Pick an element randomly:} It is not immediately clear
  what the depth of this is, but intuitively, when we choose a random
  pivot, the size of each side is not far from $n/2$ in expectation.
  This doesn't give us a proof but it gives us hope that this strategy
  will result in a tree of depth $O(\log n)$ in expectation or with
  high probability.  Indeed, picking a random pivot gives us expected
  $O(n \log n)$ work and $O(\log^2 n)$ span for $\cd{quicksort}$ and an
  expected $O(\log n)$-depth tree, as we will show.
\end{itemize}

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}

\end{dilUnit}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilUnit}
\begin{dilFieldTitle}
Analysis of Quick Sort
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Analysis of Quick Sort
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9
\end{dilFieldUnique}
\begin{dilLabel}
unit:15210:S18:CH11:SEC4:UN9
\end{dilLabel}
\begin{dilNo}
9
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR1
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC4:UN9:GR1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
Intuition
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Intuition
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR1:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC4:UN9:GR1:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>We can reason about the work and span of quick sort in a way similar to the randomized <span class="math inline">\(\texttt{select}\)</span> algorithm that we considered above.</p>
<p>Recall that rank of an element in a sequence is the position of the element in the corresponding sorted sequence and consider the rank of the pivot selected at a step. If we select a pivot whose rank is greater <span class="math inline">\(n/4\)</span> and less than <span class="math inline">\(3n/4\)</span> then the size of the input to the recursive call is at most <span class="math inline">\(3n/4\)</span>. Since all elements are equally likely to be selected as a pivot this probability is <span class="math display">\[\frac{3n/4 - n/4}{n} = 1/2.\]</span> The figure below illustrates this.</p>

<p><img src="./media/qsort-span-intuition.jpg" alt="image" style="width:4in" /></p>
<p>This observations implies that at each recursive call, the size of the input sequence sent to the recursive call decreases by a constant fraction of at least <span class="math inline">\(3/4\)</span> with probability <span class="math inline">\(1/2\)</span>. Thus if we are lucky, we successfully decrease the input size by a constant fraction. But what if we are unlucky?</p>
<p>Consider two successive recursive calls, the probability that the input size decreases by <span class="math inline">\(3/4\)</span> after two calls is the probability that it decreases at either call, which is at least <span class="math inline">\(1-\frac{1}{2} \cdot 
\frac{1}{2} = \frac{3}{4}\)</span>. More generally, after <span class="math inline">\(c &gt; 1\)</span> such successive calls, the probability that the input size decreases by a factor of <span class="math inline">\(\frac{3}{4}\)</span> is <span class="math inline">\(1 - 
\frac{1}{2^c}\)</span>, which quickly approaches <span class="math inline">\(1\)</span> as <span class="math inline">\(c\)</span> increases. For example if <span class="math inline">\(c = 10\)</span> then this probability is <span class="math inline">\(0.999\)</span>. In other words, chances of getting unlucky at any given step is reasonably high (<span class="math inline">\(1/4\)</span>) but chances of getting unlucky over and over again is low and we only need to get lucky once.</p>
<p>This means that with quite high probability, a constant number <span class="math inline">\(c\)</span> of recursive calls is almost guaranteed to decrease the input size by a constant fraction. Thus we informally expect <span class="math inline">\(\texttt{quicksort}\)</span> to complete after <span class="math inline">\(c\log{n}\)</span> levels for some constant <span class="math inline">\(c\)</span>.</p>
<p>By inspecting the algorithm, we see that each recursive call performs linear work and logarithmic span to filter the input sequence and then recurs and then performs a linear work constant span append operation. Therefore, we can write the total work is a geometrically decreasing sum totaling up to <span class="math inline">\(O(n)\)</span> and span is <span class="math inline">\(O(\log^2{n})\)</span>.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
We can reason about the work and span of quick sort in a way similar
to the randomized $\cd{select}$ algorithm that we considered above.

Recall that rank of an element in a sequence is the position of the
element in the corresponding sorted sequence and consider the rank of
the pivot selected at a step.
%
% 
If we select a pivot whose rank is greater $n/4$ and less than $3n/4$
then the size of the input to the recursive call is at most $3n/4$.
%
Since all elements are equally likely to be selected as a pivot this
probability is 
\[
\frac{3n/4 - n/4}{n} = 1/2.
\]
The figure below illustrates this.

\begin{center}
\centering
\includegraphics[width=4in]{./media/qsort-span-intuition.jpg}
\end{center}

%% \begin{question}
%% What does this say about what the span of $\cd{quicksort}$ may be? 
%% \end{question}

This observations implies that at each recursive call, the size of the
input sequence sent to the recursive call decreases by a constant
fraction of at least $3/4$ with probability $1/2$.
%
Thus if we are lucky, we successfully decrease the input size by a
constant fraction.  But what if we are unlucky? 

Consider two successive recursive calls, the probability that the
input size decreases by $3/4$ after two calls is the probability that
it decreases at either call, which is at least $1-\frac{1}{2} \cdot
\frac{1}{2} = \frac{3}{4}$.
%
More generally, after $c > 1$ such successive calls, the probability
that the input size decreases by a factor of $\frac{3}{4}$ is $1 -
\frac{1}{2^c}$, which quickly approaches $1$ as $c$ increases.  For
example if $c = 10$ then this probability is $0.999$.
%
In other words, chances of getting unlucky at any given step is
reasonably high ($1/4$) but chances of getting unlucky over and over
again is low and we only need to get lucky once.

This means that with quite high probability, a constant number $c$ of
recursive calls is almost guaranteed to decrease the input size by a
constant fraction.
%
Thus we informally expect $\cd{quicksort}$ to complete after
$c\log{n}$ levels for some constant $c$.
%

By inspecting the algorithm, we see that each recursive call performs
linear work and logarithmic span to filter the input sequence and then
recurs and then performs a linear work constant span append operation.
%
Therefore, we can write the total work is a geometrically decreasing
sum totaling up to $O(n)$ and span is $O(\log^2{n})$.

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR2
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC4:UN9:GR2
\end{dilLabel}
\begin{dilNo}
2
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR2:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC4:UN9:GR2:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>The rest of this section makes this intuition more precise. There are many methods of analysis that we can use. We consider one in detail, which is based on counting, and outline another, which is based establishing a recurrence, which can then be solved.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
The rest of this section makes this intuition more precise.  There are
many methods of analysis that we can use. 
%
We consider one in detail, which is based on counting, and outline
another, which is based establishing a recurrence, which can then be
solved.
%

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR3
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC4:UN9:GR3
\end{dilLabel}
\begin{dilNo}
3
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR3:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC4:UN9:GR3:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>For the analysis, we assume a priority-based selection technique for pivots. At the start of the algorithm, we assign each key a random priority uniformly at random from the real interval <span class="math inline">\([0, 1]\)</span> such that each key has a unique priority. We then pick in Line 5 the key with the highest priority. Notice that once the priorities are decided, the algorithm is completely deterministic.</p>
<p>In addition, we assume a version of quicksort that compares the pivot <span class="math inline">\(p\)</span> to each key in the input <span class="math inline">\(a\)</span> once (instead of 3 times, once to generate each of <span class="math inline">\(a_1\)</span>, <span class="math inline">\(a_2\)</span>, and <span class="math inline">\(a_3\)</span>).</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
For the analysis, we assume a priority-based selection technique for
pivots.
%
At the start of the algorithm, we assign each key a random priority
uniformly at random from the real interval $[0, 1]$ such that each key
has a unique priority.  
%
We then pick in Line~\randomizedqsortpivot{} the key with the highest
priority.  Notice that once the priorities are decided, the algorithm
is completely deterministic.
%

In addition, we assume a version of quicksort that compares the pivot
$p$ to each key in the input $a$ once (instead of 3 times, once to
generate each of $a_1$, $a_2$, and $a_3$).
%

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR4
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC4:UN9:GR4
\end{dilLabel}
\begin{dilNo}
4
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomExercise}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR4:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC4:UN9:GR4:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>Rewrite the quicksort algorithm so to use the comparison once when comparing the pivot with each key at a recursive call.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
Rewrite the quicksort algorithm so to use the comparison once when
  comparing the pivot with each key at a recursive call.

\end{dilFieldBodyDex}
\end{dilAtomExercise}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR5
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC4:UN9:GR5
\end{dilLabel}
\begin{dilNo}
5
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomExample}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR5:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC4:UN9:GR5:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>An execution of <span class="math inline">\(\texttt{quicksort}\)</span> with priorities and its pivot tree, which is a binary-search-tree, illustrated.</p>
<p><img src="./media/qsort-bst-example-with-p.jpg" alt="image" style="width:4in" /></p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
An execution of $\cd{quicksort}$ with priorities and its pivot tree, which is a binary-search-tree, illustrated. 

\begin{center}
\includegraphics[width=4in]{./media/qsort-bst-example-with-p.jpg}
\end{center}

\end{dilFieldBodyDex}
\end{dilAtomExample}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR6
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC4:UN9:GR6
\end{dilLabel}
\begin{dilNo}
6
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomExercise}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR6:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC4:UN9:GR6:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>Convince yourself that the two presentations of randomized <span class="math inline">\(\texttt{quicksort}\)</span> are fully equivalent (modulo the technical details about how we might store the priority values).</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
Convince yourself that the two
presentations of randomized $\cd{quicksort}$ are fully equivalent (modulo the
technical details about how we might store the priority values).

\end{dilFieldBodyDex}
\end{dilAtomExercise}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR7
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC4:UN9:GR7
\end{dilLabel}
\begin{dilNo}
7
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR7:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC4:UN9:GR7:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>Before we get to the analysis, let’s observe some properties of <span class="math inline">\(\texttt{quicksort}\)</span>. For these observations, it might be helpful to consider the example shown above.</p>
<ul>
<li><p>In <span class="math inline">\(\texttt{quicksort}\)</span>, a comparison always involves a pivot and another key.</p></li>
<li><p>Since, the pivot is not sent as part of the input to a recursive call, a key is selected to be a pivot at most once.</p></li>
<li><p>Each key is selected to be pivot.</p></li>
</ul>
<p>Based on these observations, we conclude that each pair of keys is compared at most once. <a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>We need only the first two observations to establish this conclusion.<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</section>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
Before we get to the analysis, let's observe some properties of
$\cd{quicksort}$.  For these observations, it might be helpful to consider the
example shown above.  
\begin{itemize}
\item In $\cd{quicksort}$, a comparison always involves a
pivot and another key.  

\item Since, the pivot is not sent as part of the input to a recursive
  call, a key is selected to be a pivot at most once.

\item Each key is selected to be pivot.
 
\end{itemize}

Based on these observations, we conclude that each pair of keys is
compared at most once.
%
\footnote{We need only the first two observations to establish this
  conclusion.}
%

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR8
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC4:UN9:GR8
\end{dilLabel}
\begin{dilNo}
8
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
Expected Work for Quick Sort
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Expected Work for Quick Sort
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR8:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC4:UN9:GR8:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>We are now ready to analyze the expected work of randomized <span class="math inline">\(\texttt{quicksort}\)</span> by counting the number of comparisons that it performs.</p>
<p>We define the random variable <span class="math inline">\(Y(n)\)</span> as the number of comparisons <span class="math inline">\(\texttt{quicksort}\)</span> makes on input of size <span class="math inline">\(n\)</span>. For the analysis, we will find an upper bound on <span class="math inline">\(\mathbf{E}\left[{Y(n)}\right]\)</span>. In particular we will show it is in <span class="math inline">\(O(n \log n)\)</span>. <span class="math inline">\(\mathbf{E}\left[{Y(n)}\right]\)</span> will not depend on the order of the input sequence.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
We are now ready to analyze the expected work of randomized
$\cd{quicksort}$ by counting the number of comparisons
that it performs.
%

We define the random variable $Y(n)$ as the number of
comparisons~$\cd{quicksort}$~makes on input of size~$n$.  For the
analysis, we will find an upper bound on $\expct{Y(n)}$.
%
 In
particular we will show it is in $O(n \log n)$.
%
 $\expct{Y(n)}$ will not depend on the order of the input sequence.

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR9
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC4:UN9:GR9
\end{dilLabel}
\begin{dilNo}
9
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR9:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC4:UN9:GR9:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>Consider the final sorted order of the keys <span class="math inline">\(t = \texttt{sort}(a)\)</span> and for any element element <span class="math inline">\(t_i\)</span>, let <span class="math inline">\(p_i\)</span> denote the priority of <span class="math inline">\(t_i\)</span>. Consider two positions <span class="math inline">\(i, j \in \{1, \dots, n\}\)</span> in the sequence <span class="math inline">\(t\)</span> and define following random variable <span class="math display">\[\begin{aligned}
X_{ij}  
% 
&amp; = &amp; 
% 
\left\{\begin{array}{ll} 
1 &amp; \text{if}~t_i~\text{and}~t_j~\text{are compared by} \texttt{quicksort} 
\\ 
0 &amp; \mbox{otherwise} 
\end{array}\right. \end{aligned}\]</span></p>
<p>Because in any run of <span class="math inline">\(\texttt{quicksort}\)</span>, each pair of keys is compared at most once, <span class="math inline">\(Y(n)\)</span> is equal to the sum of all <span class="math inline">\(X_{ij}\)</span>’s, i.e., <span class="math display">\[Y(n) \;\; \leq \;\; \sum_{i=1}^n \sum_{j=i+1}^n X_{ij}\]</span></p>
<p>We only consider the case that <span class="math inline">\(i &lt; j\)</span> since we only want to count each comparison once. By linearity of expectation, we have <span class="math display">\[\mathbf{E}\left[{Y(n)}\right] \leq \sum_{i=1}^n \sum_{j=i+1}^n \mathbf{E}\left[{X_{ij}}\right]\]</span> Since each <span class="math inline">\(X_{ij}\)</span> is an indicator random variable, <span class="math inline">\(\mathbf{E}\left[{X_{ij}}\right] = \mathbf{P}\left[{X_{ij} = 1}\right]\)</span>.</p>
<p>The drawing below illustrates the possible relationships between the selected pivot <span class="math inline">\(p\)</span>, <span class="math inline">\(t_i\)</span> and <span class="math inline">\(t_j\)</span>.</p>
<p><img src="./media/qsort-cases.jpg" alt="image" style="width:3in" /></p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
Consider the final sorted order of the keys $t = \cd{sort}(a)$
%
and
%
for any element element $t_i$, let $p_i$ denote the  priority of $t_i$.
%
Consider two positions $i, j \in \{1, \dots, n\}$ in the sequence $t$
and define following random variable
%
\begin{eqnarray*}
X_{ij} 
%
& = &
%
\left\{\begin{array}{ll}
1 & \text{if}~t_i~\text{and}~t_j~\text{are compared by} \cd{quicksort}
\\
0 & \mbox{otherwise}
\end{array}\right.
\end{eqnarray*}
%

Because in any run of $\cd{quicksort}$, each pair of keys is compared at most
once, $Y(n)$ is equal to the sum of all $X_{ij}$'s, i.e.,
\[
Y(n) \;\; \leq \;\; \sum_{i=1}^n \sum_{j=i+1}^n X_{ij}
\]


We only consider the case that $i < j$ since we only want to count
each comparison once. 
%
By linearity of expectation, we have
\[
 \expct{Y(n)} \leq \sum_{i=1}^n \sum_{j=i+1}^n \expct{X_{ij}}
\]
%
Since each $X_{ij}$ is an indicator random variable,
$\expct{X_{ij}} = \prob{X_{ij} = 1}$.  


The drawing below illustrates the possible relationships between the
selected pivot $p$, $t_i$ and $t_j$.
\begin{center}
\includegraphics[width=3in]{./media/qsort-cases.jpg}
%\label{fig:lec18::cases}
\end{center}

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR10
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC4:UN9:GR10
\end{dilLabel}
\begin{dilNo}
10
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR10:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC4:UN9:GR10:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>To compute the probability that <span class="math inline">\(t_i\)</span> and <span class="math inline">\(t_j\)</span> are compared (i.e., <span class="math inline">\(\mathbf{P}\left[{X_{ij}=1}\right]\)</span>), let’s take a closer look at the <span class="math inline">\(\texttt{quicksort}\)</span> algorithm to gather some intuitions.</p>
<p>Notice that the first recursive call takes as its pivot <span class="math inline">\(p\)</span> the element with highest priority. Then, it splits the sequence into two parts, one with keys larger than <span class="math inline">\(p\)</span> and the other with keys smaller than <span class="math inline">\(p\)</span>. For each of these parts, we run <span class="math inline">\(\texttt{quicksort}\)</span> recursively; therefore, inside it, the algorithm will pick the highest priority element as the pivot, which is then used to split the sequence further.</p>
<p>For any one call to <span class="math inline">\(\texttt{quicksort}\)</span> there are three possibilities (as illustrated in the drawing above) for <span class="math inline">\(X_{ij}\)</span>, where <span class="math inline">\(i &lt; j\)</span>:</p>
<ul>
<li><p>The pivot (highest priority element) is either <span class="math inline">\(t_i\)</span> or <span class="math inline">\(t_j\)</span>, in which case <span class="math inline">\(t_i\)</span> and <span class="math inline">\(t_j\)</span> are compared and <span class="math inline">\(X_{ij} = 1\)</span>.</p></li>
<li><p>The pivot is element between <span class="math inline">\(t_i\)</span> and <span class="math inline">\(t_j\)</span>, in which case <span class="math inline">\(t_i\)</span> is in <span class="math inline">\(a_1\)</span> and <span class="math inline">\(t_j\)</span> is in <span class="math inline">\(a_3\)</span> and <span class="math inline">\(t_i\)</span> and <span class="math inline">\(t_j\)</span> will never be compared and <span class="math inline">\(X_{ij} = 0\)</span>.</p></li>
<li><p>The pivot is less than <span class="math inline">\(t_i\)</span> or greater than <span class="math inline">\(t_j\)</span>. Then <span class="math inline">\(t_i\)</span> and <span class="math inline">\(t_j\)</span> are either both in <span class="math inline">\(a_1\)</span> or both in <span class="math inline">\(a_3\)</span>, respectively. Whether <span class="math inline">\(t_i\)</span> and <span class="math inline">\(t_j\)</span> are compared will be determined in some later recursive call to <span class="math inline">\(\texttt{quicksort}\)</span>.</p></li>
</ul>
<p>With this intuition in mind, we can establish the following lemma.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
To compute the probability that $t_i$ and $t_j$ are compared (i.e.,
$\prob{X_{ij}=1}$), 
%
let's take a closer look at the $\cd{quicksort}$ algorithm to gather some
intuitions.  
%

Notice that the first recursive call takes as its pivot $p$ the
element with highest priority.  
%
Then, it splits the sequence into two parts, one with keys larger than
$p$ and the other with keys smaller than $p$.  For each of these
parts, we run $\cd{quicksort}$ recursively; therefore, inside it, the
algorithm will pick the highest priority element as the pivot, which
is then used to split the sequence further.

For any one call to $\cd{quicksort}$ there are three possibilities
(as illustrated in the drawing above)
%(illustrated in Figure~\ref{fig:lec18::cases})
for $X_{ij}$, where $i < j$:
\begin{itemize}
\item The pivot (highest priority element) is either $t_i$ or $t_j$,
  in which case $t_i$ and $t_j$ are compared and $X_{ij} = 1$.
\item The pivot is element between $t_i$ and $t_j$, in which case
  $t_i$ is in $a_1$ and $t_j$ is in $a_3$ and $t_i$ and $t_j$ will
  never be compared and $X_{ij} = 0$.
\item The pivot is less than $t_i$ or greater than $t_j$.  Then 
  $t_i$ and $t_j$ are either both in $a_1$ or both in $a_3$,
  respectively.  Whether $t_i$ and $t_j$ are compared will be
  determined in some later recursive call to $\cd{quicksort}$.
\end{itemize}

With this intuition in mind, we can establish the following lemma.

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR11
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC4:UN9:GR11
\end{dilLabel}
\begin{dilNo}
11
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomLemma}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR11:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC4:UN9:GR11:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>For <span class="math inline">\(i &lt; j\)</span>, <span class="math inline">\(t_i\)</span> and <span class="math inline">\(t_j\)</span> are compared if and only if <span class="math inline">\(p_i\)</span> or <span class="math inline">\(p_j\)</span> has the highest priority among <span class="math inline">\(\{p_i, p_{i+1},\dots, p_j\}\)</span>.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
For $i < j$, $t_i$ and $t_j$ are compared if and only if $p_i$ or
  $p_j$ has the highest priority among $\{p_i, p_{i+1},\dots, p_j\}$.

\end{dilFieldBodyDex}
\end{dilAtomLemma}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomProof}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR11:AT2
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC4:UN9:GR11:AT2
\end{dilLabel}
\begin{dilNo}
2
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>Assume first that <span class="math inline">\(t_i\)</span> (<span class="math inline">\(t_j\)</span>) has the highest priority. In this case, all the elements in the subsequence <span class="math inline">\(t_i \ldots t_j\)</span> will move together in the pivot tree until <span class="math inline">\(t_i\)</span> (<span class="math inline">\(t_j\)</span>) is selected as pivot. When it is selected as pivot, <span class="math inline">\(t_i\)</span> and <span class="math inline">\(t_j\)</span> will be compared. This proves the first half of the claim.</p>
<p>For the second half, assume that <span class="math inline">\(t_i\)</span> and <span class="math inline">\(t_j\)</span> are compared. For the purposes of contradiction, assume that there is a key <span class="math inline">\(t_k\)</span>, <span class="math inline">\(i &lt; 
k &lt; j\)</span> with a higher priority between them. In any collection of keys that include <span class="math inline">\(t_i\)</span> and <span class="math inline">\(t_j\)</span>, <span class="math inline">\(t_k\)</span> will become a pivot before either of them. Since <span class="math inline">\(t_i \leq t_k \leq t_j\)</span> it will separate <span class="math inline">\(t_i\)</span> and <span class="math inline">\(t_j\)</span> into different buckets, so they are never compared. This is a contradiction; thus we conclude there is no such <span class="math inline">\(t_k\)</span>.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
Assume first that $t_i$ ($t_j$) has the highest priority.  In this
case, all the elements in the subsequence $t_i \ldots t_j$ will move
together in the pivot tree until $t_i$ ($t_j$) is selected as pivot.
%
When it is selected as pivot, $t_i$ and $t_j$ will be compared. 
%
This proves the first half of the claim.

For the second half, assume that $t_i$ and $t_j$ are compared.  For
the purposes of contradiction, assume that there is a key $t_k$, $i <
k < j$ with a higher priority between them.  In any collection of keys
that include $t_i$ and $t_j$, $t_k$ will become a pivot before either
of them.  Since $t_i \leq t_k \leq t_j$ it will separate $t_i$ and
$t_j$ into different buckets, so they are never compared.  This is a
contradiction; thus we conclude there is no such $t_k$.

\end{dilFieldBodyDex}
\end{dilAtomProof}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR12
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC4:UN9:GR12
\end{dilLabel}
\begin{dilNo}
12
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR12:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC4:UN9:GR12:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>Therefore, for <span class="math inline">\(t_i\)</span> and <span class="math inline">\(t_j\)</span> to be compared, <span class="math inline">\(p_i\)</span> or <span class="math inline">\(p_j\)</span> has to be bigger than all the priorities in between. Since there are <span class="math inline">\(j-i+1\)</span> possible keys in between (including both <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>) and each has equal probability of being the highest, the probability that either <span class="math inline">\(i\)</span> or <span class="math inline">\(j\)</span> is the highest is <span class="math inline">\(2/(j-i+1)\)</span>. Therefore, <span class="math display">\[\begin{aligned}
  \mathbf{E}\left[{X_{ij}}\right] &amp;= \mathbf{P}\left[{X_{ij} = 1}\right] \\ 
  &amp;= \mathbf{P}\left[{p_i \text{ or } p_j \text{ is the maximum among } \{p_i, \dots, p_j\}}\right] \\ 
  &amp;= \frac{2}{j - i + 1}. \end{aligned}\]</span></p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
Therefore, for $t_i$ and $t_j$ to be compared, $p_i$ or $p_j$ has to
be bigger than all the priorities in between.  
%
Since there are $j-i+1$ possible keys in between (including both $i$
and $j$) and each has equal probability of being the highest, the
probability that either $i$ or $j$ is the highest is $2/(j-i+1)$.
%
Therefore,
%
\begin{align*}
  \expct{X_{ij}} &= \prob{X_{ij} = 1} \\
  &= \prob{p_i \text{ or } p_j \text{ is the maximum among } \{p_i, \dots, p_j\}} \\
  &= \frac{2}{j - i + 1}.
\end{align*}

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR13
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC4:UN9:GR13
\end{dilLabel}
\begin{dilNo}
13
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR13:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC4:UN9:GR13:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>We can write the expected number of comparisons made in randomized <span class="math inline">\(\texttt{quicksort}\)</span> is <span class="math display">\[\begin{aligned}
  \mathbf{E}\left[{Y(n)}\right] &amp;\leq \sum_{i=1}^{n-1} \sum_{j=i+1}^n \mathbf{E}\left[{X_{ij}}\right] 
\\ 
  &amp;= \sum_{i=1}^{n-1}  \sum_{j=i+1}^n \frac{2}{j-i+1} 
\\ 
  &amp;= \sum_{i=1}^{n-1}  \sum_{k=2}^{n-i+1} \frac{2}{k}  
\\ 
  &amp; \leq 2n \sum_{i=1}^{n-1}  H_n  
\\ 
  &amp;= 2n H_n \in O(n\log n). \end{aligned}\]</span> Note that in the derivation of the asymptotic bound, we used the fact that <span class="math inline">\(H_n = \ln{n} + O(1)\)</span>.</p>
<p>Indirectly, we have also shown that the average work for the basic deterministic <span class="math inline">\(\texttt{quicksort}\)</span> (always pick the first element) is also <span class="math inline">\(O(n 
\log n)\)</span>. Just shuffle the data randomly and then apply the basic <span class="math inline">\(\texttt{quicksort}\)</span> algorithm. Since shuffling the input randomly results in the same input as picking random priorities and then reordering the data so that the priorities are in decreasing order, the basic <span class="math inline">\(\texttt{quicksort}\)</span> on that shuffled input does the same operations as randomized <span class="math inline">\(\texttt{quicksort}\)</span> on the input in the original order. Thus, if we averaged over all permutations of the input the work for the basic <span class="math inline">\(\texttt{quicksort}\)</span> is <span class="math inline">\(O(n 
\log n)\)</span> on average.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
We can write the expected number of comparisons made in randomized
$\cd{quicksort}$ is
\begin{align*}
  \expct{Y(n)} &\leq \sum_{i=1}^{n-1} \sum_{j=i+1}^n \expct{X_{ij}}
\\
  &= \sum_{i=1}^{n-1}  \sum_{j=i+1}^n \frac{2}{j-i+1}
\\
  &= \sum_{i=1}^{n-1}  \sum_{k=2}^{n-i+1} \frac{2}{k} 
\\
  & \leq 2n \sum_{i=1}^{n-1}  H_n 
\\
  &= 2n H_n \in O(n\log n).
\end{align*}
Note that in the derivation of the asymptotic bound, we used the fact
that $H_n = \ln{n} + O(1)$.

Indirectly, we have also shown that the average work for the basic
deterministic $\cd{quicksort}$ (always pick the first element) is also $O(n
\log n)$.  Just shuffle the data randomly and then apply the basic
$\cd{quicksort}$ algorithm.  Since shuffling the input randomly results in the
same input as picking random priorities and then reordering the data
so that the priorities are in decreasing order, the basic $\cd{quicksort}$ on
that shuffled input does the same operations as randomized $\cd{quicksort}$
on the input in the original order.  Thus, if we averaged over all
permutations of the input the work for the basic $\cd{quicksort}$ is $O(n
\log n)$ on average.

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR14
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC4:UN9:GR14
\end{dilLabel}
\begin{dilNo}
14
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomRemark}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR14:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC4:UN9:GR14:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>The bound <span class="math display">\[\mathbf{E}\left[{X_{ij}}\right] = \frac{2}{j - i + 1}\]</span> indicates that the closer two keys are in the sorted order (<span class="math inline">\(t\)</span>) the more likely it is that they are compared. For example, the keys <span class="math inline">\(t_i\)</span> is compared to <span class="math inline">\(t_{i+1}\)</span> with probability 1. It is easy to understand why if we consider the corresponding pivot tree.</p>
<p>One of <span class="math inline">\(t_i\)</span> and <span class="math inline">\(t_{i+1}\)</span> must be an ancestor of the other in the pivot tree: there is no other element that could be the root of a subtree that has <span class="math inline">\(t_i\)</span> in its left subtree and <span class="math inline">\(t_{i+1}\)</span> in its right subtree. Regardless, <span class="math inline">\(t_i\)</span> and <span class="math inline">\(t_{i+1}\)</span> will be compared.</p>
<p>If we consider <span class="math inline">\(t_{i}\)</span> and <span class="math inline">\(t_{i+2}\)</span> there could be such an element, namely <span class="math inline">\(t_{i+1}\)</span>, which could have <span class="math inline">\(t_i\)</span> in its left subtree and <span class="math inline">\(t_{i+2}\)</span> in its right subtree. That is, with probability <span class="math inline">\(1/3\)</span>, <span class="math inline">\(t_{i+1}\)</span> has the highest probability of the three and <span class="math inline">\(t_i\)</span> is not compared to <span class="math inline">\(t_{i+2}\)</span>, and with probability <span class="math inline">\(2/3\)</span> one of <span class="math inline">\(t_i\)</span> and <span class="math inline">\(t_{i+2}\)</span> has the highest probability and, the two are compared.</p>
<p>In general, the probability of two elements being compared is inversely proportional to the number of elements between them when sorted. The further apart the less likely they will be compared. Analogously, the further apart the less likely one will be the ancestor of the other in the related pivot tree.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
The bound
\[
  \expct{X_{ij}} = \frac{2}{j - i + 1}
\]
indicates that the closer two keys are in the sorted order ($t$) the
more likely it is that they are compared.  For example, the keys $t_i$
is compared to $t_{i+1}$ with probability~1. It is easy to understand
why if we consider the corresponding pivot tree.
%

One of $t_i$ and $t_{i+1}$ must be an ancestor of the other in the
pivot tree: there is no other element that could be the root of a
subtree that has $t_i$ in its left subtree and $t_{i+1}$ in its right
subtree.
%
Regardless, $t_i$ and $t_{i+1}$ will be compared.

If we consider $t_{i}$ and $t_{i+2}$ there could be such an element,
namely $t_{i+1}$, which could have $t_i$ in its left subtree and
$t_{i+2}$ in its right subtree. That is, with probability $1/3$,
$t_{i+1}$ has the highest probability of the three and $t_i$ is not
compared to $t_{i+2}$, and with probability $2/3$ one of $t_i$ and
$t_{i+2}$ has the highest probability and, the two are compared.  

In general, the probability of two elements being compared is
inversely proportional to the number of elements between them when
sorted.  The further apart the less likely they will be compared.
Analogously, the further apart the less likely one will be the
ancestor of the other in the related pivot tree.

\end{dilFieldBodyDex}
\end{dilAtomRemark}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR15
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC4:UN9:GR15
\end{dilLabel}
\begin{dilNo}
15
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
Analysis of Quick Sort: Span
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Analysis of Quick Sort: Span
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR15:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC4:UN9:GR15:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>We now analyze the span of <span class="math inline">\(\texttt{quicksort}\)</span>. All we really need to calculate is the depth of the pivot tree, since each level of the tree has span <span class="math inline">\(O(\log n)\)</span>—needed for the filter. We argue that the depth of the pivot tree is <span class="math inline">\(O(\log n)\)</span> by relating it to the number of contraction steps of the randomized <span class="math inline">\(\texttt{select}\)</span> we considered earlier. We refer to the <span class="math inline">\(i^{th}\)</span> node of the pivot tree as the node corresponding to the <span class="math inline">\(i^{th}\)</span> smallest key. This is also the <span class="math inline">\(i^{th}\)</span> node in an in-order traversal.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
We now analyze the span of $\cd{quicksort}$.  All we really need to
calculate is the depth of the pivot tree, since each level of the tree
has span $O(\log n)$---needed for the filter.  We argue that the depth
of the pivot tree is $O(\log n)$ by relating it to the number of
contraction steps of the randomized $\cd{select}$ we considered
earlier.
%in \secref{section:15210:S18:CH11:SEC3:sec:randomized::select}.  
%
We refer to the $i^{th}$ node of
the pivot tree as the node corresponding to the $i^{th}$ smallest key.
This is also the $i^{th}$ node in an in-order traversal.

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR16
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC4:UN9:GR16
\end{dilLabel}
\begin{dilNo}
16
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomLemma}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR16:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC4:UN9:GR16:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>The path from the root to the <span class="math inline">\(i^{th}\)</span> node of the pivot tree is the same as the steps of <span class="math inline">\(\texttt{select}\)</span> on <span class="math inline">\(k = i\)</span>. That is to the say that the distribution of pivots selected along the path and the sizes of each problem is identical.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
The path from the root to the $i^{th}$ node of the pivot tree is the
same as the steps of $\cd{select}$ on $k = i$.  That is to the
say that the distribution of pivots selected along the path and the
sizes of each problem is identical.

\end{dilFieldBodyDex}
\end{dilAtomLemma}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomProof}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR16:AT2
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC4:UN9:GR16:AT2
\end{dilLabel}
\begin{dilNo}
2
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>Note that that <span class="math inline">\(\texttt{select}\)</span> is the same as <span class="math inline">\(\texttt{quicksort}\)</span> except that it only goes down one of the two recursive branches—the branch that contains the <span class="math inline">\(k^{th}\)</span> key.</p>
<p>Recall that for <span class="math inline">\(\texttt{select}\)</span>, we showed that the length of the path is more than <span class="math inline">\(10 \lg n\)</span> with probability at most <span class="math inline">\(1/n^{3.15}\)</span>. This means that the length of any path being longer that <span class="math inline">\(10\lg{n}\)</span> is tiny.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
Note that that $\cd{select}$ is the same as $\cd{quicksort}$ except that
it only goes down one of the two recursive branches---the branch that
contains the $k^{th}$ key.
%

Recall that for $\cd{select}$, we showed that the length of the
path is more than $10 \lg n$ with probability at most $1/n^{3.15}$.
%
This means that the length of any path being longer that $10\lg{n}$ is
tiny.
%
%% \begin{question}
%% Can we thus say that there can be no long paths.
%% \end{question}

\end{dilFieldBodyDex}
\end{dilAtomProof}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR17
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC4:UN9:GR17
\end{dilLabel}
\begin{dilNo}
17
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR17:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC4:UN9:GR17:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>This does not suffice to conclude, however, that there are no paths longer than <span class="math inline">\(10\lg{n}\)</span>, because there are many paths in the pivot tree, and because we only need one to be long to impact the span. Luckily, we don’t have too many paths to begin with. We can take advantage of this property by using the union bound, which says that the probability of the union of a collection of events is at most the sum of the probabilities of the events.</p>
<p>To apply the union bound, consider the event that the depth of a node along a path is larger <span class="math inline">\(10 \lg n\)</span>, which is <span class="math inline">\(1/n^{3.5}\)</span>. The total probability that any of the <span class="math inline">\(n\)</span> leaves have depth larger than <span class="math inline">\(10 \lg n\)</span> is <span class="math display">\[\mathbf{P}\left[{\text{depth of}~\texttt{quicksort}~\text{pivot tree}}\right] &gt; 10 \lg{n}  
\leq  
\frac{n}{n^{3.15}} = \frac{1}{n^{2.15}}.\]</span> We thus have our high probability bound on the depth of the pivot tree.</p>
<p>The overall span of randomized <span class="math inline">\(\texttt{quicksort}\)</span> is therefore <span class="math inline">\(O(\log^2 n)\)</span> with high probability. As in <span class="math inline">\(\texttt{select}\)</span>, we can establish an expected bound by using the Total Expectation Theorem.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
This does not suffice to conclude, however, that there are no paths
longer than $10\lg{n}$, because there are many paths in the pivot
tree, and because we only need one to be long to impact the span.
%
Luckily, we don't have too many paths to begin with.
%
We can take advantage of this property by using the union bound,
%
which
%
says that the probability of the union of a collection of events is at
most the sum of the probabilities of the events.
%

To apply the union bound, consider the event that the depth of a node
along a path is larger $10 \lg n$, which is $1/n^{3.5}$.
%
The total probability that any of the $n$ leaves have depth larger
than $10 \lg n$ is
\[
\prob{\text{depth of}~\cd{quicksort}~\text{pivot tree}} > 10 \lg{n} 
\leq 
\frac{n}{n^{3.15}} = \frac{1}{n^{2.15}}.
\]
%
We thus have our high probability bound on the depth of the pivot tree.

The overall span of randomized $\cd{quicksort}$ is therefore $O(\log^2 n)$
with high probability.
%
As in $\cd{select}$, we can establish an expected bound by using the
Total Expectation Theorem.
%

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR18
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC4:UN9:GR18
\end{dilLabel}
\begin{dilNo}
18
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomExercise}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR18:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC4:UN9:GR18:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>Complete the span analysis of proof by showing how to apply the Total Expectation Theorem.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
Complete the span analysis of proof by showing how to apply the Total
Expectation Theorem.

\end{dilFieldBodyDex}
\end{dilAtomExercise}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR19
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC4:UN9:GR19
\end{dilLabel}
\begin{dilNo}
19
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomTeachNote}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN9:GR19:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC4:UN9:GR19:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>Using the high probability bound, we can bound the expected span by using the total expectation theorem. For brevity let the random variable <span class="math inline">\(Y\)</span> be defined as <span class="math inline">\(Y = Y_{10\lg n}\)</span>, <span class="math display">\[\begin{array}{lll} 
\mathbf{E}\left[{S}\right] &amp; = &amp; \sum_{y}\mathbf{P}_{Y}(y) \mathbf{E}\left[{S{\:\mid\:}Y = y}\right]. 
\\ 
&amp; = &amp;  
\sum_{y \le 1}{\mathbf{P}_{Y}(y) \mathbf{E}\left[{S{\:\mid\:}Y = y}\right]} 
 +  
\sum_{y &gt;1}{\mathbf{P}_{Y}(y) \mathbf{E}\left[{S{\:\mid\:}Y = y}\right]} 
\\ 
&amp; \le &amp;  
(1 - n^{-2.5}) O(\lg^2{n})  
 +  
n^{-2.5} O(n^2) 
\\ 
&amp; = &amp; 
O(\lg^2{n}).  
\end{array}\]</span></p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
Using the high probability bound, we can bound the expected span by
using the total expectation theorem.
%
For brevity let the random variable $Y$ be defined as $Y = Y_{10\lg n}$,
%
\[
\begin{array}{lll}
\expct{S} & = & \sum_{y}\pmf{Y}(y) \expct{S \given Y = y}.
\\
& = & 
\sum_{y \le 1}{\pmf{Y}(y) \expct{S \given Y = y}}
 + 
\sum_{y >1}{\pmf{Y}(y) \expct{S \given Y = y}}
\\
& \le & 
(1 - n^{-2.5}) O(\lg^2{n}) 
 + 
n^{-2.5} O(n^2)
\\
& = &
O(\lg^2{n}). 
\end{array}
\]

\end{dilFieldBodyDex}
\end{dilAtomTeachNote}
\end{dilGroup}

\end{dilUnit}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilUnit}
\begin{dilFieldTitle}
Alternative Analysis of Quick Sort
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Alternative Analysis of Quick Sort
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN10
\end{dilFieldUnique}
\begin{dilLabel}
unit:15210:S18:CH11:SEC4:UN10
\end{dilLabel}
\begin{dilNo}
10
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN10:GR1
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC4:UN10:GR1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN10:GR1:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC4:UN10:GR1:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>Another way to analyze the work of <span class="math inline">\(\texttt{quicksort}\)</span> is to write a recurrence for the expected work (number of comparisons) directly. This is the approach taken by Tony Hoare in his original paper. For simplicity we assume there are no equal keys (equal keys just reduce the cost). The recurrence for the number of comparisons <span class="math inline">\(Y(n)\)</span> done by <span class="math inline">\(\texttt{quicksort}\)</span> is then: <span class="math display">\[\begin{aligned}
Y(n) &amp; =  Y(X(n)) + Y(n-X(n)-1) + n -1 \end{aligned}\]</span> where the random variable <span class="math inline">\(Y(n)\)</span> is the size of the set <span class="math inline">\(a_1\)</span> (we use <span class="math inline">\(X(n)\)</span> instead of <span class="math inline">\(Y_n\)</span> to avoid double subscripts). We can now write an equation for the expectation of <span class="math inline">\(X(n)\)</span>. <span class="math display">\[\begin{aligned}
\mathbf{E}\left[{Y(n)}\right] &amp; = \mathbf{E}\left[{Y(X(n)) + Y(n-X(n)-1) + n -1}\right]\\ 
             &amp; = \mathbf{E}\left[{Y(X(n))}\right] + \mathbf{E}\left[{Y(n-X(n)-1)}\right] + n -1\\ 
             &amp; = \frac{1}{n} \sum_{i=0}^{n-1}(\mathbf{E}\left[{Y(i)}\right] + \mathbf{E}\left[{Y(n-i-1)}\right]) + n -1 \end{aligned}\]</span> where the last equality arises since all positions of the pivot are equally likely, so we can just take the average over them. This can be by guessing the answer and using substitution. It gives the same result as our previous method. We leave this as exercise.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
Another way to analyze the work of $\cd{quicksort}$ is to write a recurrence
for the expected work (number of comparisons) directly.  This is the
approach taken by Tony Hoare in his original paper.  For simplicity we
assume there are no equal keys (equal keys just reduce the cost).  The
recurrence for the number of comparisons $Y(n)$ done by $\cd{quicksort}$ is
then:
%
\begin{align*}
Y(n) & =  Y(X(n)) + Y(n-X(n)-1) + n -1
\end{align*}
%
where the random variable $Y(n)$ is the size of the set $a_1$ (we
use $X(n)$ instead of $Y_n$ to avoid double subscripts).  We can now write
an equation for the expectation of $X(n)$.
%
\begin{align*}
\expct{Y(n)} & = \expct{Y(X(n)) + Y(n-X(n)-1) + n -1}\\
             & = \expct{Y(X(n))} + \expct{Y(n-X(n)-1)} + n -1\\
             & = \frac{1}{n} \sum_{i=0}^{n-1}(\expct{Y(i)} + \expct{Y(n-i-1)}) + n -1
\end{align*}
%
where the last equality arises since all positions of the pivot are
equally likely, so we can just take the average over them.  
%
This can be by guessing the answer and using substitution.  It gives
the same result as our previous method.  We leave this as exercise.

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN10:GR2
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC4:UN10:GR2
\end{dilLabel}
\begin{dilNo}
2
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomGram}
\begin{dilFieldTitle}
Span Analysis
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Span Analysis
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN10:GR2:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC4:UN10:GR2:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>We can use a similar strategy to analyze span. Recall that in randomized <span class="math inline">\(\texttt{quicksort}\)</span>, at each recursive call, we partition the input sequence <span class="math inline">\(a\)</span> of length <span class="math inline">\(n\)</span> into three subsequences <span class="math inline">\(a_1\)</span>, <span class="math inline">\(a_2\)</span>, and <span class="math inline">\(a_3\)</span>, such that elements in the subsequences are less than, equal, and greater than the pivot, respectfully. Let the random variable <span class="math inline">\(X(n) 
= \max\{|a_1|, |a_2|\}\)</span>, which is the size of larger subsequence.</p>
<p>The span of <span class="math inline">\(\texttt{quicksort}\)</span> is determined by the sizes of these larger subsequences. For ease of analysis, we will assume that <span class="math inline">\(|a_2| = 0\)</span>, as more equal elements will only decrease the span. As this partitioning uses <span class="math inline">\(\texttt{filter}\)</span> we have the following recurrence for span for input size <span class="math inline">\(n\)</span></p>
<p><span class="math display">\[S(n) = S(X(n)) + O(\log n).\]</span></p>
<p>For the analysis, we shall condition the span on the random variable denoting the size of the maximum half and apply the total expectation theorem.</p>
<p><span class="math display">\[\mathbf{E}\left[{S(n)}\right] = \sum_{m=n/2}^{n}{\mathbf{P}\left[{X(n)=m}\right] \cdot \mathbf{E}\left[{S(n){\:\mid\:}(X(n)=m)}\right]}.\]</span></p>
<p>The rest is algebra <span class="math display">\[\begin{aligned}
\mathbf{E}\left[{a_n}\right]  
&amp;=  \sum_{m=n/2}^{n}{\mathbf{P}\left[{M(n)=m}\right] \cdot \mathbf{E}\left[{S(n){\:\mid\:}(M(n)=m)}\right]} 
\\ 
&amp;\leq  
\mathbf{P}\left[{X(n) \leq \frac{3n}{4}}\right] \cdot \mathbf{E}\left[{S({\frac{3n}{4}})}\right] +  
\mathbf{P}\left[{X(n) &gt; \frac{3n}{4}}\right] \cdot \mathbf{E}\left[{S(n)}\right] + c\cdot \log n 
\\ 
&amp;\leq \frac{1}{2} \mathbf{E}\left[{S({\frac{3n}{4}})}\right] + \frac{1}{2} \mathbf{E}\left[{S(n)}\right] 
\\ 
&amp;\implies \mathbf{E}\left[{S(n)}\right] \leq \mathbf{E}\left[{S(\frac{3n}{4})}\right] + 2c \log n. 
\\ \end{aligned}\]</span> This is a recursion in <span class="math inline">\(\mathbf{E}\left[{S(\cdot)}\right]\)</span> and solves easily to <span class="math inline">\(\mathbf{E}\left[{S(n)}\right] = O(\log^2 n)\)</span>.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
We can use a similar strategy to analyze span.
%
Recall that in randomized $\cd{quicksort}$, at each recursive call, we
partition the input sequence $a$ of length $n$ into three subsequences
$a_1$, $a_2$, and $a_3$, such that elements in the subsequences are
less than, equal, and greater than the pivot, respectfully.  
%
Let the random variable $X(n)
= \max\{|a_1|, |a_2|\}$, which is the size of larger subsequence. 
%

The span of $\cd{quicksort}$ is determined by the sizes of these larger
subsequences. For ease of analysis, we will assume that $|a_2| = 0$, as
more equal elements will only decrease the span. As this partitioning
uses $\cd{filter}$ we have the following recurrence for span for input
size $n$

\[ 
S(n) = S(X(n)) + O(\log n). 
\]

For the analysis, we shall condition the span on the random variable
denoting the size of the maximum half and apply the total expectation
theorem.
%%
%% As we did for \sml{SmallestK}, we will bound $E[a_n]$ by considering
%% the $\prob{Y_n \leq 3n/4}$ and $\prob{Y_n > 3n/4}$ and use the maximum
%% sizes in the recurrence to upper bound $\expct{a_n}$. Now, the
%% $\prob{Y_n \leq 3n/4} = 1/2$, since half of the randomly chosen pivots
%% results in the larger partition to be at most $3n/4$ elements: Any
%% pivot in the range $T_{n/4}$ to $T_{3n/4}$ will do, where $T$ is the
%% sorted input sequence.

\[
\expct{S(n)} = \sum_{m=n/2}^{n}{\prob{X(n)=m} \cdot \expct{S(n) \given (X(n)=m)}}.
\]

The rest is algebra
\begin{align*}
\expct{a_n} 
&=  \sum_{m=n/2}^{n}{\prob{M(n)=m} \cdot \expct{S(n) \given (M(n)=m)}}
\\
&\leq 
\prob{X(n) \leq \frac{3n}{4}} \cdot \expct{S({\frac{3n}{4}})} + 
\prob{X(n) > \frac{3n}{4}} \cdot \expct{S(n)} + c\cdot \log n
\\
&\leq \frac{1}{2} \expct{S({\frac{3n}{4}})} + \frac{1}{2} \expct{S(n)}
\\
&\implies \expct{S(n)} \leq \expct{S(\frac{3n}{4})} + 2c \log n.
\\
\end{align*}
This is a recursion in $\expct{S(\cdot)}$ and solves easily to 
 $\expct{S(n)} = O(\log^2 n)$.

% That is, with probability $1/2$ we will be
% lucky and the subproblem size will go down by at least $3n/4$ and with
% probability $1/2$ we will be unlucky and we have to start again.  In
% the end,
% the expected span is twice what it would be if we could
% guarantee partition sizes of $n/4$ and $3n/4$.


% \newcommand{\sbar}{\overline{S}}
%  Let $\sbar(n)$ denote
% $\expct{S(n)}$. As we did for \sml{SmallestK} we will bound $\sbar(n)$
% by considering the $\prob{Y_n \leq 3n/4}$ and $\prob{Y_n > 3n/4}$ and use the maximum sizes in the recurrence to upper
% bound $\expct{S(n)}$. Now, the $\prob{Y_n \leq 3n/4} = 1/2$, since
% half of the randomly chosen pivots results in the larger partition
% to be at most $3n/4$ elements: Any pivot in the range $T_{n/4}$ to
% $T_{3n/4}$ will do, where $T$ is the sorted input sequence.

% So then, by the definition of expectation, we have
% \begin{align*}
%   \sbar(n) &= \leq \sum_{i} \prob{Y_n = i } \cdot \sbar(i) + c\log n \\
%   &\leq \prob{Y_n \leq \tfrac{3n}4} \sbar(\tfrac{3n}4) + \prob{Y_n >
%     \tfrac{3n}4} \sbar(n) + c\cdot \log n\\
%   &\leq \tfrac12 \sbar(\tfrac{3n}4) + \tfrac12 \sbar(n) + c\cdot \log n\\
%   &\implies (1- \tfrac12)\sbar(n) \leq \tfrac12 \sbar(\tfrac{3n}4) + c\log n \\
%   &\implies \sbar(n) \leq \sbar(\tfrac{3n}4) + 2c \log n,
% \end{align*}
% which we know is a balanced cost tree and solves to $O(\log^2 n)$.

% That is, with probability $1/2$ we will be
% lucky and the subproblem size will go down by at least $3n/4$ and with
% probability $1/2$ we will be unlucky and we have to start again.  In
% the end,
% the expected span is twice what it would be if we could
% guarantee partition sizes of $n/4$ and $3n/4$.

\end{dilFieldBodyDex}
\end{dilAtomGram}
\end{dilGroup}

\end{dilUnit}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilUnit}
\begin{dilFieldTitle}
Concluding Remarks
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
Concluding Remarks
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN11
\end{dilFieldUnique}
\begin{dilLabel}
unit:15210:S18:CH11:SEC4:UN11
\end{dilLabel}
\begin{dilNo}
11
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN11:GR1
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC4:UN11:GR1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomRemark}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN11:GR1:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC4:UN11:GR1:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>Quick sort is one of the earliest and most famous algorithms. It was invented and analyzed by Tony Hoare around 1960. This was before the big-O notation was used to analyze algorithms. Hoare invented the algorithm while an exchange student at Moscow State University while studying probability under Kolmogorov—one of the most famous researchers in probability theory. The analysis we will cover is different from what Hoare used in his original paper, although we will mention how he did the analysis. It is interesting that while Quick sort is often used as an quintessential example of a recursive algorithm, at the time, no programming language supported recursion and Hoare spent significant space in his paper explaining how to simulate recursion with a stack.</p>
<p>We note that our presentation of quick sort algorithm differs from Hoare’s original version which sequentially partitioned the input by using two fingers that moved from each end and by swapping two keys whenever a key was found on the left greater than the pivot and on the right less than the pivot.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
\Qsort{} is one of the earliest and most famous algorithms. It was
invented and analyzed by Tony Hoare around 1960.  This was before the
big-O notation was used to analyze algorithms.  Hoare invented the
algorithm while an exchange student at Moscow State University while
studying probability under Kolmogorov---one of the most famous
researchers in probability theory.  The analysis we will cover is
different from what Hoare used in his original paper, although we will
mention how he did the analysis.  It is interesting that while
\Qsort{} is often used as an quintessential example of a recursive
algorithm, at the time, no programming language supported recursion
and Hoare spent significant space in his paper explaining how to
simulate recursion with a stack.

We note that our presentation of \qsort{} algorithm 
%(as shown in \algref{randomized::qsort})
differs from Hoare's original version which
sequentially partitioned the input by using two fingers that moved
from each end and by swapping two keys whenever a key was found on the
left greater than the pivot and on the right less than the pivot.

\end{dilFieldBodyDex}
\end{dilAtomRemark}
\end{dilGroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilGroup}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN11:GR2
\end{dilFieldUnique}
\begin{dilLabel}
group:15210:S18:CH11:SEC4:UN11:GR2
\end{dilLabel}
\begin{dilNo}
2
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dilAtomRemark}
\begin{dilFieldTitle}
...NO.TITLE...
\end{dilFieldTitle}
\begin{dilFieldTitleDex}
...NO.TITLE...
\end{dilFieldTitleDex}
\begin{dilFieldUnique}
15210:S18:CH11:SEC4:UN11:GR2:AT1
\end{dilFieldUnique}
\begin{dilLabel}
atom:15210:S18:CH11:SEC4:UN11:GR2:AT1
\end{dilLabel}
\begin{dilNo}
1
\end{dilNo}
\begin{dilParents}
...NO.PARENTS...
\end{dilParents}
\begin{dilFieldBody}
<p>In later chapters we will see that the analysis of quick sort presented here is is effectively identical to the analysis of a certain type of balanced tree called Treaps. It is also the same as the analysis of “unbalanced” binary search trees under random insertion.</p>

\end{dilFieldBody}
\begin{dilFieldBodyDex}
In later chapters we will see that the analysis of \qsort{} presented
here is is effectively identical to the analysis of a certain type of
balanced tree called Treaps.  
%
It is also the same as the analysis of ``unbalanced'' binary search
trees under random insertion.

\end{dilFieldBodyDex}
\end{dilAtomRemark}
\end{dilGroup}

\end{dilUnit}
\end{dilSection}
\end{dilChapter}
\end{dilBook}
\end{dilCourse}
