This is a lecture outline as I thought in Spring 2018.
It should be added to the notes.

* Talk about offline scheduling as an abstract graph problem.

* Thus far in offline scheduling, we did not account for the cost of
finding the schedule.  How to do this? 

* We need to be able to talk about multiple "threads" of
execution. And talk about what they do.  For example, thread 1 starts
executing.  When it executes the root vertex, two new vertices become
ready.  Thread 1 puts these into a shared queue.  Thread 2 might at
this point access the shared queue to receive a new vertex.

* The algorithms of the sort described above are concurrent.  
  In this class so far we have not seen such algorithms.
  Our algorithms all accept a "sequential semantics" 
  We can reason about them as if they are sequential.
  Except for efficiency.  

* We will dive into writing such concurrent programs in a minute.
But first let's remark one more thing.  How can we implement the
offline (not the online yet) algorithm efficiently?  The challenge is
that we need to implement somehow the sharing of vertices.  

Puzzle: Any ideas??

It turns out that there are several techniques

**: keep a central pool of data structure.  How should this data structure operate? 
   it should allow multiple "threads" to operate on it at the same time.

   for example, as one thread inserts a vertex, another one might remove a vertex.
   or multiple threadls may remove vertices at the same time.

   We call data structures like this as _concurrent data structures_.
   As we shall see they can be quite tricky to design.

**. It turns out keeping a centralized pool like this does not work
well because no matter what we do, it ends be being a bottleneck once
we have more than a handful of threads.
 
   A better approach turns out to be use distributed queues...

   Doubly ended queues.

   Still concurrent data structures...
   
* Relevance of the scheduling problem? 
 
   What is this DAG (graph) and what is scheduling about

   It turns out parallel programs are dags and scheduling is what we
   want to do when we want to execut a parallel program.

**. Examples of parallel programs as dags.
   Fork join programs. 
   Recursive reduce. 

   
* But we don't know the DAG of a parallel program.  The proble is
circular. Enter online scheduling algorithms. This is not that
different than scheduling a DAG it turns out. 

# How do we write concurrent programs?

  There are many ways.  

  In this class, we will explore a standard interface: POSIX Threads
  or pthroads.

  THis is a standard threading API.

  Allows creating threads as well as many kinds of operations for
  threads to synchronize (wait for each other).

* Give the example code in tapp/code/simple/threads-allwrite.cppp
  Mention non-determinism. 

  Race condition.  All threads are writing into a shared resource STDIO.
  Must control it if we want detererministic behavior.

  __Cricitical section:__ A part of the code that must be executed by
  at most one thread at a time.

  Critical sections requile __mutual exclusion__ (one thread at a time).

* How to unsure mutual exclusion?
 
  Use synchorinzation operations.  There are many sorts. Examples include
 
  ** locks (a.k.a., mutex): spin locks and blocking locks
  ** semaphores
  ** monitors
  ** condition variables
  ** nonblockig read-modify-write operations.


* In this course: non-blocking read-modify write operations.

  RMW (location, f(location, v)): atomically replace the value at location
  with f(location, v)

  Fetch-And-Add: f (location, v) = *location + v

  Compare-And-Swap f(location, (a,b)) = if *location = a then b else a
  Return true if the swap happened otherwise false.

* In C++ you have what is known as _atomic_ class that supports atomic operations.
  Use mutex/cas-driver.cpp

* You can use compare and exchange to implement locks
  use mutex/biglock-driver.cpp

* Such big locks are not always the best way because they serialize
  operations.  Creating a bottleneck.  Imagine implementing our
  scheduler with a big lock.  Every access to the shared pool data
  sturcutre will serialize and there will be no benefit to parallelism. 

* Let's dive into this example.  Let's assume that our shared data
  structure is a stack.

* Run through stack example.
  Use stack-int.h, nb-stack-int-aba.h, nb-stack-int.h

  
SECOND LECTURE