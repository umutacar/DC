\documentclass{course}
\title{Parallel and Sequential Algorithms}

% Course number must be unique in the database
\coursenumber{15210}

\semester{Spring 2018}
\picture{/210/course/air-pavilion.jpg}
\website{http://www.cs.cmu.edu/~15210}

% Provides book
% This must be provided
% The name should be relative to course number.
\providesbook{S18}

% Start counting chapters from 
% This is optional. Will start counting at 1.
\provideschapter{101}
\providessection{1}

15-210 aims to teach methods for designing, analyzing, and programming
sequential and parallel algorithms and data structures. The emphasis
is on teaching fundamental concepts applicable across a wide variety
of problem domains, and transferable across a reasonably broad set of
programming languages and computer architectures. This course also
includes a significant programming component in which students will
program concrete examples from domains such as engineering, scientific
computing, graphics, data mining, and information retrieval (web
search).

Unlike a traditional introduction to algorithms and data structures,
this course puts an emphasis on parallel thinking â€” i.e., thinking
about how algorithms can do multiple things at once instead of one at
a time. The course follows up on material learned in 15-122 and 15-150
but goes into significantly more depth on algorithmic issues. 


\begin{book}
\title{Algorithm Design: Parallel and Sequential}
\authors{Umut A. Acar and Guy Blelloch}

\begin{chapter}[Graph Contraction]
\label{ch:graph-search}

\picture{./media/stary-night.jpg}


In earlier chapters, we have mostly covered techniques for solving
problems on graphs that were developed in the context of sequential
algorithms.  Some of them are easy to parallelize while others are
not.  For example, we saw that BFS has some parallelism since each
level can be explored in parallel, but there was no parallelism in
DFS.
%% Umut: this is unclear to me also sounds forced.  We are ok with 
%% sequential algorithms anyway.  We want to cover them.
%%
% \footnote{In
%   reality, there is parallelism in DFS when graphs are dense---in
%   particular, although vertices need to visited sequentially, with
%   some care, the edges
%   can be processed in parallel.}  
%%
There was limited parallelism in Dijkstra's algorithm, but there was
plenty of parallelism in the Bellman-Ford algorithm.
%
In this chapter we cover a technique called ``graph contraction''
that was specifically designed to be used in parallel algorithms and
that allows obtaining poly-logarithmic span for certain graph problems.

\begin{section}[Preliminaries]

\begin{unit}

\begin{teachnote}
  Please review the material on graphs (\secref{graph-theory}) before
  proceeding with the rest of this chapter, especially the sections on
  subgraphs and connectivity.
\end{teachnote}



\begin{gram}
This chapter relies on graph terminology presented is an
earlier chapter:
%
\url{http://diderot-cmu.appspot.com/chapters/10/#anchor-section-17}
%

%% We review here the definition of graph partition.
%% %
%% Recall that a partition of a set $A$ is a set $P$ of non-empty subsets
%% of $A$ such that each element of $A$ is in exactly one subset $B \in
%% P$; each susbet $B \in P$ is called a~\defn{part} or~\defn{block.}
%% %
One important concept used in this chapter is the notion of graph
partition, which is a collection of graphs defined as vertex-induced
graphs of a set partition of its vertices.
\end{gram}

\begin{definition}
Given a graph $G$, 
%
a~\defn{graph partition} of $G$ is a collection of graphs 
%
$H_0 = (V_0, E_0), \ldots, H_{k-1} = (V_{k_1}, E_{k-1})$
%
such that 
%
% $\bigcup_{i=0}{V_i} =V$, $\forall i. 0 \le i,j < k, V_i \not= \emptyset$
% and  $i \not= j \implies V_i \cap V_j = \emptyset$, 
%
$\{V_0, \ldots, V_{k-1}\}$ is a set partition of $V$
and 
%
$H_0, \ldots, H_{k-1}$ 
%
are vertex-induced subgraphs of $G$ with respect to $V_0, \ldots, V_{k-1}$.
%
Each subgraph $H_i$ is called a~\defn{block} or~\defn{part} of $G$.
\end{definition}
%


\begin{gram}[Internal and Cut Edges]
In a graph partition, we can distinguish between two kinds of edges:
internal edges and cut edges. 
%
We call an edge $\{v_1,v_2\}$ an~\defn{internal edge}, if $v_1\in V_i$
and $v_2 \in V_i$, and by construction $\{v_1,v_2\} \in E_i$.
%
We call an edge $\{v_1,v_2\}$ a~\defn{cut edge}, if $v_1\in V_i$
and $v_2 \in V_j$  and $i \not= j$.
%
One way to partition a graph is to make each connected component a
block. In such a partition, there are no cut edges between the
partitions.
\end{gram}

\begin{definition}[Partition Map]
Sometimes it is useful to give a name or a label to each block in a
graph partition.  
%
A graph partition can then be described as a set of labels for the
blocks and~\defn{partition map} that maps each vertex to the label of
its block.  
%
The labels can be chosen arbitrarily but sometimes it is conceptually
and computationally easier to use a vertex inside a block as a name
(representative) for that block.
%
\end{definition}

\begin{example}
\label{ex:preliminaries::partition1}
The partition 
%
$\cset{\cset{\vname{a},\vname{b},\vname{c}},\cset{\vname{d}},\cset{\vname{e},\vname{f}}}$
%
of the vertices
%
$\cset{\vname{a},\vname{b},\vname{c},\vname{d},\vname{e},\vname{f}}$, 
%
defines three blocks as vertex-induced subgraphs.
\begin{center}
  \includegraphics[width=5in]{./media/contract-example8.jpg}
\end{center}
The edges $\cset{\vname{a},\vname{b}}$, $\cset{\vname{a},\vname{c}}$,
and $\cset{\vname{e},\vname{f}}$ are internal edges, and the edges
$\cset{\vname{c},\vname{d}}$, $\cset{\vname{b},\vname{d}}$,
$\cset{\vname{b},\vname{e}}$ and $\cset{\vname{d},\vname{f}}$ are
cut edges.

By labeling the blocks $\cstr{abc},\cstr{d}$ and
$\cstr{ef}$, we can specify the graph partition with following partition map:
\\
%
$ (\cset{\vname{abc}, \vname{d}, \vname{ef}}, ~\cset{\vname{a} \mapsto
  \vname{abc}, \vname{b} \mapsto \vname{abc}, \vname{c} \mapsto
  \vname{abc}, \vname{d} \mapsto \vname{d}, \vname{e} \mapsto
  \vname{ef}, \vname{f} \mapsto \vname{ef}}).$

Instead of assigning a fresh label to each block, we can choose a
representative vertex.
%
For example, by picking $\vname{a}, \vname{d}$, and $\vname{c}$ as
representatives, we can represent the partition above using the
following partition map
\[
(\cset{\vname{a},\vname{d},\vname{e}}, 
 \cset{\vname{a} \mapsto \vname{a}, \vname{b} \mapsto \vname{a}, 
       \vname{c} \mapsto \vname{a}, \vname{d} \mapsto \vname{d}, 
       \vname{e} \mapsto \vname{e}, \vname{f} \mapsto \vname{e}}).
\]

\end{example}
\end{unit}
\end{section}

\begin{section}[Graph Contraction]

\begin{unit}
\begin{gram}
Graph contraction is a technique for computing properties of graphs in
parallel.  
%
As a contraction technique, it is used to solve a problem
instance by reducing it to a smaller instance of the same problem.
%
%% \begin{teachask}
%% Can we solve graph problems using divide-and-conquer? 
%% \end{teachask}
%
Graph contraction is an important technique, because
divide-and-conquer can be difficult to apply in graph problems
efficiently.  
%
Divide-and-conquer techniques usually require partitioning graphs into
smaller graphs in a balanced fashion such that the number of cut edges
is minimized.  
%
Because graphs can be highly irregular, however, they can be difficult
to partition. In fact, graph partitioning problems are typically
NP-hard.
%
\end{gram}

\begin{gram}[Quotient Graph]
The key idea behind graph contraction is to contract the input graph
to a smaller~\defn{quotient graph}, solve the problem on the quotient
graph, and then use that solution to construct the solution for the
input graph.  
%
We can specify this technique as an inductive algorithm-design
technique (\dtref{gc::gc-technique}) as follows.
\end{gram}

\begin{gram}[Graph Contraction Design Technique]
\label{dt:gc::gc-technique}
\begin{description}
\item[Base case:] If the graph is small (e.g., it has no edges), then compute
  the desired result.
\item[Inductive case:]~
\begin{itemize}
\item Contract the graph into a smaller quotient graph.
\begin{itemize}
\item Partition the graph into blocks.
\item Contract each block to a single super-vertex.
\item Drop internal edges.
\item Reroute cut edges to corresponding super-vertices.  
\end{itemize}

\item Recursively solve the problem for the quotient graph.
\item By using the result for the quotient graph, compute the result
  for the input graph.
\end{itemize}
\end{description}
\end{gram}


% \begin{simpleexample} [An example graph contraction]

%   An illustration of how we wish to apply graph contraction to solve
%   the connectivity problem.  In order to solve the problem, as we
%   contract the graph, we do not alter the connectivity of the
%   vertices: vertices that are not connected should not become
%   connected and vice versa.

% \begin{center}
% \includegraphics[width=1.5in]{./media/contraction-example.jpg}
% \end{center}
% \end{simpleexample}


\begin{gram}
The key step of graph contraction is the construction of the quotient.
%
To this end, we partition the graph and construct a quotient
graph, where each block in the partition is represented by a
vertex in the quotient graph.
%
We can construct the quotient graph by creating a~\defn{super-vertex} for each partition
%
We then consider each edge $(u,v)$ in the graph.  If the edge is an internal
edge, then we skip it.
%
Otherwise,  we create a new edge between the
super-vertices representing the blocks containing $u$ and $v$.
%
Since there can be many
cut edges between two blocks, we may create multiple edges between
two super-vertices.  We can remove such edges or leave them in the graph, in
which case we would be working with multigraphs.  In this chapter, we
shall remove duplicate edges, because this is simpler for our
purposes.
%
The process of identifying a partition and updating the edges is
called a~\defn{round of graph contraction}. 
%
In a graph contraction, rounds are repeated until there are no edges
left.
\end{gram}

\begin{gram}
An important property of graph contraction is that it is guided by a
graph partition.  Since blocks in a graph partition are disjoint, each
vertex in the graph is mapped to one unique vertex in the quotient
graph.
%
\end{gram}

\begin{example}
\label{ex:gc::contract-example}

One round of graph contraction:
\begin{center}
  \includegraphics[width=6.5in]{./media/contract-example5.jpg}
\end{center}

Contracting a graph down to a single vertex in three rounds:
\begin{center}
\includegraphics[width=5in]{./media/graph-contraction-example-1.jpg}
\end{center}
\end{example}

\begin{gram}
As described, the graph-contraction technique is generic in the kind
of graph partition used for constructing the quotient graph.  In the
rest of this chapter, we will consider two techniques, edge
partitioning and star partitioning, and the resulting
graph-contraction algorithms.
%
The rest of this chapter describes the graph-contraction design
technique and two approaches to graph-contraction: edge contraction
and star contraction.
\end{gram}
\end{unit}
\end{section}


\begin{section}[Edge Partition and Edge Contraction]
\label{sec:gc::edge-partition}

\begin{unit}

\begin{definition}[Edge Partition and Edge Contraction]
Edge partitioning is a graph-partitioning technique.  In an~\defn{edge
  partition}, each block is either a single vertex or two vertices
connected by an edge.  We use the term~\defn{edge contraction} to
refer to a graph contraction performed by using edge partitions.
\end{definition}


\begin{example}
\label{ex:gc::ep::circle}
An example edge partition in which every block consists of two
vertices and an edge between them.  Contracting the graph based on
this partition yields a quotient graph with half as many vertices
and edges.
\begin{center}
\includegraphics[width=4.5in]{./media/edge-contraction-example-1.jpg}
\end{center}

Note that in general, blocks cannot be just pairs of vertices, because
the graph might not have an even number of vertices, but even if it
does (no pun intended), it is likely that it cannot be partitioned
into a set of pairs joined by edges.
\end{example}

\begin{gram}[Edge Partitions and Vertex Matching]
%
%% \begin{teachask}
%% How can we construct an edge-partitioning in the general case?
%% \end{teachask}
%
We can construct an edge partition by selecting an~\defn{independent
  edge set}, or~\defn{vertex matching},
where no two edges share a vertex, and placing all the
remaining vertices that are not incident an a selected edges into singleton
sets.
%
The problem of
finding a vertex matching is called the~\defn{vertex-matching
  problem}.
\end{gram}

\begin{definition} 
A~\defn{vertex matching} for an undirected graph $G = (V,E)$
is a subset of edges $M \subseteq E$ such that no two edges in $M$
share a vertex.  
\end{definition}

\begin{example}
\label{ex:gc::ec::matching}
A vertex matching for a graph (highlighted edges) and the
corresponding blocks.
\begin{center}
  \includegraphics[width=2in]{./media/matching-example.jpg}
\hspace{1in}
  \includegraphics[width=2in]{./media/matching-example-partitioned.jpg}
\end{center}
The vertex matching defines four blocks (circled), two of them defined
by the edges in the matching, $\cset{\vname{a},\vname{b}}$ and
$\cset{\vname{d},\vname{f}}$, and two of them are the unmatched
vertices $\vname{c}$ and $\vname{e}.$
\end{example}


\begin{gram}[Maximal Vertex Matching]
The problem of finding the largest vertex matching for a graph is
called the~\defn{maxima/ vertex matching} problem.  Many algorithms
for this well-studied problem have been proposed, including one that
can solve the problem in $O(\sqrt{|V|}|E|)$ work.  For graph
contraction, we do not need a maximum matching but one that it is
sufficiently large.
%
%% \begin{teachask}
%% Describe an algorithm for finding a vertex matching.
%% \end{teachask}
%

For example, we can use a greedy algorithm to construct a vertex
matching by going through the edges one by one maintaining an
initially empty set $M$ and for each edge, if no edge in $M$ is
already incident on its endpoints then add it to $M$, otherwise toss
it.  The problem with this approach is that it is sequential since
each decision depends on previous decisions.
%% %
%% \begin{teachask}
%% Describe a parallel algorithm for  vertex matching?
%% \end{teachask}
%% %

To find the vertex matching in parallel, we will need to make local
decisions at each vertex.  One possibility is for each vertex to
select one of its neighbors to match with.
%
Such a selection can be made in parallel but 
%
%% \begin{teachask}
%% What is the problem with this approach? 
%% \end{teachask}
%% %
there is one problem:  multiple vertices might select
the same vertex to match with.  
%
We therefore need a way
to \emph{break the symmetry} that arises when two vertices try to match with the same vertex.
%% %
%% \begin{teachask}
%% Can you use randomization to break the symmetry? 
%% \end{teachask}
%% %
We can use randomization to break the symmetry.   For example,
we can  flip a coin for each
edge $(u,v)$ in parallel and 
select the edge, effectively matching $u$ and $v$,  if the coin
for the edge comes up heads and all the edges incident on
$u$ and $v$ flip tails.  This guarantees that a vertex is matched with
at most one other vertex.
\end{gram}

\begin{example}[Edge contraction]
\label{ex:gc::edge-contract-example}

An example edge contraction illustrated.

\begin{center}
\includegraphics[width=5in]{./media/edge-contraction-example.jpg}
\end{center}
\end{example}

\begin{gram}
Let us analyze how effective this approach is in selecting a
reasonably large matching. As an example, we first consider cycle
graphs, consisting of a single cycle and no other edges.  In such a
graph every vertex has exactly two neighbors.
\end{gram}

\begin{example}
A graph consisting of a single cycle.    
  \begin{center}
  \includegraphics[width=1.5in]{./media/cycle-graph.jpg}
  \end{center}
Each edge flips a coin that comes up either heads ($H$) or tails
($T$).  We select an edge if it turns up heads and all other edges
incident on its endpoints are tails.  In the example the edges
$\cset{\vname{c},\vname{d}}$ and $\cset{\vname{b},\vname{f}}$ are
selected.
\end{example}

\begin{gram}
We want to determine the probability that an edge is selected in such
a graph.  Since the coins are flipped independently at random, and
each vertex has degree two, the probability that an edge picks heads
and its two adjacent edges pick tails is $\frac{1}{2} \cdot
\frac{1}{2} \cdot \frac{1}{2} = \frac{1}{8}$.  To analyze the number
of edges selected in expectation, let $R_e$ be an indicator random
variable denoting whether $e$ is selected or not, that is $R_e = 1$ if
$e$ is selected and $0$ otherwise.  Recall that the expectation of
indicator random variables is the same as the probability it has value
$1$ (true).  Therefore we have $E[R_e] = 1/8$.
%
Thus summing over all edges, we conclude that expected number of edges
deleted is $\frac{m}{8}$ (note, $m=n$ in a cycle graph). 


%% \begin{todo}
%% We need to state this as a theorem.  It is not quite stated in this way.
%% \end{todo}

In the chapter on randomized algorithms,
% (\secref{kthsmallest})
we argued that if each round of an algorithm reduces the size by a
constant fraction in expectation, and if the random choices in the
rounds are independent, then the algorithm will finish in $O(\lg n)$
rounds with high probability.  Recall that all we needed to do is
multiply the expected fraction that remain across rounds and then use
Markov's inequality to show that after some $k \lg n$ rounds the
probability that the problem size is a least 1 is very small.  For a
cycle graph, this technique leads to an algorithm for graph
contraction with linear work and $O(\lg^2{n})$ span.
%
%---left as an exercise.
\end{gram}

\begin{teachask}
  Can you think of a way to improve the expected number of edges
  deleted?
\end{teachask}

\begin{gram}
There are several ways to improve the number of deleted edges.  One
way is for each vertex to pick one of its neighbors and to select an
edge $(u,v)$ if it was picked by both $u$ and $v$.  In the case of a
circle, this increases the expected number of deleted edges to
$\frac{m}{4}$. 
%
Another way is let each edge pick a random number in some range and
then select and edge if it is the local maximum, i.e., it picked the
highest number among all the edges incident on its end points. This
increases the expected number of edges contracted in a circle to
$\frac{m}{3}$.
\end{gram}

\begin{gram}
Although edge contraction works quite well with cycle graphs, 
%or sequentially with the appropriate data structures, 
it does not work well for arbitrary graphs.  The problem is in edge
contraction, only one edge incident on a vertex can be contracted in
each round.  Therefore vertices with high degrees, will have to
contract their neighbors one at a time.  The example below illustrates
a particularly difficult graph, called a star graph, for edge
contraction.
\end{gram}
%
\begin{example}
\label{ex:gc::star}
A star graph with center $v$ and eight satellites.
  \begin{center}
  \includegraphics[width=1.5in]{./media/star-graph1.jpg}
  \end{center}
\end{example}


\begin{group}
\begin{definition}[Star Graph]
  A~\defn{star} graph $G = (V,E)$ is an undirected graph with a
 ~\defn{center} vertex $v \in V$, and a set of edges $E$ that attach
  $v$ directly to the rest of the vertices, called
 ~\defn{satellites}, i.e. $E = \cset{\cset{v,u} : u
    \in V \setminus \cset{v}}$.
\end{definition}

\begin{example}
The following are star graphs:
\begin{itemize}
\item a single vertex, and
\item a single edge.
\end{itemize}
\end{example}

\end{group}

\begin{gram}
It is not difficult to convince ourselves that on a star graph with
$n$ vertices ($1$ center and $n-1$ satellites) any edge partitioning
algorithm will take $\Omega(n)$ rounds.  To fix this problem we need
to be able to form blocks that consist of more than just edges.
\end{gram}

\begin{teachnote}
I (Umut) find the remark below quite confusing. 
\end{teachnote}

\begin{remark}
  An abstract data type called disjoint sets is often used to contract
  graphs sequentially.  Disjoint sets supply two functions:
 ~\defn{union}, which joins two components, and~\defn{find}, which
  finds what component a vertex is in.  In our framework, the union
  operation is simply edge contraction across a single edge, and the
  find is just a lookup in the partition map.  Semantically for a
  partition map $P$ we can define $\cd{union}$ as:


\[
\begin{array}{lcl}
~~\cd{union}~(P,u,v) & = & \{ u' \mapsto~\cd{if}~(v' = u)~\cd{then}~v~\cd{else}~v'
\\
 
                 & & ~: (u' \mapsto v') \in P \}
\end{array}
\]

where here we have made $v$ the new representative of the super-vertex
$\cset{u,v}$, and have updated all vertices that used to point to $u$
to now point to $v$.
Implementing the $\cd{union}$ this way, however, is inefficient since
it can require updating a lot of vertices.  It turns out that the
operations can be can be implemented much more efficiently.  Indeed
one can implement a data structure that only requires amortized
$O(\alpha(n))$ work per operation, where $\alpha(n)$ (the inverse
Ackermann function) is a function that is very close ot $O(1)$, and
$n$ is the number of operations.
\end{remark}
\end{unit}
\end{section}


\begin{section}[Star Partition and Star Contraction]
\label{sec:gc::star-partitioning}

\begin{unit}[Star Partition]

\begin{gram}
In an edge partition, if there is an edge incident on a vertex~$v$ is
selected as a block, then none of the other edges incident on $v$ can be their own
block. This limits the effectiveness of edge partitioning, because it
is unable to contract significantly graphs with high-degree vertices.
%
In this section, we describe an alternative technique: star
partition.  
\end{gram}

\begin{group}
\begin{definition}[Star Partition]
A~\defn{star partition} of a graph $G$ is a partition where each block
is a  star graph.
\end{definition}

\begin{example}
\label{ex:gc::star-partition}
Consider star graph with center $v$ and eight satellites.
  \begin{center}
  \includegraphics[width=1.5in]{./media/star-graph1.jpg}
  \end{center}

A partition consisting of the whole graph is a star partition, where 
the only block is the (star) graph itself.

Similarly, a partition where each block is an isolated vertex is a
star partition, because each block is a vertex-induced subgraph of the
vertices of a star graph.

\end{example}


\begin{example}
\label{ex:gc::star}

  In the graph shown below on the left, we first find two disjoint stars. We
  then partition the graph into two blocks, each defined as the
  vertex-induced subgraphs of the two stars.

\begin{center}
\includegraphics[width=0.9\textwidth]{./media/star-decomposition-1.jpg}
\end{center}
\end{example}
\end{group}


%% \begin{teachask}
%% How can we star-partition a graph? 
%% \end{teachask} 

\begin{gram}[Finding a Star Partition]
As with an edge partition, it is possible to construct a star partition
sequentially.  
%
One approach proceeds by adding start incrementally as follows.
\begin{itemize}

\item Choose an arbitrary vertex $v$ in the graph and make $v$ the
  center of a star.

\item Attach as satellites all the neighbors of $v$.

\item Remove $v$ and its satellites from the graph.

\item Recursively repeat the same process with the remaining graph.
\end{itemize}
%

Towards the end of this process, we might end up with some vertices
that are isolated vertices with no edges attached to them.
%
Such vertices can all be chosen as \defn{singleton stars.}



%% \begin{teachask}
%% How can we star partition a graph in parallel? 
%% \end{teachask} 
%
We can also construct a star partition in parallel by making local
independent decisions at each vertex.  As with edge partitions, we use
randomization to break symmetry.
%
%% \begin{teachask}
%% Can you think of a randomized approach for selecting stars?
%% \end{teachask}
%
One approach proceeds as follows. 
%
\begin{itemize}
\item Flip a coin for each vertex.

\item If a vertex flips heads, then it becomes the center of a star. 

%% \item If a vertex flips tails, then it attempts to become a satellite
%%   by finding a neighbor that has flipped heads.  If no such neighbor
%%   exists (all neighbors have flipped tails or the vertex is isolated),
%%   then the vertex becomes a center.
%%%% TODO: The following seems to be a better explanation
\item If a vertex flips tails, then there are two cases.
\begin{itemize}
\item The vertex is isolated.  In this case, it becomes a center.
\item The vertex has a neighbor.  In this case, the vertex looks for a
  neighbor that flipped heads.  If there is such a neighbor, then it
  chooses one arbitrarily, and becomes a satellite for that
  neighbor. Otherwise, all neighbors have flipped tails and the
  vertex becomes a center.
\end{itemize}
\end{itemize}

%
%% \begin{teachask}
%% Is this algorithm guaranteed to create the smallest number of stars?
%% \end{teachask}
%
This approach is not optimal in the sense that it might not always
create the smallest number of stars.
%
As we shall see, suboptimality is acceptable, because we only need to
reduce the size of the graph by a constant factor.
\end{gram}

\begin{group}

\begin{example}[Randomized Star Partition]
\label{ex:startpartition}
An example star partition. Vertices $\vname{a}$ and $\vname{b}$, which
flip heads, become centers. Vertices $\vname{c}$ and $\vname{e}$,
which flipped tails, attempt to become satellites by finding a center
among their neighbors, breaking ties arbitrarily.
%
If a vertex does not have a neighbor that is a center (flipped heads),
then it becomes a singleton star (e.g., vertex $d$).
%
We end up with three stars: the star with center $\vname{a}$ (with no
satellites), the star with center $\vname{b}$ (with two satellites),
and the singleton star $\vname{d}$.
%
The star partition thus yields three blocks, which are defined by the
subgraphs induced by each star.

\begin{center}
  \includegraphics[width=6in]{./media/star-find0.jpg}
\end{center}

\end{example}
\end{group}


\begin{group}
%require an undirected graph $G=(V,E)$ and round number $i$
%return $V'$ = remaining vertices after contraction,
%          $P$ = mapping from $V$ to $V'$
\begin{algorithm}[Star Partition]
\label{alg:gc::starPartition}

To specify the star-partition algorithm, we need
a source of randomness.
%
We assume that each vertex is given a (potentially infinite) sequence
of random and independent coin flips. The $i^{th}$ element of the
sequence can be accessed via the function
\[
\cheads(v,i) : V \times \mathbb{Z} \to \mathbb{B}, 
\]
which returns $\cd{true}$ if the $i^{th}$ flip on vertex $v$ is heads
and false otherwise. Since most machines don't have true sources of
randomness, in practice this can be implemented with a pseudorandom
number generator or even with a good hash function. 
\[
\begin{array}{ll}
1 & \cd{starPartition}~(G=(V,E),i) =
\\
2 & ~~~\cd{let}
\\
3 & ~~~~~~\cd{(* Find the arcs from satellites to centers. *)}
\\
4 & ~~~~~~\mathit{TH} = \csetf{(u,v) \in E}{\neg \cheads(u,i) \land \cheads(v,i)}$ %@\label{line:flip}\vspace{.1in}@
\\
5 & ~~~~~~\cd{(* Partition map from satellites to centers *)}
\\
6 & ~~~~~~P_S = \bigcup_{(u,v) \in \mathit{TH}} \cset{u \mapsto v}
% \label{line:starmerge}
\\
7 & ~~~~~~\cd{(* Centers are non-satellite vertices *)}
\\
8 & ~~~~~~V_c = V \setminus \cd{domain}(P_s)
\\
9 & ~~~~~~\cd{(* Map centers to themselves *)}
\\
10 & ~~~~~~~P_c = \cset{u \mapsto u : u \in V_c} % \label{line:self}
\\
11 & ~~~\cd{in}
\\
12 & ~~~~~~(V_c, P_s \cup P_c)
\\
13 & ~~~\cd{end}
\end{array}
\]

The pseudo-code above shows the code for star partitioning.
%
The function $\cd{starPartition}$ takes as argument a graph and a round
number, and returns a graph partition specified by a set of centers and
a partition map from all vertices to centers.
%
The algorithm starts by flipping a coin for each vertex and selecting the directed
edges that point from tails to heads---this gives
the set of edges $\mathit{TH}$.
%
In this set of edges, there can be multiple edges from the same
non-center. Since we want to choose one center for each satellite, we
remove duplicates in Line~\linegcstarmerge{}, by creating a set of
singleton tables and merging them.
%
More specifically, the union is shorthand for the following code.
%
\[
\begin{array}{ll}
  \cd{Set.reduce}
  & (\cd{Table.union}~(\cd{lambda} (x,y) \dra x))
\\
& \emptyset
\\
& \cset{\cset{u \mapsto v} : (u,v) \in \mathit{TH}}
\end{array}
\]
%
This completes the selection of satellites and their centers. 
%

Next, the algorithm determines the set of centers, which
will become the vertices of the quotient graph in contraction, as all
the vertices minus the satellites.
%
To complete the process, the algorithm maps each center to itself
(Line~\linegcstarself{}).
%
These operations effectively promote unmatched non-centers to centers,
forming singleton stars, and matches all centers with themselves.
%
Finally, the algorithm constructs the partition map by  uniting the
mapping for the satellites and the centers.
\end{algorithm}

\begin{example}
Consider the star partition illustrated below.
\begin{center}
  \includegraphics[width=6in]{./media/star-find0.jpg}
\end{center}

The  star-partition algorithm proceeds on this examplse as follows.
%
First, it computes
%
\[
\mathit{TH} =
\cset{(\vname{c},\vname{a}),(\vname{c},\vname{b}),(\vname{e},\vname{b})},
\]
%
as the edges from satellites to centers.  
%
Now, it
converts each edge into a singleton table, and merges all the tables
into one
table, which is going to become a part of the partition map:
%
\[
P = \cset{\vname{c} \mapsto \vname{b},\vname{e} \mapsto \vname{b}}.
\]
%
Note that the edge $(\vname{c},\vname{a})$ has been removed since when
uniting the tables, we selects only one element for each key in the
domain.  
%
Now for all remaining vertices
%
$V' = V \setminus \cd{domain}(P) = \cset{\vname{a},\vname{b},\vname{d}}$
we map them to themselves, giving:
%
\[
P' = \cset{\vname{a} \mapsto \vname{a}, \vname{b} \mapsto \vname{b},
  \vname{d} \mapsto \vname{d}}.
\]
%
The vertices in $P'$ are the centers.
%
Finally we merge $P$ and $P'$ to obtain the partition map
%
\[
P' \cup P = \cset{\vname{a} \mapsto \vname{a}, \vname{b} \mapsto \vname{b}, \vname{c} \mapsto \vname{b}, \vname{d} \mapsto
    \vname{d}, \vname{e} \mapsto \vname{b}}.
\]
\end{example}
\end{group}

\end{unit}

%% \begin{todo}
%% This theorem needs a proof.  What is the graph representation? 
%% It seems like it is an edge set representation.

%% The big union would require logn span do we need inject here?

%% \end{todo}


\begin{unit}[Analysis of Star Partitioning]

\begin{gram}
By examining the algorithm, we can conclude the following work and
span bounds for the star partition algorithm.
\end{gram}

\begin{theorem}[Cost of Star Partition]
Based on the array-based cost specification for sequences and
single-threaded sequences, the cost of $\cd{starPartition}$ is 
\[
O(n + m)
\]
work and 
\[
O(\lg n)
\]
span for a graph with $n$ vertices and $m$ edges.
\end{theorem}
\begin{exercise}
Prove the theorem.
\end{exercise}

\begin{teachask}
 In expectation, how big is $P$?  
\end{teachask}


\begin{gram}[Number of Satellites]
Let us also bound the number of satellites found by
$\cd{starPartition}.$
%
Note first that there is a one-to-one mapping between the satellites
and the set $P_s$ computed by  the algorithm.
%
%
The following lemma establishes that on a graph with
$\nn$~non-isolated vertices, the number of satellites is at least
$\nn/4$ in expectation.
%
As we will see this means that we can use star partitions to perform
graph contraction with logarithmic span.
\end{gram}

\begin{group}
\begin{lemma}[Number of Satelites]
  For a graph $G$ with $\nn$ non-isolated vertices, the expected
  number of satellites in a call to $\cd{starPartition}~(G,i)$ with
  any~$i$ is at least $\nn/4$.
\end{lemma}

\begin{proof}
  For any vertex $v$, let $H_v$ be the event that a vertex $v$ comes
  up heads, $T_v$ that it comes up tails, and $R_v$ that $v \in
  \cd{domain}(P)$ (i.e, it is a satellite).
%
  Consider any non-isolated vertex $v \in V(G)$.  By definition, we
  know that a non-isolated vertex $v$ has at least one neighbor $u$.
  So, we know that $T_v \land H_u$ implies $R_v$, since if $v$ is a
  tail and $u$ is a head $v$ must either join $u$'s star or some other
  star.  Therefore, $\prob{R_v} \geq \prob{T_v} \prob{H_u} = 1/4$.  By
  the linearity of expectation,  the expected number of satellites is
  \begin{align*}
    \expct{\sum_{v: v \text{ non-isolated}} \onef{R_v}} & = \sum_{v: v
      \text{ non-isolated}} \expct{\onef{R_v}} 
\\[2mm]
  & \geq \nn/4.
  \end{align*}
  The final inequality follows because we have $\nn$ non-isolated
  vertices and because the expectation of an indicator random variable
  is equal to the probability that it takes the value $1$.
\end{proof}
\end{group}

\begin{teachnote}
Consider the random variable that a vertex becomes a satellite.  This
happens if the vertex flips tails and it has a neighbor that flips
heads.  A non-isolated vertex has at least one neighbor, therefore
this probability is at least 1/4.  The bound follows.
\end{teachnote}

\end{unit}

\begin{unit}[Star Contraction]

\begin{definition}[Star Contraction]
We refer to a graph contraction that uses star partitions only as
a~\defn{star contraction}.
\end{definition}

\begin{algorithm}[Star Contraction]
\label{alg:gc::cc}
\[
\begin{array}{ll}
1 & \cd{starContract}~\cd{base}~\cd{expand}~(G = (V,E)) =
\\ 
2 & ~~~\cd{if}~|E| = 0~\cd{then}
\\
3 & ~~~~~~\cd{base}~(V)
\\
4 & ~~~\cd{else}
\\ 
5 & ~~~~~~\cd{let}
\\ 
6 & ~~~~~~~~~(V',P) = \cd{starPartition}~(V,E)~ % @\label{line:gc::cc::partition}@
\\
7 & ~~~~~~~~~E' = \csetf{(\cget{P}{u},\cget{P}{v}) : (u,v) \in  E}{\cget{P}{u} \neq \cget{P}{v}} 
% @\label{line:gc::cc::edges}@
\\
8 & ~~~~~~~~~R = \cd{starContract}~\cd{base}~\cd{expand}~(V',E')
\\
9 & ~~~~~~\cd{in}
\\
10 & ~~~~~~~~~\cd{expand}~(P, R)
\\
11 & ~~~~~~\cd{end}
\end{array}
\]

The pseudo-code above illustrates a generic, higher-order
star-contraction algorithm.
%
The algorithm takes as arguments
\begin{itemize}
\item  a function $\cd{base}$ that specifies
the computation in the base case,

\item 
another function $\cd{expand}$ that computes the result for the larger
graph from the smaller, contracted graph, and 

\item
the graph.
\end{itemize}


Each contraction on Line~\linegcscpartition{} returns the set of
(centers) super-vertices $V'$ and a table $P$ mapping every $v \in V$
to a $v' \in V'$.
%
The set $V'$ defines the super-vertices of the quotient graph.
%

Line~\linegcscedges{} completes the construction of the quotient graph:
%
\begin{itemize}
\item it computes the edges of the quotient graph by 
routing the end points of each edge to the corresponding
super-vertices in $V'$, which is specified by the table $P$;
%
\item it  removes all self edges via the  filter $\cget{P}{u}
\neq \cget{P}{v}$.
%
\end{itemize}
%

Having computed the quotient graph, the algorithm recurs.
%
Recursion bottoms out when the graph contains no edges, in which case,
the function $\cd{base}$ is applied to the remaining vertices.
%
In the base case, each connected component has been contracted down to
a singleton vertex, and thus the number of vertices in the contracted
graph is equal to the number of components in the input graph.

The result of the recursive call $R$ is ``expanded'' to compute the result
for the whole graph by calling the function $\cd{expand}$.

\end{algorithm}

\begin{exercise}
I did not work out the arguments for $\cd{base}$ and $\cd{expand}$
carefully.  Discuss here what it should be by clicking on the ``discuss''
button on the right hand corner of this block.
\end{exercise}

\begin{teachnote}
TODO [this is somewhat resolved]
This analysis is rather imprecise, because we have not written the
pseudocode for graph contraction.  How do we re-route edges and such.
This should be done.
\end{teachnote}

\begin{group}
\begin{theorem}[Work and Span of Star Contraction]
\label{thm:gc::star-contraction-cost}
  For a graph $G = (V,E)$, we can contract the graph into a number of
  isolated vertices   in $O(|V| + |E| \lg |V|)$ work and $O(\lg^2 |V|)$ span.
\end{theorem}

\begin{gram}
For the proof, we will consider work and span separately
%
and assume that
%
\begin{itemize}
   \item function $\cd{base}$ has constant span and linear work in the
     number of vertices passed as argument, and
\item function $\cd{expand}$ has linear work and logarithmic span in
  the number of vertices and edges at the corresponding step of the
  contraction.
  \end{itemize}

\end{gram}

\begin{gram}[Span of Star Contraction]
Let $\nn$ be the number of non-isolated vertices.
%
In star contraction, once a vertex becomes isolated, it remains
isolated until the final round, since contraction only removes edges.
%
Let $\nn'$ denote the number of non-isolated vertices after one round of star
contraction.
%

We can write the following recurrence for the span of star contraction.
%
\[
\begin{array}{lll}
S(\nn)  & = &
\left\{
\begin{array}{lll}
S(\nn') + O(\lg n) & \mbox{if} & \nn > 0
\\
1 & \mbox{otherwise.}
\end{array}
\right.
\end{array}
\]
%

Observe that $\nn' = \nn - X$, where $X$ is the number of satellites
(as defined earlier in the lemma about $\cd{starPartition}$), which are
removed at a step of contraction. Since $\expct{X} = \nn/4$,
$\expct{\nn'} = 3n/4$.
%
This is a familiar recurrence, which we know solves to $O(\lg^2
\nn)$, and thus $O(\lg^2 n)$, in expectation.
\end{gram}


\begin{gram}[Work of Star Contraction]
For work, we would like to show that the overall work is linear,
because we might hope that the graph size is reduced by a constant
fraction on each round.
%
Unfortunately, this is not the case.  Although we have shown that star
contraction can remove a constant fraction of the non-isolated
vertices in one round, we have not bounded the number of edges
removed.
%

%% \begin{teachask}
%% How many edges can we remove? 
%% \end{teachask}
%
Because removing a satellite also removes the edge that attaches it to
its star's center, each round  removes at least as many edges as vertices.  
%
But this does not help us bound
the number of edges removed by a linear function of $m$, because there
can be as many an $n^2$ edges in the graph.
%
Thus, all we know is that there are no more than $m-n$ after one
round of contraction.

To bound the work, we will consider non-isolated and isolated vertices
separately.
%
Let $\nn'$  denote the  number of non-isolated vertices after one
round of star contraction.
%
For the non-isolated vertices, we have the following work recurrence:
\[
\begin{array}{lll}
W(\nn, m) 
\leq 
\left\{
\begin{array}{lll}
W(\nn', m) + O(\nn+m) & \mbox{if} & \nn > 1
\\
1 & \mbox{otherwise.}
\end{array}
\right.
\end{array}
\]
%
This recursion solves to
\[
\expct{W(\nn,m)} = O(\nn + m\lg \nn) = O(n + m \lg{n}).
\]

To bound the work on isolated vertices, we note that there at most $n$
of them at each round and thus, the additional work is $O(n \lg{n}).$

We thus conclude that the total work is
\[
O(n + m\lg{n}).
\]
\end{gram}

\begin{note}
Consider as an example a star contraction where $n$ and $m$ have the
following values in each round.
\[
\begin{array}{lll}
\hline
 \mbox{round} & \mbox{vertices} & \mbox{edges}
\\
\hline
 1 & n & m 
\\
 2 & n/2 & m - n/2 
\\
 3 & n/4 & m - 3n/4 
\\
 4 & n/8 & m - 7n/8 
\\
\hline
 \end{array}
\]
It is clear that the number of edges does not drop below $m-n,$ so if
there are $m > 2n$ edges to start with, the overall work will be $O(m
\lg n)$.
%
\end{note}
%
\end{group}

\begin{teachnote}
Note that if the graph is complete, we do actually reduce the number
of edges by a constant fraction be eliminating redundancy, because we
can only have so many edges in the quotient graph. This brings up an
interesting point about when this algorithm actually performs poorly.
It might be interesting to study some real world instance.
\end{teachnote}

\begin{teachnote}
Idea: Consider a contraction along with the randomness function.
Consider each round and the blocks contracted in that round.
Add just as many edges as possible (without leading to duplicates)
between those blocks.  Make sure that you don't generate duplicates
in following rounds.  Since each block is nested inside a
logarithmic number of other blocks.  It is possible to construct
such a graph that also has a large number of edges.
\end{teachnote}
\end{unit}
\end{section}



\begin{section}[Connectivity via Graph Contraction]

%% Is this needed? 
% To apply the graph contraction to the connectivity problem, we will
% take care not to alter the connectivity properties of the input graph
% as we contract it.  This will allow us to construct the solution for
% the input graph from the solution for the contracted graph.  In
% general, graph contraction can be applied to a variety of problems,
% including for example spanning trees and minimum spanning trees.


\begin{unit}
\begin{problem}[The Graph Connectivity (GC) Problem]
Given an undirected graph $G = (V,E)$, the \defn{graph-connectivity
  problem} requires finding all of the connected components of $G$ by
specifying the set of vertices in each component.
\end{problem}
%

\begin{teachask}
  Solve the graph-connectivity problem by using one of the
  techniques recently covered earlier in the course.
\end{teachask}
%

\begin{gram}[Sequential Algorithms for Connectivity]
The graph connectivity problem can be solved by using graph search as
follows. 
\begin{itemize}
\item Start at any vertex and find, using DFS or BFS, all vertices
  reachable from that vertex and mark them visited. This creates the
  first connected component.

\item 
Select another vertex, and if it has not already been visited, then
search from that vertex to create the second component. 
%
Repeat until all the vertices are considered.
%
%
\end{itemize}

This approach leads to a perfectly sensible sequential algorithms for
graph connectivity, but they are not good parallel algorithms.
%
DFS for example is a purely sequential algorithm.
%
Using BFS for each component can yield some parallelism but still the
span of BFS is lower-bounded by the diameter of the graph.
%
Note that the diameter of a component can be as large as~$n-1$.  A
``chain'' of $n$ vertices will have diameter $n-1$.
%

Even if the diameter of the graph is small, we still have to iterate
over the components sequentially one by one.  Thus the span in the
worst case can be linear in the number of components, which can be
large.
\end{gram}


\begin{gram}
We would like to find a parallel algorithm for connectivity that has a
small span an all graphs.  To this end, we use the graph-contraction
technique with star partitions.  
%
To specify the algorithm, we use an edge-set representation for
graphs, where every edge is represented as a pair of vertices, in both
orders.  
%
This is effectively equivalent to a directed graph representation of
undirected graphs with two arcs per edge.
\end{gram}

\begin{example}
The edge-set representation of an undirected graph is shown below.

\begin{center}
  \includegraphics[width=2.2]{./media/contract-example1.jpg}
\end{center}

\[
\begin{array}{lcl}
V & = & \cset{\vname{a},\vname{b},\vname{c},\vname{d},\vname{e},\vname{f}}\\
E & = &
\{(\vname{a},\vname{b}),(\vname{b},\vname{a}),(\vname{b},\vname{d}),(\vname{b},\vname{e}),(\vname{e},\vname{b}),(\vname{d},\vname{b}),(\vname{d},\vname{f}),(\vname{a},\vname{c}),
\\
& & ~~(\vname{c},\vname{a}),(\vname{c},\vname{d}),(\vname{d},\vname{c}),(\vname{d},\vname{f}),(\vname{f},\vname{d}),(\vname{e},\vname{f}),(\vname{f},\vname{e})\}
\end{array}
\]
\end{example}

\begin{group}
\begin{algorithm}[Counting Components using Graph Contraction]
\label{alg:gc::cc}

\[
\begin{array}{ll}
1 & \cd{countComponents}~(G = (V,E)) =
\\ 
2 & ~~~\cd{if}~|E| = 0~\cd{then}
\\
3 & ~~~~~~|V|
\\
4 & ~~~\cd{else}
\\ 
5 & ~~~~~~\cd{let}
\\ 
6 & ~~~~~~~~~(V',P) = \cd{starPartition}~(V,E)
\\
7 & ~~~~~~~~~E' = \csetf{(\cget{P}{u},\cget{P}{v}) : (u,v) \in  E}{\cget{P}{u} \neq \cget{P}{v}} %
\\
8 & ~~~~~~~~~R = \cd{countComponents}~(V',E')
\\
9 & ~~~~~~\cd{in}
\\
10 & ~~~~~~~~~R
\\
11 & ~~~~~~\cd{end}
\end{array}
\]

%% \[
%% \begin{lstlisting}
%% countComponents $(G = (V,E))$ = 
%%   if $|E| = 0$ then $
%%     |V|$
%%   else 
%%     let 
%%       $(V',P)$ = starPartition $(V,E)$ @\label{line:gc::cc::partition}@
%%       $E'$ = $\csetf{(\cget{P}{u},\cget{P}{v}) : (u,v) \in E}{\cget{P}{u} \neq \cget{P}{v}}$ @\label{line:gc::cc::edges}@
%%     in
%%       countComponents $(V',E')$
%%     end
%% \end{lstlisting}



The pseudo-code above shows a graph-contraction algorithm for
determining the number of connected components in a graph.
%
Each contraction on \lineref{gc::cc::partition} returns the set of
(centers) super-vertices $V'$ and a table $P$ mapping every $v \in V$
to a $v' \in V'$.
%
The set $V'$ defines the super-vertices of the quotient graph.
%
\lineref{gc::cc::edges} completes the computation of the quotient graph.
\begin{itemize}
\item it computes the edges of the quotient graph by 
routing the end points of each edge to the corresponding
super-vertices in $V'$, which is specified by the table $P$;
%
\item it  removes all self edges via the  filter $\cget{P}{u}
\neq \cget{P}{v}$.
%
\end{itemize}
%
Having computed the quotient graph, the algorithm recursively solves
the problem on it.
%
Recursion bottoms out when the graph contains no edges, in which case,
each component has been contracted down
to a singleton vertex, and thus the number of vertices in the
contracted graph is equal to the number of components in the input graph.

\end{algorithm}

\begin{example}
The values of $V'$, $P$, and $E'$ after each round of the 
contraction shown in \exref{gc::contract-example}.
\[
\begin{array}{crcl}
  & V' & = & \cset{\vname{a},\vname{d},\vname{ef}}\\
\mbox{Round } 1 & P' & = & 
 \cset{\vname{a} \mapsto \vname{a}, 
       \vname{b} \mapsto \vname{a}, 
       \vname{c} \mapsto \vname{a}, 
       \vname{d} \mapsto \vname{d}, 
       \vname{e} \mapsto \vname{e}, 
       \vname{f} \mapsto \vname{e}}\\
  & E' & = & \cset{(\vname{a},\vname{e}),
               (\vname{e},\vname{a}),
               (\vname{a},\vname{d}),
               (\vname{d},\vname{a}),
               (\vname{d},\vname{e}),
               (\vname{e},\vname{d})}\\[.1in]
  & V' & = & \cset{\vname{a},\vname{e}}\\
\mbox{Round } 2 & P' & = & 
 \cset{\vname{a} \mapsto \vname{a}, 
       \vname{d} \mapsto \vname{abcd}, 
       \vname{e} \mapsto \vname{e}}\\
  & E' & = & \cset{(\vname{a},\vname{e}),
               (\vname{e},\vname{a})}\\[.1in]
  & V' & = & \cset{\vname{a}}\\
\mbox{Round } 3 & P' & = & 
 \cset{\vname{a} \mapsto \vname{a}, 
       \vname{e} \mapsto \vname{a}}\\
  & E' & = & \cset{}
\end{array}
\]
\end{example}

\end{group}

\begin{exercise}
  Express $\cd{countComponents}$ in terms of higher order function
  $\cd{starContract}$ by giving $\cd{base}$ and $\cd{expand}$ functions.
\end{exercise}

\begin{gram}
%
We can modify the algorithm slightly to compute the components
themselves instead of returning their count. 
%
To this end, we are going to construct the mapping from vertices to
their components recursively. This is possible because we can obtain
the mapping by composing the mapping from vertices to their
super-vertices and the mapping from super-vertices to their
components, which we obtain recursively. \algref{gc::nc} shows the
algorithm.
\end{gram}



\begin{algorithm}[Contraction-based graph connectivity]
\label{alg:gc::nc}
\[
\begin{array}{ll}
1 & \cd{connectedComponents}~(G = (V,E)) = 
\\
2 & ~~~\cd{if}~|E| = 0~\cd{then}
\\ 
3 & ~~~~~~(V, \cset{v \mapsto v : v \in V})
\\
4 & ~~~\cd{else}
\\ 
5 & ~~~~~~\cd{let}
\\
6 & ~~~~~~~~~(V',P) = \cd{starPartition}~(V,E)
\\
7 & ~~~~~~~~~E' = \csetf{(\cget{P}{u},\cget{P}{v}) : (u,v) \in E}{\cget{P}{u} \neq \cget{P}{v}}$ 
\\
8 & ~~~~~~~~~(V'',C) = \cd{connectedComponents}~(V',E')
\\
9 & ~~~~~~\cd{in}
\\
10 & ~~~~~~~~~(V'', \cset{v \mapsto C[s] : (v \mapsto s) \in P}) % @\label{line:gc::nc::back}@
\\
11 & ~~~~~~\cd{end}
\end{array}
\]
% old code : \cset{v \mapsto \cget{P'}{\cget{P}{v}} : v \in V}
\end{algorithm}


\begin{example}
\label{ex:concomp}

Applying $\cd{connectedComponents}$ to the following graph

\begin{center}
\includegraphics[width=2.2]{./media/contract-example1.jpg}
\end{center}

might return:

\begin{eqnarray*}
(\cset{\vname{a}}, ~\cset{\vname{a} \mapsto \vname{a}, 
                          \vname{b} \mapsto \vname{a}, 
                          \vname{c} \mapsto \vname{a}, 
                          \vname{d} \mapsto \vname{a}, 
                          \vname{e} \mapsto \vname{a}, 
                          \vname{f} \mapsto \vname{a}})
\end{eqnarray*}

This is because there is a single component and all vertices will map
to that component label.  In this case $\vname{a}$ was picked as the
representative, but any of the initial vertices is a valid
representative, in which case all vertices would map to it.
\end{example}

\begin{gram}
The only differences from $\cd{countComponents}$ are a modification
to the base case, and the extra line (Line~\linegcncback{}) after
the recursive call.  In the base case instead of returning the size of
$V$ returns all vertices in $V$ along with a mapping from each one to
itself.  This is a valid answer since if there are no edges each
vertex is its own component.  In the inductive case, when returning
from the recursion, Line~\linegcncback{} updates the mapping $P$
from vertices to super-vertices by looking up the component that the
super-vertex belongs to, which is given by $C$.  This simply involves
the look up $C[s]$ for every $(v \mapsto s) \in P$.  Note that if you
view a mapping as a function, then this is equivalent to function
composition, i.e., $C \circ P$.
\end{gram}

\begin{example}
Consider the following example graph.

\begin{center}
\includegraphics[width=2.2]{./media/contract-example1.jpg}
\end{center}

Suppose that $\cd{starPartition}$ returns:
\[
\begin{array}{lcl}
V' & = & \cset{\vname{a},\vname{d},\vname{e}}\\
P & = & 
 \cset{\vname{a} \mapsto \vname{a}, \vname{b} \mapsto \vname{a}, 
       \vname{c} \mapsto \vname{a}, \vname{d} \mapsto \vname{d}, 
       \vname{e} \mapsto \vname{e}, \vname{f} \mapsto \vname{e}}.
\end{array}
\]
%
This pairing corresponds to the case where $a$, $d$ and $e$ are chosen
an centers.
%

Because the graph is connected, the recursive call to
$\cd{connectedComponents}~(V',E')$ will map all vertices in $V'$ to
the same vertex.  Lets say this vertex is $\vname{a}$ giving:
\[
\begin{array}{lcl}
V'' & = & \cset{\vname{a}}\\
P' & = & \cset{\vname{a} \mapsto \vname{a}, \vname{d} \mapsto \vname{a}, \vname{e} \mapsto \vname{a}}~.
\end{array}
\]
%
Now $\cset{v \mapsto P'[s] : (v \mapsto s) \in P}$ will for each
vertex-super-vertex pair in $P$, look up what that super-vertex got
mapped to in the recursive call.  For example, vertex $\vname{f}$ maps
to vertex $\vname{e}$ in $P$ so we look up $\vname{e}$ in $P'$, which
gives us $\vname{a}$ so we know that $\vname{f}$ is in the component
$\vname{a}.$  Overall the result is:
%
\[\cset{\vname{a} \mapsto \vname{a}, 
                          \vname{b} \mapsto \vname{a}, 
                          \vname{c} \mapsto \vname{a}, 
                          \vname{d} \mapsto \vname{a}, 
                          \vname{e} \mapsto \vname{a}, 
                          \vname{f} \mapsto \vname{a}}\;.\]
\end{example}

\begin{exercise}
  Express $\cd{countComponents}$ in terms of higher order function
  $\cd{starContract}$ by giving $\cd{base}$ and $\cd{expand}$ functions.
\end{exercise}

\end{unit}
\end{section}


\begin{section}[Forest Contraction and Tree Contraction]

\begin{unit}
\begin{gram}[Contracting a Forest]
Suppose that we want to contract a forest (set of trees) instead of a
general graph.
% 
Because forests are graphs, we can use the same $\cd{starContract}$
algorithm to contract the forest.
%
Because a forest of $n$ vertices has at most $n-1$ edges, we obtain a
better work bound than for general graphs, because the number of edges
decrease geometrically (in expectation) in each round, as do the
number of vertices.
%
The overall expected work is therefore a
geometric sum of the form: 
%
\[
\expct{W(n,m)} = \sum_{i=0}^{\infty} \left(\frac{3}{4}\right)^i kn =
O(n).
\] 
%
Span of the algorithm remains the same.
%
For forests and thus trees, we thus obtain a much better work
bound---recall that for general graphs, the work bound is $O(m \lg
n)$.
%
\end{gram}


\begin{gram}[Tree Contraction]
For a graph $G = (V,E)$ consider a subset of edges $F \subset E$ that
forms a forest (i.e., has no cycles).  Such a set of edges partitions
the graph $G$, where blocks are defined as the subgraphs induced by
the trees in $F$.
%% %
%% \begin{teachask}
%% How can we perform graph contraction by using trees for partitioning?
%% \end{teachask}
%
We can thus contract a graph by identifying a forest $F$, and then use
$\cd{connectedComponents}~(V,F)$, which does
linear work as explained above, instead of our $\cd{starPartition}$
routine.  
%
This corresponds to~\defn{tree partition} instead of star partition,
which is a special kind of tree.
%
We can thus view this approach as a generalization of the star
contraction technique and may call it \defn{tree contraction}.
%
\end{gram}




\begin{example}
A graph and a subset of the edges $F$ (highligted) consisting of
three disjoint trees illustrated in the middle diagram.  Each tree
induces a block in the original graph (red blobs). 
 
\begin{center}
  \includegraphics[width=6in]{./media/tree-contract-example.jpg}
\end{center}
If we run $\cd{connectedComponents}$ on $F$, then are left with the desired partitioning with
super-vertices $\cset{\vname{a},\vname{g},\vname{i}}$ and the mapping:
%
\[
\cset{\vname{a}\mapsto\vname{a},\vname{b}\mapsto\vname{a},\vname{c}\mapsto\vname{a},\vname{d}\mapsto\vname{a},\vname{e}\mapsto\vname{a},\vname{f}\mapsto\vname{a},
  \vname{g}\mapsto\vname{g},
  \vname{h}\mapsto\vname{i},\vname{i}\mapsto\vname{u},\vname{j}\mapsto\vname{i}}.
\]

Using this partition, we can compute a quotient graph in the usual
way by re-routing edges to the super-vertices.  The resulting quotient graph is 
illustrated on the right.

\end{example}

\begin{note}
We will use this idea in an algorithm for Minimum Spanning
Trees described in \chref{mst}.
\end{note}

\end{unit}
\end{section}

\begin{section}[Concluding Remarks]

\begin{unit}
\begin{gram}
  In general the graph contraction techniques does not specify how to
  partition the graph.  In this chapter, we considered two techniques
  for graph partitioning.  Depending on the problem, other techniques
  can be used.  For graph contraction to be applicable to a problem,
  however, it is important that the quotient graph satisfy certain
  properties.  For example, when solving graph connectivity with the
  algorithms described here, we have to be careful that the graph
  partition maintains connectivity: a subgraph should be connected in
  the quotient graph, if and only if it was connected in the input
  graph.  To ensure this, we will need to use a graph-partition
  algorithm that ensures that each block is connected in the input
  graph.  
\end{gram}

\begin{example}
The pictures below illustrate two graph partitions. The first graph
partition maintains connectivity, the second one does not.
%
The partitioning on the left is appropriate for graph contraction since
each partition is connected.    The partition on the right is not
since $\cd{d}$ is not connected to $\cd{e}$ and $\cd{f}$.


\begin{center}
\includegraphics[width=2.0in]{./media/partition-example1.jpg}
\hspace{1in}
\includegraphics[width=2.2in]{./media/partition-example2.jpg}

\end{center}
\end{example}
\end{unit}
\end{section}


%% \begin{comment}
%% \section{Spanning Trees and Forests}


%% A \emph{spanning tree} of an undirected connected graph $G = (V,E)$ is
%% a tree $T = (V,E')$ where $E' \subseteq E$.  A \emph{spanning forest}
%% of a graph $G = (V,E)$ is the union of spanning trees on its connected
%% components.  We are interested in the spanning forest problem, which
%% is to find a spanning forest for a given undirected graph (the
%% spanning tree is just the special case when the input graph is
%% connected).

%% It turns out that a spanning forest of a graph $G$ can be generated
%% from our connectivity algorithm.  In particular all we need to do is
%% keep track of all the edges that we use to hook, and return the union
%% of these edges.  We will see this in more detail as we cover minimum
%% spanning trees, our next topic.
%% \end{comment}

%% \input{./media/problems} 
%% }
%% \flushchapter

\end{chapter}
\end{book}
