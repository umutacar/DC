\documentclass{course}
\title{Parallel and Sequential Algorithms}

% Course number must be unique in the database
\coursenumber{15210}

\semester{Spring 2018}
\picture{/210/course/air-pavilion.jpg}
\website{http://www.cs.cmu.edu/~15210}

% Provides book
% This must be provided
% The name should be relative to course number.
\providesbook{S18}

% Start counting chapters from 
% This is optional. Will start counting at 1.
\provideschapter{19}
\providessection{1}

15-210 aims to teach methods for designing, analyzing, and programming
sequential and parallel algorithms and data structures. The emphasis
is on teaching fundamental concepts applicable across a wide variety
of problem domains, and transferable across a reasonably broad set of
programming languages and computer architectures. This course also
includes a significant programming component in which students will
program concrete examples from domains such as engineering, scientific
computing, graphics, data mining, and information retrieval (web
search).

Unlike a traditional introduction to algorithms and data structures,
this course puts an emphasis on parallel thinking â€” i.e., thinking
about how algorithms can do multiple things at once instead of one at
a time. The course follows up on material learned in 15-122 and 15-150
but goes into significantly more depth on algorithmic issues. 


\begin{book}
\title{Algorithm Design: Parallel and Sequential}
\authors{Umut A. Acar and Guy Blelloch}


\begin{chapter}[Hash Functions and Hash Tables]
\label{ch:hash}


\picture{./media/lorenz-attractor.jpg}

\begin{section}[Introduction]

\begin{unit}

  \begin{teachnote}
    I taught this as follows.
    Used diderot example.  This was a good example to fall back on.

    To motivate why hashing is possible, I used the example of slicing
    a fruit and giving one a piece of the slice.  They can tell it is
    an apple.  They can tell an apple from a banana.  But if I make
    the slice smaller and smaller, it becomes harder to tell the
    difference.

    I described the modulus operation as slicing the apple.  The mod
    gives us the slice. Multiplication mixes things.  I think this
    intuition should be developed further.

    The weakness of hashing is that collisions can be exploited by an
    adversary.  These should be minimized and made hard to produce.
  \end{teachnote}
  
\begin{teachnote}
TODO:

I (umut) included a mathematical discussion of hash functions and added a
section on universal hashing.  
%
UH notes are synthesized from several books, including CLRS, KT, Jeff
Erickson's notes on hashing, Knuth book, my mathematics books on
number theory, and other research.
%

Given that hash functions are used for all sorts of reasons, it seems
unnecessary to tie their discussion to those of hash tables.  I tried
to separate the two and define them more mathematically.

I kept the discussion of hash tables intact for the most part, due to
time constraints.  
%
I would like to rework these to be more abstract.  
%
For example separate chaining does not have to use array of linked
lists.

\end{teachnote}

\begin{gram}
Hash functions and hash tables are one of the most widely used
techniques in computer science.  Even though their developments have
historically been intertwined, hash functions today are used for many
different purposes than for implementing hash tables.
%
In this chapter, we describe hash functions as computational
structures on their own right, and then discuss their use in hash
tables.
\end{gram}

\begin{example}

The screenshot below illustrates the deployment process of the Diderot
project on Google Cloud.
%
Deployment involves creating a virtual machine on the cloud,
installing all the needed software, and then copying the contents of
the directory from the local computer (a laptop in this case) from
which the deployment is started onto the virtual machine on the cloud.
%
The integrity of this copy operation is checked by using the ``SHA256''
hash function.
%
The hash code, a.k.a., the digest, is computed before the
transmission, transmitted along with the copied contents, and compared
against a freshly computed digest for the transmitted contents.
%

The depyloment process uses the SHA hash function to check that the
transmission operation did not corrupt the contents being copied onto
the cloud.
%

\begin{verbatim}
$ gcloud app deploy app_test.yaml --project diderot-cmu-test
Beginning deployment of service [default]
Building and pushing image for service [default]
Started cloud build [1e27052a-be74-4a38-be05-807042ca1146].
--------------- REMOTE BUILD OUTPUT ---------------
starting build "1e27052a-be74-4a38-be05-807042ca1146"
Successfully built 2abdfcbff888
Successfully tagged [...] appengine/default.20180414t103220:latest
PUSH
Pushing us.gcr.io/diderot-cmu-test/appengine/default.20180414t103220:latest
Repository: [us.gcr.io/diderot-cmu-test/appengine/default.20180414t103220]
21df82f90a72: Preparing
21df82f90a72: Waiting
67b0784928b9: Pushed
724aba9dc62d: Layer already exists
77c1da6d3730: Pushed

latest: digest: sha256:39df2d79576d3c204b8772150052610e65308706f169b72b0351c164a69c2de1 
size: 3889

DONE
-----------------------------------------------------
Updating service [default]...done.
Stopping version [diderot-cmu-test/default/20180403t134333].
Updating service [default]...done.
Updating service [default]...done.
Deployed service [default] to [https://diderot-cmu-test.appspot.com]

\end{verbatim}


\end{example}

\begin{gram}[Secure Hash Algorithm (SHA)]
  SHA stands for ``Secure Hash Algorithm.''
 
%
  Multiple generations of SHA functions have been designed by the NSA.
%
The current generation SHA-2 includes a set of cryptographic hash
functions that are 224, 256, 384, and 512 bits, which are named
respectively as SHA-224, SHA-256, SHA-384, and SHA-512.


SHA functions have the characteristic that a small change to the
contents leads to a large change in the hash code,
a.k.a., the~\defn{avalanche effect}.
\end{gram}


\begin{gram}[Applications of Hashing]
Hashing is widely employed in many applications.

\begin{enumerate}

% ref{ch:}
\item In a previous chapter, we describe how hashing can be used in
  treaps.  In particular we suggested using a hash function to hash
  the keys to generate the ``random'' priorities.  Here what was
  important is that the ordering of the priorities is somehow random
  with respect to the keys.  Our analysis assumed the priorities were
  truly random, but it can be shown that a limited form of randomness
  that arise out of relatively simple hash functions is sufficient.

\item
  In cryptography hash functions can be used to hide information.  One
  such application is in digital signatures where a secure hash
  function is used to describe a large document with a small number of
  bits.

\item A one-way hash function is used to hide a string, for example
  for password protection.  Instead of storing passwords in plain
  text, only the hash of the password is stored.  To verify whether a
  password entered is correct, the hash of the password is compared to
  the stored value.  These signatures can be used to
  \emph{authenticate} the source of the document, ensure the
  \emph{integrity} of the document as any change to the document
  invalidates the signature, and prevent \emph{repudiation} where the
  sender denies signing the document.

\item String commitment protocols use hash functions to hide to what
  string a sender has committed so that the receiver gets no
  information.  Later, the sender sends a key that allows the receiver
  to reveal the string.  In this way, the sender cannot change the
  string once it is committed, and the receiver can verify that the
  revealed string is the committed string.  Such protocols might be
  used to flip a coin across the Internet: The sender flips a coin and
  commits the result. In the mean time the receiver calls heads or
  tails, and the sender then sends the key so the receiver can reveal
  the coin flip.

\item
 Hashing can be used to approximately match documents, or even parts
 of documents. \emph{Fuzzy matching} hashes overlapping parts of a
 document and if enough of the hashes match, then it concludes that
 two documents are approximately the same. Big search engines look for
 similar documents so that on search result pages they don't show the
 many slight variations of the same document (e.g., in different
 formats). It is also used in spam detection, as spammers make slight
 changes to email to bypass spam filters or to push up a document's
 content rank on a search results page. When looking for malware,
 fuzzy hashing can quickly check if code is similar to known malware.
 Geneticists use it to compare sequences of genes fragments with a
 known sequence of a related organism as a way to assemble the
 fragments into a reasonably accurate genome.

\item Hashing is used to implement hash tables.  In hash tables one is
  given a set of keys $K \subset \alpha$ and needs to map them to a
  range of integers so they can stored at those locations in an array.
  The goal is to spread the keys out across the integers as well as
  possible to minimize the number of keys that collide in the array.
  You should not confuse the terms hash function and hash table.  They
  are two separate ideas, and the latter uses the former.
\end{enumerate}

\end{gram}
\end{unit}

\end{section}

\begin{section}[Hash Functions and Universality]
\begin{unit}[Hash Functions]


\begin{definition}[Hash Function]
A~\defn{hash fuction} $h$ is a function from a domain $\kuni{}$,
typically called the~\defn{universe} to a range $\natspre{m} = \{0, 1,
\ldots, m-1 \}$, where $m$ is a positive natural number, i.e.,
\[
h : \kuni{} \ra \natspre{m}.
\]
%
The size of the range $m$ is usually significantly smaller than the
size of the universe.
%

An element of the universe is called a~\defn{key}.
%
An element of the range is  called a~\defn{hash value},~\defn{hash
  code}, or sometimes~\defn{digest}.
%
% For example, for a key $x \in \kuni{}$, $h(x)$ is the hash value, hash
% code, or the digest of $x$.
%
\end{definition}

\begin{definition}[Collisions]
For distinct $x, y \in \kuni{}$,
%
if $h(x) = h(y)$, then we say that $h(x)$ and $h(y)$~\defn{collide}.
%
We also say sometimes that $x$ and $y$ collide.
\end{definition}

\begin{gram}[Properties of Good Hash Functions]

A good hash function, $h: \kuni{} \ra \natspre{m}$ should have at
least the following qualities.

\begin{itemize}
\item \textbf{Cost:} it should not be too difficult to compute, e.g.,
  it should require at most linear work in the size of the key.

\item \textbf{Compactness:} it should not require more than constant
  memory to store and to compute.

\item \textbf{Coverage:} its image should match its range, i.e., for
  any $0 \le i < m$, there exists $x \in \kuni{}$ such that $h(x) =
  i$.  In other words, the hash function should be surjective.

\item \textbf{Mixing:} Given
%
a small set of keys $\{x_0, x_1, \ldots,  x_{k-1} \} \subset \kuni{}$
%
and 
%
their hash codes $h(x_0) \cdots h(x_{k-1})$,
%
it should be difficult to predict the hash code $h(x_k)$ of any other
key $x_k \not\in \{x_0, x_1, \ldots,  x_{k-1} \}$.

\item \textbf{Collision Avoidance:} The function should evenly spread
  collision over its range.
\end{itemize}
\end{gram}


%% \begin{gram}[Intuition for Hash Functions]
%% A hash function can be viewed as a way of naming entities from a
%% universe. 
%% %
%% In many applications, naming entities is useful because
%% \begin{itemize}
%% \item using names, we can compare entities quickly (if two entities
%%   have different names, then they are different),
%% \item names can be chosen as natural numbers, allowing for enumeration
%%   and indexing, which enables fast random access.
%% \end{itemize}

%% In real life as well as in computing, names can be much fewer than the
%% actual items that they identify.
%% %
%% For example, there are more than 300 million people in the United
%% States of America but there are approximately 150,000 distinct last
%% names.
%% %
%% In China, the number of distinct last names is less than 10,000.
%% %

%% Given the relatively few number of names, you would expect there to
%% lots of collisions, but in many cases, we usually compare a tiny
%% fraction of the population.
%% %
%% For example, in your class only less than 10\% of you share a name
%% with somebody else.  
%% %
%% To resorve collisions, we can use additional data, e.g., the city
%% somebody is from. 

%% \end{gram}

\begin{example}
Let $\kuni{}$ be set of all natural numbers and consider the following
hash function 
\[
h(x) = x \bmod 4.
\]

This is not a good has function because, it only consider the least
significant two bits of the key, and thus does not mix well.  Just a
few different applications of the hash could reveal the behavior of
$h$, making it easy to predict the hash of any key in the universe.

More generally any hash function of the form 
\[
h(x) = x \bmod a^b,
\]
where $a, b \in \nats{}$, is not a good hash function for a similar
reason: the function treats the input key as a number base $a$ and
takes the least significiant $b$ digits.
\end{example}

\begin{example}
Let $\kuni{}$ be set of all natural numbers and let $p$ be a
prime number. Consider the hash function
\[
h(x) = x \bmod p.
\]

This is not a good hash function because it is relatively easy to
predict by for example trying out some arguments of the form $x,
x+1, x+2, x+2^2, \ldots$.
%
Thus by observing the behavior of function on logarithmically many
values, we can make a good guess for any value $y \in \kuni{}$.
%
More generally, $h(x) = h(x + cp)$ for any $c$.
%
In other words, the function does not mix well.
\end{example}


\begin{example}[Random Hash Function]
Consider a universe $\kuni{}$ and a range $\natspre{m}$.
%
We can construct a hash function for the universe by picking, for
each key, a uniformly random natural number less than $m$.
%
Such a random hash function has several important qualities.

\begin{itemize}

\item 
The function thus mixes its input keys well.
%
Because the function is random, it is difficult to predict the value
of any key from a small number of observations.
%

\item
The function evenly spreads collision over its range:
for any $x, y \in \kuni{},$ if $x \not= y$ then the probability that
$h(x)$ and $h(y)$ collide is $1/m$.
\end{itemize}

The problem with this hash function is that it is not compact: for
each key in the universe, we have to remember the hash value that it
maps to, which can require a large amounts of space (memory).
\end{example}

\begin{remark}
Even though uniformly random hash functions are not compact, they are
commonly used in the design of algorithms, because they offer a clean
theoretical model.  This is sometimes referred to as the simple
uniform hashing assumption.
\end{remark}

\begin{definition}[Simple Uniform Hashing]
The~\defn{simple uniform hashing} postulates that for any universe and
any range there is a hash function that ensures that each key has
equal probability of being mapped to any valid hash code independent
of what the other elements are mapped to.
\end{definition}


\begin{gram}[Collision Avoidance]
One key challenge is designing hash functions is avoiding collisions.
%
We can show, however, that collisions are impossible to avoid
completely even for hash functions that have a relatively large range.
%

To see this, let's recall a fun fact: the (poorly
named)~\defn{birthday paradox}.
%
~\footnote{There is nothing paradoxical about birthday paradox,
  because it is simply a consequence of counting.}
%
The ``parodox'' states that we only need 23 people in a room to have a
50\% chance that at least two people have the same birthday.
%
If we have 60 people, then we have a 99\% chance that two people have
the same birthday.
%

We can generalize the birthday paradox to show that when hashing to a
range size $m$, we expect a collision to occur with only
$\sqrt{\frac12 \pi\;m}$ keys.
%

A related question is how many key-value pairs do we need until every
hash-code in the range is taken (mapped to). 
%
By using the~\defn{coupon-collector's problem}, we can show that if
the hash function has the range $\natspre{m}$ for some $m$, and if we
have $\Theta(m \log m)$ distinct keys, then we expect every hash-code
to be taken.
\end{gram}

\begin{exercise}
Given a universe $\kuni{}$ and a range $\natspre{m}$.
%
Let $h$ be a random hash function that is constructed by selecting for
each key in the universe a hash-code in the range.
%
Prove that the hash function $h$ satisfies that
%
\begin{itemize}
\item 
for all $x \in \kuni{}$, and 
%
for all $i, 0  \le i < m$, 
\[
\prob{h(x) = i} = 1/m.
\]

\item
for all $x, y \in \kuni{}$ such that $x \not= y$
\[
\prob{h(x) = h(y)} = 1/m.
\]
\end{itemize}
\end{exercise}

\begin{exercise}
Given a universe $\kuni{}$ and a range $\natspre{m}$,
%
consider the set of all functions $\kallhash{}$ and let $h \in
\kallhash{}$ be a function that is uniformly randomly chosen from
$\kallhash{}$. 
%
Prove the following two statements


\begin{itemize}
\item 
For all $x \in \kuni{}$, and 
%
for all $i, 0  \le i < m$, 
\[
\probover{h \in \kallhash{}}{h(x) = i} = 1/m.
\]

\item
For all $x, y \in \kuni{}$ such that $x \not= y$
\[
\probover{h \in \kallhash{}}{h(x) = h(y)} = 1/m.
\]
\end{itemize}

As we will see, we refer to classes (sets) of hash functions for
which the second property hold as ``universal.''

\end{exercise}

%% \begin{solution}
%% The Number of all functions in m**u.  The number of functions fixed is
%% m**(u-1).  
%%  \end{solution}

\begin{group}
\begin{exercise}
Given the universe $\kuni{}$ and the range $\natspre{m}$,
%
construct a set of hash functions $\kallhash{}$ 
such that 
%
for all $x \in \kuni{}$, and 
%
for all $i, 0  \le i < m$, 
\[
\probover{h \in \kallhash{}}{h(x) = i} = 1/m
\]
but the following does not hold:
for all $x, y \in \kuni{}$ such that $x \not= y$
\[
\probover{h \in \kallhash{}}{h(x) = h(y)} = 1/m.
\]
\end{exercise}
\begin{solution}
Let $\kallhash{}$ be the set of all distinct constant functions, each
of which maps all the elements in the universe to a single hash code
in the range.
%
The first property holds because for a uniformly randomly hash
function, each hash code is equally likely to be selected.
%
The second property does not hold, because each hash function is a
constant function and thus  the releveant probability is $1.0$.
\end{solution}

\end{group}

\end{unit}

\begin{unit}[Universal Hashing]


\begin{gram}
We have seen in the previous section that if we construct a random
hash function by selecting for each key a uniformly random hash code,
then the function minimizes the number of collision:
%
the probability that any two keys collide is $1/m$ for the range
$\natspre{m}.$
%
We have also seen, however, that such a random hash function is not
compact, because we have to remember the mapping of keys to hash
values explicitly.
%
A natural question is whether it is possible to construct a hash
function that is compact and cheap to compute but has the same
guarantee over collisions.
%
In this section, we shall see that this is indeed possible.
\end{gram}

\begin{definition}[Universal Class of Hash Functions]
Let $\kallhash{}$ be a class (set) of hash function from a universe
$\kuni{}$ to the range $\natspre{m}$ for some $m$.
%
We say that $\kallhash{}$ is~\defn{universal} if the probability that
two distinct keys of the universe collide under a uniformly randomly
chosen hash function is $1/m$, i.e.,
\\
for all~$x, y \in \kuni{}$ such that $x \not= y$:
\[
\probover{h \in \kallhash{}}{h(x) = h(y)} = 1/m.
\] 
\end{definition}

\begin{gram}
There are at least several different techniques for obtaining classes
of universal hash functions.
%
We will state here, without proof, a few techniques that are some of
the most commonly used ones.
\end{gram}

\begin{theorem}[Multiplicative Hashing]
Consider a finite universe $\kuni{} \subset \nats{}$ and any range
$\natspre{m}.$
%
Let $p$ be a prime number that is greater than the size of the
universe, i.e., $p > |\kuni{}|.$
%
For any natural number $a$, $0 < a < p$,  let
\[
h_a(x) = (a x \bmod p) \bmod m.
\] 

The class of hash functions \kallhash{} defined as
\[
\kallhash{} = \{ h_a(x) ~|~ 0 < a < p \}
\]
is universal.
\end{theorem}

\begin{theorem}[Multiplicative Hashing with Offset)]
Consider a finite universe $\kuni{} \subset \nats{}$ and any range
$\natspre{m}.$
%
Let $p$ be a prime number that is greater than the size of the
universe, i.e., $p > |\kuni{}|.$
%
For integers $a$ and $b$,  $0 < a, b < p$,  let
\[
h_{a,b}(x) = (a x + b \bmod p) \bmod m.
\] 

The class of hash functions \kallhash{} defined as
\[
\kallhash{} = \{ h_{a,b}(x) ~|~ 0 < a < p, 0 \le b < p \}
\] 
is universal.
\end{theorem}


\begin{gram}
As the theorems above show, it is relatively easy to construct classes
of compact and efficient hash functions that are universal.
%
One somewhat concerning assumption could be that we need a prime
number larger than the size of the universe.
%
The next theorem eliminates this assumption by allowing us to work
with essentially any sufficiently large prime number.
\end{gram}

\begin{theorem}[Dot-Product Hashing]
Let $m$ be a prime number and $r$ be a positive integer.
%
Consider the universe $\kuni{} = \natspre{m^r}$ and
the range $\natspre{m}$.
%
For any natural number $a$, $0 < a < m^r$,  let
\[
h_a(x) =  \left( \sum_{i = 0}^{r-1}{a_i \cdot x_i} \right) \bmod m,
\] 
where $a_i$ and $x_i$ denote the $i^{th}$ digit of $a_i$ and $x_i$ in
base $m.$

The class of hash functions \kallhash{} defined as
\[
\kallhash{} = \{ h_a(x), ~|~ 0 < a < m^r \}
\]
is universal.
\end{theorem}

\begin{gram}[Intuition]
The idea behind the theorem is to read the keys of the universe as
numbers in base $p$, which is a prime number.
%
We select $r$ to be big enough such that all keys are natural numbers
less than $p^r$. 
%
Given some $0 < a < p^r$, we then define the hash function $h(x)$ to
the sum of the products of digits of $a$ and the key $x$ modulo
$p$.
%
\end{gram}

\begin{remark}
The theorem fixes the hash codes to be numbers congruent to a prime
$p$, i.e., the integers between $0$ and $p-1$.
%
For the theorem to be effective, we would need to select $p$ to be
close to the number of distinct hash codes that we are interested in.
%
\end{remark}

\begin{example}[Universal Hashing for Strings]
Dot-product hashing yields a natural hash function for strings.
%
Let $r$ be the maximum length of the strings and interpret each
character of the string as a natural number.
%
Select a prime $p$ to bound the value of each character and to be
large enough to reduce the probability of collision to be small.
%

For any length-$r$ string $a$, define
\[
h_a(x) = \left( \sum_{i = 0}^{r-1}{a_i \cdot x_i} \right) \bmod p,
\] 
where $a_i$ and $x_i$ denote the $i^{th}$ character of $a_i$ and $x_i$.

The class of hash functions $\kallhash{}$ defined as
\[
\kallhash{} = \{ h_a(x) ~|~ a~\text{is a string of length}~r \}
\]
is universal.
\end{example}


\begin{gram}[Bounding the Number of Collisions]
The key quantity of interest in understanding the effectiveness of
hashing is the number of collisions that a key may be involved in.
%
To understand this quantity, let's define $\xconfp{x}{y}$ to be an
indicator random variable such that
\[
\xconfp{x}{y} = 
\left\{ 
\begin{array}{ll}
1 & \mbox{if}~h(x) = h(y)
\\
0 & \mbox{otherwise}.
\end{array}
\right.
\]

Because $\xconfp{x}{y}$ is an indicator random variable its expectation
is the same as the probability that it is $1$.
%
Assuming universal hashing (or more strongly simple uniform hashing),
we know that for any $x \not= y$
\[
\expct{\xconfp{x}{y}} = \prob{\xconfp{x}{y} = 1} = 1/m.
\]

Suppose now that we have $n$ keys that we wish to hash and we wish to
bound the number of collisions that any key is involved in.
%
Define the random variable $\xconf{x}$ to be total number of keys
other than $x$ that collide with $x$. 
\[
\begin{array}{lcl}
\xconf{x} & = & \sum_{y, y \not= x}{\xconfp{x}{y}}
\\
\expct{\xconf{x}} & = & \sum_{y, y \not= x}{\expct{\xconfp{x}{y}}}
\\
\expct{\xconf{x}} & = & \frac{n-1}{m}
\\
& \le & \frac{n}{m}.
\end{array}
\]

We can similarly bound the total number of collisions across all keys.
%
To this end let $\nconf{}$ be the random variable denoting the total
number of collisions.
%
Because there are exactly ${n \choose 2}$ distinct pairs of keys that
could collide, we can bound the expectation of $\nconf{}$ as

\[
\begin{array}{lcl}
\nconf{} & = & \sum_{x,y, x \not=y}{\xconfp{x}{y}}
\\
\expct{\nconf{}} & = & \sum_{x, y, x \not= y}{\expct{\xconfp{x}{y}}}
\\
\expct{\nconf{}} & = & {n \choose 2} \cdot \frac{1}{m}
\\
\expct{\nconf{}} & \le & \frac{n^2}{2m}.
\end{array}
\]

Let's summarize these bounds.
\begin{itemize}

\item If the range of the class of the universal hash functions $m$ is
  large compared to the number keys $n$, then we expect a relatively
  small number of collisions for any key.
%
\item Summed over all keys, the expected number of collisions is a bit
  larger, but still proportional to the square of the number of keys
  hashed.
%
\end{itemize}

In many cases, it will be sufficient to bound the expected number of
collisions per key by a constant, and thus it is sufficient to
consider $m = O(n)$, e.g., $m = 2n$.
%

In other cases, it is desirable to reduce the number of collisions
further, so that for example, we have only a few collisions across all
keys.
%
This can be achieved by choosing the range of our functions to be
larger, e.g., for $m = n^2$, the expected number of collisions is
$1/2$.

\end{gram}

\begin{gram}[Bounding the Probability of a Collision]
Consider a class of universal hash functions from a universe $\kuni{}$
to a range $\natspre{m}$, we know that for a uniformly random hash
function $h$ from this class, we know that for all~$x, y \in \kuni{}$
such that $x \not= y$:
\[
\probover{h \in \kallhash{}}{h(x) = h(y)} = 1/m.
\] 

By the union bound, we can bound the probability that there exists at
least one collision among $n$ keys by considering all distinct pairs
of keys.
%
We know that the probability that a distinct pair of keys collide is
$\frac{1}{m}$ and we know that have exactly ${n \choose 2}$ distinct
pairs.
 \[
\begin{array}{lcl}
\probover{h \in \kallhash{}}{\text{There is a collision}} 
& \le & 
\sum_{x, y, x \not= y}{\probover{h \in \kallhash{}}{h(x) = h(y)}}.
\\
& \le & \sum_{x, y, x \not= y}{\frac{1}{m}}
\\
& \le & {n \choose 2} \cdot \frac{1}{m}
\\
& \le & \frac{n(n-1)}{2m}
\\
& \le & \frac{n^2}{2m}.
\end{array}
\]

This means that if we choose the range of our hash functions to be
u$n^2$, the probability that there is a (at least one) collision is
$\frac{1}{2}$.
%

\end{gram}

\begin{remark}
The bound on the probability of collisions has interesting
consequences.
%
It means for example that we can hash $n$ known keys perfectly with
no collisions.
%
To this, we first choose $m = n^2$, and then pick a hash function
uniformly randomly from the class of universal hash functions.
%
The chosen random function will avoid all collisions with probability
$\frac{1}{2}$.
%
If it does, then we stop---we have found the perfect hash function.
%
But there might be collisions, in which case, we can retry by picking
another random hash function.
%
After expected $2$ tries, we will find a perfect hash function
with no collisions.  This idea is the key idea behind a technique
called perfect hashing that we shall see later in this chapter.
\end{remark}

\begin{exercise}
We bounded the probability of a collision by direct calculation. 
%
This bound can also be obtained by applying Markov's inequality.
Describe how. 
\end{exercise}

\begin{exercise}
Consider a universal class of hash function $\kallhash{}$ for some
universe $\kuni{}$ and $T \subseteq U$ be any subset of $U$.
%
Prove that for any key $x \in U$, the number of keys whose hash value
collide with $x$ under a uniformly randomly chosen hash function $h \in
\kallhash{}$ is constant. 
\end{exercise}

\begin{teachnote}
There is something unsatisfactory with the prime choice, we should be
able to scale it back to mod m for any m.  I think jeff erickson's
notes do this but not i a direct way perhaps.
\end{teachnote}

\end{unit}



\end{section}

\begin{section}[Hash Tables]
\begin{unit}[Interface and Collisions]


\begin{gram}[Interface]
A hash table is an abstract data type that supports the following
operations on key-value pairs, where keys are drawn from a universe
(e.g., integers, strings, records) and accept an equality test
(function).


\begin{itemize}
\item The $\cd{createTable}$ function takes as argument an equality
  function on keys, a hash function generator that returns a hash
  function given a natural number that specifies the size of its range,
  and an initial size and creates an empty hash table of that given
  size.

\item The $\cd{insert}$ function takes as argument a hash table and a
  key-value pair and inserts the pair into the table.

\item The $\cd{lookup}$ function takes a hash table and a key and returns
  the value for the key stored in the hash table if any, or indicates
  that the key is not found.

\item The $\cd{loadAndSize}$ function takes a hash table and returns
  the number of key-value pairs stored in the table and the size of
  the table.

\item The $\cd{resize}$ function takes a hash table and a new size,
  usually double or half the current size, and returns a new hash
  table that contains the same key-value pairs as in the original
  paper, nothing less and nothing more. 
\end{itemize}

Hash tables enable us to maintain a dynamically changing mapping from
keys to values.
%
In this sense, they are a special case of table data type that we have
seen in the past.
%
They differ from tables in several ways.
\begin{itemize}
\item Hash tables don't require the keys to be totally ordered and
  don't demand a comparison function on keys.  Instead they require
  the keys to be hashable.
\item They support a narrower set of operations that revolve around
  insertions and deletions.
\end{itemize}
%
\end{gram}

\begin{gram}[Collisions]
The main challenge in designing hash tables is resolving collisions,
where two keys hash to the same hash code.
%
There are several well-studied collision resolution strategies.
\begin{itemize}
\item \textbf{Nested tables:} use an outer table to map each hash code
  to an inner table that contains the key-value pairs that map to that
  hash code.  The inner table can be represented in several different
  ways, including as a lists, or as another hash table.  
%
If the inner
  table is a list, the technique is called ``separate chaining.''
%

\item \textbf{Flat Tables or Open Addressing}: Use a single, flat
  table mapping keys to entries. 
\end{itemize}

Between the two possibilities, the nested tables are more flexible and
more amenable to analysis.


%% Between the two possibilities, the nested tables are usually the
%% preferred method, because of their elegance, flexibility, and their
%% amenability to analysis.  
%% %
%% In practice, flat tables have certain advantages such as better
%% locality, that can make the
\end{gram}

%% \begin{example}
%% Different types of hash tables.  The grey indicates the location is
%% already full with another key.
%% \begin{center}\hspace*{-.25in}
%% \begin{tabular}{cccc}
%% \includegraphics[height=2in]{./media/separate-chaining.jpg} &
%% \includegraphics[height=2in]{./media/open-addressing.jpg} &
%% \includegraphics[height=2in]{./media/perfect-hashing.jpg} &
%% \includegraphics[height=2in]{./media/cuckoo-hashing.jpg} \\
%% separate chaining & open addressing & perfect hashing & cuckoo hashing
%% \end{tabular}
%% \end{center}
%% \end{example}

\begin{definition}[Load Factor]
For a hash table of size $m$ with $n$ key-value pairs stored in the
table, the \defn{load factor}, written as $\kloadfac{}$, is defined as
\[
\kloadfac{} = \frac{n}{m}.
\]
\end{definition}

\end{unit}


\begin{unit}[Nested Tables: A Parametric Implementation]
\begin{gram}
The basic structure of a nested table implementation is naturally
recursive: keep an outer table that maps each key to an inner table,
which can be structures as desired.
%
Given a key, we  use an outer hash function to determine the inner
table that the key maps to.
%
We then use the inner table to resolve the collisions.
%
Because the outer hash function maps keys to a prefix of the natural
numbers, the domain of the outer table is natural numbers less than
the current size $m$. 
%
We can thus use an array to represent the outer table and locate the
inner table efficiently with constant work.
\end{gram}

\begin{example}
Consider the following table mapping keys to values.

\[
\begin{alignat}{2}
\{
 & 
\cstr{aa} \mapsto \cstr{a},
\cstr{bb} \mapsto \cstr{b}, 
\cstr{cc} \mapsto \cstr{b}, 
\cstr{dd} \mapsto \cstr{d}, 
\cstr{ee} \mapsto \cstr{e},
\\
 & 
\cstr{ff} \mapsto \cstr{f}, 
\cstr{gg} \mapsto \cstr{g}, 
\cstr{hh} \mapsto \cstr{h}, 
\cstr{ii} \mapsto \cstr{i}, 
\cstr{jj} \mapsto \cstr{j}
\}.
\end{alignat}
\]

Let
\[
h(x) = \left(\sum{\mathit{pos}(x[i])}\right) \bmod m
\]
be a hash function that maps each string to a hash code by summing up
the positions of its characters in the alphabet (counting from zero)
modulo the table size $m = 5$.

We can use the following nested hash table for our  key-value
pairs.

\[
\begin{alignat}{1}
\{
& 0 \mapsto \{ \cstr{aa} \mapsto \cstr{a}, \cstr{ff} \mapsto \cstr{f} \},
\\
& 1 \mapsto \{ \cstr{dd} \mapsto \cstr{d}, \cstr{ii} \mapsto \cstr{i} \},
\\
& 2 \mapsto \{ \cstr{bb} \mapsto \cstr{b}, \cstr{gg} \mapsto \cstr{g} \},  
\\
& 3 \mapsto \{ \cstr{ee} \mapsto \cstr{e}, \cstr{jj} \mapsto \cstr{j} \},  
\\
& 4 \mapsto \{ \cstr{cc} \mapsto \cstr{c}, \cstr{hh} \mapsto \cstr{h} \},  
\\
\} & .
\end{alignat}
\]
\end{example}



\begin{gram}[Bounding the Size Inner Tables]
The key quantity of interest in understanding the efficiency of nested
tables is the size of an inner table.
%
Since any inner table stores the key-value pairs that collide with
each other, we can bound the their size in terms of conflicts.
%

Conflicts can be very high in general but not so if we use universal
hash functions.
%
Recall that we bounded the expected number of conflicts for any key
$x$ in terms of the 
\[
\expct{\xconf{x}} \le \frac{n}{m} = \kloadfac{}.
\]
This means that the size of an inner table in $O(1 + \kloadfac{})$ in
expectation.

Thus, if we ensure that the load factor of the table remains a
constant by making sure for example that $n \le cm$, for some constant
$c$, then we know that the size of each inner table is constant in
expectation.



%%  the operations such as \cd{insert},
%% \cd{delete}, and \cd{lookup}, can be performed with constant work.
\end{gram}



\begin{teachnote}
It is an interesting exercise to try to do the same analysis by trying
to bound the length of an inner table at position i.  It should fail
because you need a max over expectaitons but then you need
independence.

Note also that the analysis does not bound the expected maximum length
of the table, and thus cannot be interpreted as expected worst-case time.
\end{teachnote}


\begin{gram}[Keeping the Load Factor Small]
Because the size of the table $m$ is fixed and $n$ changes, the load
factor can increase as  a result of insertions.
%
To keep the load factor from growing, we can resize the table, by for
example doubling it every time the load factor exceeds the desired
bound.
%
The cost of the resize operation can be amortized because doubling
ensures that the new keys pay for the old ones.
\end{gram}

\begin{exercise}
Describe how you can implement the hash table interface specified
above by using nested tables. 
%
For the inner tables use the Table ADT that you have learned about
earlier but leave out the implementation and thus the costs
unspecified.
\end{exercise}

\begin{exercise}
Does it make sense to reduce the size of the hash table?  If so, then
under what conditions and how?
\end{exercise}
\end{unit}

\begin{unit}[Separate Chaining]

\begin{gram}
The parametric implementation that we outlined above uses an array to
represent the outer table but does not specify how to implement the
inner table.
%

Perhaps the simplest way to implement the inner table is to use a list
representation that stores at each node one a key-value pairs.
%
Such an implementations is called \defn{separate chaining} or simply
as \defn{chaining}.

In separate chaining, insertion proceeds by first locating the inner
table, a list, and then inserting the key-value pair at the head of
the list; this requires constant work.
%
Lookups could proceed by first looking up the list using the hash code
of the key being searched, and then searching for the key from the
head of the list using the key equality function; this requires work
linear in the length of the list.
%
Deletions could proceed by first looking up the key and then deleting
it, again requiring work linear in the length of the list.
\end{gram}

\begin{example}
Recall the example,  where we are given the following table mapping keys to values.

\[
\begin{alignat}{2}
\{
 & 
\cstr{aa} \mapsto \cstr{a},
\cstr{bb} \mapsto \cstr{b}, 
\cstr{cc} \mapsto \cstr{b}, 
\cstr{dd} \mapsto \cstr{d}, 
\cstr{ee} \mapsto \cstr{e},
\\
 & 
\cstr{ff} \mapsto \cstr{f}, 
\cstr{gg} \mapsto \cstr{g}, 
\cstr{hh} \mapsto \cstr{h}, 
\cstr{ii} \mapsto \cstr{i}, 
\cstr{jj} \mapsto \cstr{j}
\}.
\end{alignat}
\]

Let
\[
h(x) = \left(\sum{\mathit{pos}(x[i])}\right) \bmod m
\]
be a hash function that
maps each string to a hash code by summing up the positions of its
characters in the alphabet (counting from zero) modulo the table size
$m = 5$.

Using chaining, we represent this table as

\[
\begin{alignat}{1}
\{
& 0 \mapsto [ (\cstr{aa},\cstr{a}), (\cstr{ff}, \cstr{f})], 
\\
& 1 \mapsto [ (\cstr{dd} , \cstr{d}), (\cstr{ii} , \cstr{i}) ],
\\
& 2 \mapsto [ (\cstr{bb} , \cstr{b}), (\cstr{gg} , \cstr{g}) ],  
\\
& 3 \mapsto [ (\cstr{ee} , \cstr{e}), (\cstr{jj} , \cstr{j}) ],  
\\
& 4 \mapsto [ (\cstr{cc} , \cstr{c}), (\cstr{hh} , \cstr{h}) ]
\\
\} & .
\end{alignat}
\]
\end{example}


\begin{gram}[Cost Analysis of Separate Chaining]
As described, $\cd{insert}, \cd{delete},$ and $\cd{lookup}$ operations all
spend $O(1+\kloadfac{})$ work traversing the chain.
%
Because the hash function takes constant work, total expected work for
these operations is is $O(1+\kloadfac{})$.
%
Thus assuming that $\kloadfac{}$ is a constant, the total expected
work for these operations is is $O(1)$.
\end{gram}

\begin{exercise}
Describe how to implement the resize operation and bound its cost.
\end{exercise}

\end{unit}

\begin{unit}[Perfect Hashing]
\begin{gram}
Nested tables with separate chaining gives us expected constant time
bounds on the key hash table operations.
%
Consider now the special case where we know exactly the set of keys
that we wish to store in the table.
%
In other words, we only wish to perform $\cd{lookup}$ operations on a
static set of keys.
%

In this special case, we can achieve worst case work for $\cd{lookup}$
operations by using a nested hash table, where the inner table itself
is a  hash table with chaining.
%
To ensure  constant-work in the worst case, we will make sure that all
chains (lists) in the inner table has length at most one, i.e., they
contain a single key-value pair or they are empty.
%
In other words, for the inner table, we guarantee the absence of
collisions.

To this end, we are going to use a result from universal hashing. 
%
Recall that for a hash table with range-size $m$, the expected number
of collisions among $n$ key is 
\[
\expct{\nconf{}} \le \frac{n^2}{2m}
\]
and the probability that there is a collision is at most
\[
\frac{n^2}{2m}.
\]

Thus, if we choose $m = n^2$, then we have 
\[
\expct{\nconf{}} \le \frac{1}{2}
\]
and the probability that there is a collision is at most
\[
\frac{1}{2}.
\]
%
This is a lot of space of course and can be unaffordable, but we can
imagine applying this approach to each inner table, because we expect
them to be small.
%

\end{gram}

\begin{group}
\begin{algorithm}[Perfect Hashing]

We are given $n$ key-value pairs that we wish to store in a hash
table. 
%
We can construct a perfect hash table for the set of key-value
pairs as follows.
%
\begin{itemize}
\item Choose a uniformly random hash function $h$ from a universal
  hash family with a range of $n$, the total number of key-value pairs
  that we wish to store.

\item Use $h$ for the outer table and determine the key-value pairs
  for each inner table $T_i$, $0 \le i < n$.  Let $n_i$ be the number
  of key-value pairs in the inner table.

\item For each inner table $T_i$ with $n_i$ key-value pairs, select a
  hash function whose range is $n_i^2$ from a universal hash family.
  Check that the hash function guarantees absence of collisions for
  the keys in $T_i$.  If there are collisions, choose another hash
  function. Step when a hash function $h_i$ that guarantees the
  absence of collisions is found.

\item Represent each inner table $T_i$ by using a hash table with
  chaining and the hash function $h_i$ that guarantees absence of
  collisions.
\end{itemize}
\end{algorithm}

\begin{example}
Consider the following table mapping keys to values with $n = 10$
key-value pairs.

\[
\begin{alignat}{2}
\{
 & 
\cstr{aa} \mapsto \cstr{a},
\cstr{bb} \mapsto \cstr{b}, 
\cstr{cc} \mapsto \cstr{b}, 
\cstr{dd} \mapsto \cstr{d}, 
\cstr{ee} \mapsto \cstr{e},
\\
 & 
\cstr{ff} \mapsto \cstr{f}, 
\cstr{gg} \mapsto \cstr{g}, 
\cstr{hh} \mapsto \cstr{h}, 
\cstr{ii} \mapsto \cstr{i}, 
\cstr{jj} \mapsto \cstr{j}
\}.
\end{alignat}
\]

Let
\[
h(x) = \left(\sum{\mathit{pos}(x[i])}\right) \bmod m
\]
be a hash function that maps each string to a hash code by summing up
the positions of its characters in the alphabet (counting from zero)
modulo the table size $m$.
%

In perfect hashing we select $m = n$, thus $m = 10$.
%
First, we build the outer table, determining for each hash-code the
key-value pairs that map to that hash code.
%
This gives us the following hash table.

\[
\begin{alignat}{1}
\{
& 0 \mapsto \{ \cstr{aa} \mapsto \cstr{a}, \cstr{ff} \mapsto \cstr{f} \},
\\
& 1 \mapsto \{ \},
\\
& 2 \mapsto \{ \cstr{bb} \mapsto \cstr{b}, \cstr{gg} \mapsto \cstr{g} \},  
\\
& 3 \mapsto \{ \}
\\
& 4 \mapsto \{ \cstr{cc} \mapsto \cstr{c}, \cstr{hh} \mapsto \cstr{h} \},  
\\
& 5 \mapsto \{ \}
\\
6 \mapsto \{ \cstr{dd} \mapsto \cstr{d}, \cstr{ii} \mapsto \cstr{i} \}
\\
7 \mapsto \{ \}
\\
8 \mapsto \{ \cstr{ee} \mapsto \cstr{e}, \cstr{jj} \mapsto \cstr{j} \}
\\
9 \mapsto \{ \}
\\
\} & .
\end{alignat}
\]

Next, we select for each inner table a new hash function uniformly at
random from our class of universal functions.
%
In our case, we can select hash functions of the form 
Let
\[
h_i(x) = \left(\sum{\mathit{pos}(a \cdot x[i])}\right) \bmod m_i,
\]
where $m_i$ is the size of the $i^{th}$ inner hash table and $0 \le a < m_i$.
%
Recall that $m_i$'s are square of the number of key-value pairs in
that table.
%
In our case, we have 
%
$m_0 = m_2 = m_4 = m_6 = m_8 = 4$
%
and 
%
$m_1 = m_3 = m_5 = m_7 = m_8 = 0$.

For simplicity, we shall choose the following hash function for all
inner tables.  
\[
h_i(x) = \left(\sum{\mathit{pos}(x[i])}\right) \bmod 4
\]
%
This gives us the perfect hashing for each inner table.

\[
\begin{alignat}{1}
\{
& 0 \mapsto \{ 0 \mapsto [\cstr{aa} \mapsto \cstr{a}], 2 \mapsto [\cstr{ff} \mapsto \cstr{f}] \},
\\
& 1 \mapsto \{ \},
\\
& 2 \mapsto \{ 0 \mapsto [\cstr{gg} \mapsto \cstr{g}], 2 \mapsto [\cstr{bb} \mapsto \cstr{b}] \},  
\\
& 3 \mapsto \{ \}
\\
& 4 \mapsto \{ 0 \mapsto [\cstr{cc} \mapsto \cstr{c}], 2 \mapsto [\cstr{hh} \mapsto \cstr{h}] \},  
\\
& 5 \mapsto \{ \}
\\
& 6 \mapsto \{ 0 \mapsto [\cstr{ii} \mapsto \cstr{i}], 2 \mapsto [\cstr{dd} \mapsto \cstr{d}] \}
\\
& 7 \mapsto \{ \}
\\
& 8 \mapsto \{ 0 \mapsto [\cstr{ee} \mapsto \cstr{e}], 2 \mapsto [\cstr{jj} \mapsto \cstr{j}] \}
\\
& 9 \mapsto \{ \}
\\
\} & .
\end{alignat}
\]

\end{example}

\end{group}

\begin{gram}[Analysis of Perfect Hashing]

By construction, perfect hashing guarantees the absence of collisions
in the inner table, it therefore supports $O(1)$ lookup time.

Perhaps the most interesting quantity that we are interested in the
size of the hash table, including of course the outer and the inner
tables.
%
We prove that this is linear, i.e., $O(n)$, in expectation when
storing $n$ key-value pairs.
%
To establish this bound we need to sum up the sizes of all inner
tables, each of which is quadratic in the number of key-value pairs
that it stores.
%
To this end, imagine the complete directed graph with $n$ vertices,
where each vertex represents a key-value pair stored and each distinct
pair of vertices is connected by two edges, one in each direction, and
each vertex has one self-loop.
%
Observe now that the total space of the inner tables corresponds
exactly to the number edges between vertices that are within the same
inner table.
%
Next observe that the two endpoints of an edge are within the same
inner table if the outer hash code of the corresponding keys collide.
%
In other words, the number of such edges is two times the total number
of collisions, plus $n$ to account for the self loops.
%
Because we use universal hashing, we know that the total expected
number of collisions in the outer hash table is $\frac{n^2}{2m}$,
where $m$ is the size of the range of the hash function.
%
Since we know that $m = n$, the bound on the expected space usage is 
\[
2 \frac{n^2}{2n} + n = 2n.
\] 

\end{gram}

\begin{exercise}
What is the probability that a perfect hash table uses more than $2n$,
say $16n$ space? 
\end{exercise}

\begin{exercise}
Analyze the work required to construct a hash table for $n$ key-value
pairs.

\end{exercise}

\end{unit}

\begin{unit}[Flat Tables or Open Addressing]

\begin{gram}
When using flat tables, we store all key-value pairs in a single table
that maps keys to key-value pairs. 
%
We minimize the impact of collisions by keeping the load factor of the
table low.
%
Because the table is flat, however, keys that map to the same
hash-code can interact in interesting ways, e.g., when two keys
collide and map to the same hash code, only one could be mapped by the
hash code.
%
We therefore have to be careful about dealing with collisions.
%

The basic idea behind flat hash tables is to perform a sequence of
``probes'' until a suitable position in the hash table is found.
%
More precisely, consider a hash table of size $m$.
%
To insert a key-value pair into the table, we repeatedly~\defn{probe}
the table in different position until we find an available position
and claim that position.
%
We refer to the sequence of probes as a \defn{probe sequence}, and for
correctness require it to try out all positions in the table.
%
As we shall see, probe sequences can be generated in several different
ways.
\end{gram}

\begin{definition}[Probe Sequence]
For a hash table of with $m$ entries, a~\defn{probe sequence} is a
permutation of $\natspre{m} = \{0, 1, \ldots, m-1\}$.
\end{definition}

\begin{datastr}[A Parametric Implementation]
We present an implementation of open addressing by assuming that for
the current hash function of size $m$, we have $m$ hash function 
\[
h_o(x), h_1(x), \ldots, h_{m-1}(x)
\]
that generate the probe sequence for any key $x$.
%

To specify the iplementation, we assume that 
we are given the types for keys and values, $\cd{keyType}$ and
$\cd{valueType}$ respectively.
%
We also assume the existence of a function $\cd{eqKey}$ for checking
that two keys are equal.

We define the type of a hash table as
\[
\begin{array}{lcl}
\cd{type entryType} & = & \cd{Empty} 
\\
& | & \cd{Dead} 
\\
& | & \cd{Live of keyType  * valueType}
\\
\cd{type hasTableType} & = & \cd{entryType array}.
\end{array}
\]
%
The first variant $\cd{Empty}$ of $\cd{entryType}$ indicates an
empty entry, the second $\cd{Dead}$ indicates that the entry has been deleted, and
the third indicates that the entry is live and has the given key and
value.

Keeping track of deleted entries enables the implementation to find a
key when its probe sequence interleaves with the probe sequence of
another key, which may later be deleted.
%

\[
\begin{array}{ll}
1 & \cd{lookup}~(T, k) = 
\\
2 &  ~~~\cd{let}
\\
3 &  ~~~~~~\cd{lookup}~(i) =
\\
4 &  ~~~~~~~~~\cd{case}~T[h_i(k)]~\cd{of}
\\
5 &  ~~~~~~~~~~~~\cd{Empty} \dra \cd{None}
\\
6 &  ~~~~~~~~~|~\cd{Dead} \dra \cd{lookup}~(i+1)
\\
6 &  ~~~~~~~~~|~\cd{Live}(k', v') \dra  
\\
7 &  ~~~~~~~~~~~~\cd{if}~(\cd{keyEqual}(k, k')~\cd{then}~\cd{Some(v)}
\\
8 &  ~~~~~~~~~~~~\cd{else}~\cd{lookup'}~(i+1)
\\
9 & ~~~\cd{in} 
\\
10 & ~~~~~~\cd{lookup'}~(0) 
\\
11 & ~~~\cd{end}
\end{array}
\]

The $\cd{insert}$ function is very similar to $\cd{lookup}$ but it updates
the table with the given key-value pair.
%
For simplicity, we assume that key is not in the table, which can be
checked by using a $\cd{lookup}$ first.

\[
\begin{array}{ll}
1 & \cd{insert}~(T, k, v) = 
\\
2 &  ~~~\cd{let}
\\
3 &  ~~~~~~\cd{insert'}~(i) =
\\
4 &  ~~~~~~~~~\cd{case}~T[h_i(k)]~\cd{of}
\\
5 &  ~~~~~~~~~~~~\cd{Empty} \dra \cd{update} (T, h_i(k), \cd{Live}(k, v))
\\
6 &  ~~~~~~~~~|~\cd{Dead} \dra \cd{update} (T, h_i(k), \cd{Live}(k, v))
\\
6 &  ~~~~~~~~~|~\cd{Live}((k', v') \dra  
\\
7 &  ~~~~~~~~~~~~\cd{if}~(\cd{keyEqual} (k, k')~\cd{then}~\cd{raise}~\cd{DuplicateKey}
\\
8 &  ~~~~~~~~~~~~\cd{else}~\cd{insert'}~(i+1)
\\
9 & ~~~\cd{in} 
\\
10 & ~~~~~~\cd{insert'}~(0) 
\\
11 & ~~~\cd{end}
\end{array}
\]

The delete function is similar. For simplicity, we assume that the key
is indeed in the table; this can be checked by performing a
$\cd{lookup}$ first.

\[
\begin{array}{ll}
1 & \cd{delete}~(T, k) = 
\\
2 &  ~~~\cd{let}
\\
3 &  ~~~~~~\cd{delete'}~(i) =
\\
4 &  ~~~~~~~~~\cd{case}~T[h_i(k)]~\cd{of}
\\
5 &  ~~~~~~~~~~~~\cd{Empty} \dra \cd{raise NotFound}
\\
6 &  ~~~~~~~~~|~\cd{Dead} \dra \cd{delete} (i+1)
\\
6 &  ~~~~~~~~~|~\cd{Live}((k', v') \dra  
\\
7 &  ~~~~~~~~~~~~\cd{if}~(\cd{keyEqual}(k, k')~\cd{then}~\cd{update}~(T, k, \cd{Dead})
\\
8 &  ~~~~~~~~~~~~\cd{else}~\cd{delete'}~(i+1)
\\
9 & ~~~\cd{in} 
\\
10 & ~~~~~~\cd{delete'}~(0) 
\\
11 & ~~~\cd{end}
\end{array}
\]

\end{datastr}


\begin{example}
Let $T$ be the following table 
\begin{center}
\begin{tabular}{c c c c c c c c }
      0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\ 
\hline
        & B &   & D & E & A &   & F \\ 
\hline
\end{tabular}
\end{center}
if key E has the probe sequence 
\[
\cseq{7, 4, 2, \cdots},
\]
$\cd{lookup}(T,\mbox{E})$ would first visit position 7, which is full,
and then position 4 where it finds E.
\end{example}


\begin{example}
Let $T$ be the following table, where * indicates a deleted entry.
\begin{center}
\begin{tabular}{c c c c c c c c }
      0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\ 
\hline
        & B &   & D & * & A &   & F \\ 
\hline
\end{tabular}
\end{center}
if key D has the probe sequence 
\[
\cseq{7, 4, 3, \cdots},
\]
$\cd{lookup}(T,\mbox{E})$ would first visit position 7, which is full,
and then position 4, which is deleted, and then position 3, where it
finds D.
\end{example}

\begin{example}
Suppose the hash table has the following keys:
\begin{center}
\begin{tabular}{c c c c c c c c }
        0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\ 
\hline
          & B &    &    & E & A &    & F \\
\hline
\end{tabular}
\end{center}
Now if for a key $D$ we had the probe sequence $\cseq{1, 5, 3,
  \cdots}$, then we would find position 1 and 5 full (with $B$ and
$E$) and place $D$ in position 3 giving:
\begin{center}
\begin{tabular}{c c c c c c c c }
     0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\
\hline
       & B &   & D & E & A &   & F \\ 
\hline
\end{tabular}
\end{center}
\end{example}


\begin{gram}[Cost Analysis of Flat Tables]
The cost analysis of flat tables becomes tricky because of the impact
of deleted keys and the interaction between keys that collide.
%
Here we present an informal analysis for a table of size $m$ with $n$
stored key-value pairs, where $n \le m$, and thus the load factor
$\kloadfac{} \le 1$.
%
We make several assumptions.
%
\begin{itemize}
\item We assume that the probe sequence executed by an operation is a
  uniformly randomly chosen permutation of $0, \ldots, m-1$.

\item We assume simple uniform hashing, which postulates that each key
  is given a uniformly randomly chosen hash-code independently of all
  the others.

\item We assume that there are no deletions, and thus the table
  entries are either empty or occupied but not marked deleted or dead.

\end{itemize}


Under these assumptions, let's first bound the number of probes needed
until we find an empty cell in the hash table.
%
This is a Bernoulli trial with a success probability of $1-\kloadfac{}$.
%
Therefore, the expected number of trial is $\frac{1}{1 - \kloadfac{}}$.

This means that an insertion and an unsuccessful lookup will require
$\frac{1}{1 - \kloadfac{}}$ work in expectation.

For a successful lookup, consider some key $x$ that is the $i^{th}$
key to be inserted into the table.
%
To insert the key, we first find an empty cell, which requires
$\frac{1}{1-i/m} = \frac{m}{m-i}$, because the load factor for the
table is $i/m$.
%
Now, observe that the probe sequence for a key is always deterministic.
%
Thus a successful search will repeat the same probe sequence as the
insertion and find the key. 
%
Thus, we the successful search for the $i^{th}$ key requires
$\frac{m}{m-i}$ work in expectation.
%

We can write the average expected cost over all keys as
\[
\frac{1}{n}\sum_{i = 0}^{n}\frac{m}{m-i}.
\]
This is bounded by 
\[
\frac{1}{\kloadfac{}}\left(\ln{\frac{1}{1-\kloadfac{}}}\right)
\]
because
%
\[
\begin{array}{lcl}
\frac{1}{n}\sum_{i = 0}^{n}\frac{m}{m-i} 
& = & \frac{m}{n}\left(\sum_{i = 0}^{m}\frac{1}{i}  - \sum_{i = 0}^{m-n}\frac{1}{i} \right)
\\ 
& = & \frac{m}{n}\left(H_m - H_{m-n}\right)
\\
& \le & \frac{m}{n}\left(\ln{\frac{m}{m-n}}\right)
\\
& \le & \frac{1}{\kloadfac{}}\left(\ln{\frac{1}{1-\kloadfac{}}}\right).
\end{array}
\]
(The bound on $H_m - H_{m-n}$ can be obtained by using integration.)
\end{gram}


\begin{exercise}
Show that the parametric implementation of the flat hash table above
can be implemented by using just a single higher-order function, which
in turn can be used to implement $\cd{lookup}$, $\cd{insert}$, and
$\cd{delete}$.
\end{exercise}


\begin{exercise}
Complete the implementation of the parametric flat hash table by
describing the algorithms and writing the pseudo-code for the
remaining operations, e.g., $\cd{resize}.$ 
\end{exercise}

\end{unit}

\begin{unit}[Linear Probing]

\begin{gram}
Linear probing is a flat table implementation.
%
In linear probing, the probe sequence is defined by $m$ hash function
of the form 
\[
h_i(k) = (h(k) + i) \mod m.
\] 
%
Each position in the table determines a single probe sequence, so
there are only $m$ possible probe sequences.
\end{gram}

\begin{teachask}
What are some advantages and disadvantages of linear probing?
\end{teachask}

\begin{gram}[Primary Clustering]
The problem with linear probing is that keys tend to cluster.  It
suffers from~\defn{primary clustering}: Any key that hashes to any
position in a cluster (not just collisions), must probe beyond the
cluster and adds to the cluster size.  Worse yet, primary clustering
not only makes the probe sequence longer, it also makes it
more likely that it will be lengthen further.

What is the impact of clustering for an unsuccessful search? Let's
consider two extreme examples when the table is half full,
$\kloadfac{}=1/2$ (or equivalently, $m = 2n$).  Clustering is minimized
when every other location in the table is empty.  In this case, the
average number of probes needed to insert a new key $k$ is $3/2$: One
probe to check cell $h(k)$, and with probability $1/2$ that cell is
full and it needs to look at the next location which, by construction,
must be empty.  In the worst case, all the keys are clustered, let's
say at the end of the table. If $k$ hashes to any of the first $n$
locations, only one probe is needed.  But hashing to the $n^{th}$
location would require probing all $n$ full locations before finally
wrapping around to find an empty location.  Similarly, hashing to the
second full cell, requires probing $(n-1)$ full cells plus the first
empty cell, and so forth.  Thus, under uniform hashing the average
number of probes needed to insert a key would be

\[1 + [n + (n-1) + (n-2) + .... + 1]/m = 1 + n(n+1)/2m  \approx n/4 \]

Even though the average cluster length is 2, the cost for an
unsuccessful search is $n/4$. In general, each cluster $j$ of length
$n_j$ contributes $n_j(n_j+1)/2$ towards the total number of probes
for all keys.  Its contribution to the average is proportional the
\emph{square} of the length of the cluster, making long cluster
costly.


%% We won't attempt to analyze the cost of successful and unsuccessful
%% searches, as considering cluster formation during linear probing is
%% quite difficult be we will state a known lemma that quantifies the
%% difficulty.

\end{gram}

%% \begin{lemma} When using linear probing in a hash table of size $m$
%% that contains $n = \kloadfac{} m$ keys, the average number of probes needed
%% for an unsuccessful search or an insert is
%% \[\frac12\left(1 + \frac1{(1-\kloadfac{})^2}\right) \]
%% and for a successful search is
%% \[\frac12\left(1 + \frac1{1-\kloadfac{}}\right). \]
%% \end{lemma}

%% \begin{example}
%% As you can see from the following table, which shows the expected
%% number of probes under uniform hashing, the performance of linear
%% probing degrades significantly when the load factor increases:

%% \begin{tabular}{l r r r r r}
%% \toprule
%% $\mathbf{\kloadfac{}}$    & 1/4 & 1/2 & 2/3 & 3/4 & 9/10 \\ \midrule
%% \textbf{successful}  & 1.2 & 1.5 & 2.0 & 3.0 &  5.5 \\
%% \textbf{unsuccessful}& 1.4 & 2.5 & 5.0 & 8.5 & 50.5 \\
%% \bottomrule
%% \end{tabular}
%% \end{example}

\begin{remark}
Although it can perform poorly in the worst case, linear probing is
known to be quite competitive, when the load factors are in the range
30-70\% as clusters tend to stay small. In addition, a few extra
probes is mitigated when sequential access is much faster than random
access, as in the case of caching.  Because of primary clustering,
though, it is sensitive to quality of the hash function or the
particular mix of keys that result in many collisions or clumping.
Therefore, it may not be a good choice for general purpose hash
tables.
\end{remark}

\end{unit}

\begin{unit}[Quadratic Probing]


\begin{gram}
Quadratic probing is a flat-table implementation, where the probe
sequence cause probes to move away from clusters, by making increasing
larger jumps. The probe sequence is defined by functions
\[ 
h_i(k) = (h(k) + i^2) \mod m.
\]
\end{gram}

\begin{teachask}
What are some advantages and disadvantages of quadratic probing?
\end{teachask}

\begin{gram}
Although, quadratic probing avoids primary clustering, it still
has~\defn{secondary clustering}: When two keys hash to the same
location, they have the same probe sequence. Since there are only $m$
locations in the table, there are only $m$ possible probe sequences.

One problem with quadratic probing is that probe sequences do not
probe all locations in the table. But since there are $(p+1)/2$
quadratic residues when $p$ is prime, we can make the following
guarantee.
\end{gram}

\begin{group}
\begin{lemma} 
If $m$ is prime and the table is at least half empty, then quadratic
probing will always find an empty location. Furthermore, no locations
are checked twice.
\end{lemma}

\begin{proof} 
Consider two probe locations $h(k) + i^2$ and $h(k) + j^2, 0 \leq i,j
< \lceil {m/2} \rceil$. Suppose the locations are the same but $i \neq j$. Then
\begin{align*}
h(k) +i^2 &\equiv (h(k) + j^2)\mod m \\
i^2 &\equiv j^2 \mod m \\
i^2 - j^2 &\equiv 0 \mod m \\
(i-j)(i+j) &\equiv 0 \mod m
\end{align*}
Therefore, since $m$ is prime either $i-j$ or $i+j$ are divisible by
$m$. But since both $i-j$ and $i+j$ are less than $m$, they cannot be
divisible by $m$. This is  a contradiction.

Thus the first $\lceil {m/2} \rceil$ probes are distinct and
guaranteed to find an empty location.
\end{proof}
\end{group}

\begin{gram}
Computing the next probe is only slightly more expensive than linear
probing as it can be computed without using multiplication:
\begin{align*}
h_i - h_{i-1} &\equiv (i^2 - (i-1)^2)\mod m \\
h_i &\equiv (h_{i-1} + 2i - 1) \mod m
\end{align*}

Unfortunately, requiring that the table remains less than half full
makes quadratic probing space inefficient.
\end{gram}
\end{unit}

\begin{unit}[Double Hashing]

\begin{gram}
Double hashing is a flat-table implementation that uses two hash
functions $h(\cdot)$ and $hh(\cdot)$, one to find the initial
location to place the key and a second to determine the size of the
jumps in the probe sequence.
%
The probe sequence is defined by hash functions of the form
\[
h_i(k) = (h(k) + i\cdot hh(k)) \mod m.
\] 
%
Keys that hash to the same location, are likely to hash to a different
jump size, and so will have different probe sequences.  Thus, double
hashing avoids secondary clustering by providing as many as $m^2$
probe sequences.

How do we ensure every location is checked? Since each successive
probe is offset by $hh(k)$, every cell is probed if $hh(k)$ is
relatively prime to $m$. Two possible ways to ensure $hh(k)$
is relatively prime to $m$ are, either make $m=2^k$ and design $hh(k)$
so it is always odd, or make $m$ prime and ensure $hh(k) < m$. Of
course, $hh(k)$ cannot equal zero.
\end{gram}

%% \begin{gram}
%% Double hashing behaves quite closely to simple uniform hashing for
%% careful choices of $h$ and $hh$. Under uniform hashing the average
%% number of probes for an unsuccessful search or an insert is at most
%% \[
%% 1 + \kloadfac{} + \kloadfac{}^2 +... =
%% \left(\frac1{1-\kloadfac{}}\right) 
%% \]
%% and for a successful search is at most
%% \[
%% \frac1{\kloadfac{}} \left (1+\ln
%% \biggl(\frac1{1-\kloadfac{}}\biggr)\right).
%%  \]

%% The former bound is because the probability of needing more than $i$
%% probes is at most $\kloadfac{}^i$. 
%% %
%% A search always needs one probe, and with
%% probability $\kloadfac{}$ needs a second probe, and with probability
%% $\kloadfac{}^2$ needs a third probe, and so on.  
%% %
%% The bound for a
%% successful search for a key $k$ follows the same probe sequences as
%% when it was first inserted. So if $k$ was the $(j+1)^{th}$ key inserted
%% the cost for inserting it is at most $1/(1-j/m)$.  Therefore
%% the average cost of a successful search is at most
%% \begin{align*}
%% \frac1{n} \sum_{j=0}^{n-1} \frac{1}{1-j/m} &=
%% \frac{m}{n}\sum_{j=0}^{n-1} \frac{1}{m-j} \\
%% &=\tfrac1{\kloadfac{}}\left(\sum_{j=0}^{m} \frac{1}{j} + \sum_{j=0}^{m-n} \frac{1}{j}\right)\\
%% &= \tfrac1{\kloadfac{}}(H_m - H_{m-n}) \\
%% &\leq \tfrac1{\kloadfac{}}(\ln m + 1 - \ln(m-n)) \\
%% &= \tfrac1{\kloadfac{}}\Bigr(1+\ln \Bigl(\tfrac1{1-\kloadfac{}}\Bigr)\Bigr)
%% \end{align*}


%% The table below shows the expected number of probes under the
%% assumption of simple uniform hashing and is the best one can expect by
%% open addressing.

%% \begin{tabular}{l r r r r r r}
%% \toprule
%% $\mathbf{\kloadfac{}}$    & 1/4 & 1/2 & 2/3 & 3/4 & 9/10 \\ \midrule
%% \textbf{successful}  & 1.2 & 1.4 & 1.6 & 1.8 & 2.6 \\
%% \textbf{unsuccessful}& 1.3 & 2.0 & 3.0 & 4.0 & 10.0 \\
%% \bottomrule
%% \end{tabular}

%% Comparing these numbers with the numbers in the table for linear
%% probing, the linear probing numbers are remarkable close when the load
%% factor is 50\% or below.
%% \end{gram}

\begin{teachask}
What are some advantages and disadvantages of double hashing?
\end{teachask}

\begin{gram}
The main advantage with double hashing is that it allows for smaller
tables (higher load factors) than linear or quadratic probing, but at
the expense of higher costs to compute the next probe.  The higher
cost of computing the next probe may be preferable to longer probe
sequences, especially when testing two keys equal is expensive.
\end{gram}

\end{unit}
\end{section}

\begin{section}[Concluding Remarks]
\begin{unit}

\begin{gram}
Hash functions are a very important technique in computer science that
is used in a very broad array of applications. 
%
Although their development was intertwined with that of hash tables in
the initial years of computer science, recent developments in hash
functions are primarily driven by security, privacy, and error
detection and correction.
\end{gram}

\begin{gram}
Hash tables are classic data structures that are broadly employed in
many real-world systems.
%
They are a classic example of a space-time tradeoff: increase the
space so table operations are faster; decrease the space but table
operations are slower.
%

Of the different methods for implementing hash table, nested tables
and separate chaining are perhaps the simplest and are less sensitive
to the quality of the hash function or load factors.
%
They are therefore usually the choice when it is unknown how many and
how frequently keys may be inserted or deleted from the hash table.
%

Flat tables and open addressing can be more space efficient than
nested tables, though the space efficiency of nested tables can
also improved by using blocking techniques. 
%
Linear probing has the advantage that it has small constants and works
well with modern architectures due to better locality (the memory
locations accessed are typically on the same cache line). 
%
But it suffers from primary clustering, which means its performance is
sensitive to collisions and to high load factors.  
%
Quadratic probing, on the other hand, avoids primary clustering, but
still suffers from secondary clustering and requires rehashing as soon
as the load factor reaches 50\%. 
%
Although double hashing reduces clustering, so high load factors are
possible, finding suitable pairs of hash functions is somewhat more
difficult and increases the cost of a probe.
\end{gram}

\end{unit}
\end{section}
\end{chapter}
\end{book}
