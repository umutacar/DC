
\documentclass{course}
\title{Parallel and Sequential Algorithms}
\label{15210}
\no{15210}
\unique{15210}
\parent{...NOT.PROVIDED.PARENTS...}

\coursenumber{15210}
\picture{/210/course/air-pavilion.jpg}
\providesbook{S18}
\provideschapter{101}
\providessection{1}
\providesunit{1}
\providesassignment{1}
\semester{Spring 2018}
\website{http://www.cs.cmu.edu/~15210}
15-210 aims to teach methods for designing, analyzing, and programming
sequential and parallel algorithms and data structures. The emphasis
is on teaching fundamental concepts applicable across a wide variety
of problem domains, and transferable across a reasonably broad set of
programming languages and computer architectures. This course also
includes a significant programming component in which students will
program concrete examples from domains such as engineering, scientific
computing, graphics, data mining, and information retrieval (web
search).

Unlike a traditional introduction to algorithms and data structures,
this course puts an emphasis on parallel thinking â€” i.e., thinking
about how algorithms can do multiple things at once instead of one at
a time. The course follows up on material learned in 15-122 and 15-150
but goes into significantly more depth on algorithmic issues.
\begin{book}
\title{Algorithm Design: Parallel and Sequential}
\label{book:15210:S18}
\no{0}
\unique{15210:S18}
\parent{...NOT.PROVIDED.PARENTS...}
\authors{Umut A. Acar and Guy Blelloch}

\begin{chapter}[Example Chapter: Binary Search Trees]
\label{chapter:15210:S18:CH101:ch:bsts}
\no{101}
\unique{15210:S18:CH101}
\parent{...NOT.PROVIDED.PARENTS...}

\picture{/210/bsts/abstract-tree.jpg}

...NOT.PROVIDED.INTRO...

\begin{section}[Motivation]
\label{section:15210:S18:CH101:SEC1}
\no{1}
\unique{15210:S18:CH101:SEC1}
\parent{...NOT.PROVIDED.PARENTS...}

Searching is one of the most important operations in computer science.
Of the many search data structures that have been designed and are
used in practice, search trees, more specifically balanced binary
search trees, occupy a coveted place because of their broad
applicability to many different sorts of problems.  For example, in
this book, we rely on binary search trees to implement set and table
(dictionary) abstract data types
%(\chref{sets-tables}),
which are then used in the
implementation of many algorithms, including for example graph
algorithms.

\begin{unit}[...NOT.PROVIDED.TITLE...]
\label{unit:15210:S18:CH101:SEC1:UN1}
\no{1}
\unique{15210:S18:CH101:SEC1:UN1}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC1:UN1:GR1}
\no{1}
\unique{15210:S18:CH101:SEC1:UN1:GR1}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC1:UN1:GR1:AT1}
\no{1}
\unique{15210:S18:CH101:SEC1:UN1:GR1:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

If we are interested in searching a static or unchanging collection of
elements, then we can use a simpler data structure such as sequences.
%
For example, we can use a sequence with the array-based cost
specification to implement an efficient search function by
representing the collection as a sorted sequence and by using binary
search.  
%
Such an implementation would yield a logarithmic-work search
operation.
%
If, however, we want to support dynamic collections, where for
example, we insert new elements and delete existing elements,
sequences would require linear work.
% 
Binary search trees, or~\defn{BSTs} for short, make it possible to
compute with dynamic collections by using insertions, deletions, as
well as searches all in logarithmic number of tree operations.
\end{gram}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC1:UN1:GR2}
\no{2}
\unique{15210:S18:CH101:SEC1:UN1:GR2}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC1:UN1:GR2:AT1}
\no{1}
\unique{15210:S18:CH101:SEC1:UN1:GR2:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

In the traditional treatment of algorithms, which focuses on
sequential algorithms, binary search trees revolve around three
operations: insertion, deletion, and search.
%
While these operations are important, they are not sufficient for
parallelism, since they perform a single update at a time.
% 
We therefore consider aggregate update operations, such as union and
difference, which can be used to insert and delete (respectively) many
elements atonce.
\end{gram}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC1:UN1:GR3}
\no{3}
\unique{15210:S18:CH101:SEC1:UN1:GR3}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC1:UN1:GR3:AT1}
\no{1}
\unique{15210:S18:CH101:SEC1:UN1:GR3:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

The rest of this chapter is organized as follows.  We first define
binary search trees
%(\secref{bst::prelim})
and present an ADT for them.
%(\secref{bst::adt}). 
%
We then present a parametric implementation of the ADT
%(\secref{bst::parametric})
by using only two operations, $\cd{split}$
and $\cd{join}$, which respectively split a tree at a given key and
join two trees.
%
%In \secref{bst::cost},
Next, we present a cost specification based on the
parametric implementation, which achieves strong bounds as long as the
$\cd{split}$ and $\cd{join}$ operations have logarithmic work and
span.
%
As a result, we are able to reduce the problem of implementing the BST
ADT to the problem of implementing just the functions $\cd{split}$
and $\cd{join}$.  
%
We finish the chapter by presenting a specific instance of the
parametric implementation using Treaps.
%(\secref{bst::treaps}).
%
Other possible implementation techniques are also described.
%(\secref{bst::bsts-review})
\end{gram}
\end{group}
\end{unit}
\end{section}

\begin{section}[Preliminaries]
\label{section:15210:S18:CH101:SEC2:sec:bst::prelim}
\no{2}
\unique{15210:S18:CH101:SEC2}
\parent{...NOT.PROVIDED.PARENTS...}

...NOT.PROVIDED.INTRO...

\begin{unit}[...NOT.PROVIDED.TITLE...]
\label{unit:15210:S18:CH101:SEC2:UN2}
\no{2}
\unique{15210:S18:CH101:SEC2:UN2}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC2:UN2:GR1}
\no{1}
\unique{15210:S18:CH101:SEC2:UN2:GR1}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC2:UN2:GR1:AT1}
\no{1}
\unique{15210:S18:CH101:SEC2:UN2:GR1:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

We start with some basic definitions and terminology involving rooted
and binary search trees.  Recall first that a rooted tree is a tree
with a distinguished root node.
%(\defref{prelim::rootedtree}). 
%
A~\defn{full binary tree} is a rooted tree, where each node is either
a~\defn{leaf}, which has no children, or an~\defn{internal node},
which have a left and a right child.
%(\defref{bst::binarytree}).
%
\end{gram}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC2:UN2:GR2}
\no{2}
\unique{15210:S18:CH101:SEC2:UN2:GR2}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{definition}[Full Binary Tree]
\label{atom:15210:S18:CH101:SEC2:UN2:GR2:AT1:def:bst::binarytree}
\no{1}
\unique{15210:S18:CH101:SEC2:UN2:GR2:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

A~\defn{full binary tree} is an ordered rooted tree in which every
internal node has exactly two children: the first or the~\defn{left
  child} and the second or the~\defn{right child}.  
%
The~\defn{left subtree} of a node is the subtree rooted at the left
child, and the~\defn{right subtree} the one rooted at the right child.
\end{definition}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC2:UN2:GR3}
\no{3}
\unique{15210:S18:CH101:SEC2:UN2:GR3}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC2:UN2:GR3:AT1}
\no{1}
\unique{15210:S18:CH101:SEC2:UN2:GR3:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

A binary search tree is a full binary tree, where each internal node
$u$ has a unique key~$k$ such that each node in its left subtree has a
key less than $k$ and each node in its right subtree has a key greater
that $x$.  
%
\end{gram}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC2:UN2:GR4}
\no{4}
\unique{15210:S18:CH101:SEC2:UN2:GR4}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{definition}[Binary Search Tree (BST)]
\label{atom:15210:S18:CH101:SEC2:UN2:GR4:AT1:def:bst::bst}
\no{1}
\unique{15210:S18:CH101:SEC2:UN2:GR4:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

A~\defn{binary search tree} (BST) over a totally ordered set $S$ is a
full binary tree that satisfies the following conditions.
\begin{enumerate}
 \item There is a one-to-one mapping $k(v)$ from internal tree nodes to elements in $S$.
 \item for every $u$ in the left subtree of $v$, $k(u) < k(v)$
 \item for every $u$ in the right subtree of $v$, $k(u) > k(v)$
\end{enumerate}
%
In the definition, conditions 2 and 3 are referred to as the~\defn{BST
  property}.  We often refer to the elements of $S$ in a BST as keys,
and use $\dom{T}$ to indicate the domain (keys) in a BST $T$.
The~\defn{size} of a BST is the number of keys in the tree,
i.e. $|S|$.


% A BST can equivalently be defined recursively as:
% \[
% \cd{BST}(S) = \left\{\begin{array}{ll}
% \cd{Leaf} & S = \emptyset\\
% \cd{iNode}(\cd{BST}(S_L), k, \cd{BST}(S_R)) & 
% \underbrace{(S = S_L \cup \cset{k} \cup S_R)}_{\mbox{one to one (inclusion)}} \wedge \underbrace{(S_L < k < S_R)}_{\mbox{BST property}}
% \end{array}\right.
% \]
\end{definition}

\begin{example}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC2:UN2:GR4:AT2:ex:bst}
\no{2}
\unique{15210:S18:CH101:SEC2:UN2:GR4:AT2}
\parent{...NOT.PROVIDED.PARENTS...}

An example binary search tree over the set of natural numbers
$\{1,3,4,5,6,7,8,9\}$ is shown below.

\begin{tabular}{ccc}
\vspace*{0in}
\begin{minipage}[t]{2.5in}
\vspace*{0in}
\includegraphics[width=2.5in]{/media/210/bsts/bst2.jpg}
\end{minipage}
%
& \quad\quad\quad &
%
\vspace*{0in}
\begin{minipage}[t]{2.5in}
\vspace*{0in}
\includegraphics[width=1.8in]{/media/210/bsts/bst3.jpg}
\end{minipage}
\end{tabular}
On the left the $L$ and $R$ indicate the left (first) and right
(second) child, respectively.  All internal nodes (white) have a key
associated with them while the leaves (black) are empty.  The keys
satisfy the BST property---for every node, the keys in the left
subtree are less, and the ones in the right subtree are greater. 


\smallskip 

In the illustration on the left, the edges are oriented away from the
root. They could have also been oriented towards the root.
%
When illustrating binary search trees, we usually replace the directed
arcs with undirected edges, leaving the orientation to be implicit.
%
We also draw the left and right subtrees of a node on its left and
right respectively.
%
Following this convention, we can draw the tree on the left above as
shown an the right.
%
We use this convention in future figures.
\end{example}
\end{group}
\end{unit}
\end{section}

\begin{section}[The BST Abstract Data Type]
\label{section:15210:S18:CH101:SEC3:sec:bst::adt}
\no{3}
\unique{15210:S18:CH101:SEC3}
\parent{...NOT.PROVIDED.PARENTS...}

...NOT.PROVIDED.INTRO...

\begin{unit}[...NOT.PROVIDED.TITLE...]
\label{unit:15210:S18:CH101:SEC3:UN3}
\no{3}
\unique{15210:S18:CH101:SEC3:UN3}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC3:UN3:GR1}
\no{1}
\unique{15210:S18:CH101:SEC3:UN3:GR1}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC3:UN3:GR1:AT1}
\no{1}
\unique{15210:S18:CH101:SEC3:UN3:GR1:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

The Abstract Data Type specification below describes an ADT for BSTs
parametrized by a totally ordered key set. We briefly describe this
ADT and present some examples. As we shall see, the BST ADT can be
implemented in many ways.  In order to present concrete examples, we
assume an implementation but do not specify it.
\end{gram}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC3:UN3:GR2}
\no{2}
\unique{15210:S18:CH101:SEC3:UN3:GR2}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{datatype}[Binary Search Tree (BST)]
\label{atom:15210:S18:CH101:SEC3:UN3:GR2:AT1:adt:bst::adt}
\no{1}
\unique{15210:S18:CH101:SEC3:UN3:GR2:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

For a universe of totally ordered keys $\kkk$, the BST ADT
consists of a type $\bstt$ representing a power set of keys and the
functions whose types are specified as follows. 

\[
\begin{array}{lcl}
\texttt{empty} & : &\bstt
\\
%
\cd{singleton} & : & \kkk \ra \bstt
\\
%
\cd{find}
& : & \bstt \ra \kkk \ra \bbb
\\
%
\cd{delete}
& : & \bstt \ra \kkk \ra  \bstt
\\
\cd{insert}
& : & \bstt \ra \kkk \ra  \bstt
\\
%
\cd{intersection}
& : & \bstt \ra \bstt \ra  \bstt
\\
%
\cd{difference}
& : & \bstt \ra \bstt \ra  \bstt
\\
%
\cd{union}
& : & \bstt \ra \bstt \ra  \bstt
\\
%
\cd{split}
& : & \bstt \ra \kkk \ra (\bstt \times \bbb \times \bstt)
\\
\cd{join}
& : & \bstt \ra \bstt \ra \bstt 
\end{array}
\]
\end{datatype}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC3:UN3:GR3}
\no{3}
\unique{15210:S18:CH101:SEC3:UN3:GR3}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC3:UN3:GR3:AT1}
\no{1}
\unique{15210:S18:CH101:SEC3:UN3:GR3:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

The ADT supports two constructors: $\cd{empty}$ creates an empty BST
  and $\cd{singleton}$ creates a BST with a single key.
%

The function $\cd{find}$ searches for a given key and returns a
boolean indicating success.

The functions $\cd{insert}$ and $\cd{delete}$ insert and delete a
given key into or from the BST.
\end{gram}

\begin{example}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC3:UN3:GR3:AT2}
\no{2}
\unique{15210:S18:CH101:SEC3:UN3:GR3:AT2}
\parent{...NOT.PROVIDED.PARENTS...}

Searching in BSTs illustrated.

\begin{itemize}
\item 
Searching for  $5$  in the input tree returns $\cd{true}.$

$\cd{find}$
$\leftparen{2cm}$
\includegraphics[width=3cm]{/media/210/bsts/bst4.jpg}
$\rightparen{2cm}$
~~$\cd{5}$
$~=~$
$\cd{true}$

\item 
Searching for  $6$  in the input tree returns $\cd{false}.$
%

$\cd{find}$
$\leftparen{2cm}$
\includegraphics[width=3cm]{/media/210/bsts/bst4.jpg}
$\rightparen{2cm}$
~~$\cd{6}$
$~=~$ 
$\cd{false}$

\end{itemize}
\end{example}

\begin{example}[Insertion]
\label{atom:15210:S18:CH101:SEC3:UN3:GR3:AT3}
\no{3}
\unique{15210:S18:CH101:SEC3:UN3:GR3:AT3}
\parent{...NOT.PROVIDED.PARENTS...}

Inserting the key $6$  into the input tree returns a new tree including $6$.
%

$\cd{insert}$
$\leftparen{2cm}$
\includegraphics[width=3cm]{/media/210/bsts/bst4.jpg}
$\rightparen{2cm}$
~~$\cd{6}$
=
\includegraphics[width=3cm]{/media/210/bsts/bst5.jpg}
\end{example}

\begin{example}[Deletion]
\label{atom:15210:S18:CH101:SEC3:UN3:GR3:AT4}
\no{4}
\unique{15210:S18:CH101:SEC3:UN3:GR3:AT4}
\parent{...NOT.PROVIDED.PARENTS...}

Deleting the key  $6$ from the input tree returns a tree without it.
%

$\cd{delete}$
$\leftparen{2cm}$
\includegraphics[width=3cm]{/media/210/bsts/bst5.jpg}
$\rightparen{2cm}$
~~$\cd{6}$
= 
\includegraphics[width=3cm]{/media/210/bsts/bst4.jpg}
\end{example}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC3:UN3:GR4}
\no{4}
\unique{15210:S18:CH101:SEC3:UN3:GR4}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[Union]
\label{atom:15210:S18:CH101:SEC3:UN3:GR4:AT1}
\no{1}
\unique{15210:S18:CH101:SEC3:UN3:GR4:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

The function $\cd{union}$ takes two BSTs and returns a BST that
contains all the keys in them; $\cd{union}$ is an aggregate insert
operation.
% 
The function $\cd{intersection}$ takes two BSTs and returns a BST
that contains the keys common in both.
% 
The function $\cd{difference}$ takes two BSTs $t_1$ and $t_2$ and returns
a BST that contains the keys in $t_1$ that are not in $t_2$;
$\cd{difference}$ is an aggregate delete operation.
\end{gram}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC3:UN3:GR5}
\no{5}
\unique{15210:S18:CH101:SEC3:UN3:GR5}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[Split]
\label{atom:15210:S18:CH101:SEC3:UN3:GR5:AT1}
\no{1}
\unique{15210:S18:CH101:SEC3:UN3:GR5:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

The function $\cd{split}$ takes a tree $t$ and a key $k$ and
splits $t$ into two trees: one consisting of all the keys of $t$ less
than $k$, and another consisting of all the keys of $t$ greater than
$k$.  
%
It also returns a Boolean value indicating whether $k$ appears in $t$.
%
The exact structure of the trees returned by $\cd{split}$ can differ
from one implementation to another: the specification only requires
that the resulting trees to be valid BSTs and that they contain the
keys less than $k$ and greater than $k$, leaving their structure
otherwise unspecified.
\end{gram}

\begin{example}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC3:UN3:GR5:AT2}
\no{2}
\unique{15210:S18:CH101:SEC3:UN3:GR5:AT2}
\parent{...NOT.PROVIDED.PARENTS...}

The function $\cd{split}$ illustrated.

\begin{itemize}
\item 
Splitting the input tree at $6$ yields two trees, consisting of the
keys less that $6$ and those greater that $6$, indicating also that
$6$ is not in the input tree. 

$\cd{split}$
$\leftparen{2cm}$
\includegraphics[width=3cm]{/media/210/bsts/bst4.jpg}
$\rightparen{2cm}$
~~$\cd{6}$
$~~=~~$
$\leftparen{1.5cm}$
\includegraphics[width=1cm]{/media/210/bsts/bst4a.jpg}~~,
$\cd{False}$~,~~
\includegraphics[width=2cm]{/media/210/bsts/bst4b.jpg}
$\rightparen{1.5cm}$

\item 
Splitting the input tree at $5$ yields two trees, consisting of the
keys less than $5$ and those greater than $5$, indicating also that
$5$ is found in the input tree. 

$\cd{split}$
$\leftparen{2cm}$
\includegraphics[width=3cm]{/media/210/bsts/bst4.jpg}
$\rightparen{2cm}$
~~$\cd{5}$
%
$~~=~~$
%
$\leftparen{2cm}$
{\includegraphics[width=7mm]{/media/210/bsts/bst4c.jpg}}~~,
~$\cd{True}$~,~~
{\includegraphics[width=2cm]{/media/210/bsts/bst4b.jpg}}
$\rightparen{2cm}$
\end{itemize}
\end{example}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC3:UN3:GR6}
\no{6}
\unique{15210:S18:CH101:SEC3:UN3:GR6}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[Join]
\label{atom:15210:S18:CH101:SEC3:UN3:GR6:AT1}
\no{1}
\unique{15210:S18:CH101:SEC3:UN3:GR6:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

The function $\cd{join}$ takes two trees $t_1$ and $t_2$
such that all the keys in $t_1$ are less than the keys in $t_2$.  The
function returns a tree that contains all the keys in $t_1$ and $t_2$.
%
The exact structure of the tree returned by $\cd{join}$ can differ
from one implementation to another: the specification only requires
that the resulting tree is a valid BST and that it contains all the
keys in the trees joined.
\end{gram}

\begin{example}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC3:UN3:GR6:AT2}
\no{2}
\unique{15210:S18:CH101:SEC3:UN3:GR6:AT2}
\parent{...NOT.PROVIDED.PARENTS...}

The function $\cd{join}$ illustrated.

$\cd{join}$
$\leftparen{2cm}$
\includegraphics[width=1.0cm]{/media/210/bsts/bst4a.jpg}
$\rightparen{2cm}$
%
$\leftparen{2cm}$
{\includegraphics[width=3cm]{/media/210/bsts/bst6.jpg}}
$\rightparen{2cm}$
%
$~~=~~$
%
{\includegraphics[width=3cm]{/media/210/bsts/bst4.jpg}}
\end{example}
\end{group}
\end{unit}
\end{section}

\begin{section}[Implementation via Balancing]
\label{section:15210:S18:CH101:SEC4:sec:bst::bsts-review}
\no{4}
\unique{15210:S18:CH101:SEC4}
\parent{...NOT.PROVIDED.PARENTS...}

...NOT.PROVIDED.INTRO...

\begin{unit}[...NOT.PROVIDED.TITLE...]
\label{unit:15210:S18:CH101:SEC4:UN4}
\no{4}
\unique{15210:S18:CH101:SEC4:UN4}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC4:UN4:GR1}
\no{1}
\unique{15210:S18:CH101:SEC4:UN4:GR1}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[The Main Ideas]
\label{atom:15210:S18:CH101:SEC4:UN4:GR1:AT1}
\no{1}
\unique{15210:S18:CH101:SEC4:UN4:GR1:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

The key idea behind the implementation of BSTs is to organize the
keys to facilitate
\begin{enumerate}
\item
finding a key by following a branch in the tree, performing key
comparisons along the way, and

\item moving the keys that together form a contiguous range in the
  sorted ordering of the keys in the tree by performing constant work.
\end{enumerate}
\end{gram}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC4:UN4:GR2}
\no{2}
\unique{15210:S18:CH101:SEC4:UN4:GR2}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[Searching a Tree]
\label{atom:15210:S18:CH101:SEC4:UN4:GR2:AT1}
\no{1}
\unique{15210:S18:CH101:SEC4:UN4:GR2:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

To see how we can search in a tree, consider searching for a key $k$
in a tree $t$ whose root is $r$.
%
We can start at the root $r$ and if $k$ equals the key at the root,
$k(r)$, then we have found our key, otherwise if $k < k(r)$, then we
know that $k$ cannot appear in the right subtree, so we only need to
search the left subtree, and if $k > k(r)$, then we only have to
search the right subtree.  Continuing the search, we will either find
the key or reach a leaf and conclude that the key is not in the tree.
%
In both cases we have followed a single path through the BST starting
at the root.
\end{gram}

\begin{example}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC4:UN4:GR2:AT2}
\no{2}
\unique{15210:S18:CH101:SEC4:UN4:GR2:AT2}
\parent{...NOT.PROVIDED.PARENTS...}

A successful search for $7$ and an unsuccessful search for $4$ in the
given tree.  Search paths are highlighted.

\begin{center}
\includegraphics[width=3cm]{/media/210/bsts/bst5-search-7.jpg}\
%
\hspace*{3cm}
%
\includegraphics[width=3cm]{/media/210/bsts/bst5-search-4.jpg}
\end{center}
\end{example}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC4:UN4:GR3}
\no{3}
\unique{15210:S18:CH101:SEC4:UN4:GR3}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[Operating on Ranges]
\label{atom:15210:S18:CH101:SEC4:UN4:GR3:AT1}
\no{1}
\unique{15210:S18:CH101:SEC4:UN4:GR3:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

To see how we can operate on a range of keys, note first that each
subtree in a binary tree contains all the keys that are within a
specific range. 
%
We can find such a range by performing a search as
described---in fact, a search as described identifies a possibly empty
range. Once we find a range of keys, we can operate on them as a group
by handling their root.
%
For example, we can move the whole subtree to another location by
linking the root to another parent.
\end{gram}

\begin{example}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC4:UN4:GR3:AT2}
\no{2}
\unique{15210:S18:CH101:SEC4:UN4:GR3:AT2}
\parent{...NOT.PROVIDED.PARENTS...}

Consider the tree shown below on the left, we can handle all the keys
that are less than $8$ by holding the subtree rooted at $5$, the left
child of $8$.  For example, we can make $5$ the left child of $9$ and
delete $8$ from the tree. Note that if $8$ remains in the tree, the
resulting tree would not be a valid BST.

\begin{center}
\begin{minipage}[t]{10cm}
\begin{minipage}[t]{3cm}
\includegraphics[width=3cm]{/media/210/bsts/bst5-search-5.jpg}
\end{minipage}
%
%\hspace{3cm}
%
\begin{minipage}[t]{3cm}
\includegraphics[width=2.5cm]{/media/210/bsts/bst5-move-5.jpg}
\end{minipage}
\end{minipage}
\end{center}
\end{example}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC4:UN4:GR4}
\no{4}
\unique{15210:S18:CH101:SEC4:UN4:GR4}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[Balancing]
\label{atom:15210:S18:CH101:SEC4:UN4:GR4:AT1}
\no{1}
\unique{15210:S18:CH101:SEC4:UN4:GR4:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

By finding a range of keys by traversing a path in the BST, and by
moving ranges with constant work, it turns out to be possible to
implement all the operations in the BST ADT efficiently as long as the
paths traversed are not too long.
%
To guarantee the absence of long paths, we can make sure that the tree
remains balanced, i.e., the longest paths have approximately the same
length.
\end{gram}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC4:UN4:GR5}
\no{5}
\unique{15210:S18:CH101:SEC4:UN4:GR5}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{definition}[Perfectly Balanced BSTs]
\label{atom:15210:S18:CH101:SEC4:UN4:GR5:AT1}
\no{1}
\unique{15210:S18:CH101:SEC4:UN4:GR5:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

A binary tree is~\defn{perfectly balanced} if it has the
minimum possible height.  
%
For a binary search tree with $n$ keys, a perfectly balanced tree has
height exactly $\lceil \lg (n + 1) \rceil$.
\end{definition}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC4:UN4:GR6}
\no{6}
\unique{15210:S18:CH101:SEC4:UN4:GR6}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC4:UN4:GR6:AT1}
\no{1}
\unique{15210:S18:CH101:SEC4:UN4:GR6:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

Ideally we would like to use only perfectly balanced trees.  
%
If we never make changes to the tree, we could balance it
once and for all.
%
If, however, we want to update the tree by, for example, inserting new
keys, then maintaining such perfect balance is costly.  
%
In fact, it turns out to be impossible to maintain a perfectly
balanced tree while allowing insertions in $O(\lg n)$ work.  
%
BST data structures therefore aim to keep approximate balance instead
of a perfect one.
\end{gram}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC4:UN4:GR7}
\no{7}
\unique{15210:S18:CH101:SEC4:UN4:GR7}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{definition}[Nearly Balanced BSTs]
\label{atom:15210:S18:CH101:SEC4:UN4:GR7:AT1}
\no{1}
\unique{15210:S18:CH101:SEC4:UN4:GR7:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

We refer to a BST data structure as~\defn{nearly balanced} or simply
as~\defn{balanced} if all trees with $n$ elements have height $O(\lg
n)$, perhaps in expectation or with high probability.
\end{definition}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC4:UN4:GR8}
\no{8}
\unique{15210:S18:CH101:SEC4:UN4:GR8}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[Balanced BST Data Structures]
\label{atom:15210:S18:CH101:SEC4:UN4:GR8:AT1}
\no{1}
\unique{15210:S18:CH101:SEC4:UN4:GR8:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

There are many balanced BST data structures.  Most either try to
maintain height balance (the children of a node are about the same
height) or weight balance (the children of a node are about the same
size).  Here we list a few such data structures:

\begin{enumerate}
\item \defn{AVL trees} are the earliest nearly balanced BST data
  structure (1962).  It maintains the invariant that the two children
  of each node differ in height by at most one, which in turn implies
  approximate balance.

\item \defn{Red-Black trees} maintain the invariant that all leaves
  have a depth that is within a factor of 2 of each other.  The depth
  invariant is ensured by a scheme of coloring the nodes red and
  black.

\item \defn{Weight balanced (BB[$\alpha$]) trees} maintain the
  invariant that the left and right subtrees of a node of size $n$
  each have size at least $\alpha n$ for $0 < \alpha \leq
  1 - \frac{1}{\sqrt{2}}$.  The BB stands for bounded balance, and adjusting
  $\alpha$ gives a tradeoff between search and update costs.

\item \defn{Treaps} associate a random priority with every key and
  maintain the invariant that the keys are stored in heap order with
  respect to their priorities (the term ``Treap'' is short for ``tree
  heap'').  Treaps guarantee approximate balance with
  high-probability.

\item \defn{Splay trees} are an amortized data structure that does not
  guarantee approximate balance, but instead guarantees that for any sequence
  of $m$ insert, find and delete operations each does $O(\lg n)$
  amortized work.
\end{enumerate}
There are dozens of other balanced BST data structures (e.g. scapegoat
trees and skip lists), as well as many that allow larger degrees,
including 2--3 trees, brother trees, and B trees.  
%
\end{gram}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC4:UN4:GR9}
\no{9}
\unique{15210:S18:CH101:SEC4:UN4:GR9}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{remark}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC4:UN4:GR9:AT1}
\no{1}
\unique{15210:S18:CH101:SEC4:UN4:GR9:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

Many of the existing BST data structures were developed for sequential
computing.   
%
Some of these data structures such as Treaps, which we describe here,
generalize naturally to parallel computing. 
%
But some others, such as data structures that rely on amortization
techniques can be challenging to support in the parallel setting.
\end{remark}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC4:UN4:GR10}
\no{10}
\unique{15210:S18:CH101:SEC4:UN4:GR10}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{teachnote}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC4:UN4:GR10:AT1}
\no{1}
\unique{15210:S18:CH101:SEC4:UN4:GR10:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

Bob Harper comment:
misleading discussion? pages 226-227 the discussion of splay trees,
etc ignores the important issue of single-threaded vs multi-threaded
data structures; amortization doesn't work for the multithreaded case.
do the treaps work for that case?

It might indeed be worthwhile to discuss the weakness of amortization
with respect to persistence.  Though this seems a bit advanced for
this material.  Further thought is needed...
\end{teachnote}
\end{group}
\end{unit}
\end{section}

\begin{section}[A Parametric Implementation]
\label{section:15210:S18:CH101:SEC5:sec:bst::parametric}
\no{5}
\unique{15210:S18:CH101:SEC5}
\parent{...NOT.PROVIDED.PARENTS...}

We describe a minimalist implementation of the BST ADT based on two
functions, $\cd{split}$ and $\cd{join}$. 
%
Since the implementation depends on just these two functions, we refer
to it as a parametric implementation.

\begin{unit}[...NOT.PROVIDED.TITLE...]
\label{unit:15210:S18:CH101:SEC5:UN5}
\no{5}
\unique{15210:S18:CH101:SEC5:UN5}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC5:UN5:GR1}
\no{1}
\unique{15210:S18:CH101:SEC5:UN5:GR1}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{algorithm}[Implementing the BST ADT with $\cd{split}$ and $\cd{join}$]
\label{atom:15210:S18:CH101:SEC5:UN5:GR1:AT1:ds:bst::parametric}
\no{1}
\unique{15210:S18:CH101:SEC5:UN5:GR1:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

\[
\begin{array}{ll}
1 & \cd{type $\tttt$ = Leaf | Node of ($\tttt$ $\times$ $\kkk$ $\times$ $\tttt$)}
\\
&
\\
2 & \cd{split $t$ $k$ = ... (* as given *)}
\\
&
\\
3 & \cd{join $t_1$ $t_2$ = ... (* as given *)}
\\
&
\\
4 & \cd{joinM $t_1$ $k$ $t_2$ = join $t_1$ (join (singleton $k$) $t_2$)}
\\
&
\\
5 & \cd{empty = Leaf}
\\
&
\\
6 & \cd{singleton $(k)$ = Node(Leaf,$k$,Leaf)  }
\\
&
\\
7 & \cd{find $t$ $k$ = let (_,$v$,_) = split $t$ $k$  in $v$ end }
\\
&
\\
8 & \cd{delete $t$ $k$ = let $(l,\_,r)$ = split $t$ $k$ in join $l$ $r$ end}
\\
&
\\
9 & \cd{insert $t$ $k$ = let $(l,\_,r)$ = split $t$ $k$ in joinM $l$ $k$ $r$ end }
\\
&
\\
10 & \cd{intersect $t_1$ $t_2$ =}
\\
11 & ~~\cd{case ($t_1$,$t_2$) }
\\
12 & ~~\cd{| (Leaf,_) $\Rightarrow$ Leaf}
\\
13 & ~~\cd{| (_,Leaf) $\Rightarrow$ Leaf}
\\
14 & ~~\cd{| (Node $(l_1,k_1,r_1)$,_) $\Rightarrow$ }
\\
15 & ~~~~\cd{let $(l_2,b,r_2)$ = split $t_2$ $k_1$}
\\
16 & ~~~~~~~\cd{$(l,r)$ = (intersect $l_1$ $l_2$) || (intersect $r_1$ $r_2$)}
\\
17 & ~~~~\cd{in if $b$ then joinM $l$ $k_1$ $r$ else join $l$ $r$ end}
\\
&
\\
18 & \cd{difference $t_1$ $t_2$ =}
\\
19 & ~~\cd{case ($t_1$, $t_2$)}
\\
20 & ~~\cd{| (Leaf,_) $\Rightarrow$ Leaf}
\\
21 & ~~\cd{| (_,Leaf) $\Rightarrow$ $t_1$}
\\
22 & ~~\cd{| (Node $(l_1,k_1,r_1)$,_) $\Rightarrow$ }
\\
23 & ~~~~\cd{let $(l_2,b,r_2)$ = split $t_2$ $k_1$}
\\
24 & ~~~~~~~~\cd{$(l,r)$ = (difference $l_1$ $l_2$) || (difference $r_1$ $r_2$)}
\\
25 & ~~~~\cd{in if $b$ then join $l$ $r$ else joinM $L$ $k_1$ $r$ end}
\\
&
\\
26 & \cd{union $t_1$ $t_2$ =}
\\
27 & ~~\cd{case ($t_1$,$t_2$)}
\\
28 & ~~\cd{| (Leaf,_) $\Rightarrow$ $t_2$}
\\
29 & ~~\cd{| (_,Leaf) $\Rightarrow$ $t_1$}
\\
30 & ~~\cd{| (Node $(l_1,k_1,r_1)$,_) $\Rightarrow$   }
\\
31 & ~~~~\cd{let $(l_2,\_,r_2)$ = split $t_2$ $k_1$}
\\
32 & ~~~~~~~~\cd{$(l,r)$ = (union $l_1$ $l_2$) || (union $r_1$ $r_2$)}
\\
33 & ~~~~\cd{in joinM $l$ $k_1$ $r$ end    }
\\
\end{array}
\]
\end{algorithm}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC5:UN5:GR2}
\no{2}
\unique{15210:S18:CH101:SEC5:UN5:GR2}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC5:UN5:GR2:AT1}
\no{1}
\unique{15210:S18:CH101:SEC5:UN5:GR2:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

The implementation given above assumes that the implementation of
$\cd{split}$ and $\cd{join}$ are supplied.
%
%\dsref{...}
%

The implementation defines the tree type as consisting of
leaves or internal nodes with left and right subtrees and a key.
%

The auxiliary function, $\cd{joinM}$ takes two trees $t_1$ and $t_2$
and a ``middle'' key $k$ that is sandwiched between the two
trees---that is $k$ is greater than all the keys in $t_1$ and less
than all the keys in $t_2$---and returns a tree that contains all the
keys in $t_1$ and $t_2$ as well as $k$.


The function $\cd{find}$ is easily implementable with a
$\cd{split}$, which indicates whether the key used for splitting is
found in the tree or not.
%
To implement $\cd{insert}$ of a key $k$ into a tree, we first
$\cd{split}$ the tree at $k$ and then join the two returned
trees along with key $k$ using $\cd{joinM}.$
%
To implement $\cd{delete}$ of a key $k$ from a tree, we first
$\cd{split}$ the tree at $k$ and then join the two returned
trees with $\cd{join}$. 
%
If the key $k$ was found, this gives us a tree that does not contain
the $k$; otherwise we obtain a tree of the same set of keys (though
the structure of the tree may be different internally depending on the
implementation of $\cd{split}$ and $\cd{join}$).
\end{gram}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC5:UN5:GR3}
\no{3}
\unique{15210:S18:CH101:SEC5:UN5:GR3}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[Union]
\label{atom:15210:S18:CH101:SEC5:UN5:GR3:AT1}
\no{1}
\unique{15210:S18:CH101:SEC5:UN5:GR3:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

The implementation of $\cd{union}$ uses divide and conquer.  The idea is
to split both trees at some key $k$, recursively union the two parts
with keys less than $k$, and the two parts with keys greater than $k$
and then join them.  
%
There are different ways to select the key $k$ used to split the tree.
%
One way is to use the key at the root of one of the two trees, for
example the first tree, and split the second tree with it; this is the
approach take in the parametric implementation.
\end{gram}

\begin{example}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC5:UN5:GR3:AT2:example:tree-union}
\no{2}
\unique{15210:S18:CH101:SEC5:UN5:GR3:AT2}
\parent{...NOT.PROVIDED.PARENTS...}

The union of tree $t_1$ and $t_2$ illustrated.

\begin{center}
  \includegraphics[width=14cm]{/media/210/bsts/union-dia1.jpg}
\end{center}
\end{example}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC5:UN5:GR4}
\no{4}
\unique{15210:S18:CH101:SEC5:UN5:GR4}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[Intersection]
\label{atom:15210:S18:CH101:SEC5:UN5:GR4:AT1}
\no{1}
\unique{15210:S18:CH101:SEC5:UN5:GR4:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

The implementation of $\cd{intersection}$ uses a divide-and-conquer
approach similar to that  of $\cd{union}$.  
%
As in $\cd{union}$, we split both trees by using the key $k_1$ at the
root of the first tree, and compute intersections recursively.
%
We then compute the result by joining the results from the recursive
calls and including the key $k_1$ if it is found in both trees.
%
\begin{teachask}
How does the implementation accounts for all possible shared keys when
we divide the input trees into two and calculate intersections
recursively?  What if a key on the left matches a key on the right
half of the other tree?
\end{teachask}
%
Note that since the trees are BSTs, checking for the intersections of
left and right subtrees recursively and is guaranteed to find all
shared keys because the $\cd{split}$ operation places all keys less
than and greater than the given key to two separate trees.
\end{gram}

\begin{exercise}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC5:UN5:GR4:AT2}
\no{2}
\unique{15210:S18:CH101:SEC5:UN5:GR4:AT2}
\parent{...NOT.PROVIDED.PARENTS...}

Prove correct the functions $\cd{intersection}$, $\cd{difference}$, and
  $\cd{union}$.
\end{exercise}
\end{group}
\end{unit}
\end{section}

\begin{section}[Cost Specification]
\label{section:15210:S18:CH101:SEC6:sec:bst::cost}
\no{6}
\unique{15210:S18:CH101:SEC6}
\parent{...NOT.PROVIDED.PARENTS...}

...NOT.PROVIDED.INTRO...

\begin{unit}[...NOT.PROVIDED.TITLE...]
\label{unit:15210:S18:CH101:SEC6:UN6}
\no{6}
\unique{15210:S18:CH101:SEC6:UN6}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC6:UN6:GR1}
\no{1}
\unique{15210:S18:CH101:SEC6:UN6:GR1}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC6:UN6:GR1:AT1}
\no{1}
\unique{15210:S18:CH101:SEC6:UN6:GR1:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

There are many ways to implement an efficient data structure that
matches our BST ADT, many of these implementation more or less match
the same cost specification, with the main difference being whether
the bounds are worst-case, expected case (probabilistic), or
amortized.  
%
These implementations all use balancing techniques to ensure that the
depth of the BST remains $O(\lg{n})$, where $n$ is the number of keys
in the tree.  
%
For the purposes specifying the costs, we don't distinguish between
worst-case, amortized, and probabilistic bounds, because we can always
rely on the existence of an implementation that matches the desired
cost specification.  
%
When using specific data structures that match the specified bounds in
an amortized or randomized sense, we will try to be careful when
specifying the bounds.
\end{gram}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC6:UN6:GR2}
\no{2}
\unique{15210:S18:CH101:SEC6:UN6:GR2}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{costspec}[BSTs]
\label{atom:15210:S18:CH101:SEC6:UN6:GR2:AT1:cost:bst::costs}
\no{1}
\unique{15210:S18:CH101:SEC6:UN6:GR2:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

The~\defn{BST} cost specification is defined as follows.  The
  variables $n$ and $m$ are defined as $n = \max{(|t_1|,|t_2|)}$ and
  $m = \min{(|t_1|,|t_2|)}$ when applicable.
%  \setlength{\extrarowheight}{1ex}
\[
\begin{array}{l@{\qquad}c@{\qquad}c@{\qquad}}
& \mathbf{Work} & \mathbf{Span}
\\ 

\cd{empty}
& \bigoh{1}
& \bigoh{1}
\\ 

\cd{singleton}~k
& \bigoh{1}
& \bigoh{1}
\\ 

\cd{split}~t~k
& \bigoh{\lg{|t|}}
& \bigoh{\lg{|t|}}
\\ 

\cd{join}~t_1~t_2
& \bigoh{\lg{\left(|t_1|+|t_2|\right)}}
& \bigoh{\lg{\left(|t_1|+|t_2|\right)}}
\\ 

\cd{find}~t~k
& \bigoh{\lg{|t|}}
& \bigoh{\lg{|t|}}
\\ 

\cd{insert}~t~k
& \bigoh{\lg{|t|}}
& \bigoh{\lg{|t|}}
\\ 

\cd{delete}~t~k
& \bigoh{\lg{|t|}}
& \bigoh{\lg{|t|}}
\\ 

\cd{intersect}~t_1~t_2
& \bigoh{m \cdot \lg{\frac{n+m}{m}}}
& \bigoh{\lg{n}}
\\ 

\cd{difference}~t_1~t_2
& \bigoh{m \cdot \lg{\frac{n+m}{m}}}
& \bigoh{\lg{n}}
\\ 

\cd{union}~t_1~t_2
& \bigoh{m \cdot \lg{\frac{n+m}{m}}}
& \bigoh{\lg{n}}
\end{array}
\]
\end{costspec}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC6:UN6:GR3}
\no{3}
\unique{15210:S18:CH101:SEC6:UN6:GR3}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC6:UN6:GR3:AT1}
\no{1}
\unique{15210:S18:CH101:SEC6:UN6:GR3:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

The cost specification above shows the costs for the BST ADT as can be
realized by several balanced BST data structures such as Treaps (in
expectation), red-black trees (in the worst case), and splay trees
(amortized). As may be expected the cost of $\cd{empty}$ and
$\cd{singleton}$ are constant.
%

For the rest of the operations, we justify the cost bounds by assuming
the existence of logarithmic time $\cd{split}$ and $\cd{join}$ operations,
and by using our parametric implementation described above.
%
The work and span costs of $\cd{find}$, $\cd{insert},$ and
$\cd{delete}$ are determined by the $\cd{split}$ and $\cd{join}$
operation and are thus logarithmic in the size of the tree.
%

The cost bounds on $\cd{union}$, $\cd{intersection}$, and
$\cd{difference}$, which are similar are more difficult to see.
\end{gram}
\end{group}
\end{unit}

\begin{unit}[Cost of Union, Intersection, and Difference]
\label{unit:15210:S18:CH101:SEC6:UN7}
\no{7}
\unique{15210:S18:CH101:SEC6:UN7}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC6:UN7:GR1}
\no{1}
\unique{15210:S18:CH101:SEC6:UN7:GR1}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC6:UN7:GR1:AT1}
\no{1}
\unique{15210:S18:CH101:SEC6:UN7:GR1:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

Let's analyze the cost for $\cd{union}$ as implemented by the parametric
implementation. It is easy to apply a similar analysis to
$\cd{intersection}$ and $\cd{difference}$.
\end{gram}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC6:UN7:GR2}
\no{2}
\unique{15210:S18:CH101:SEC6:UN7:GR2}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[Figuring out the Recurrence]
\label{atom:15210:S18:CH101:SEC6:UN7:GR2:AT1}
\no{1}
\unique{15210:S18:CH101:SEC6:UN7:GR2:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

Consider now a call to $\cd{union}$ with parameters
$t_1$ and $t_2$.  To simplify the analysis, we will make the following
assumptions:
\begin{enumerate}
\item $t_1$ is perfectly balanced (i.e., the left and right subtrees of
  the root have size at most $|t_1|/2$), 
\item each time a key from $t_1$ splits $t_2$, it splits the tree in
  exactly in half, and
\item $|t_1| < |t_2|$.
\end{enumerate}
%
Later we will relax these assumptions.  

Let us define $m = |t_1|$ and
$n = |t_2|$ (recall the size of a tree is the number of keys in it).
With these assumptions and examining the algorithm we can then write
the following recurrence for the work of $\cd{union}$:
\begin{align*}
  W_{\mbox{union}}(m, n) &\leq 2W_{\mbox{union}}(m/2,n/2) + W_{\mbox{split}}(n)
+ W_{\mbox{join}}(n+m) + O(1)\\
   & \leq  2W_{\mbox{union}}(m/2, n/2) + O(\lg n)~.
\end{align*}  

The size for join is the sum of the two sizes, $m+n$, but since 
$m \leq n$, $O(\lg (n + m))$ is equivalent to $O(\lg n)$.
We also have the base case
\begin{align*}
  W_{\mbox{union}}(1, n) & \leq 2W_{\mbox{union}}(0,n/2) + W_{\mbox{split}}(n)
+ W_{\mbox{join}}(n) + O(1)\\
         & \leq O(\lg n)~.
\end{align*}
The final inequality holds because $2W_{\mbox{union}}(0,n) = O(1)$.
\end{gram}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC6:UN7:GR3}
\no{3}
\unique{15210:S18:CH101:SEC6:UN7:GR3}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[Solving the Recurrence]
\label{atom:15210:S18:CH101:SEC6:UN7:GR3:AT1}
\no{1}
\unique{15210:S18:CH101:SEC6:UN7:GR3:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

We can draw the recurrence tree showing the work performed by the
splitting of $t_2$ and by the joining of the results as follows.  For
simplicity of the argument, let's assume that the leaves of the tree
correspond to the case for $m = 1$.

%This figure should be changed so "n" is replaced by "N" except the
%bottom level should be "each costs log (1 + n/m)"

\begin{center}
  \includegraphics[width=4.5]{/media/210/bsts/recurtree2.jpg}
\end{center}
\end{gram}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC6:UN7:GR4}
\no{4}
\unique{15210:S18:CH101:SEC6:UN7:GR4}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[Brick Method]
\label{atom:15210:S18:CH101:SEC6:UN7:GR4:AT1}
\no{1}
\unique{15210:S18:CH101:SEC6:UN7:GR4:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

Let's analyze the structure of the recursion tree shown above.
%

We can find the number of leaves in the tree by examining the work
recurrence.  Notice that in the recurrence, the tree bottoms out when
$m = 1$ and before that, $m$ always gets split in half (remember that
$t_1$ is perfectly balanced).  The tree $t_2$ does not affects the
shape of the recursion tree or the stopping condition. Thus, there are
exactly $m$ leaves in the tree.  
%
In fact, the recursion can be rewritten as a recursion of the form
$W(m) = 2W(m/2) + \ldots $, which means that there are $m$ leaves.
%


By the same reasoning, we can see that the leaves are $(1 + \lg m)$
deep.

Let's now determine the size of $t_2$ at the leaves.  
%
We have $m$ keys in $t_1$ to start with, and they split $t_2$ evenly
all the way down to the level of the leaves (by assumption). Thus, the
leaves have all the same size of $\frac{n}{m}$.
%
Therefore, each leaf adds a $O(\lg (1+\frac{n}{m}))$ term to the work
(the $1+$ is needed to deal with the case that $n = m$).  Since there
are $m$ leaves, the whole bottom level costs 
%
\[
O(m \lg (1+ \frac{n}{m})).
\]  

We will now prove that the cost at the bottom level is indeed
asymptotically the same as the total work.  In other words, the tree
is leaves-dominated.  It is possible to prove that the tree is
leaves-dominated by computing the ratio of the work at adjacent
levels, i.e., the ratio
\[
\frac{2^{i-1}  \lg{n/2^{i-1}}}{2^i 
  \lg{n/2^{i}}} = \frac{1}{2} \frac{\lg{n} - i + 1 }{\lg{n} - i},
\]
where $i \le \lg{m} < \lg{n}$. This ratio is less than $1$ for all
levels except for the last level, where by taking $i = \lg{n} - 1$ we
have
\[
 \frac{1}{2} \frac{\lg{n} - i +1 }{\lg{n} - i}
 \le
 \frac{1}{2} \frac{1}{\lg{n} - \lg{n} +1 + 1}{\lg{n} - \lg{n}+1}
= \frac{1}{1}.
\]
Thus the total work is asymptotically dominated by the total work of
the leaves, which is $\bigoh{m\lg{n/m}}$.
\end{gram}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC6:UN7:GR5}
\no{5}
\unique{15210:S18:CH101:SEC6:UN7:GR5}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[Direct Derivation]
\label{atom:15210:S18:CH101:SEC6:UN7:GR5:AT1}
\no{1}
\unique{15210:S18:CH101:SEC6:UN7:GR5:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

We can establish the same fact more precisely.  Let's start by writing
the total cost by summing over all levels, omitting for simplicity the
constant factors, and assuming that $n = 2^a$ and $m = 2^b$,
\[
W(n,m) = \sum_{i = 0}^{b}{2^i \lg{\frac{n}{2^i}}}.
\]
We can rewrite this sum as 
\[
\sum_{i = 0}^{b}{2^i \lg{\frac{n}{2^i}}} = 
\lg{n}\sum_{i = 0}^{b}{2^i} - \sum_{i = 0}^{b}{i\,2^i}.
= a\sum_{i = 0}^{b}{2^i} - \sum_{i = 0}^{b}{i\,2^i}.
\]
Let's now focus on the second term. Note that 
\[
\sum_{i = 0}^{b}{i\,2^i} 
=
\sum_{i = 0}^{b}{\sum_{j=i}^{b}2^j} 
=
\sum_{i = 0}^{b}{\left(\sum_{j=0}^{b}{2^j} - \sum_{k=0}^{i-1}2^k\right). }
\]
Substituting the closed form for each inner summation and simplifying
leads to
\[
\begin{array}{ll}
= & \sum_{i = 0}^{b}{\left( (2^{b+1}-1) - (2^{i}-1) \right)}.
\\[2mm]
= &  (b+1) (2^{b+1}-1) - \sum_{i = 0}^{b}{(2^{i}-1)}
\\[2mm]
= &  (b+1) (2^{b+1}-1) - \left( 2^{b+1}-1 - (b+1) \right)
\\[2mm]
= &  (b+1) (2^{b+1}-1) - \left( 2^{b+1}-1 - (b+1) \right)
\\[2mm]
= &  b\,2^{b+1} + 1.
\end{array}
\]

Let's now go back and plug this into the original work bound and simplify
\[
\begin{array}{lll}
W(n,m) & = &\sum_{i = 0}^{b}{2^i \lg{\frac{n}{2^i}}}
\\[2mm]
& = & a\sum_{i = 0}^{b}{2^i} - \sum_{i = 0}^{b}{i\,2^i}
\\[2mm]
& = & a\,(2^{b+1}-1) -  (b\,2^{b+1} + 1)
\\[2mm]
& = & a\,2^{b+1}-a - b\,2^{b+1} -1
%\\
= 2m (a-b) - a -1
\\[2mm]
& = & 2m (\lg{n}-\lg{m}) - a -1
%\\
= 2m \lg{\frac{n}{m}} - a -1
\\[2mm]
& = & \bigoh{m \, \lg{\frac{n}{m}}}.
\end{array}
\]

While the direct method may seem complicated, it is more robust than
the brick method, because it can be applied to analyze essentially any
algorithm, whereas the Brick method requires establishing a geometric
relationship between the cost terms at the levels of the tree.
%
\end{gram}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC6:UN7:GR6}
\no{6}
\unique{15210:S18:CH101:SEC6:UN7:GR6}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[Removing the Assumptions]
\label{atom:15210:S18:CH101:SEC6:UN7:GR6:AT1}
\no{1}
\unique{15210:S18:CH101:SEC6:UN7:GR6:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

Of course, in reality, our keys in $t_1$ won't split subtrees of $t_2$
in half every time.  But it turns out that any unevenness in the
splitting only helps reduce the work---i.e., the perfect split is the
worst case.  We won't go through a rigorous argument, but if we keep
the assumption that $t_1$ is perfectly balanced, then the shape of the
recursion tree stays the same.  What is now different is the cost at
each level.  Let us try to analyze the cost at level~$i$.  At this
level, there are $k = 2^i$ nodes in the recursion tree. Say the sizes
of $t_2$ at these nodes are $n_1, \dots, n_k$, where $\sum_j n_j =
n$. Then, the total cost for this level is
\[
c \cdot \sum_{j=1}^k \lg (n_j) \;\;\leq\;\; c \cdot \sum_{j=1}^k \lg (n/k) =
c\cdot 2^i \cdot \lg (n/2^i),
\]
where we used the fact that the logarithm function is
concave\footnote{This is also known as Jensen's inequality.}.  Thus,
the tree remains leaf-dominated and the same reasoning shows that the
total work is $O(m \lg (1 + \frac{n}{m}))$.

Still, in reality, $t_1$ doesn't have to be perfectly balanced as we
assumed. To generalize the analysis, we just need a tree with
$O(\lg{m})$ height.  Thus, we only need $t_1$ to be approximately
balanced. 

Finally, we assumed that $t_1$ is larger that $t_2$.  If it is
smaller, then we can reverse the order of arguments, so in this case,
there is no loss of generality.  If they are the same size, we need to
be a bit more precise in our handling of the base case in our
summation but this is all.

We end by remarking that as described, the span of $\cd{union}$ is
$O(\lg^2 n)$, but this can be improved to $O(\lg n)$ by changing the
algorithm slightly.

In summary, $\cd{union}$ can be implemented in 
\[
O(m \lg (1 + \tfrac{n}m))
\]
work and 
\[
O(\lg n)
\]
span.  

Essentially the same analysis applies to the functions
$\cd{intersection}$ and $\cd{difference}$, whose structures are the same
as $\cd{union}$, except for an additional constant work and span for
the conditional ($\cd{if}$) expression.
\end{gram}
\end{group}
\end{unit}
\end{section}

\begin{section}[Treaps]
\label{section:15210:S18:CH101:SEC7:sec:bst::treaps}
\no{7}
\unique{15210:S18:CH101:SEC7}
\parent{...NOT.PROVIDED.PARENTS...}

Our parametric implementation established an interesting fact: to
implement the BST ADT efficiently, we only need to provide efficient
$\cd{split}$ and $\cd{join}$ operations.  In this section, we
present a data structure called~\defn{Treaps} that can support
$\cd{split}$ and $\cd{join}$ operations in expected logarithmic work
and span.
%
Treaps achieve their efficiency by maintaining BSTs that are
probabilistically balanced. Of the many balanced BST data structures,
Treaps are likely the simplest, but, since they are randomized, they
only guarantee approximate balance with high probability.

\begin{unit}[...NOT.PROVIDED.TITLE...]
\label{unit:15210:S18:CH101:SEC7:UN8}
\no{8}
\unique{15210:S18:CH101:SEC7:UN8}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC7:UN8:GR1}
\no{1}
\unique{15210:S18:CH101:SEC7:UN8:GR1}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{teachnote}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC7:UN8:GR1:AT1}
\no{1}
\unique{15210:S18:CH101:SEC7:UN8:GR1:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

To see the idea behind Treaps, let's first discuss how we can map a
  sequence of keys to a BST. Given $S$, a sequence of unique keys,
  let's start with an empty BST and insert the keys in $S$ into the
  BST one by one starting from left to right.  To insert a key $k$, we
  perform a search for $k$ in the current BST and let $u$ be the leaf
  where the (unsuccessful) search terminates.  To insert $k$ into the
  tree, we replace $u$ with a new node with key $k$.



We can map the sequence $\cseq{8,9,5,6,1,7}$ to a BST by inserting the keys from
left to right.

\begin{center}
\includegraphics[width=14cm]{/media/210/bsts/bst5-build.jpg}
\end{center}


What can we say about the height of such a tree? 


Note now that the height of the BST will be directly determined by the
input sequence.  Specifically, the specific order---or
permutation---in which the keys are inserted will determine the
height.  For most permutations, the tree will be reasonably well
balanced because we get an unbalanced tree only in cases where an
element partitions the following keys unevenly.  Since we have
many more even partitions for a given set of keys (many ``middle''
keys), many permutations create balanced trees.


Can we take advantage of this observation somehow? 

%
Recall that the BST ADT does not care about the specific structure of
the BST but only the set of keys in the tree. We can take advantage of
this observation by selecting a (uniformly) random permutation of the keys and
constructing the BST based on this permutation.  Since most
permutations give us reasonably balanced trees, this approach should
give us a balanced tree.



Suppose that we were given the keys in the sequence one by one instead
of all at once.  Can you think of a way to select a uniformly random
permutation?


Observe now that we can select a uniformly random permutation even if
we don't have all the keys by assigning a random priority to each key
as it arrives and building our BST by considering the keys in the
priority order.  By assigning priorities randomly, we essentially
guarantee that we always build our tree on a uniformly randomly
selected permutation.
%
This is one of the main ideas behind Treaps: Treaps can be viewed as
maintaining a BST on a uniformly random permutation of the keys in the
tree. To achieve this ``imitation of randomly ordered insertions''
we associate a priority with each key and require the BST to be ``heap
ordered'' with respect to priorities.
\end{teachnote}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC7:UN8:GR2}
\no{2}
\unique{15210:S18:CH101:SEC7:UN8:GR2}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[Idea behind Treaps]
\label{atom:15210:S18:CH101:SEC7:UN8:GR2:AT1}
\no{1}
\unique{15210:S18:CH101:SEC7:UN8:GR2:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

The idea behind Treaps is to associate a uniformly randomly selected
priority to each key and maintain a priority order between keys in
addition to the binary-search-tree order.  The priority order between
keys resemble the order used in binary heaps, leading to the name
``Tree Heap'' or ``Treap.''
\end{gram}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC7:UN8:GR3}
\no{3}
\unique{15210:S18:CH101:SEC7:UN8:GR3}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{definition}[Treap]
\label{atom:15210:S18:CH101:SEC7:UN8:GR3:AT1}
\no{1}
\unique{15210:S18:CH101:SEC7:UN8:GR3:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

A Treap is a binary search tree over a set $K$ along with
  a~\defn{priority} for each key given by
%
  \[
  p : \kkk \rightarrow  \tyint
  \]
%
  that in addition to satisfying the BST property on the
  keys $K$, satisfies the heap property on the priorities $p(k), k \in
  K$, i.e., for every internal node $v$ with left and right children
  $u$ and $w$:
\[
p(k(v)) \geq p(k(u)) \mbox{ and } p(k(v)) \geq p(k(w)),
\]
where $k(v)$ denotes the key of a node.
\end{definition}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC7:UN8:GR4}
\no{4}
\unique{15210:S18:CH101:SEC7:UN8:GR4}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{example}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC7:UN8:GR4:AT1}
\no{1}
\unique{15210:S18:CH101:SEC7:UN8:GR4:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

The following key-priority pairs $(k,p(k))$,
\[ (a,3), (b,9), (c, 2), (e,6), (f, 5)~,\] where the keys are ordered
alphabetically, form the following Treap:
\begin{center}
  \includegraphics[width=2.0in]{/media/210/bsts/treap-examp.jpg}
\end{center}
since $9$ is larger than $3$ and $6$, and $6$ is larger than $2$ and
$5$.
\end{example}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC7:UN8:GR5}
\no{5}
\unique{15210:S18:CH101:SEC7:UN8:GR5}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{exercise}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC7:UN8:GR5:AT1}
\no{1}
\unique{15210:S18:CH101:SEC7:UN8:GR5:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

Prove that if the priorities are unique, then there is exactly one tree
structure that satisfies the Treap properties.
\end{exercise}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC7:UN8:GR6}
\no{6}
\unique{15210:S18:CH101:SEC7:UN8:GR6}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[Assigning Priorities]
\label{atom:15210:S18:CH101:SEC7:UN8:GR6:AT1}
\no{1}
\unique{15210:S18:CH101:SEC7:UN8:GR6:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

So how do we assign priorities?  As we briefly suggested in the
informal discussion above, it turns out that if the priorities are
selected uniformly randomly then the tree is guaranteed to be near
balanced, i.e. $O(\lg |S|)$ height, with high probability.  We will
show this shortly.
\end{gram}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC7:UN8:GR7}
\no{7}
\unique{15210:S18:CH101:SEC7:UN8:GR7}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{teachask}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC7:UN8:GR7:AT1}
\no{1}
\unique{15210:S18:CH101:SEC7:UN8:GR7:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

How can we maintain the heap order as we modify the tree?
\end{teachask}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC7:UN8:GR8}
\no{8}
\unique{15210:S18:CH101:SEC7:UN8:GR8}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC7:UN8:GR8:AT1}
\no{1}
\unique{15210:S18:CH101:SEC7:UN8:GR8:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

Based on our parametrized implementation, we can implement the BST ADT
with Treaps simply by implementing the $\cd{split}$ and $\cd{join}$
functions.  
%
%\dsref{bst::treaps} shows such an implementation.  
The \pml{} code for such an implementation is given below.
%
For the implementation we assume, without loss of generality, that the
priorities are integers.  We present only the code for $\cd{split}$
and $\cd{join}$; the rest of the implementation is essentially the
same as in the parametric implementation.
% \dsref{bst::parametric}
%
The only exception that since the nodes now carry priorities, we will
need to account for them as we pattern match on nodes and create new
ones.  In implementing the rest of the functions, there are no
interesting operations on priorities: they simply follow the key that
they belong to.
\end{gram}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC7:UN8:GR9}
\no{9}
\unique{15210:S18:CH101:SEC7:UN8:GR9}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{algorithm}[Implementing \adt{BST} with Treaps]
\label{atom:15210:S18:CH101:SEC7:UN8:GR9:AT1:ds:bst::treaps}
\no{1}
\unique{15210:S18:CH101:SEC7:UN8:GR9:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

\[
\begin{array}{ll}
1 & \cd{type $\tttt$ = Leaf | Node of ($\tttt$ $\times$ $\kkk$ $\times$ $\tyint$ $\times$ $\tttt$)}
\\
~&~
\\
2 & \cd{empty = Leaf}
\\
&\\
3 & \cd{singleton $k$ = Node(Leaf, $k$, randomInt(), Leaf)}
\\
&\\
4 & \cd{split $t$ $k$ = }
\\
5 & ~~\cd{case $t$ }
\\
6 & ~~\cd{| Leaf $\Rightarrow$ (Leaf, False, Leaf)}
\\
7 & ~~\cd{| Node $(l, k', p', r)$ =}
\\
8 & ~~~~~~\cd{case compare $(k, k')$}
\\
9 & ~~~~~~\cd{| LESS $\Rightarrow$} %\label{line:bst::treaps::split-less}
\\
10 & ~~~~~~~~~~\cd{let $(l', x, r')$ = split $l$ $k$}
\\
11 & ~~~~~~~~~~\cd{in $(l', x, \cd{Node}(r', k', p', r))$ end}% \label{line:bst::treaps::splitnode1}}
\\
12 & ~~~~~~\cd{| EQUAL $\Rightarrow$ $($$l$, true, $r$$)$}
\\
13 & ~~~~~~\cd{| GREATER $\Rightarrow$} %\label{line:bst::treaps::split-greater}
\\
14 & ~~~~~~~~~~\cd{let $(l', x, r')$ = split $r$ $k$}
\\
15 & ~~~~~~~~~~\cd{in (Node $(l, k', p', l'), x, r'$) end} %\label{line:bst::treaps::splitnode2}
\\
&\\
16 & \cd{join $t_1$ $t_2$ =}
\\
17 & ~~\cd{case $(t_1, t_2)$}
\\
18 & ~~\cd{| (Leaf,  _) $\Rightarrow$ $t_2$}
\\
19 & ~~\cd{| (_ ,  Leaf) $\Rightarrow$ $t_1$}
\\
20 & ~~\cd{| (Node $(l_1, k_1, p_1, r_1)$,  Node $(l_2, k_2, p_2, r_2)$) $\Rightarrow$}
\\
21 & ~~~~~~\cd{if ($p_1 > p_2$) then}  %\label{line:bst::treaps::cpri}
\\
22 & ~~~~~~~~\cd{Node ($l_1$, $k_1$, $p_1$,  join $r_1$ $t_2$)}
\\
23 & ~~~~~~\cd{else}
\\
24 & ~~~~~~~~\cd{Node (join $t_1$ $l_2$,  $k_2$, $p_2$, $r_2$)}
\end{array}
\]
\end{algorithm}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC7:UN8:GR10}
\no{10}
\unique{15210:S18:CH101:SEC7:UN8:GR10}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC7:UN8:GR10:AT1}
\no{1}
\unique{15210:S18:CH101:SEC7:UN8:GR10:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

To implement the function $\cd{singleton},$ we rely on a function
$\cd{randomInt},$ which when called returns a (pseudo-)random number.
Such functions are broadly provided by programming languages.
\end{gram}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC7:UN8:GR11}
\no{11}
\unique{15210:S18:CH101:SEC7:UN8:GR11}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[Split]
\label{atom:15210:S18:CH101:SEC7:UN8:GR11:AT1}
\no{1}
\unique{15210:S18:CH101:SEC7:UN8:GR11:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

The $\cd{split}$ algorithm recursively traverses the tree from the
root to the key $k$ splitting along the path, and then when returning
from the recursive calls, it puts the subtrees back together.  When
putting back the trees along the path being split through, the
function does not have to compare priorities 
%because $\cd{Node}$ on
%\linereftwo{bst::treaps::splitnode1}{bst::treaps::splitnode2},
the priority $p'$ is the highest priority in the input tree $T$ and is
therefore larger than the priorities of either of the subtrees on the
left and right.  Hence $\cd{split}$ maintains the heap property of
treaps.
\end{gram}

\begin{example}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC7:UN8:GR11:AT2}
\no{2}
\unique{15210:S18:CH101:SEC7:UN8:GR11:AT2}
\parent{...NOT.PROVIDED.PARENTS...}

A $\cd{split}$ operation on a Treap and key~$c$, which is not in the
Treap.  The $\cd{split}$ traverses the path $\cseq{a,e,b,d}$ turning
right at $a$ and $b$ 
%(\lineref{bst::treaps::split-greater} 
of the Data Structure
%~\ref{ds:bst::treaps}) 
and turning left at $e$ and $d$.
%
%(\lineref{bst::treaps::split-less}).  
%
The pieces are put back together into the two
resulting trees on the way back up the recursion.
\begin{center}
  \includegraphics[width=5.7in]{/media/210/bsts/bstsplit.jpg}
\end{center}
%The actual way the trees will be put back together will depend on the
%balancing scheme.
\end{example}
\end{group}

\begin{group}[Join]
\label{group:15210:S18:CH101:SEC7:UN8:GR12}
\no{12}
\unique{15210:S18:CH101:SEC7:UN8:GR12}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC7:UN8:GR12:AT1}
\no{1}
\unique{15210:S18:CH101:SEC7:UN8:GR12:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

Unlike the implementation of $\cd{split}$, the implementation of
$\cd{join}$ operates on priorities in order to ensure that the
resulting Treap satisfies the heap priority of Treaps.  Specifically,
given two trees, $\cd{join}$ first compares the priorities of the two
roots, making the larger priority the new root. It then recursively
joins the Treaps consisting of the other tree and the appropriate side
of the new root.
%

The path from the root to the leftmost node in a BST is called
the~\defn{left spine}, and the path from the root to the rightmost
node is called the~\defn{right spine}.  The function
$\cd{join}~t_1~t_2$ merges the right spine of $t_1$ with the left
spine of $t_2$ based on the priority order.  This ensures that the
priorities are in decreasing order down the path.
\end{gram}

\begin{example}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC7:UN8:GR12:AT2:ex:bst::treap-join}
\no{2}
\unique{15210:S18:CH101:SEC7:UN8:GR12:AT2}
\parent{...NOT.PROVIDED.PARENTS...}

An illustration of $\cd{join}~t_1~t_2$ on Treaps.  
%
If $p(k_1) > p(k_2)$, then the function recurs with
$\cd{join}~R_1~t_2$ and the result becomes the right child of
$k_1$.
\begin{center}
  \includegraphics[width=15cm]{/media/210/bsts/treap-join.jpg}
\end{center}
\end{example}

\begin{example}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC7:UN8:GR12:AT3}
\no{3}
\unique{15210:S18:CH101:SEC7:UN8:GR12:AT3}
\parent{...NOT.PROVIDED.PARENTS...}

An illustration of $\cd{join}$ for Treaps applied to $t_1$ and $t_2$ in
  more detail.  The right spine of $t_1$ consisting of $(b,9)$,
  $(d,6)$ and $(e,5)$ is merged by priority with the left spine of
  $t_2$ consisting of $(h,8)$ and $(g,4)$.  Note that splitting the
  result with the key $f$ will return the original two trees.
\begin{center}
  \includegraphics[width=5.8in]{/media/210/bsts/treap-examp-join.jpg}
\end{center}
\end{example}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC7:UN8:GR13}
\no{13}
\unique{15210:S18:CH101:SEC7:UN8:GR13}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{teachnote}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC7:UN8:GR13:AT1}
\no{1}
\unique{15210:S18:CH101:SEC7:UN8:GR13:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{lemma}
\label{thm:treapuniqueness}
  For a set of keys $S$, if their priorities $p(s) : s \in S$ are unique,
  then there is exactly one Treap (i.e. shape) for $S$.

\begin{proof} (By induction on size)
An empty tree is a leaf (base case).  Otherwise, the unique key $k$
with the highest priority in $S$ must be at the root.  This fixes the
keys to the left ($\csetf{k' \in S}{k' < k}$) and to the right
($\csetf{k' \in S}{k' > k}$).  By induction these are unique, so the
whole tree is unique.
\end{proof}
\end{lemma}
\end{teachnote}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC7:UN8:GR14}
\no{14}
\unique{15210:S18:CH101:SEC7:UN8:GR14}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[Work of Split and Join]
\label{atom:15210:S18:CH101:SEC7:UN8:GR14:AT1}
\no{1}
\unique{15210:S18:CH101:SEC7:UN8:GR14:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

Let's bound now the work for $\cd{split}$ and $\cd{join}$.  
%
Each one does constant work on each recursive call.  
%
For $\cd{split}$ each recursive call goes to one of the children, so the
number of recursive calls is at most the height of $t$.  
%
For $\cd{join}$ each recursive call either goes down one level in $t_1$
or one level in $t_2$.  
%
Therefore the number of recursive calls is bounded by the sum of the
heights of the two trees.  
%
Hence the work of $\cd{split}$ $t$ $k$ is $O(h(t))$ and the work of
$\cd{join}$$(t_1,m,t_2)$ is $O(h(t_1)+h(t_2))$.  
%
Thus all that is left to do is to bound the height of a Treap.
\end{gram}
\end{group}
\end{unit}

\begin{unit}[Height Analysis of Treaps]
\label{unit:15210:S18:CH101:SEC7:UN9}
\no{9}
\unique{15210:S18:CH101:SEC7:UN9}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC7:UN9:GR1}
\no{1}
\unique{15210:S18:CH101:SEC7:UN9:GR1}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC7:UN9:GR1:AT1}
\no{1}
\unique{15210:S18:CH101:SEC7:UN9:GR1:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

We can analyze the height of a Treap by relating it to quicksort,
 which we analyzed before.
%
% in \chref{randomized}.
%
\end{gram}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC7:UN9:GR2}
\no{2}
\unique{15210:S18:CH101:SEC7:UN9:GR2}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{algorithm}[Treap Generating Quicksort]
\label{atom:15210:S18:CH101:SEC7:UN9:GR2:AT1}
\no{1}
\unique{15210:S18:CH101:SEC7:UN9:GR2:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

The following variant of quick sort generates a treap.
%
This algorithm is almost identical to our previous quicksort except that it
uses $\cd{Node}$ instead of $\cd{append},$ and
% on \lineref{bst::qsnode},
%
$\cd{Leaf}$ instead of an empty sequence in the base case.
%
Because it is generating a treap consisting of unique keys, the
algorithm retains only one key equaling the pivot.
%

\[
\begin{array}{ll}
1 & \cd{qsTree $a$ =}
\\
2 & ~~\cd{if $|a| = 0$ then Leaf}
\\
3 & ~~\cd{else}
\\
4 & ~~~~\cd{let}
\\
5 & ~~~~~~\cd{$x$ = the key $k \in a$ for which $p(k)$ is the largest}
\\
6 & ~~~~~~\cd{$b$ = $\cseqf{y \in a}{y < x}$}
\\
7 & ~~~~~~\cd{$c$ = $\cseqf{y \in a}{y > x}$}
\\
8 & ~~~~~~\cd{$(l,r)$ = (qsTree $b$ ) || (qsTree $c$)}
\\
9 & ~~~~\cd{in}
\\
10 & ~~~~~~\cd{Node $(l, x, r)$} %\label{line:bst::qsnode}
\\
11 & ~~~~\cd{end}
\\
\end{array}
\]
\end{algorithm}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC7:UN9:GR3}
\no{3}
\unique{15210:S18:CH101:SEC7:UN9:GR3}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC7:UN9:GR3:AT1}
\no{1}
\unique{15210:S18:CH101:SEC7:UN9:GR3:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

The tree generated by $\cd{qsTree}(a)$ is the Treap for the sequence
$a$.  
%
This can be seen by induction.  
%
It is true for the base case.
%
Now assume by induction it is true for the trees returned by the two
recursive calls.  
%
The tree returned by the main call is then also a Treap since the
pivot $x$ has the highest priority, and therefore is correctly placed
at the root, the subtrees and in heap order by induction, and because
the keys in $l$ are less than the pivot, and the keys in $r$ are
greater than the pivot, the tree has the BST property.

Based on this isomorphism, we can bound the height of a Treap by the
recursion depth of quicksort.
%  In \chref{randomized}, 
% 
Recall that we proved that if we pick the priorities at random, the
recursion depth is $O(\lg{n})$ with high probability.  Therefore we
know that the height of a Treap is $O(\lg{n})$ with high probability.
\end{gram}
\end{group}
\end{unit}
\end{section}

\begin{section}[Augmenting Trees]
\label{section:15210:S18:CH101:SEC8}
\no{8}
\unique{15210:S18:CH101:SEC8}
\parent{...NOT.PROVIDED.PARENTS...}

Thus far in this chapter, the only interesting information that we
stored in BSTs were keys. While such trees can be useful, we
sometimes wish to augment trees with more information. 
%
In this section, we describe how we might augment BSTs with
additional information such as key-value pairs, subtree sizes, and
reduced values in general.

\begin{unit}[Augmenting with Key-Value Pairs]
\label{unit:15210:S18:CH101:SEC8:UN10}
\no{10}
\unique{15210:S18:CH101:SEC8:UN10}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC8:UN10:GR1}
\no{1}
\unique{15210:S18:CH101:SEC8:UN10:GR1}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{teachnote}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC8:UN10:GR1:AT1}
\no{1}
\unique{15210:S18:CH101:SEC8:UN10:GR1:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

Can you think of an application where we would want to associate a
value with each key?

For example, you might want to maintain an address book as a BST keyed
by the names of your friends and associate each name with a phone
number or email.  Such a BST would allow you to locate the phone
number of your friend quickly, while also allowing you to do other
operations such as joining two address books, and updating entries.

%
How can we change a BST data structure to associate values with keys?
\end{teachnote}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC8:UN10:GR2}
\no{2}
\unique{15210:S18:CH101:SEC8:UN10:GR2}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC8:UN10:GR2:AT1}
\no{1}
\unique{15210:S18:CH101:SEC8:UN10:GR2:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

Perhaps the simplest form of augmentation involves storing in the BST
a key-value pair instead of just a key.
%
Implementing BSTs augmented with key-value pairs is relatively
straightforward by updating the relevant parts of the ADT.
%
For example, to accommodate the key, we can change the BST data type
to a key-value pair, and update the implementation of the functions to
pass the value around with the key as needed, making sure that a
key-value pair is never separated.  For functions such as $\cd{find}$
and $\cd{split}$ that may return the value, we make sure to do so.
\end{gram}
\end{group}
\end{unit}

\begin{unit}[Augmenting with Size]
\label{unit:15210:S18:CH101:SEC8:UN11}
\no{11}
\unique{15210:S18:CH101:SEC8:UN11}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC8:UN11:GR1}
\no{1}
\unique{15210:S18:CH101:SEC8:UN11:GR1}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC8:UN11:GR1:AT1}
\no{1}
\unique{15210:S18:CH101:SEC8:UN11:GR1:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

As a more complex augmentation, we might want to associate with each
node in the tree a size field that tells us how large the subtree
rooted at that node is.
%
As a motivating example for this form of augmentation, suppose that we
wish to extend the BST ADT
% (\adtref{bst::adt}) 
with the following
additional functions.
%
\begin{itemize}
\item Function $\cd{rank}~t~k$ returns the rank of the key $k$ in
  the tree, i.e., the number of keys in $t$ that are less than or
  equal to $k$.

\item Function $\cd{select}~T~i$ returns the key with the rank $i$ in $t$.

\end{itemize}
%
Such functions arise in many applications.  For example, we can use
them to implement sequences.
%
% the sequence interface discussed in \chref{sequences}. 

If we have a way to count the number of nodes in a subtree, then we
can easily implement these functions.
%
%\algref{bst::augment::size} 
%
The algorithm below shows such an implementation by using a size
operation for computing the size of a tree, written $|t|$ for tree
$t$.
%
%
With balanced trees such as Treaps, the $\cd{rank}$ and $\cd{select}$
functions require logarithmic span but linear work, because computing
the size of a subtree takes linear time in the size of the subtree.
%

%
If, however, we augment the tree so that at each node, we store the
size of the subtree rooted at that node, then work becomes
logarithmic, because we can find the size of a subtree in constant
work.
\end{gram}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC8:UN11:GR2}
\no{2}
\unique{15210:S18:CH101:SEC8:UN11:GR2}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{algorithm}[Rank]
\label{atom:15210:S18:CH101:SEC8:UN11:GR2:AT1}
\no{1}
\unique{15210:S18:CH101:SEC8:UN11:GR2:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

\[
\begin{array}{ll}
1 & \cd{rank $t$ $k$ =}
\\
2 & ~~\cd{case $t$ }
\\
3 & ~~\cd{| Leaf $\Rightarrow$ $0$}
\\
4 & ~~\cd{| Node $(l,k',r)$ $\Rightarrow$}
\\
5 & ~~~~~~\cd{case compare $(k,k')$ }
\\
6 & ~~~~~~\cd{| LESS $\Rightarrow$ rank $l$ $k$}
\\
7 & ~~~~~~\cd{| EQUAL $\Rightarrow$ $|l|$}
\\
8 & ~~~~~~\cd{| GREATER $\Rightarrow$ $|l| + 1 +$ rank $r$ $k$}
\\
~&~
\\
9 & \cd{select $t$ $i$ =}
\\
10 & ~~\cd{case $t$ }
\\
11 & ~~\cd{| Leaf $\Rightarrow$ raise exception OutOfRange}
\\
12 & ~~\cd{| Node $(l,k,r)$ $\Rightarrow$}
\\
13 & ~~~~~~\cd{case compare $(i,|l|)$ of}
\\
14 & ~~~~~~~~\cd{LESS $\Rightarrow$ select $l$ $i$}
\\
15 & ~~~~~~~~\cd{EQUAL $\Rightarrow$ $k$}
\\
16 & ~~~~~~~~\cd{GREATER $\Rightarrow$ select $r$ $(i-|l|-1)$}
\\
\end{array}
\]
\end{algorithm}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC8:UN11:GR3}
\no{3}
\unique{15210:S18:CH101:SEC8:UN11:GR3}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{teachask}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC8:UN11:GR3:AT1}
\no{1}
\unique{15210:S18:CH101:SEC8:UN11:GR3:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

Can you implement these functions by using a BST? 

What is the work and span of these functions?

Can we compute size of subtrees more efficiently?
\end{teachask}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC8:UN11:GR4}
\no{4}
\unique{15210:S18:CH101:SEC8:UN11:GR4}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{example}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC8:UN11:GR4:AT1}
\no{1}
\unique{15210:S18:CH101:SEC8:UN11:GR4:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

An example BST, where keys are ordered lexicographically and the nodes
are augmented with the sizes of subtrees.
%
The path explored by $\cd{rank (T,n)}$ and $\cd{select (T,4)}$ is
highlighted.

\begin{center}
  \includegraphics[width=2.5in]{/media/210/bsts/rankSelect.jpg}
\end{center}
\end{example}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC8:UN11:GR5}
\no{5}
\unique{15210:S18:CH101:SEC8:UN11:GR5}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{teachask}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC8:UN11:GR5:AT1}
\no{1}
\unique{15210:S18:CH101:SEC8:UN11:GR5:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

But how can we maintain the sizes of the subtrees as we perform
  various operations on the BST such as possibly aggregate insertions,
  deletions, splits, and joins?
\end{teachask}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC8:UN11:GR6}
\no{6}
\unique{15210:S18:CH101:SEC8:UN11:GR6}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC8:UN11:GR6:AT1}
\no{1}
\unique{15210:S18:CH101:SEC8:UN11:GR6:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

To implement a size-augmented tree, we need to keep $\cd{size}$ field at
each node and compute the size of the nodes as they are created.
%
In our parametric implementation, we can incorporate the $\cd{size}$
field by changing the definition of a node and initializing it to $1$,
when a singleton tree is created. 
%
When $\cd{split}$ and $\cd{join}$ functions create a new node, they
can compute its size by summing the sizes of its children.
%

In addition to the $\cd{rank}$ and $\cd{select}$ functions, we can
also define the function $\cd{splitRank}(T,i)$, which splits the
tree into two by returning the trees $t_1$ and $t_2$ such that $t_1$
contains all keys with rank less than $i$ and $t_2$ contains all keys
with rank is greater or equal to $i$. 
%
Such a function can be used for example to write divide-and-conquer
algorithms on imperfectly balanced trees.
\end{gram}
\end{group}
\end{unit}

\begin{unit}[Augmenting with Reduced Values]
\label{unit:15210:S18:CH101:SEC8:UN12}
\no{12}
\unique{15210:S18:CH101:SEC8:UN12}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC8:UN12:GR1}
\no{1}
\unique{15210:S18:CH101:SEC8:UN12:GR1}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC8:UN12:GR1:AT1}
\no{1}
\unique{15210:S18:CH101:SEC8:UN12:GR1:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

To compute rank-based properties of keys in a BST, we augmented the
BST so that each node stores the size of its subtree.  More generally,
we might want to associate with each node a~\defn{reduced value} that
is computed by reducing over the subtree rooted at the node by a user
specified function.  In general, there is no restriction on how the
reduced values may be computed, they can be based on keys or
additional values that the tree is augmented with.
%
To compute reduced values, we simply store with every node $u$ of a
binary search tree, the reduced value of its subtree (i.e. the sum of
all the reduced values that are descendants of $u$, possibly also the
value at $u$ itself).
\end{gram}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC8:UN12:GR2}
\no{2}
\unique{15210:S18:CH101:SEC8:UN12:GR2}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{example}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC8:UN12:GR2:AT1}
\no{1}
\unique{15210:S18:CH101:SEC8:UN12:GR2:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

The following drawing shows a tree with key-value pairs on the left,
  and the augmented tree on the right, where each node additionally
  maintains the sum of it subtree.
\begin{center}
  \includegraphics[width=4in]{/media/210/bsts/augtree.jpg}
\end{center}
The sum at the root ($13$) is the sum of all values in the tree ($3 +
1 + 2 + 2 + 5$).    It is also the sum of the reduced values of its
two children ($6$ and $5$) and its own value $2$.
\end{example}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC8:UN12:GR3}
\no{3}
\unique{15210:S18:CH101:SEC8:UN12:GR3}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{algorithm}[Treaps with Reduced Values]
\label{atom:15210:S18:CH101:SEC8:UN12:GR3:AT1:fig:bst::reducedjoin}
\no{1}
\unique{15210:S18:CH101:SEC8:UN12:GR3:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

\[
\begin{array}{ll}
1 & \cd{(* type of the reduced value, as specified. *)}
\\
2 & \cd{type rv = ...                   }
\\
~&~
\\
3 & \cd{(* associative reducer function, as specified. *)}
\\
4 & \cd{$f$(x: rv, y: val, z: rv): rv = ...  }
\\
~&~
\\
5 & \cd{(* identity for the reducer function, as specified. *)}
\\
6 & \cd{$id_f$ : rv = ...                 }
\\
~&~
\\
7 & \cd{type treap = }
\\
8 & ~~~\cd{Leaf }
\\
9 & ~\cd{| Node of (Treap $\times$ key $\times$ priority  $\times$ (val $\times$ rv) $\times$ Treap) }
\\
~&~
\\
10 & \cd{rvOf $t$ =}
\\
11 & ~~\cd{case $t$}
\\
12 & ~~\cd{| Leaf $\Rightarrow$ $id_f$}
\\
13 & ~~\cd{| Node (_,_,_(_,w),_) $\Rightarrow$ w}
\\
~&~
\\
14 & \cd{mkNode $(l,k,p,v,r)$ = Node ($l$,$k$,$p$,($v$,$f$ (rvOf $l$,$v$,rvOf $r$)),$r$) }
\\
~&~
\\
15 & \cd{split $t$ $k$ = }
\\
16 & ~~\cd{case t }
\\
17 & ~~\cd{| Leaf $\Rightarrow$ (Leaf,false,Leaf)}
\\
18 & ~~\cd{| Node $(l,k',p',(v',w'),r)$ =}
\\
19 & ~~~~~~\cd{case compare $(k,k')$}
\\
20 & ~~~~~~\cd{| LESS $\Rightarrow$ }
\\
21 & ~~~~~~~~~~\cd{let $(l',x,r')$ = split $l$ $k$}
\\
22 & ~~~~~~~~~~\cd{in $(l',x,\cd{mkNode}~(r',k',p',v',r))$ end }
\\
23 & ~~~~~~\cd{| EQUAL $\Rightarrow$ $($$l$,true,$r$$)$}
\\
24 & ~~~~~~\cd{| GREATER $\Rightarrow$}
\\
25 & ~~~~~~~~~~\cd{let $(l',x,r')$ = split $r$ $k$}
\\
26 & ~~~~~~~~~~\cd{in (mkNode $(l,k',p',v',l'),x,r'$) end }
\\
~&~
\\
27 & \cd{join $t_1$ $t_2$ =}
\\
28 & ~~\cd{case $(t_1,t_2)$ of}
\\
29 & ~~~~\cd{(Leaf, _) $\Rightarrow$ $t_2$}
\\
30 & ~~\cd{| (_, Leaf) $\Rightarrow$ $t_1$}
\\
31 & ~~\cd{| (Node $(l_1,k_1,p_1,(v_1,w_1),r_1)$, Node $(l_2,k_2,p_2,(v_2,w_2),r_2)$) $\Rightarrow$}
\\
32 & ~~~~~~\cd{if $p_1$ > $p_2$) then mkNode
  ($l_1$,$k_1$,$p_1$,$v_1$,join $r_1$ $t_2$)       } %  \label{line:bst::reducedjoin::cpri} 
\\
33 & ~~~~~~\cd{else mkNode (join $t_1$ $l_2$,$k_2$,$v_2$,$r_2$)}
\\
\end{array}
\]
\end{algorithm}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC8:UN12:GR4}
\no{4}
\unique{15210:S18:CH101:SEC8:UN12:GR4}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC8:UN12:GR4:AT1}
\no{1}
\unique{15210:S18:CH101:SEC8:UN12:GR4:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

The value of each reduced value in a tree can be calculated as the sum
of its two children plus the value stored at the node.  This means
that we can maintain these reduced values by simply taking the
``sum'' of three values whenever creating a node.  We can thus change
a data structure to support reduced values by changing the way we
create nodes.  In such a data structure, if the function that we use
for reduction performs constant work, then the work and the span bound
for the data structure remains unaffected.

As an example, the data structure above
%
%\figref{bst::reducedjoin} 
%
describes an extension of the parametric implementation of Treaps to
support reduced values.  The description is parametric in the values
paired with keys and the function $\cd{f}$ used for reduction.
%
The type for Treaps is extended to store the value paired with the key
as well as the reduced value.  Specifically, in a $\cd{Node},$ the
first data entry is the value paired by the key and the second is the
reduced value.
%


To compute reduced values as the structure of the tree changes, the
implementation relies on an auxiliary function $\cd{mkNode}$ (read
``make node'') that takes the key-value pair as well as the left and
right subtrees and computes the reduced value by applying reducer
function to the values of the left and right subtrees as well as the
value.
%
The only difference in the implementation of $\cd{split}$ and
$\cd{join}$ functions from the parametric implementations is the use
of $\cd{mkNode}$ instead of $\cd{Node}.$
\end{gram}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC8:UN12:GR5}
\no{5}
\unique{15210:S18:CH101:SEC8:UN12:GR5}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{example}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC8:UN12:GR5:AT1}
\no{1}
\unique{15210:S18:CH101:SEC8:UN12:GR5:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

The following diagram shows an example of splitting an augmented tree.
\begin{center}
  \includegraphics[width=5in]{/media/210/bsts/augtree-split.jpg}
\end{center}
The tree is split by the key $c$, and the reduced values on the
internal nodes need to be updated.  This only needs to happen along
the path that created the split, which in this case is $e$, $b$, and
$d$.  The node for $d$ does not have to be updated since it is a leaf.
The $\cd{makeNode}$ for $e$ and $b$ are what will update the reduced
values for those nodes.
\end{example}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC8:UN12:GR6}
\no{6}
\unique{15210:S18:CH101:SEC8:UN12:GR6}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{gram}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC8:UN12:GR6:AT1}
\no{1}
\unique{15210:S18:CH101:SEC8:UN12:GR6:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

We note that this idea can be used with any binary search tree, not
just Treaps.  We only need to replace the function for creating a node
so that as it creates the node, it also computes a reduced value for
the node by summing the reduced values of the children and the value
of the node itself.
\end{gram}
\end{group}

\begin{group}[...NOT.PROVIDED.TITLE...]
\label{group:15210:S18:CH101:SEC8:UN12:GR7}
\no{7}
\unique{15210:S18:CH101:SEC8:UN12:GR7}
\parent{...NOT.PROVIDED.PARENTS...}

\begin{remark}[...NOT.PROVIDED.TITLE...]
\label{atom:15210:S18:CH101:SEC8:UN12:GR7:AT1}
\no{1}
\unique{15210:S18:CH101:SEC8:UN12:GR7:AT1}
\parent{...NOT.PROVIDED.PARENTS...}

In an imperative implementation of binary search trees, when a child
  node is side affected, the reduced values for the nodes on the path
  from the modified node to the root must be recomputed.
\end{remark}
\end{group}
\end{unit}
\end{section}
\end{chapter}

\end{book}
