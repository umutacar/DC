%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{course}
\title{Parallel and Sequential Algorithms}
\label{15210}
\no{15210}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

\coursenumber{15210}
\picture{/210/course/air-pavilion.jpg}
\providesbook{S18}
\provideschapter{12}
\providessection{1}
\providesunit{1}
\providesassignment{1}
\semester{Spring 2018}
\website{http://www.cs.cmu.edu/~15210}
15-210 aims to teach methods for designing, analyzing, and programming
sequential and parallel algorithms and data structures. The emphasis
is on teaching fundamental concepts applicable across a wide variety
of problem domains, and transferable across a reasonably broad set of
programming languages and computer architectures. This course also
includes a significant programming component in which students will
program concrete examples from domains such as engineering, scientific
computing, graphics, data mining, and information retrieval (web
search).

Unlike a traditional introduction to algorithms and data structures,
this course puts an emphasis on parallel thinking â€” i.e., thinking
about how algorithms can do multiple things at once instead of one at
a time. The course follows up on material learned in 15-122 and 15-150
but goes into significantly more depth on algorithmic issues.
\begin{book}
\title{Algorithm Design: Parallel and Sequential}
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}
\authors{Umut A. Acar and Guy Blelloch}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{chapter}[Binary Search Trees]
\label{ch:bsts}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

\picture{bstpic}

%% UPDATE LOG (Umut)
%%%% Umut: Updates in Fall 2016
%% consistency in notation
%% simplified adt
%%
%%%% Umut: Updates from 2015 to 2016
%%   Feb 2016: Bunch of corrections.
%%             Augmentation part (- reduced values) updated.
%%
%%%% Umut: Updates from 2014 to 2015 (body-2014.tex)
%% (balancing) scheme --> data structure
%% (I worry that ``scheme'' can piss people off.)
%%
%% skip trees --> skip lists.
%%
%% Define full ADT.
%% Major edit on the rest of the section
%% saving body-2014.tex in directory for recovery of 
%% material.
%% The section at the end also contains
%% some (probably all of the left unused material).
%%
%% A summary of edits:
%% o defined ADT also mathematically, all functions included
%% o simplified join interface: considering only two trees.
%%   this was a significant change that affected a lot of code and 
%%   discussion.  i probably missed some.
%% o repositioned review of prior work
%% o deleted unbalanced version
%% o Using the word ``parametric'' implementation to refer to a functor
%% o Rewrote the parametric implementation section
%% o added ``setdifference'' function diff everywhere
%% o Changed treap intro to talk about random insertions.
%% o Rewrote treap section, gave full code based on parametric.
%% o Separate cost spec section, justified by parametric analysis.
%% o Rewrote augmentation section
%% o Worked out a direct proof of the union bound.
%% TODO: Flesh out the join bound and the log-span implementation.

Searching is one of the most important operations in computer science.
Of the many search data structures that have been designed and are
used in practice, search trees, more specifically balanced binary
search trees, occupy a coveted place because of their broad
applicability to many different sorts of problems.  For example, in
this book, we rely on binary search trees to implement set and table
(dictionary) abstract data types
%(\chref{sets-tables}),
which are then used in the
implementation of many algorithms, including for example graph
algorithms.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{section}[Motivation]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

...NO.INTRO...
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{unit}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{gram}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

If we are interested in searching a static or unchanging collection of
elements, then we can use a simpler data structure such as sequences.
%
For example, we can use a sequence with the array-based cost
specification to implement an efficient search function by
representing the collection as a sorted sequence and by using binary
search.  
%
Such an implementation would yield a logarithmic-work search
operation.
%
If, however, we want to support dynamic collections, where for
example, we insert new elements and delete existing elements,
sequences would require linear work.
% 
Binary search trees, or \defn{BSTs} for short, make it possible to
compute with dynamic collections by using insertions, deletions, as
well as searches all in logarithmic number of tree operations.
\end{gram}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{gram}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

In the traditional treatment of algorithms, which focuses on
sequential algorithms, binary search trees revolve around three
operations: insertion, deletion, and search.
%
While these operations are important, they are not sufficient for
parallelism, since they perform a single update at a time.
% 
We therefore consider aggregate update operations, such as union and
difference, which can be used to insert and delete (respectively) many
elements atonce.
\end{gram}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{gram}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

The rest of this chapter is organized as follows.  We first define
binary search trees
%(\secref{bst::prelim})
and present an ADT for them.
%(\secref{bst::adt}). 
%
We then present a parametric implementation of the ADT
%(\secref{bst::parametric})
by using only two operations, $\cd{split}$
and $\cd{join}$, which respectively split a tree at a given key and
join two trees.
%
%In \secref{bst::cost},
Next, we present a cost specification based on the
parametric implementation, which achieves strong bounds as long as the
$\cd{split}$ and $\cd{join}$ operations have logarithmic work and
span.
%
As a result, we are able to reduce the problem of implementing the BST
ADT to the problem of implementing just the functions $\cd{split}$
and $\cd{join}$.  
%
We finish the chapter by presenting a specific instance of the
parametric implementation using Treaps.
%(\secref{bst::treaps}).
%
Other possible implementation techniques are also described.
%(\secref{bst::bsts-review})
\end{gram}
\end{unit}
\end{section}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{section}[Preliminaries]
\label{sec:bst::prelim}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

...NO.INTRO...
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{unit}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{gram}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

We start with some basic definitions and terminology involving rooted
and binary search trees.  Recall first that a rooted tree is a tree
with a distinguished root node.
%(\defref{prelim::rootedtree}). 
%
A \defn{full binary tree} is a rooted tree, where each node is either
a \defn{leaf}, which has no children, or an \defn{internal node},
which have a left and a right child (\defref{bst::binarytree}).
%
\end{gram}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{definition}[Full Binary Tree]
\label{def:bst::binarytree}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

A \defn{full binary tree} is an ordered rooted tree in which every
internal node has exactly two children: the first or the \defn{left
  child} and the second or the \defn{right child}.  
%
The \defn{left subtree} of a node is the subtree rooted at the left
child, and the \defn{right subtree} the one rooted at the right child.
\end{definition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{gram}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

A binary search tree is a full binary tree, where each internal node
$u$ has a unique key~$k$ such that each node in its left subtree has a
key less than $k$ and each node in its right subtree has a key greater
that $x$.  
%
\end{gram}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{group}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{definition}[Binary Search Tree (BST)]
\label{def:bst::bst}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

A \defn{binary search tree} (BST) over a totally ordered set $S$ is a
full binary tree that satisfies the following conditions.
\begin{enumerate}
 \item There is a one-to-one mapping $k(v)$ from internal tree nodes to elements in $S$.
 \item for every $u$ in the left subtree of $v$, $k(u) < k(v)$
 \item for every $u$ in the right subtree of $v$, $k(u) > k(v)$
\end{enumerate}
%
In the definition, conditions 2 and 3 are referred to as the \defn{BST
  property}.  We often refer to the elements of $S$ in a BST as keys,
and use $\dom{T}$ to indicate the domain (keys) in a BST $T$.  The
\defn{size} of a BST is the number of keys in the tree, i.e. $|S|$.


% A BST can equivalently be defined recursively as:
% \[
% \cd{BST}(S) = \left\{\begin{array}{ll}
% \cd{Leaf} & S = \emptyset\\
% \cd{iNode}(\cd{BST}(S_L), k, \cd{BST}(S_R)) & 
% \underbrace{(S = S_L \cup \cset{k} \cup S_R)}_{\mbox{one to one (inclusion)}} \wedge \underbrace{(S_L < k < S_R)}_{\mbox{BST property}}
% \end{array}\right.
% \]
\end{definition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}[...NO.TITLE...]
\label{ex:bst}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

An example binary search tree over the set of natural numbers
$\{1,3,4,5,6,7,8,9\}$ is shown below.
\begin{center}
\begin{minipage}[t]{2.5in}
\vspace*{0in}
\includegraphics[width=2.5in]{/media/210/bsts/bst2.jpg}
\end{minipage}
%
\hspace{1in}
%
\begin{minipage}[t]{2.5in}
\vspace*{0in}
\includegraphics[width=1.8in]{/media/210/bsts/bst3.jpg}
\end{minipage}
\end{center}
On the left the $L$ and $R$ indicate the left (first) and right
(second) child, respectively.  All internal nodes (white) have a key
associated with them while the leaves (black) are empty.  The keys
satisfy the BST property---for every node, the keys in the left
subtree are less, and the ones in the right subtree are greater. 


\smallskip 

In the illustration on the left, the edges are oriented away from the
root. They could have also been oriented towards the root.
%
When illustrating binary search trees, we usually replace the directed
arcs with undirected edges, leaving the orientation to be implicit.
%
We also draw the left and right subtrees of a node on its left and
right respectively.
%
Following this convention, we can draw the tree on the left above as
shown an the right.
%
We use this convention in future figures.
\end{example}
\end{group}
\end{unit}
\end{section}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{section}[The BST Abstract Data Type]
\label{sec:bst::adt}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

...NO.INTRO...
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{unit}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{gram}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

The Abstract Data Type specification below describes an ADT for BSTs
parametrized by a totally ordered key set. We briefly describe this
ADT and present some examples. As we shall see, the BST ADT can be
implemented in many ways.  In order to present concrete examples, we
assume an implementation but do not specify it.
\end{gram}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{datatype}[Binary Search Tree (BST)]
\label{adt:bst::adt}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

For a universe of totally ordered keys $\kkk$, the BST ADT
consists of a type $\bstt$ representing a power set of keys and the
functions whose types are specified as follows. 

\[
\begin{array}{lcl}
\texttt{empty} & : &\bstt
\\
%
\cd{singleton} & : & \kkk \ra \bstt
\\
%
\cd{find}
& : & \bstt \ra \kkk \ra \bbb
\\
%
\cd{delete}
& : & \bstt \ra \kkk \ra  \bstt
\\
\cd{insert}
& : & \bstt \ra \kkk \ra  \bstt
\\
%
\cd{intersection}
& : & \bstt \ra \bstt \ra  \bstt
\\
%
\cd{difference}
& : & \bstt \ra \bstt \ra  \bstt
\\
%
\cd{union}
& : & \bstt \ra \bstt \ra  \bstt
\\
%
\cd{split}
& : & \bstt \ra \kkk \ra (\bstt \times \bbb \times \bstt)
\\
\cd{join}
& : & \bstt \ra \bstt \ra \bstt 
\end{array}
\]
\end{datatype}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{group}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{gram}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

The ADT supports the constructor operations
$\cd{empty}$ and $\cd{singleton}$ for creating an empty BST and BST
with a single key. 
%

The function $\cd{find}$ searches for a given key and returns a
boolean indicating success.

The functions $\cd{insert}$ and $\cd{delete}$ insert and delete a
given key into or from the BST.
\end{gram}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

Searching in BSTs illustrated.

\begin{itemize}
\item 
Searching for  $5$  in the input tree returns $\cd{true}.$

$\cd{find}$
$\leftparen{2cm}$
\includegraphics[width=3cm]{/media/210/bsts/bst4.jpg}
$\rightparen{2cm}$
~~$\cd{5}$
$~=~$
$\cd{true}$

\item 
Searching for  $6$  in the input tree returns $\cd{false}.$
%

$\cd{find}$
$\leftparen{2cm}$
\includegraphics[width=3cm]{/media/210/bsts/bst4.jpg}
$\rightparen{2cm}$
~~$\cd{6}$
$~=~$ 
$\cd{false}$

\end{itemize}
\end{example}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}[Insertion]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

Inserting the key $6$  into the input tree returns a new tree including $6$.
%

$\cd{insert}$
$\leftparen{2cm}$
\includegraphics[width=3cm]{/media/210/bsts/bst4.jpg}
$\rightparen{2cm}$
~~$\cd{6}$
=
\includegraphics[width=3cm]{/media/210/bsts/bst5.jpg}
\end{example}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}[Deletion]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

Deleting the key  $6$ from the input tree returns a tree without it.
%

$\cd{delete}$
$\leftparen{2cm}$
\includegraphics[width=3cm]{/media/210/bsts/bst5.jpg}
$\rightparen{2cm}$
~~$\cd{6}$
= 
\includegraphics[width=3cm]{/media/210/bsts/bst4.jpg}
\end{example}
\end{group}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{gram}[Union]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

The function $\cd{union}$ takes two BSTs and returns a BST that
contains all the keys in them; $\cd{union}$ is an aggregate insert
operation.
% 
The function $\cd{intersection}$ takes two BSTs and returns a BST
that contains the keys common in both.
% 
The function $\cd{difference}$ takes two BSTs $t_1$ and $t_2$ and returns
a BST that contains the keys in $t_1$ that are not in $t_2$;
$\cd{difference}$ is an aggregate delete operation.
\end{gram}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{group}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{gram}[Split]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

The function $\cd{split}$ takes a tree $t$ and a key $k$ and
splits $t$ into two trees: one consisting of all the keys of $t$ less
than $k$, and another consisting of all the keys of $t$ greater than
$k$.  
%
It also returns a Boolean value indicating whether $k$ appears in $t$.
%
The exact structure of the trees returned by $\cd{split}$ can differ
from one implementation to another: the specification only requires
that the resulting trees to be valid BSTs and that they contain the
keys less than $k$ and greater than $k$, leaving their structure
otherwise unspecified.
\end{gram}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

The function $\cd{split}$ illustrated.

\begin{itemize}
\item 
Splitting the input tree at $6$ yields two trees, consisting of the
keys less that $6$ and those greater that $6$, indicating also that
$6$ is not in the input tree. 

$\cd{split}$
$\leftparen{2cm}$
\includegraphics[width=3cm]{/media/210/bsts/bst4.jpg}
$\rightparen{2cm}$
~~$\cd{6}$
$~\xRightarrow{~~~~}$ 
$~\xrightarrow{~~~~}$ %% Which one of these works
$\leftparen{1.5cm}$
\includegraphics[width=0.69cm]{/media/210/bsts/bst4a.jpg}~~,
$\cd{False}$~,~~
\includegraphics[width=2cm]{/media/210/bsts/bst4b.jpg}
$\rightparen{1.5cm}$

\item 
Splitting the input tree at $5$ yields two trees, consisting of the
keys less than $5$ and those greater than $5$, indicating also that
$5$ is found in the input tree. 

$\cd{split}$
$\leftparen{2cm}$
\includegraphics[width=3cm]{/media/210/bsts/bst4.jpg}
$\rightparen{2cm}$
~~$\cd{5}$
%
$\xrightarrow{~~~~}$
$\xRightarrow{~~~~}$
%
$\leftparen{2cm}$
{\includegraphics[width=2cm]{/media/210/bsts/bst4c.jpg}}~~,
~$\cd{True}$~,~~
{\includegraphics[width=2cm]{/media/210/bsts/bst4b.jpg}}
$\rightparen{2cm}$
\end{itemize}
\end{example}
\end{group}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{group}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{gram}[Join]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

The function $\cd{join}$ takes two trees $t_1$ and $t_2$
such that all the keys in $t_1$ are less than the keys in $t_2$.  The
function returns a tree that contains all the keys in $t_1$ and $t_2$.
%
The exact structure of the tree returned by $\cd{join}$ can differ
from one implementation to another: the specification only requires
that the resulting tree is a valid BST and that it contains all the
keys in the trees joined.
\end{gram}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

The function $\cd{join}$ illustrated.

$\cd{join}$
$\leftparen{2cm}$
\includegraphics[width=1.5cm]{/media/210/bsts/bst4a.jpg}
$\rightparen{2cm}$
%
$\leftparen{2cm}$
{\includegraphics[width=3cm]{/media/210/bsts/bst6.jpg}}
$\rightparen{2cm}$
%
$\xrightarrow$
%
{\includegraphics[width=3cm]{/media/210/bsts/bst4.jpg}}
\end{example}
\end{group}
\end{unit}
\end{section}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{section}[Implementation via Balancing]
\label{sec:bst::bsts-review}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

...NO.INTRO...
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{unit}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{gram}[The Main Ideas]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

The key idea behind the implementation of BSTs is to organize the
keys such that
\begin{enumerate}
\item
a specific key can be located by following a branch in the tree,
performing key comparisons along the way, and

\item a set of keys that constitute a contiguous range in a sorted
  order of keys in the tree can be moved as a chunk by performing
  constant work.
\end{enumerate}
\end{gram}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{group}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{gram}[Searching a Tree]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

To see how we can search in a tree, consider searching for a key $k$
in a tree $t$ whose root is $r$.
%
We can start at the root $r$ and if $k$ equals the key at the root,
$k(r)$, then we have found our key, otherwise if $k < k(r)$, then we
know that $k$ cannot appear in the right subtree, so we only need to
search the left subtree, and if $k > k(r)$, then we only have to
search the right subtree.  Continuing the search, we will either find
the key or reach a leaf and conclude that the key is not in the tree.
%
In both cases we have followed a single path through the BST starting
at the root.
\end{gram}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

A successful search for $7$ and an unsuccessful search for $4$ in the
given tree.  Search paths are highlighted.

\begin{center}
\includegraphics[width=3cm]{/media/210/bsts/bst5-search-7.jpg}\
%
\hspace*{3cm}
%
\includegraphics[width=3cm]{/media/210/bsts/bst5-search-4.jpg}
\end{center}
\end{example}
\end{group}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{group}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{gram}[Operating on Ranges]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

To see how we can operate on a range of keys, note first that each
subtree in a binary tree contains all the keys that are within a
specific range. 
%
We can find such a range by performing a search as
described---in fact, a search as described identifies a possibly empty
range. Once we find a range of keys, we can operate on them as a group
by handling their root.
%
For example, we can move the whole subtree to another location by
linking the root to another parent.

By finding a range of keys by traversing a path in the BST, and by
moving ranges with constant work, it turns out to be possible to
implement all the operations in the BST ADT efficiently as long as the
paths traversed are not too long.
%
One way to guarantee absence of long paths is to make sure that the
tree remains balanced, i.e., the longest paths have approximately
the same length.
\end{gram}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

Consider the tree shown below on the left, we can handle all the keys
that are less than $8$ by holding the subtree rooted at $5$, the left
child of $8$.  For example, we can make $5$ the left child of $9$ and
delete $8$ from the tree. Note that if $8$ remains in the tree, the
resulting tree would not be a valid BST.

\begin{center}
\begin{minipage}[t]{3cm}
\includegraphics[width=3cm]{/media/210/bsts/bst5-search-5.jpg}
\end{minipage}
%
\hspace{3cm}
%
\begin{minipage}[t]{3cm}
\includegraphics[width=2.5cm]{/media/210/bsts/bst5-move-5.jpg}
\end{minipage}
\end{center}
\end{example}
\end{group}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{definition}[Perfectly Balanced BSTs]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

A binary tree is \defn{perfectly balanced} if it has the
minimum possible height.  
%
For a binary search tree with $n$ keys, a perfectly balanced tree has
height exactly $\lceil \lg (n + 1) \rceil$.
\end{definition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{gram}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

Ideally we would like to use only perfectly balanced trees.  
%
If we never make changes to the tree, we could balance it
once and for all.
%
If, however, we want to update the tree by, for example, inserting new
keys, then maintaining such perfect balance is costly.  
%
In fact, it turns out to be impossible to maintain a perfectly
balanced tree while allowing insertions in $O(\lg n)$ work.  
%
BST data structures therefore aim to keep approximate balance instead
of a perfect one.
\end{gram}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{definition}[Nearly Balanced BSTs]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

We refer to a BST data structure as \defn{nearly balanced} or simply
as \defn{balanced} if all trees with $n$ elements have height $O(\lg
n)$, perhaps in expectation or with high probability.
\end{definition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{gram}[Balanced BST Data Structures]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

There are many balanced BST data structures.  Most either try to
maintain height balance (the children of a node are about the same
height) or weight balance (the children of a node are about the same
size).  Here we list a few such data structures:

\begin{enumerate}
\item \defn{AVL trees} are the earliest nearly balanced BST data
  structure (1962).  It maintains the invariant that the two children
  of each node differ in height by at most one, which in turn implies
  approximate balance.

\item \defn{Red-Black trees} maintain the invariant that all leaves
  have a depth that is within a factor of 2 of each other.  The depth
  invariant is ensured by a scheme of coloring the nodes red and
  black.

\item \defn{Weight balanced (BB[$\alpha$]) trees} maintain the
  invariant that the left and right subtrees of a node of size $n$
  each have size at least $\alpha n$ for $0 < \alpha \leq
  1 - \frac{1}{\sqrt{2}}$.  The BB stands for bounded balance, and adjusting
  $\alpha$ gives a tradeoff between search and update costs.

\item \defn{Treaps} associate a random priority with every key and
  maintain the invariant that the keys are stored in heap order with
  respect to their priorities (the term ``Treap'' is short for ``tree
  heap'').  Treaps guarantee approximate balance with
  high-probability.

\item \defn{Splay trees} are an amortized data structure that does not
  guarantee approximate balance, but instead guarantees that for any sequence
  of $m$ insert, find and delete operations each does $O(\lg n)$
  amortized work.
\end{enumerate}
There are dozens of other balanced BST data structures (e.g. scapegoat
trees and skip lists), as well as many that allow larger degrees,
including 2--3 trees, brother trees, and B trees.  
%
\end{gram}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{remark}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

Many of the existing BST data structures were developed for sequential
computing.   
%
Some of these data structures such as Treaps, which we describe here,
generalize naturally to parallel computing. 
%
But some others, such as splay trees, could be more difficult because
their bounds rely on amortization techniques that can be challenging
to support in parallel computing.
\end{remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{teachnote}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

Bob Harper comment:
misleading discussion? pages 226-227 the discussion of splay trees,
etc ignores the important issue of single-threaded vs multi-threaded
data structures; amortization doesn't work for the multithreaded case.
do the treaps work for that case?

It might indeed be worthwhile to discuss the weakness of amortization
with respect to persistence.  Though this seems a bit advanced for
this material.  Further thought is needed...
\end{teachnote}
\end{unit}
\end{section}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{section}[A Parametric Implementation]
\label{sec:bst::parametric}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

We describe a minimalist implementation of the BST ADT based on two
functions, $\cd{split}$ and $\cd{join}$. 
%
Since the implementation depends on just these two functions, we refer
to it as a parametric implementation.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{unit}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{gram}[Implementing the BST ADT with $\cd{split}$ and $\cd{join}$]
\label{ds:bst::parametric}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

\begin{lstlisting}
type $\tttt$ = Leaf | Node of ($\tttt$ ** $\kkk$ ** $\tttt$)
split $t$ $k$ = ... (* as given *)
join $t_1$ $t_2$ = ... (* as given *)

joinM $t_1$ $k$ $t_2$ = join $t_1$ (join (singleton $k$) $t_2$)
empty = Leaf
singleton $(k)$ = Node(Leaf,$k$,Leaf)  
find $t$ $k$ = let (_,$v$,_) = split $t$ $k$  in $v$ end 
delete $t$ $k$ = let $(l,\_,r)$ = split $t$ $k$ in join $l$ $r$ end
insert $t$ $k$ = let $(l,\_,r)$ = split $t$ $k$ in joinM $l$ $k$ $r$ end 

intersect $t_1$ $t_2$ =
  case ($t_1$,$t_2$) 
  | (Leaf,_) => Leaf
  | (_,Leaf) => Leaf
  | (Node $(l_1,k_1,r_1)$,_) => 
    let $(l_2,b,r_2)$ = split $t_2$ $k_1$
       $(l,r)$ = (intersect $l_1$ $l_2$) || (intersect $r_1$ $r_2$)
    in if $b$ then joinM $l$ $k_1$ $r$ else join $l$ $r$ end

difference $t_1$ $t_2$ =
  case ($t_1$, $t_2$)
  | (Leaf,_) => Leaf
  | (_,Leaf) => $t_1$
  | (Node $(l_1,k_1,r_1)$,_) => 
    let $(l_2,b,r_2)$ = split $t_2$ $k_1$
        $(l,r)$ = (difference $l_1$ $l_2$) || (difference $r_1$ $r_2$)
    in if $b$ then join $l$ $r$ else joinM $L$ $k_1$ $r$ end

union $t_1$ $t_2$ =
  case ($t_1$,$t_2$)
  | (Leaf,_) => $t_2$
  | (_,Leaf) => $t_1$
  | (Node $(l_1,k_1,r_1)$,_) =>   
    let $(l_2,\_,r_2)$ = split $t_2$ $k_1$
        $(l,r)$ = (union $l_1$ $l_2$) || (union $r_1$ $r_2$)
    in joinM $l$ $k_1$ $r$ end    
\end{lstlisting}
\end{gram}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{gram}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

The implementation given above assumes that the implementation of
$\cd{split}$ and $\cd{join}$ are supplied.
%
%\dsref{...}
%

The implementation defines the tree type as consisting of
leaves or internal nodes with left and right subtrees and a key.
%

The auxiliary function, \cd{joinM} takes two trees $t_1$ and $t_2$
and a ``middle'' key $k$ that is sandwiched between the two
trees---that is $k$ is greater than all the keys in $t_1$ and less
than all the keys in $t_2$---and returns a tree that contains all the
keys in $t_1$ and $t_2$ as well as $k$.


The function \cd{find} is easily implementable with a
$\cd{split}$, which indicates whether the key used for splitting is
found in the tree or not.
%
To implement \cd{insert} of a key $k$ into a tree, we first
$\cd{split}$ the tree at $k$ and then join the two returned
trees along with key $k$ using \cd{joinM}.
%
To implement \cd{delete} of a key $k$ from a tree, we first
$\cd{split}$ the tree at $k$ and then join the two returned
trees with $\cd{join}$. 
%
If the key $k$ was found, this gives us a tree that does not contain
the $k$; otherwise we obtain a tree of the same set of keys (though
the structure of the tree may be different internally depending on the
implementation of $\cd{split}$ and $\cd{join}$).
\end{gram}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{group}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{gram}[Union]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

The implementation of $\cd{union}$ uses divide and conquer.  The idea is
to split both trees at some key $k$, recursively union the two parts
with keys less than $k$, and the two parts with keys greater than $k$
and then join them.  
%
There are different ways to select the key $k$ used to split the tree.
%
One way is to use the key at the root of one of the two trees, for
example the first tree, and split the second tree with it; this is the
approach take in the parametric implementation.
\end{gram}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

The union of tree $t_1$ and $t_2$ illustrated.

\begin{center}
  \includegraphics[scale=.7]{/media/210/bsts/union-dia1.jpg}
\end{center}
\label{example:tree-union}
\end{example}
\end{group}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{group}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{gram}[Intersection]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

The implementation of $\cd{intersection}$ uses a divide-and-conquer
approach similar to that  of $\cd{union}$.  
%
As in $\cd{union}$, we split both trees by using the key $k_1$ at the
root of the first tree, and compute intersections recursively.
%
We then compute the result by joining the results from the recursive
calls and including the key $k_1$ if it is found in both trees.
%
\begin{teachask}
How does the implementation accounts for all possible shared keys when
we divide the input trees into two and calculate intersections
recursively?  What if a key on the left matches a key on the right
half of the other tree?
\end{teachask}
%
Note that since the trees are BSTs, checking for the intersections of
left and right subtrees recursively and is guaranteed to find all
shared keys because the $\cd{split}$ operation places all keys less
than and greater than the given key to two separate trees.
\end{gram}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{exercise}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

Prove correct the functions $\cd{intersection}$, $\cd{difference}$, and
  $\cd{union}$.
\end{exercise}
\end{group}
\end{unit}
\end{section}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{section}[Cost Specification]
\label{sec:bst::cost}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

...NO.INTRO...
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{unit}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{gram}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

There are many ways to implement an efficient data structure that
matches our BST ADT, many of these implementation more or less match
the same cost specification, with the main difference being whether
the bounds are worst-case, expected case (probabilistic), or
amortized.  
%
These implementations all use balancing techniques to ensure that the
depth of the BST remains $O(\lg{n})$, where $n$ is the number of keys
in the tree.  
%
For the purposes specifying the costs, we don't distinguish between
worst-case, amortized, and probabilistic bounds, because we can always
rely on the existence of an implementation that matches the desired
cost specification.  
%
When using specific data structures that match the specified bounds in
an amortized or randomized sense, we will try to be careful when
specifying the bounds.
\end{gram}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{costspec}[BSTs]
\label{cost:bst::costs}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

The \defn{BST} cost specification is defined as follows.  The
  variables $n$ and $m$ are defined as $n = \max{(|t_1|,|t_2|)}$ and
  $m = \min{(|t_1|,|t_2|)}$ when applicable.
%  \setlength{\extrarowheight}{1ex}
\[
\begin{array}{l@{\qquad}c@{\qquad}c@{\qquad}}
\toprule
%& \multicolumn{2}{c|}{\bf Balanced~BST}\\ \hline 
& \emph{Work} & \emph{Span}
\\ \toprule

\cd{empty}
& \bigoh{1}
& \bigoh{1}
\\ \midrule

\cd{singleton}~k
& \bigoh{1}
& \bigoh{1}
\\ \midrule

\cd{split}~t~k
& \bigoh{\lg{|t|}}
& \bigoh{\lg{|t|}}
\\ \midrule

\cd{join}~t_1~t_2
& \bigoh{\lg{\left(|t_1|+|t_2|\right)}}
& \bigoh{\lg{\left(|t_1|+|t_2|\right)}}
\\ \midrule

\cd{find}~t~k
& \bigoh{\lg{|t|}}
& \bigoh{\lg{|t|}}
\\ \midrule

\cd{insert}~t~k
& \bigoh{\lg{|t|}}
& \bigoh{\lg{|t|}}
\\ \midrule

\cd{delete}~t~k
& \bigoh{\lg{|t|}}
& \bigoh{\lg{|t|}}
\\ \midrule

\cd{intersect}~t_1~t_2
& \bigoh{m \cdot \lg{\frac{n+m}{m}}}
& \bigoh{\lg{n}}
\\ \midrule

\cd{difference}~t_1~t_2
& \bigoh{m \cdot \lg{\frac{n+m}{m}}}
& \bigoh{\lg{n}}
\\ \midrule

\cd{union}~t_1~t_2
& \bigoh{m \cdot \lg{\frac{n+m}{m}}}
& \bigoh{\lg{n}}
\\
\bottomrule
\end{array}
\]
\end{costspec}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{gram}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

The cost specification above shows the costs for the BST ADT as can be
realized by several balanced BST data structures such as Treaps (in
expectation), red-black trees (in the worst case), and splay trees
(amortized). As may be expected the cost of \cd{empty} and
\cd{singleton} are constant.
%

For the rest of the operations, we justify the cost bounds by assuming
the existence of logarithmic time $\cd{split}$ and $\cd{join}$ operations,
and by using our parametric implementation described above.
%
The work and span costs of \cd{find}, \cd{insert}, and
\cd{delete} are determined by the $\cd{split}$ and $\cd{join}$
operation and are thus logarithmic in the size of the tree.
%

The cost bounds on $\cd{union}$, $\cd{intersection}$, and
\cd{difference}, which are similar are more difficult to see.
\end{gram}
\end{unit}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{unit}[Cost of Union, Intersection, and Difference]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{gram}[...NO.TITLE...]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

Let's analyze the cost for $\cd{union}$ as implemented by the parametric
implementation. It is easy to apply a similar analysis to
\cd{intersection} and $\cd{difference}$.
\end{gram}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{gram}[Figuring out the Recurrence]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

Consider now a call to $\cd{union}$ with parameters
$t_1$ and $t_2$.  To simplify the analysis, we will make the following
assumptions:
\begin{enumerate}
\item $t_1$ is perfectly balanced (i.e., the left and right subtrees of
  the root have size at most $|t_1|/2$), 
\item each time a key from $t_1$ splits $t_2$, it splits the tree in
  exactly in half, and
\item $|t_1| < |t_2|$.
\end{enumerate}
%
Later we will relax these assumptions.  

Let us define $m = |t_1|$ and
$n = |t_2|$ (recall the size of a tree is the number of keys in it).
With these assumptions and examining the algorithm we can then write
the following recurrence for the work of $\cd{union}$:
\begin{align*}
  W_{\mbox{union}}(m, n) &\leq 2W_{\mbox{union}}(m/2,n/2) + W_{\mbox{split}}(n)
+ W_{\mbox{join}}(n+m) + O(1)\\
   & \leq  2W_{\mbox{union}}(m/2, n/2) + O(\lg n)~.
\end{align*}  

The size for join is the sum of the two sizes, $m+n$, but since 
$m \leq n$, $O(\lg (n + m))$ is equivalent to $O(\lg n)$.
We also have the base case
\begin{align*}
  W_{\mbox{union}}(1, n) & \leq 2W_{\mbox{union}}(0,n/2) + W_{\mbox{split}}(n)
+ W_{\mbox{join}}(n) + O(1)\\
         & \leq O(\lg n)~.
\end{align*}
The final inequality holds because $2W_{\mbox{union}}(0,n) = O(1)$.
\end{gram}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{gram}[Solving the Recurrence]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

We can draw the recursion tree showing the work performed by the
splitting of $t_2$ and by the joining of the results as follows.  For
simplicity of the argument, let's assume that the leaves of the tree
correspond to the case for $m = 1$.

%This figure should be changed so "n" is replaced by "N" except the
%bottom level should be "each costs log (1 + n/m)"

\begin{center}
  \includegraphics[width=4.5]{/media/210/bsts/recurtree2.jpg}
\end{center}
\end{gram}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{gram}[Brick Method]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

Let's analyze the structure of the recursion tree shown above.
%
\begin{teachask}
How many leaves are there?
\end{teachask}
%
We can find the number of leaves in the tree by examining the work
recurrence.  Notice that in the recurrence, the tree bottoms out when
$m = 1$ and before that, $m$ always gets split in half (remember that
$t_1$ is perfectly balanced).  The tree $t_2$ does not affects the
shape of the recursion tree or the stopping condition. Thus, there are
exactly $m$ leaves in the tree.  
%
In fact, the recursion can be rewritten as a recursion of the form
$W(m) = 2W(m/2) + \ldots $, which means that there are $m$ leaves.
%
\begin{teachask}
How deep are the leaves?
\end{teachask}
%
By the same reasoning, we can see that the leaves are $(1 + \lg m)$
deep.

Let's now determine the size of $t_2$ at the leaves.  
%
We have $m$ keys in $t_1$ to start with, and they split $t_2$ evenly
all the way down to the level of the leaves (by assumption). Thus, the
leaves have all the same size of $\frac{n}{m}$.
%
Therefore, each leaf adds a $O(\lg (1+\frac{n}{m}))$ term to the work
(the $1+$ is needed to deal with the case that $n = m$).  Since there
are $m$ leaves, the whole bottom level costs $O(m \lg (1+
\frac{n}{m}))$.  

We will now prove that the cost at the bottom level is indeed
asymptotically the same as the total work.  In other words, the tree
is leaves-dominated.  It is possible to prove that the tree is
leaves-dominated by computing the ratio of the work at adjacent
levels, i.e., the ratio $\frac{2^{i-1}  \lg{n/2^{i-1}}}{2^i 
  \lg{n/2^{i}}} = \frac{1}{2} \frac{\lg{n} - i + 1 }{\lg{n} - i}$,
where $i \le \lg{m} < \lg{n}$. This ratio is less than $1$ for all
levels except for the last level, where by taking $i = \lg{n} - 1$ we
have
\[
 \frac{1}{2} \frac{\lg{n} - i +1 }{\lg{n} - i}
 \le
 \frac{1}{2} \frac{1}{\lg{n} - \lg{n} +1 + 1}{\lg{n} - \lg{n}+1}
= \frac{1}{1}.
\]
Thus the total work is asymptotically dominated by the total work of
the leaves, which is $\bigoh{m\lg{n/m}}$.
\end{gram}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{gram}[Direct Derivation]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

We can establish the same fact more precisely.  Let's start by writing
the total cost by summing over all levels, omitting for simplicity the
constant factors, and assuming that $n = 2^a$ and $m = 2^b$,
\[
W(n,m) = \sum_{i = 0}^{b}{2^i \lg{\frac{n}{2^i}}}.
\]
We can rewrite this sum as 
\[
\sum_{i = 0}^{b}{2^i \lg{\frac{n}{2^i}}} = 
\lg{n}\sum_{i = 0}^{b}{2^i} - \sum_{i = 0}^{b}{i\,2^i}.
= a\sum_{i = 0}^{b}{2^i} - \sum_{i = 0}^{b}{i\,2^i}.
\]
Let's now focus on the second term. Note that 
\[
\sum_{i = 0}^{b}{i\,2^i} 
=
\sum_{i = 0}^{b}{\sum_{j=i}^{b}2^j} 
=
\sum_{i = 0}^{b}{\left(\sum_{j=0}^{b}{2^j} - \sum_{k=0}^{i-1}2^k\right). }
\]
Substituting the closed form for each inner summation and simplifying
leads to
\[
\begin{array}{ll}
= & \sum_{i = 0}^{b}{\left( (2^{b+1}-1) - (2^{i}-1) \right)}.
\\[2mm]
= &  (b+1) (2^{b+1}-1) - \sum_{i = 0}^{b}{(2^{i}-1)}
\\[2mm]
= &  (b+1) (2^{b+1}-1) - \left( 2^{b+1}-1 - (b+1) \right)
\\[2mm]
= &  (b+1) (2^{b+1}-1) - \left( 2^{b+1}-1 - (b+1) \right)
\\[2mm]
= &  b\,2^{b+1} + 1.
\end{array}
\]

Let's now go back and plug this into the original work bound and simplify
\[
\begin{array}{lll}
W(n,m) & = &\sum_{i = 0}^{b}{2^i \lg{\frac{n}{2^i}}}
\\[2mm]
& = & a\sum_{i = 0}^{b}{2^i} - \sum_{i = 0}^{b}{i\,2^i}
\\[2mm]
& = & a\,(2^{b+1}-1) -  (b\,2^{b+1} + 1)
\\[2mm]
& = & a\,2^{b+1}-a - b\,2^{b+1} -1
%\\
= 2m (a-b) - a -1
\\[2mm]
& = & 2m (\lg{n}-\lg{m}) - a -1
%\\
= 2m \lg{\frac{n}{m}} - a -1
\\[2mm]
& = & \bigoh{m \, \lg{\frac{n}{m}}}.
\end{array}
\]

While the direct method may seem complicated, it is more robust than
the brick method, because it can be applied to analyze essentially any
algorithm, whereas the Brick method requires establishing a geometric
relationship between the cost terms at the levels of the tree.
%
\end{gram}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{gram}[Removing the Assumptions]
\label{...NO.LABEL...}
\no{0}
\unique{...NO.UNIQUE...}
\parent{...NO.PARENTS...}

Of course, in reality, our keys in $t_1$ won't split subtrees of $t_2$
in half every time.  But it turns out that any unevenness in the
splitting only helps reduce the work---i.e., the perfect split is the
worst case.  We won't go through a rigorous argument, but if we keep
the assumption that $t_1$ is perfectly balanced, then the shape of the
recursion tree stays the same.  What is now different is the cost at
each level.  Let us try to analyze the cost at level~$i$.  At this
level, there are $k = 2^i$ nodes in the recursion tree. Say the sizes
of $t_2$ at these nodes are $n_1, \dots, n_k$, where $\sum_j n_j =
n$. Then, the total cost for this level is
\[
c \cdot \sum_{j=1}^k \lg (n_j) \;\;\leq\;\; c \cdot \sum_{j=1}^k \lg (n/k) =
c\cdot 2^i \cdot \lg (n/2^i),
\]
where we used the fact that the logarithm function is
concave\footnote{This is also known as Jensen's inequality.}.  Thus,
the tree remains leaf-dominated and the same reasoning shows that the
total work is $O(m \lg (1 + \frac{n}{m}))$.

Still, in reality, $t_1$ doesn't have to be perfectly balanced as we
assumed. To generalize the analysis, we just need a tree with
$O(\lg{m})$ height.  Thus, we only need $t_1$ to be approximately
balanced. 

Finally, we assumed that $t_1$ is larger that $t_2$.  If it is
smaller, then we can reverse the order of arguments, so in this case,
there is no loss of generality.  If they are the same size, we need to
be a bit more precise in our handling of the base case in our
summation but this is all.

We end by remarking that as described, the span of \cunion{} is
$O(\lg^2 n)$, but this can be improved to $O(\lg n)$ by changing the
algorithm slightly.

In summary, \cunion{} can be implemented in $O(m \lg (1 +
\tfrac{n}m))$ work and span $O(\lg n)$.  

Essentially the same analysis applies to the functions
\cd{intersection} and $\cd{difference}$, whose structures are the same
as $\cd{union}$, except for an additional constant work and span for
the conditional (\cd{if}) expression.
\end{gram}
\end{unit}
\end{section}
\end{chapter}

\end{book}
