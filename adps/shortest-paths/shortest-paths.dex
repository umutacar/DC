%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% TODO
%%
%% Make clear the simple path (allowing no reps) and path distinction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{course}
\title{Parallel and Sequential Algorithms}

% Course number must be unique in the database
\coursenumber{15210}

\semester{Spring 2018}
\picture{/210/course/air-pavilion.jpg}
\website{http://www.cs.cmu.edu/~15210}

% Provides book
% This must be provided
% The name should be relative to course number.
\providesbook{S18}

% Start counting chapters from 
% This is optional. Will start counting at 1.
\provideschapter{16}
\providessection{1}

15-210 aims to teach methods for designing, analyzing, and programming
sequential and parallel algorithms and data structures. The emphasis
is on teaching fundamental concepts applicable across a wide variety
of problem domains, and transferable across a reasonably broad set of
programming languages and computer architectures. This course also
includes a significant programming component in which students will
program concrete examples from domains such as engineering, scientific
computing, graphics, data mining, and information retrieval (web
search).

Unlike a traditional introduction to algorithms and data structures,
this course puts an emphasis on parallel thinking â€” i.e., thinking
about how algorithms can do multiple things at once instead of one at
a time. The course follows up on material learned in 15-122 and 15-150
but goes into significantly more depth on algorithmic issues. 


\begin{book}
\title{Algorithm Design: Parallel and Sequential}
\authors{Umut A. Acar and Guy Blelloch}

\begin{chapter}[Shortest Paths]
\label{ch:shortest-paths}
\picture{./media/lord-of-the-rings-paths.jpg}


Given a graph where edges are labeled with weights (or distances) and
a source vertex, what is the shortest path between the source and some
other vertex?  Problems requiring us to answer such queries are
broadly known as~\defn{shortest-paths problems}. 
%
In this chapter, we define several flavors of shortest-path problems
and present two algorithms for the single-source shortest path
problem: Dijkstra's and Bellman-Ford's algorithms.  
%
Dijkstra's algorithm is more efficient but it is mostly sequential and
it works only for graphs where edge weights are non-negative.
%
Bellman-Ford's algorithm is a good parallel
algorithm and works for all graphs but performs significantly more work.


\begin{section}[Weighted Graphs and Shortest Paths]

\begin{unit}[Preliminaries]

\begin{gram}
Consider a weighted graph $G = (V, E, w)$, where $V$ is the set of
vertices, $E$ is the set of edges, and $w\!  : E \to \R$ is a function
mapping each edge to a real number, or a weight. 
%
The graph can either be directed or undirected.  
%
For convenience we define $w(u,v) = \infty$ if $(u,v) \not\in E$.  
\end{gram}

\begin{group}
\begin{definition}[Path Weight]
Given a weighted graph, we define the~\defn{weight of the path} in the
graph as the sum of the weights of the edges along that path.
\end{definition}

\begin{example}
\label{ex:shortestpath::pathlength}
In the following graph the weight of the path $\cseq{s,a,b,e}$ is $6$.
The weight of the path $\cseq{s,a,b,s}$ is $10$.
\begin{center}
\includegraphics[width=3.5in]{./media/weighted-graph.jpg}
\end{center}

\end{example}
\end{group}

\begin{group}
\begin{definition}[Shortest Paths and Distance]
For a weighted graph $G(V,E,w)$ a~\defn{shortest weighted path}, or~\defn{shortest path} from vertex $u$ to vertex $v$ is a path from $u$
to $v$ with minimal weight.
%
In other words, a shortest path is the path with the smallest weight
among all paths from $u$ to $v$.
%
Note that there could be multiple paths with equal weight; if so they
are all shortest paths from $u$ to $v$.
%

We define the~\defn{distance} from $u$ to $v$, written
$\delta_G(u,v)$, as the weight of a shortest path from $u$ to $v$.
%
If there is no path from $u$ to $v$, then the distance is infinity,
i.e., $\delta_G(u,v) = \infty$.
\end{definition}

\begin{note}
Historically, the term ``shortest path'' is used as a brief form for
``shortest weighted path'' even though the term ``shortest path'' is
inconsistent with the use of the term ``weight.''
%
Some authors use the term ``length'' instead of weight; this approach
has the disadvantage that length are usually thought to be
non-negative.
\end{note}

\begin{example}
\label{ex:shortestpath::pathlength}
In the following graph, the shortest path from $s$ to $e$ is $\langle
s,a,b,e \rangle$ with weight $6$.
\begin{center}
\includegraphics[width=3.5in]{./media/weighted-graph.jpg}
\end{center}
\end{example}
\end{group}

\begin{group}
\begin{gram}[Negative Edge Weights]
Negative edge weights have significant influence on shortest paths,
because they could cause the weight of paths to become non-monotonic:
we can decrease the weight of a path by adding a negative-weight edge
to the path.
%
Furthermore, negative-weight edges allow for cycles with negative
total weight.
%
Such cycles in turn can lead to shortest paths with total
weight~$-\infty$.

For these reasons, we will need to be careful about negative edge
weights when computing shortest paths.
%
As we will discover, even if there are no negative weight cycles,
negative edge weights make finding shortest paths more difficult.
%
% For this reason, we will first consider the problem of finding
% shortest paths when there are no negative edge weights.  
%
\end{gram}

\begin{example}[Negative Weights and Cycles]
In the graph below, the cycle $\cseq{s, a, b, s}$ has a weight of $-4$
%
and 
%
the cycle $\cseq{s, a, b, s, a, b, s}$ has a weight of $-8$.
%
This means that the shortest path from $s$ to $e$ has distance $-\infty.$ 
\begin{center}
\includegraphics[width=3.5in]{media/weighted-graph-negative.jpg}
\end{center}
\end{example}
\end{group}



\end{unit}


\begin{unit}[Shortest Path Problems]

\begin{gram}
Shortest path problems comes in several flavors, such as single-pair,
single-source, and all-pairs problems.
\end{gram}

\begin{problem}[Single-Pair Shortest Paths]
Given a graph, the~\defn{single-pair} shortest path problem requires
finding the shortest path between given a source and a given
destination vertex in the graph.
\end{problem}



\begin{problem}[Single-Source Shortest Paths (\sssp{})]
\label{prob:sp::sssp}
  Given a weighted graph $G = (V,E,w)$ and a source vertex $s$,
  the~\defn{single-source shortest path (\sssp{}) problem} is to find
  a shortest weighted path from $s$ to every other vertex in $V$.
\end{problem}



\begin{problem}[All-Pairs Shortest Paths]
\label{prob:sp::apsp}
Given a graph, the~\defn{all-pairs shortest path problem} requires
finding the shortest paths between all pairs of vertices in the graph.
\end{problem}

\begin{note}
Shortest-path problems typically require finding only one of the
possibly many shortest paths between two vertices considered.
%
In some cases, we only care about the distance $\delta(u,v)$ but not
the path itself.
\end{note}

%%%% Umut: not sure if this is necessary but it might be good to -
%%%% discuss some examples.
%%
%% Beyond the perhaps obvious applications to finding directions (for
%% walking, biking, public-transit, or driving) between places of
%% interest, shortest paths have many othre applications.
\end{unit}
\end{section}

\begin{section}[Dijkstra's Algorithm]

\begin{unit}

\begin{gram}
In the previous chapter, we saw how Breadth-First Search (BFS) can be
used to solve the single-source shortest path problem on graphs
without edge weights, or, equivalently, where all edges have weight
$1.$
%
BFS does not work on general weighted graphs.
\end{gram}

\begin{example}
Consider the following directed graph with $3$ vertices.
%
\begin{center}
  \includegraphics[width=2.4in]{./media/breaking-bfs-1.jpg}
\end{center}
%

In this graph, a BFS first visits $b$ and then $a$.  When it visits~$b$,
it assigns $b$ an incorrect weight of~$3$.
%
Since BFS never visit~$b$ again, it will not find the shortest path
going trough~$a$, which happens to be shorter.
\end{example}

\begin{teachnote}
Another graph where BFS fails to find the shortest paths correctly
%
\begin{center}
  \includegraphics[width=3.5in]{./media/breaking-bfs-2.jpg}
\end{center}
%
\end{teachnote}


\begin{teachask}
Can you see why BFS works on graphs without edge weights?
\end{teachask}

\begin{gram}
The reason why BFS works on unweighted graphs is quite interesting and
helpful for understanding other shortest path algorithms.  The key
idea behind BFS is to visit vertices in order of their distance from
the source, visiting closest vertices first, and the next closest, and
so on.  More specifically, for each frontier $F_i$ (at round $i$), BFS
has the correct distance from the source to each vertex in the
frontier.  It can then determine the correct distance for
unencountered neighbors that are distance one further away (on the
next frontier). Dijkstra's algorithm uses a similar idea for weighted
paths.
\end{gram}


\begin{teachnote}
One way to think of BFS, especially when reasoning about shortest
paths, is to think of a network of pipes where each vertex is a bucket
and each edge is a pipe that connects the buckets. Imagine now
flooding this network from a ``source'' bucket.  
%
The way water floods the system simulates how Dikstra's discovers the
paths and vertices.  The flood waters reach a vertex through the
shortest path first, because they discover all paths simultaneously
arriving at each place at the earliest possible time.
\end{teachnote}

\begin{problem}[\ssspp{}]
Consider a variant of the \sssp{} problem, where all the weights on
the edges are non-negative (i.e. $w : E \to \R^+)$.  We refer to this
as the~\defn{\ssspp{} problem}. 
%
\end{problem}

\begin{gram}
Dijkstra's algorithm solves the \ssspp{} problem.  It is an important
algorithm, because it is efficient, and because it is an elegant
example of a greedy algorithm that can find optimal results.
%
In this section, we are going to (re-)discover this algorithm by
taking advantage of properties of graphs and shortest paths.  
\end{gram}

\begin{gram}[Brute Force]
Let's start by noting that since no edges have negative weights, there
cannot be a negative-weight cycle.  
%
One can therefore never make a path shorter by visiting a vertex
twice---i.e., a path that cycles back to a vertex cannot have less
weight than the path that ends at the first visit to the vertex. 
%
When searching for a shortest path, we thus have to consider only the
simple paths.

%\begin{teachask}
%Give a brute-force algorithm for finding the shortest path?
%\end{teachask}

Let us start with a brute-force algorithm for the \ssspp{} problem,
that, for each vertex, considers all simple paths between the source
and the vertex and selects the shortest such path.
%
%\begin{teachask}
%How many simple paths can there be between two vertices in a graph?
%\end{teachask}
%
Unfortunately there can be a large number (exponential in the number
of vertices) of paths between any pair of vertices, so any algorithm that
tries to look at all paths is not likely to scale beyond very small
instances.
%
\end{gram}

\begin{teachask}
Why does the brute-force algorithm does redundant work? 
\end{teachask}

\begin{group}
\begin{gram}[Sub-Paths Property]
Let's try to reduce the work.
%
Observe that the brute-force algorithm
does redundant work, because it does not take advantage of a crucial
property of shortest paths: any sub-path of a shortest path is a
shortest path (between its end vertices).
%
We refer to this property as the~\defn{sub-paths property} of shortest
paths.
%

This property means that we can build shortest paths from smaller
shortest paths.
%
We are going to use this property to derive both Dijkstra's algorithm,
and also Bellman-Ford's algorithm, which solves the \sssp{} problem on
general graphs, allowing for negative edge weights.
\end{gram}

\begin{example}[Subpaths property]
If a shortest path from Pittsburgh to San Francisco goes through
Chicago, then that shortest path includes the shortest path from
Pittsburgh to Chicago.   
\end{example}
\end{group}


\begin{gram}[Applying Sub-Paths Property]
\label{gr:shortestpath::allbutone}
To see how sub-paths property can find help shortest pants, consider
the graph $G$ shown below.
%
Suppose that an oracle has told us the shortest paths to all vertices
except for the vertex~$v$.  We want to find the shortest path to~$v$.
%
In the graph, each vertex is labeled with its distance from~$s$. 


By inspecting the graph, we know that the shortest path to~$v$ goes
through either one of $a$, $b$, or $c$. 
%
Furthermore, by sub-paths property, we know that the shortest path
to~$v$ consists of the shortest path to one of $a$, $b$, or~$c$, and
the edge to~$v$. 
%
Thus, all we have to do is to find the vertex $u$~among the
in-neighbors of~$v$ that minimizes the distance to~$v$, i.e.,
$\dist_G(s,u)$ plus the additional edge weight to get to~$v$.
%
The weight of the shortest path to~$v$ is therefore
%
\[
\min{
\left(
\dist_G(s,a) + 3,
%
\dist_G(s,b) + 6,
%
\dist_G(s,c)+5
\right).
}
\]
%

The shortest path therefore goes through the vertex~$b$, which
minimizes the weight.

\begin{center}
\includegraphics[width=3.5in]{./media/shortest-paths-last.jpg}
\end{center}

\end{gram}

\begin{note}
Could the shortest-path be going through~$v$?  If so, then are we
still guaranteed to find a shortest path?
%
Recall that because all edge weights are non-negative, there is a
shortest path from $s$ to $v$ that is simple, i.e., contains no
repeated vertices.
\end{note}

%% \begin{notesonly}
%% Let's now consider the case where the oracle gave us the shortest path
%% to all but two of the vertices~$u,v$. 

%% %\begin{teachask}
%% % Can we find the shortest path to any one of $u$ or $v$?
%% %\end{teachask}

%% Let~$X$ denote the set of vertices for which we know the shortest
%% paths.
%% %
%% Let's calculate for $u$ (and $v$) the shortest path that goes through
%% a neighbor in~$X$ and then goes to $u$ ($v$). Consider now the
%% shortest of these two distances and assume without loss of generality
%% that it is the path to~$u$.  Assume that the path is strictly smaller.

%% %\begin{teachask}
%% %Could the shortest path to~$u$ be this path? 
%% %\end{teachask}

%% Since source is in $X$, we know that the shortest paths to~$u$ cross
%% out from~$X$ via some edge that goes to either~$u$ or~$v$. Since we
%% know that the weight of such a path to~$u$ is the smaller, and since
%% all edge weights are non-negative, there cannot be a shorter path that
%% goes to~$v$ and than comes back to~$u$ again.  
%% %
%% If the path is not strictly smaller, again the path that goes out to
%% $v$ and comes back to $u$ is going to be no shorter.
%% %
%% We thus conclude that the shortest path to~$u$ is the path that we
%% have found by considering all incoming edges
%% from~$X$. \exref{shortestpath::allbuttwo} illustrates this.


%% \begin{example}
%% \label{ex:shortestpath::allbuttwo}
%% In the following graph, suppose that we have found the shortest paths
%% from the source $s$ to all the vertices in $X$.  The shortest paths
%% are indicated by labels next to the vertices.  
%% %
%% The shortest path from the source $s$ to vertex $u$ in $Y$ is the
%% path to vertex $d$ with weight $5$ followed by the edge $(d,u)$ with
%% weight $4$ and hence total weight $9$.  
%% %
%% If edge weights are non-negative there cannot be any shorter way to
%% get to $u$, whatever $w(v,u)$ is, therefore we know that $\delta(s,u)
%% = 9$.
%% \begin{center}
%% \includegraphics[width=3.5in]{./media/shortest-paths-all-but-two.jpg}
%% \end{center}

%% \end{example}

%% \begin{teachask}
%% Can you see how we can generalize this idea?  
%% \end{teachask}

%% \end{notesonly}

\begin{group}
\begin{gram}[Applying Sub-Paths Property (Continued)]
Let's generalize the argument and consider the case where the oracle
tells us the shortest paths from $s$ to to some subset of the vertices
$X \subset V$ with $s \in X$.
%
Also let's define $Y$ to be the vertices not in $X$, i.e.,
\[
Y = V\setminus X$.
\]  
%

Consider now this question: can we efficiently determine the shortest
path to any one of the vertices in~$Y$.

%% \begin{teachask}
%% What would be the point of doing that? 
%% \end{teachask}


If we could do this, then we would have an algorithm to add new
vertices to $X$ repeatedly, until we are done.  


%
To see if thiscan be done, let's define the \defn{frontier} as the set
of vertices that are neighbors of $X$ but not in $X$, i.e. $N^+(X)
\setminus X$.
%
Observe that any path that leaves $X$ must go through a frontier
vertex on the way out.  Therefore for every $v \in Y$ the shortest
path from $s$ must start in $X$, since $s \in X$, and then leave $X$
via a vertex in the frontier.
%
%% \begin{teachask}
%% Can you use this property to identify a vertex $v \in Y$ that is no
%% farther from the source than any other vertex in $Y$?
%% \end{teachask}

Consider now the vertex $v \in Y$ that has an edge to some already
visited vertex $x \in X$ and that minimizes the sum $\dist_G(s,x) +
w(x,v)$.  
%
We can think of the vertex $v$ as the vertex closest to the set $X$.
%
Observe now that no other vertex in $Y$ can be closer to the source
than $x$ because 
\begin{itemize}
\item all paths to $Y$ must go through the frontier when exiting $X$, and
\item edge weights are non-negative, a sub-path cannot be longer than
  the full path.
\end{itemize}
%
%See \exref{shortestpath::some}. 
%

We now know that all other vertices in $Y$ are farther than $v$.
%
We also know that all edges are non-negative.
%
This means that the shortest path for $v$ is $\dist_G(s,x) + w(x,v)$.
%

We thus have answered the question that we set out to answer: we can
indeed determine the shortest path one one vertex---the vertex closest
to $X$.
%
We could thus iterate this reasoning to find the shortest path to all
vertices.
\end{gram}

\begin{example}
\label{ex:shortestpath::some}

  In the following graph suppose that we have found the shortest paths
  from the source $s$ to all the vertices in $X$ (marked by numbers
  next to the vertices).  The shortest path from the source $s$ to a
  vertex $u$ in $Y$ is the path to vertex $d$ with weight $5$ followed by
  the edge $(d,u)$ with weight $4$ and hence total weight $9$.  If
  edge weights are non-negative there cannot be any shorter way to get
  to $u$, whatever $w(v,u)$ is, therefore we know that $\delta(s,u) =
  9$.
\begin{center}
\includegraphics[width=3.5in]{./media/shortest-paths-some.jpg}
\end{center}

\end{example}
\end{group}

\begin{gram}
The intuition that we have developed thus far is stated more precisely
and proved in following Lemma.
%  \lemref{sp::djk}.  
%
The Lemma tells us that once we know the shortest paths to a set $X$
we can add more vertices to $X$.  
%
This gives us a crank that can churn
out at least one new vertex on each round. Dijkstra's algorithm simply
does exactly this: it turns the crank until all vertices are visited.
\end{gram}

\begin{group}
\begin{lemma}[Dijkstra's Property] 
\label{lem:sp::djk}
%
Consider a (directed) weighted graph $G = (V,E, w)$, $w\!: E \to \R^*$, and a source vertex $s \in V$.  
Consider any partitioning of the vertices $V$ into $\visited$ and
$Y = V \setminus \visited$ with $s \in \visited$,  and let
\[p(v) \equiv \min_{x \in X} (\dist_G(s,x) + w(x,v))\]
then $\displaystyle \min_{y \in Y} p(y) = \min_{y \in Y} \dist_G(s, y)$.
\vspace{-.2in}
\begin{center}
  \includegraphics[width=4.0in]{./media/dijkstra-prop.jpg}
\end{center}
\end{lemma}

\begin{note}[The Lemma Explained in plain English]
The overall shortest-path weight from $s$ via a vertex in $X$ directly
to a neighbor in $Y$ (in the frontier) is as short as any path from
$s$ to any vertex in $Y$.
\end{note}

\begin{proof}
  Consider a vertex $v_m \in Y$ such that $\dist_G(s,v_m) = \min_{v
    \in Y} \dist_G(s, v)$, and a shortest path from $s$ to $v_m$ in
  $G$.  The path must go through an edge from a vertex $v_x \in X$ to
  a vertex $v_t$ in $Y$ (see the figure).  Since there are no negative
  weight edges, and the path to $v_t$ is a sub-path of the path to
  $v_m$, $\dist_G(s,v_t)$ cannot be any greater than $\dist_G(s,v_m)$
  so it must be equal.  We therefore have
%
\[
 \min_{y \in Y} p(y) \leq \dist_G(s,v_t) = \dist_G(s,v_m) =\min_{y \in
   Y} \dist_G(s, y),
\]
%
but the leftmost term cannot be less than the rightmost, so they must
be equal.
\begin{comment}
  The path must cross from $X$ to $Y$ at some point using some edge
  $(v_X,v_T)$. Since sub-paths of shortest paths are shortest paths,
  the sub-path from $s$ to $v_T$ is a shortest path to $v_T$ and,
  since edges are non-negative, path weights do not decrease along the
  path implying $\dist_G(s,v_T) \leq \dist_G(s,v_m)$ (it could be that
  $v_T = v_m$).  Furthermore $\min_{x \in X} (\dist_G(s,x) + w(x,v_T))
  = \dist_G(s,v_T)$ by assumption, which gives the desired result
  since:
\[
d_{XY} \leq \min_{x \in X} (\dist_G(s,x) + w(x,v_t)) = \dist_G(s,v_t) \leq \dist_G(s,v_m)
= \min_{y \in Y} \dist_G(s, y)\, .\]
and
\[d_{XY} \geq \min_{y \in Y} \dist_G(s, y)\, ,\]
since $d_{XY}$ are path weights to $Y$ and the right hand side is the
overall shortest path to $Y$, so together we have an equality.
\end{comment}
\end{proof}

\begin{note}
The lemma gives us a way to easily find $\dist_G(s,v)$ for at least
one vertex $v \in Y$.  In particular for all $v \in Y$ where $p(v) =
\min_{y \in Y} p(y)$, it must be the case that $p(v) = \dist_G(s,v)$
since there cannot be a shorter path.  Also we need only consider the
frontier vertices in calculating $p(v)$, since for others in $Y$,
$p(y)$ is infinite.
\end{note}

\end{group}


\begin{gram}
The reader might have noticed that the terminology that we used in
explaining Dijkstra's algorithm closely relates to that of graph
search.
%
%% \begin{teachask}
%%   Does this algorithm remind you of a particular graph search that we
%%   discussed?
%% \end{teachask}
%
More specifically, recall that priority search is a graph search,
where each round visits the frontier vertices with the highest
priority.  
%
If as usual we denote the visited set by~$X$, we can define the
priority for a vertex~$v$ in the frontier,~$p(v)$, as the weight of
the shortest-path weight consisting of a path to~$x \in X$ and an
additional edge from~$x$ to~$v$.  In other words, this algorithm is
actually an instance of priority-first search.
%
We can thus define precisely Dijkstra's algorithm in terms of graph
search.
\end{gram}

\begin{algorithm}[Dijkstra's Algorithm]
  For a weighted graph $G=(V,E,w)$ and a source $s$, Dijkstra's
  algorithm is priority search on $G$ that 
%
\begin{itemize}
\item starts at $s$ with $d(s) = 0$, 
\item uses priority $\displaystyle p(v) = \min_{x \in X} (d(x) +
  w(x,v))$ (to be minimized), and 
\item sets $d(v) = p(v)$ when $v$ is visited.
\end{itemize}
\end{algorithm}
%
\begin{note}
Dijkstra's algorithm visits vertices in non-decreasing shortest-path
weight since on each round it visits unvisited vertices that have the
minimum shortest-path weight from~$s$.
\end{note}


%% \begin{comment}
%% \begin{figure}
%% \begin{pseudocode}
%% ~\\
%% \begin{lstlisting}
%% function $\sssppd(G, s) =$ 
%% let
%%    $D = \cset{s \mapsto 0}$     
%%    $F = N(s)$        % the frontier
%%   while $|F| \neq 0$ with $(D,F)$     % $D$ is a table from $v \in X$ to $\dist_G(s,v)$ 
%%      $\displaystyle D_F = \cset{y \mapsto \min_{(x \mapsto d) \in D}(d + w(x,y)) : y \in F}$
%%      $\displaystyle d_{XY} = \min_{(y \mapsto d) \in D_F} d$
%%      $Y = \csetf{(y \mapsto d) \in D_F}{d = d_{XY}}$
%%      $D = D \cup Y$
%%      $F = (F \cup N_G(Y)) \setminus \mbox{domain}(D)$
%% in
%%   $X$
%% end
%% \end{lstlisting}
%% \label{alg:pfsdjk}
%% \end{pseudocode}
%% \end{figure}
%% \end{comment}


\begin{note}
It may appear that Dijkstra's algorithm visits vertices strictly in
increasing order of shortest-path weight from the source, visiting
vertices with equal shortest-path weight on the same round.  This is
not true. To see this, consider the example below and convince yourself
that it does not contradict our reasoning.

\begin{center}
  \includegraphics[width=1.7in]{./media/dijkstra-zero-weight-counter-example.jpg}
\end{center}
\end{note}

\begin{group}
\begin{lemma}[Correctness of Dijkstra's Algorithm]
  Dijkstra's algorithm returns~$d(v) = \delta_G(s,v)$ for~$v$ 
  reachable from~$s$.
\end{lemma}
\begin{proof}
  We show that for each step in the algorithm, for all $x \in X$ (the
  visited set), $d(x) = \delta_G(s,x)$.  This is true at the start
  since $X = \cset{s}$ and $d(s) = 0$.  On each step the search adds
  vertices $v$ that minimizes $P(v) = \min_{x \in X} (d(x) + w(x,v))$.
  By our assumption we have that $d(x) = \delta_G(s,x)$ for $x \in X$.
  By \lemref{sp::djk}, $p(v) = \delta_G(s,v)$, giving $d(v) =
  \delta_G(s,v)$ for the newly added vertices, maintaining the
  invariant.  As with all priority-first searches, it will eventually
  visit all reachable $v$.
\end{proof}
\end{group}
\end{unit}



\begin{unit}[Implementation of Dijkstra's Algorithm]

\begin{gram}
We describe and analyze an implementation of Dijkstra's algorithm using
priority queues.
\end{gram}

\begin{group}
\begin{algorithm}[Dijkstra's Algorithm using Priority Queues]
\label{alg:sp::dijkstra}

An implementation of Dijkstra's algorithm that uses a priority
queue to maintain $p(v)$ is shown below.
%
The priority queue supports $\cd{deleteMin}$ and $\cd{insert}$
operations.
%

\[
\begin{array}{ll}
1 & \cd{dijkstraPQ} (G,s) = 
\\
2 & \cd{let}
\\
3 & ~~~\cd{requires:}
\\
4 & ~~~~~~
\begin{array}[t]{lcl}
\forall {(x \mapsto d) \in X}, d & = & \dist_G(s,x)
\\
\csetf{(d,y) \in Q}{y \in V \setminus X} & = &
\begin{array}[t]{lcl}
\{ (d + w(x,y), y) & : & (x \mapsto d) \in X,
\\
& & y \in N_G^+(x) \setminus X        \}
\end{array}
\end{array}
%%
%% 4 & ~~~~~~\forall {(x \mapsto d) \in X}, d = \dist_G(s,x)
%% \\
%% 5 & ~~~~~~\begin{array}{ll}
%%        \csetf{(d,y) \in Q}{y \in V \setminus X} = 
%%        \{ (d + w(x,y), y): & (x \mapsto d) \in X,
%%        \\
%%        & y \in N_G^+(x) \setminus X 
%%        \}
%%       \end{array}
\\
5 & ~~~\cd{returns:} \cset{x \mapsto \dist_G(s,x) : x \in R_G(s)} 
\\
6 & ~~~\cd{dijkstra} (X, Q) = 
\\
7 & ~~~~~~\cd{case}~\underline{\cd{PQ.deleteMin}}(Q)~\cd{of} %\label{line:dijkstra::min}       
\\
8 & ~~~~~~(\cnone, \_) \dra X
\\
0 & ~~~|~(\csome{(d,v)}, Q') \dra
\\  
10 & ~~~~~~~~~\cd{if}~\underline{(v,\_) \in X}~\cd{then}
%  \label{line:dijkstra::find}
\\        
11 & ~~~~~~~~~~~~~\cd{dijkstra}~(X, Q')
\\
12 & ~~~~~~~~~\cd{else}
\\
13 & ~~~~~~~~~~~~\cd{let} %  @\label{line:dijkstra::let}@ 
\\
14 & ~~~~~~~~~~~~~~~X' = \underline{X \cup \{(v,d)\}}
%\label{line:dijkstra::insert}
\\
15 & ~~~~~~~~~~~~~~~\cd{relax} (Q, (u, w)) = \underline{\cd{PQ.insert}}~(d+w, u) Q
% \label{line:dijkstra::pqinsert}
\\   
16 & ~~~~~~~~~~~~~~~Q'' =
\underline{\cd{iterate}}~\cd{relax}~Q'~\underline{N_G^+(v)}
%\label{line:dijkstra::iter}@
\\
17 & ~~~~~~~~~~~~\cd{in}~\cd{dijkstra}~(X', Q'')~\cd{end}
\\      
18 & ~~~\cd{in}
\\
19 & ~~~~~~\cd{dijkstra}~(\{\}, \underline{\cd{PQ.insert}}~(0, s) \{\})  % @\label{line:dijkstra::end}@
\\
20 & ~~~\cd{end}
\end{array}
\]

%% \begin{lstlisting}[numbers=left]
%% dijkstraPQ ($G$,$s$) = 
%% let
%%   requires:
%%      $\forall {(x \mapsto d) \in X}, d = \dist_G(s,x)$     
%%      $
%%      \begin{array}{ll}
%%       \csetf{(d,y) \in Q}{y \in V \setminus X} = 
%%           \{ (d + w(x,y), y): & (x \mapsto d) \in X,
%%       \\
%%       & y \in N_G^+(x) \setminus X 
%%            \}
%%      \end{array}
%%      $
%%   returns: $\cset{x \mapsto \dist_G(s,x) : x \in R_G(s)} $
%%   function dijkstra ($X$, $Q$) = 
%%     case @\fbox{PQ.deleteMin}@($Q$) of  @\label{line:dijkstra::min}@        
%%      ($\cnone$, _) => $X$
%%     | $(\csome{(d,v)}, Q')$ =>  
%%         if @\fbox{$(v,\_) \in X$}@ then  @\label{line:dijkstra::find}@        
%%           dijkstra ($X$, $Q'$)
%%         else
%%           let  @\label{line:dijkstra::let}@ 
%%            $X'$ = @\fbox{$X \cup \{(v,d)\}$}@ @\label{line:dijkstra::insert}@
%%            relax ($Q$, ($u$, $w$)) = @\fbox{PQ.insert}@($d+w$, $u$) $Q$ @\label{line:dijkstra::pqinsert}@   
%%            $Q''$ = @\fbox{iterate}@ relax $Q'$ @\fbox{$N_G^+(v)$}@ @\label{line:dijkstra::iter}@
%%           in dijkstra ($X'$, $Q''$) end      
%% in
%%   dijkstra ($\{\}$, @\fbox{PQ.insert}@($0$, $s$) $\{\}$)   @\label{line:dijkstra::end}@
%% end
%% \end{lstlisting}
\end{algorithm}

\begin{note}
This algorithm only adds one vertex at a time even if
there are multiple vertices with equal distance.
\end{note}

\begin{example}
An example run of Dijkstra's algorithm. Note that after
visiting~$s$,~$a$, and~$b$, the queue~$Q$ contains two distances
for~$c$ corresponding to the two paths from~$s$ to~$c$ discovered thus
far.  The algorithm takes the shortest distance and adds it to~$X$.  A
similar situation arises when $c$ is visited, but this time
for~$d$. Note that when~$c$ is visited, an additional distance for~$a$
is added to the priority queue even though it is already visited.
Redundant entries for both are removed next before visiting~$d$.  The
vertex~$e$ is never visited as it is unreachable from~$s$.  Finally,
notice that the distances in~$X$ never decrease.


\begin{center}
\includegraphics[width=3.0in]{./media/dijkstra-0.jpg} 

\includegraphics[width=3.0in]{./media/dijkstra-1.jpg}

\includegraphics[width=3.0in]{./media/dijkstra-2.jpg}

\includegraphics[width=3.0in]{./media/dijkstra-3.jpg}

\includegraphics[width=3.0in]{./media/dijkstra-4.jpg}

\includegraphics[width=3.0in]{./media/dijkstra-5.jpg}

\includegraphics[width=3.0in]{./media/dijkstra-6.jpg}

\includegraphics[width=3.0in]{./media/dijkstra-7.jpg}
\end{center}

\end{example}
\end{group}



\begin{gram}
The algorithm maintains the visited set $\visited$ as a table mapping
each visited vertex $u$ to $d(u) = \delta_G(s,u)$.  It also maintains
a priority queue $Q$ that keeps the frontier prioritized based on the
shortest distance from $s$ directly from vertices in $\visited$.  On each
round, the algorithm selects the vertex $x$ with least distance $d$ in
the priority queue (Line~\linedijkstramin{} in the code) and, if it
hasn't already been visited, visits it by adding $(x \mapsto d)$ to
the table of visited vertices (Line~\linedijkstrainsert{}), and then
adds all its neighbors $v$ to $Q$ along with the priority $d(x) +
w(x,v)$ (i.e. the distance to $v$ through $x$)
(Lines~\linedijkstrapqinsert{}~\linedijkstraiter{}).  
%

Note that a
neighbor might already be in $Q$ since it could have been added by
another of its in-neighbors.  $Q$ can therefore contain duplicate
entries for a vertex, but what is important is that the minimum
distance will always be pulled out first.  Line~\linedijkstrafind{}
checks to see whether a vertex pulled from the priority queue has
already been visited and discards it if it has.  This algorithm is just
a concrete implementation of the previously described Dijkstra's
algorithm.
\end{gram}

\begin{remark}
There are a couple other variants on Dijkstra's algorithm using
priority queues.
%

One variant checks whether $u$ is already in $X$ inside the
$\cd{relax}$ function, and if not, inserts it into the priority queue.
%
This does not affect the asymptotic work bounds but probably would
give some improvement in practice.  
%

Another variant decreases the priority of the neighbors instead of
adding duplicates to the priority queue.  This requires a more
powerful priority queue that supports a $\cd{decreaseKey}$ function.
\end{remark}
\end{unit}

\begin{unit}[Work and Span of Dijkstra's Algorithm]

\begin{gram}[Data Structures]
To analyze the work and span of priority-queue based implementation of
Dijkstra's algorithm shown above,
%
%\algref{sp::dijkstra}
%
let's first consider the priority queue ADT's that we use.
%
For the priority queue, we assume $\cd{PQ.insert}$ and
$\cd{PQ.deleteMin}$ have $O(\lg n)$ work and span.  
%

Since Dijkstra's algorithm makes no updates to the graph, we can
represent the input graph simply either by using a table or a
sequence, mapping vertices to their out-neighbors along with the
weight of the corresponding edge.
%
As we shall see, it suffices to use the tree based costs for tables.
%
To represent the mapping of visited vertices to their distances, we
can use a table, an array sequence, or a single-threaded array
sequences.
%
\end{gram}

\begin{gram}
\label{sp::dijkstra-costs}
To analyze the work, we calculate the work for each different kind of
operation and sum them up to find the total work.  The table belowe
summarizes the costs of the operations, along with the number of calls
made to each operation.

%\begin{tabular}{llc|c|c|c|c|c} 
\begin{tabular}{llcccccc} 
\hline
%
& Operation & Line & \# of calls & PQ & Tree Table & Array & ST Array
\\ 
%
\hline
%
& $\cd{deleteMin}$ & Line~\linedijkstramin{}
& $O(m)$           & $O(\lg m)$ & - & - & - \\
& $\cd{insert}$    & Line~\linedijkstrapqinsert{}
& $O(m)$ & $O(\lg m)$ & - & - & -\\ 
\hline
%%
% \multicolumn{3}{l|}{\bf Priority Q total}
% &        & $O(m \lg m)$ & - & - & -
% \\
% \hline
%%
& $\cd{find}$ & Line~\linedijkstrafind{}
& $O(m)$     & -           & $O(\lg n)$ & $O(1)$ & $O(1)$ \\
& $\cd{insert}$ & Line~\linedijkstrainsert{}
& $O(n)$   & -           & $O(\lg n)$ & $O(n)$ & $O(1)$ \\ 
\midrule
%%
% \multicolumn{3}{l|}{\bf Distances total}
% &        & -             & $O(m \lg n)$ & $O(n^2)$ & $O(m)$ \\ 
%\hline
%%
& $N_G^+(v)$ & Line~\linedijkstraiter{}
& $O(n)$      & -           & $O(\lg n)$ & $O(1)$ & - \\
& $\cd{iterate}$ & Line~\linedijkstraiter{}
& $O(m)$     & -           & $O(1)$ & $O(1)$ & - \\ 
%%
%\hline
%\multicolumn{3}{l|}{\bf Graph access total}
%&        & -             & $O(m + n \lg n)$ & $O(m)$ & -\\ 
%%
\hline
%\bf Total & - & $O(m \lg n)$ & $O(m \lg n)$ & $O(n^2)$ & $O(m)$ \\ \hline
\end{tabular}

In the algorithm, each operation on the graph $G$, the set of visited
vertices $X$, or the priority queue $\cd{PQ}$ are underlined.  The
$\cd{PQ.insert}$ in Line~\linedijkstraend{} is called only once, so we
can safely ignore it.  Of the remaining operations, The $\cd{iterate}$
and $N_G^+(v)$ on Line~\linedijkstraiter{} are on the graph,
Lines~\linedijkstrafind{}~and~\linedijkstrainsert{} are on the table
of visited vertices $X$, and
Lines~\linedijkstramin{}~and~\linedijkstrapqinsert{} are on the
priority queue $Q$.

We can calculate the total number of calls to each operation by noting
that the body of the let starting on Line~\linedijkstralet{} is only
run once for each vertex.  Thus, Line~\linedijkstrainsert{} and
$N_G^+(v)$ on Line~\linedijkstraiter{} are only called $O(n)$ times.
All other operations are performed once for each edge.  The total work
for Dijkstra's algorithm using a tree table is therefore 
\[
O(m \lg m + m \lg n + m + n \lg n).
\]
Since $m \le n^2$, the total work is 
\[
O(m \lg n).
\]

Since the algorithm is sequential, the span is the same as the work.
\end{gram}


\begin{remark}
By inspecting the table above, we can observe that when using either
tree tables or single threaded sequences, the cost is no more than the
cost of the priority queue operations.
%
Therefore, there is no asymptotic advantage to using one over the
other; there might, however, be differences in constant factors.
%

Note also that using regular purely functional arrays is not a good
idea, because the cost is then dominated by the insertions and the
algorithm runs in $\Theta(n^2)$ work.
\end{remark}

\end{unit}
\end{section}

\begin{section}[The Bellman-Ford Algorithm]

Dijkstra's algorithm solves the single source shortest path problem on
graphs with non-negative edge weights.
%
For graphs with negative edge weights, we can use the Bellman-Ford
algorithm.

\begin{unit}[Graphs with Negative Edge Weights]

\begin{gram}
Negative edge weights might appear to be unnatural. 
%
For example, in a ``map'' graph, which represents places of interest
and the roads between them, quantities of interest such as the travel
time, or the length of the road between two vertices are non-negative.
%
Negative weights, do however, arise when we consider more complex
properties and when we reduce other problems to shortest paths.
%
\end{gram} 

\begin{gram}[Impact of Negative Weights on Shortest Paths]
Consider a graph with negative edge weights.
%
Are shortest paths on this graph always well defined? 
%

To answer this question, consider two cases.
%
\begin{itemize}
\item
First, assume that the graph does not have any cycles with total
negative weight, i.e., for any cycle the sum of the weights of the
edges on that cycle is not less than zero.
%
In this case, we can conclude that there is a shortest path between
any two vertices that is simple, i.e., contains no cycles.

\item
Second, assume that the graph has a cycle with total
negative weight.  In this case, any shortest simple path that passes
through a vertex in this cycle can be made shorter by extending the
path with the cycle.
%
Furthermore, this process can be repeated, ultimately allowing us
prove that the shortest path is $-\infty$.
\end{itemize}

Based on this analysis, we expect a shortest path algorithm to alert
us when it encounters a negative-weight cycle.
%
In other words, if there is a relevant negative-weight cycle, then the
algorithm should terminate and indicate the presence of such a cycle,
otherwise, the algorithm should compute the shortest paths as desired.
\end{gram}

\begin{exercise}
Prove that if a graph has no negative-weight cycles, then there is a
shortest path between any two vertices that is simple.
\end{exercise}

%% \begin{notesonly}
%% \begin{todo}
%%  In the graph above, one student has found a cycle that allows
%% dollar to be converted more dollars.  You might want to update the example. 

%% These rates from Oct 26, 2016.

%% \end{todo}

%% \begin{exercise}
%%   Consider the following~\defn{currency exchange} problem: given a set
%%   currencies, a set of exchange rates between them, and a source
%%   currency $s$, find for each other currency $v$ the best sequence of
%%   exchanges to get from $s$ to $v$.  Hint: how can you convert
%%   multiplication to addition.

%% Here is an example:

%% \includegraphics[width=3.5in]{./media/currency-exchange.jpg}

%% In the general case, you can imagine having $n$ currencies and finding
%% the best way to convert a currency into another.


%% \end{exercise}




%% We can convert multiplication to addition by taking logs.  But we also
%% need to convert longest, which is what we want, to shortest.  So we
%% have to use negative log. 


%% \begin{exercise}
%% In your solution to the previous exercise can you get negative weight
%% cycles?   If so, what does this mean?
%% \end{exercise}

%% \end{notesonly}

\begin{group}
\begin{gram}[Dijkstra with Negative Edge Weights]
Recall that Dijkstra's assumes non-negative edge weights. 
%
This assumption allows the algorithm to consider simple paths (with no
cycles) only and more importantly plays a critical role in its
correctness.  
%
More specifically, Dijkstra's algorithm assumes that the shortest path
to the vertex $v$ in the frontier that is closest to the set of
visited vertices, whose distances have been determined, can be
calculated by considering only the incoming edges of $v$. 
%
With negative edge weights, this assumption is not true anymore,
because there can be a shorter path that ventures out of the frontier
and then comes back to $v$.
\end{gram}

\begin{example}
To see where Dijkstra's property fails with negative edge weights
consider the following example.
\begin{center}
  \includegraphics[width=6.0in]{./media/dijkstra-negative.jpg}
\end{center}
Dijkstra's algorithm would visit $b$ then $a$ and leave $b$ with a
distance of $2$ instead of the correct distance $1$.  
%
The problem is that when Dijkstra visits $b$, it fails to consider the
possibility of there being a shorter path from $a$ to $b$ (which is
impossible with non-negative edge weights).
\end{example}
\end{group}


\end{unit}

\begin{unit}[Bellman-Ford Algorithm]

\begin{teachask}
Why can't we use Dijkstra's algorithm to compute shortest path when
there are negative edges?
\end{teachask}

\begin{teachask}
How can we find shortest paths on a graph with negative weights?
\end{teachask}
\begin{teachask}
Recall that for Dijkstra's algorithm, we started with the brute-force
algorithm and realized a key property of shortest paths.  Do you
recall the property?
\end{teachask}


\begin{gram}[Intuition behind Bellman-Ford]
To develop some intuition for finding shortest paths in graphs with
negative edge weights, let's recall the sub-path property of shortest
paths.
%
This property states that any sub-path of a
shortest path is a shortest path (between its end vertices).
%
Sub-paths property holds regardless of the edge weights.

Dijkstra's algorithm exploits this property by building longer paths
from shorter ones, i.e., by building shortest paths in non-decreasing
order.  
%
With negative edge weights this does not work anymore, because
paths can get shorter as we add edges.
%
%% \begin{teachask}
%% Is there another way to use this property?
%% \end{teachask}
%

But there is another way to exploit the same property: building paths
that contain more and more edges. 
%
To see how, suppose that we have found the shortest paths from source
to all vertices with~$k$ or fewer edges.
%% %
%% \begin{teachask}
%%   How can you update the shortest paths for $k+1$ edges? That is you
%%   want to find the shortest paths that contain $k+1$ or fewer edges.
%% \end{teachask}
%% %
We can compute the shortest paths with $k+1$ or fewer edges by
extending all paths by one edge if doing so leads to a shorter path
and leaving them as they are otherwise.
%

To make this idea more precise, define~\defn{k-distance}, written
$\delta^k_G(s,t)$, as distance from $s$ to $t$ considering all paths
with at most $k$ edges.
%
Suppose now that we know that a vertex $v$ has a shortest path from a
source $s$ with $k+1$ edges and that we have calculated shortest paths
for all vertices with $k$ edges.
%
To find the shortest path to a vertex $v$ with $k+1$ edges, all we
need is to consider the incoming edges of $v$ and pick the shortest
path to that vertex that arrives at a neighbor $u$ using $k$ edges and
then takes the edge $(u,v)$.
%

Of course, in general, we don't know the number of edges that are is a
shortest path, but this is relatively easy to deal with by iterating
over all possibilities.
%  
\begin{itemize}
\item Start by determining $\delta^0_G(s,v)$ for all $v \in V$.
  Since no vertex other than the source is reachable with a path of
  length $0$, we have:

\begin{itemize}
\item $\delta^0_G(s,s) = 0$, and 
\item  $\delta^0_G(s,v) = \infty$ for all $v \not= s$.
%
\end{itemize}

\item Next, iteratively compute for all $v \in V$ and for all $k > 0$,
  $\delta^{k+1}_G(s,v)$ based on all $\delta^{k}_G(s,v)$.
%
To calculate the distances $k+1$ of fewer edges, we can use the
shortest path with $k$ edges to each of its in-neighbors and then add
in the weight of the one additional edge.  More precisely, for each
vertex $v$,
\[
\delta^{k+1}(v) = \min(\delta^{k}(v),\min_{x \in N^-(v)}
(\delta^{k}(x) + w(x,v))\;).
\]
Recall that $N^-(v)$ indicates the in-neighbors of vertex $v$.
\end{itemize}

This algorith has an interesting convergence property: because the
distance for a vertex at $k+1$ is calculated in terms of the distances
at $k$, if the distances for all vertices remain the same at $k+1$ as
at $k$, then the distances at $k+2$ will also remain unchanged.
%
Therefore, we can stop, when an iteration produces no change in the
distances of vertices.
%

Convergence, however is not guaranteed, because a negative-weight
cycles will decrease distances at each iteration.
%
But, we can detect negative cycles by checking that the distances have
not converged after $|V|$ iterations, because a simple (acyclic) path
in a graph can include at most $|V|$ edges and, in the absence of
negative-weight cycles, there always exist a simple shortest path.
\end{gram}

\begin{example}
\label{ex:shortestpath::allbutone-negative}

  In the following graph $G$, suppose that we have found the shortest
  paths from the source $s$ to vertices using $k$ or fewer edges; each
  vertex $u$ is labeled with its $k$-distance to $s$, written
  $\dist_G^k(s,u)$. The weight of the
  shortest path to~$v$ using $k+1$ or fewer edges is 
%
\[
\min
\left(
\dist_G^k(s,v),\min{\dist_G^k(s,a)+3, \dist_G^k(s,b)-6,\dist_G^k(s,c)+5}
\right)
.
\]
%
The shortest path with at most $k+1$ edges has weight $-2$ and goes
through vertex $b$.

\begin{center}
\includegraphics[width=3.0in]{./media/shortest-paths-last-negative.jpg}
\end{center}

\end{example}

\begin{group}
\begin{algorithm}[Bellman Ford Algorithm]
\label{alg:sp::bf-code}

The pseudo-code below show the Bellman-Ford algorithm for computing
shortest paths in weighted graphs, where edge weights can be negative.
%
The algorithm terminates and returns either the weight of the shortest
paths for all vertices reachable from the source $s$ or it indicates
that the graph has a negative-weight cycle.

The algorithm runs until convergence or until $|V|$ iterations have
been performed.
%
If after $|V|$ iterations, and the distances does not converge, then
the algorithm concludes that there is a negative-weight cycle that is
reachable from the source vertex and returns~$\cnone{}$
(Line~\linebfnegcycle{}).
%

\[
\begin{array}{ll}
1 & \cd{BellmanFord}~(G=(V,E),s) =
\\ 
2 & ~~~\cd{let}
\\
3 & ~~~~~~\cd{requires:}~\forall {v \in V}, D_v = \delta_G^k(s,v)$
\\
4 & ~~~~~~\cd{BF}~(D,k) =
\\
5 & ~~~~~~~~~\cd{let}
\\
6 & ~~~~~~~~~~~~D' = \{v \mapsto \min(D[v],\min_{u \in N_G^-(v)} (D[u] +
w(u,v))) : v \in V \} % @\label{line:bf::distances}@
\\ 
7 & ~~~~~~~~~\cd{in}
\\ 
8 & ~~~~~~~~~~~~\cd{if}~(k = |V|)~\cd{then}
\\
9 & ~~~~~~~~~~~~~~~\cnone{} % @\label{line:bf::negcycle}@
\\
10 & ~~~~~~~~~~~~\cd{else if}~\left(\cd{all}~\cset{D[v] = D'[v] : v \in
  V}\right)~\cd{then} % @\label{line:bf::if}@
\\
11  & ~~~~~~~~~~~~~~~\csome{D}
\\
12 & ~~~~~~~~~~\cd{else}
\\
13 & ~~~~~~~~~~~~~~~~\cd{BF}~(D',~k+1)
\\
14 & ~~~~~~~~~\cd{end} 
\\
15 & ~~~~~~D = \cset{v \mapsto~\infty : v \in V \setminus \{s\}} \cup
\cset{s \mapsto 0}
\\
16 & ~~~\cd{in}
\\
17 & ~~~~~~\cd{BF}~(D,0)
\\
18 & ~~~\cd{end}
\end{array}
\]
\end{algorithm}


\begin{example}
\label{ex:sp::bf}

Several steps of the Bellman Ford algorithm are shown below.  The
numbers with squares indicate the current distances and highlight
those that has changed on each step.


\begin{center}
\includegraphics[width=2.4in]{./media/bf-0.jpg} 

\includegraphics[width=2.4in]{./media/bf-1.jpg}

\includegraphics[width=2.4in]{./media/bf-2.jpg}

\includegraphics[width=2.4in]{./media/bf-3.jpg}

\includegraphics[width=2.4in]{./media/bf-4.jpg}
\end{center}

\end{example}
\end{group}

\begin{group}
\begin{theorem}[Correctness of Bellman-Ford]
 Given a directed weighted graph $G = (V,E,w)$, $w : E \to R$, and a
  source $s$, the $\cd{BellmanFord}$ algorithm returns either
  $\dist_G(s,v)$ for all vertices reachable from $s$, or indicates
  that there is a negative weight-cycle in $G$ that is reachable from
  $s$.
\end{theorem}

\begin{proof}
  By induction on the number of edges $k$ in a path.  The base case is
  correct since $D_s = 0$.  For all $v \in V\setminus{s}$, on each
  step a shortest $(s,v)$ path with up to $k+1$ edges must consist of
  a shortest $(s,u)$ path of up to $k$ edges followed by a single edge
  $(u,v)$.  Therefore if we take the minimum of these we get the
  overall shortest path with up to $k+1$ edges.  For the source the
  self edge will maintain $D_s = 0$.    The algorithm can only proceed
  to $n$ rounds if there is a reachable negative-weight cycle.   
  Otherwise a shortest path to every $v$ is simple and can consist of
  at most $n$ vertices and hence $n-1$ edges.
\end{proof}  
\end{group}
\end{unit}

\begin{unit}[Cost Analysis of Bellman-Ford]

\begin{gram}[Data Structures]
To analyze the cost Bellman-Ford, let us first determine the
representations that we want to use for each structure in the
algorithm.
%
For the distance structure $D$, we use a table mapping vertices
to their distances.
%
For graphs, we consider two different representations, one
with tables and another with sequences.
%
When using tables, we represent the graph as a table that maps each
vertex $u$ to a table of its out-neighbors; this table maps each
out-neighbor $v$ to the weight of the edge from $u$ to $v$.
%
\end{gram}

\begin{gram}[Cost with Tables]
Consider the cost of one call to $BF$, not including the recursive
calls.  The only nontrivial computations are on
Lines~\linebfdistances{}~and~\linebfif{}. 
%

Line~\linebfdistances{} consists of a $\cd{tabulate}$ over the vertices.
%
As the cost specification for tables indicate, to calculate the work
for a $\cd{tabulate},$ we take the sum of the work for each vertex, and for
the span we take the maximum of the spans, and add $O(\lg n)$.  
%

Now consider what the algorithm does for each vertex.  
\begin{itemize}
\item First, it has
to find the neighbors in the graph, using a $\cd{find G v}.$  This
requires $O(\lg |V|)$ work and span. 
%
\item Second, the algorithm performs a $\cd{map}$ over the  in-neighbors. 
%
Each instance of the $\cd{map}$ requires finding in the distance table
to obtain $D[u]$, finding in the weight table, the weight, and an
addition operation.
%
The find operations take $O(\lg |V|)$ work and span.  
%
\item Third, there is a reduce for finding the shortest path through
  an in-neighbor.  The reduce takes $O(1 + |N_G(v)|)$ work and $O(\lg
  |N_G(v)|)$ span.
%
\item Finally, the value calculated by mapping over the neighbors is
  compared against the current distance $D[v]$ and the minimum is
  taken.  This requires $O(\lg |V|)$ work and span.
\end{itemize}
%

Using $n = |V|$ and $m = |E|$, we can write the work
as follows
\[
\begin{array}{lcl}
W_{BF}(n,m)
& = & O\left(\sum_{v \in V} \left(\lg n + |N_G^-(v)| + \sum_{u
  \in N_G^-(v)} (1 + \lg n)\right)\right) 
\\
& = & O\left((n + m) \lg n\right). 
\end{array}
\]
%
The first term is for looking up the current distance, the second term
is for reduce, and the third term is the cost for mapping over the
neighbors.


Similarly,  we can write the span as follows
\[
\begin{array}{lcl}
S_{BF}(n,m)
& = & O\left(\max_{v \in V} \left(\lg n + \lg |N_G^-(v)| + \max_{u
  \in N_G^-(v)} (1 + \lg n)\right)\right) 
\\
& = & O(\lg n). 
\end{array}
\]
%
The work and span of Line~\linebfif{} is simpler to analyze since it
only involves a tabulate and a reduction: it requires $O(n \lg n)$
work and $O(\lg n)$ span.

Since the number of calls to $BF$ is bounded by $n$, as discussed
earlier.  Since the calls to $\cd{BF}$ are performed sequentially, we
can multiply the work and span for each call by the number of calls to
compute the total work and span, which, assuming $m \ge n$, are
\[
\begin{array}{lcl}
W_{BF}(n,m) & = & O(n m \lg n)\\
S_{BF}(n,m) & = & O(n \lg n).\\
\end{array}
\]
\end{gram}

\begin{gram}[Cost with Sequences]
Let's now analyze the cost with a sequence representation of graphs.
%
If we assume that the graphs is enumerable, then the vertices are
identified by the integers $\{0,1,\ldots,|V|-1\}$ and we can use
sequences to represent the graph. 
%
Instead of using a $\cd{find}$ for a table, which requires $O(\lg
n)$ work, we can use $\cd{nth}$ (indexing) requiring only $O(1)$
work.  
%
This improvement in costs can be applied for looking up in the
graph to find the neighbors of a vertex, and looking up in the
distance table to find the current distance.  By using the improved
costs we obtain:
\[
\begin{array}{lcl}
W_{BF}(n,m) & = & O\left(\sum_{v \in V} \left(1 + |N_G^-(v)| + \sum_{u \in N_G^-(v)} 1\right)\right) \\
  & = & O(n+m) \\
S_{BF}(n,m) & = & O\left(\max_{v \in V} \left(1 + \lg |N_G^-(v)| + \max_{u \in N_G^-(v)} 1\right)\right) \\
  & = & O(\lg n) \\
\end{array}
\]
and hence the overall complexity for $\cd{BellmanFord}$ with array
sequences is and assuming $m \ge n$,
\[
\begin{array}{lcl}
W(n,m) & = & O(n m)\\
S(n,m) & = & O(n \lg n)\\
\end{array}
\]
By using array sequences we have reduced
the work by a $O(\lg n)$ factor.
\end{gram}
\end{unit}

%% %% \section{SML code}

%% %% Here we present the SML code for Dijkstra.


%% %% \begin{small}
%% %% \verbatiminput{../code/dijkstra.sml}
%% %% \end{small}

%% \begin{comment}
%% With this assumption, we seem close to an algorithm.  We have already
%% found a way to compute the shortest path to a vertex by using the
%% sub-paths property.  We can now iterate this process by taking out
%% vertices one by one and finding their shortest paths. Or inversely, we
%% can compute the shortest paths to some subset of the vertices and then
%% extend the set by adding another vertex.

%% \begin{teachask}
%% Would this algorithm work? 
%% \end{teachask}

%% This algorithm could work but only with a lot of luck.  To see why we
%% need a lot of luck, consider the example of the shortest path from
%% Pittsburgh to San Francisco and suppose that it goes through Chicago.
%% If in our algorithm, we consider San Francisco before Chicago, we will
%% not be able to find the shortest path to San Francisco.  Thus we now
%% have to solve the problem of how to order the vertices so that we can
%% find all shortest paths correctly.


%% \begin{teachask}
%% Can you pick an ordering on vertices so that all the shortest paths
%% would be computed correctly by this algorithm?
%% \end{teachask}

%% To solve this problem, suppose that we know the distance---that is the
%% weight of the shortest path---of each vertex to our source.  One idea
%% would be to visit the vertices in the order of their distances to the
%% source.  Let's see if this would work.  Suppose that we have found the
%% distance for all vertices up to distance~$d$ and we are now
%% considering a vertex~$v$ at distance~$d$; an example is shown below.

%% \begin{example}
%% \label{ex:dijkstra::monotone}

%% Finding the shortest path to a vertex~$v$ at distance~$d$ after
%% visiting all the vertices that are closer.  We can find a shortest
%% path to~$v$ by computing
%% $\min
%% \left(
%% \dist_G(s,a)+1,\dist_G(s,b)+\dist_G(s,c)+3
%% \right)
%% $.

%% \begin{center}
%% \includegraphics[width=3.5in]{./media/shortest-paths-distance-order.jpg}
%% \end{center}
%% \end{example}

%% \begin{teachask}
%% How can we find the shortest path to $v$?
%% \end{teachask}

%% Consider a shortest path to~$v$ and let $(u,v)$ be the last edge on
%% the path.  By sub-paths property, the shortest path to $u$ contains
%% the shortest path from source to~$u$.  Since all edges have positive
%% weights, $u$~is at a distance less than~$d$.  We therefore know
%% that~$u$ has been visited.  Thus to find the shortest path to~$v$, all
%% we have to do is to consider all incoming edges of~$v$ from the
%% visited set and compute the shortest path by taking the minimum of the
%% possibilities as shown in the example above.


%% The last piece of the puzzle remaining is to find the distance of the
%% vertices to the source, which we need in order to determine the
%% ordering for the shortest paths.  But this is a circular problem, how
%% can we find the ordering without finding the shortest paths?

%% \begin{teachask}
%% Can you think of a way of figuring out the order?
%% \end{teachask}

%% The key observation to solving this problem is to notice that we don't
%% have to know the order for all the vertices, we just have to know the
%% ``next'' closest vertex to visit (at any point in the algorithm).

%% We are now going to show that having visited all the vertices at
%% distance less than~$d$, if we always pick the vertex closest to the
%% visited set, then we would be visiting vertices in increasing order of
%% distance from the source and we would find a shortest path to a vertex
%% when we visit that vertex.


%% \begin{example}
%% The distance of~$v$ to the visited set~$X$, denoted $\dist_{G,X}$ is
%% computed as $\dist_{G,X} = \min \left(\dist_G(s,a)+1, \dist_G(s,b)+2,
%% \dist_G(s,c)+3, \dist_G(s,d)+4 \right)$.  If~$v$ is the vertex in the
%% frontier with smallest such distance, then the distance of~$v$ to the
%% source is the smallest of all the remaining vertices. 

%% \begin{center}
%% \includegraphics[width=3.5in]{./media/shortest-paths-distance-order-multi.jpg}
%% \end{center}
%% \end{example}

%% To prove this crucial property, let~$v$ be the vertex closest to the
%% visited set.  Assume for contradiction that there is another vertex~$w$,
%% not yet visited, whose distance is no less than $d$ but that is closer
%% to the source.  Consider a shortest path to~$w$ from the source and
%% let~$u$ be the first vertex outside~$X$.  Such a vertex exists because
%% each sub-path of the shortest path to~$w$ being considered is a
%% shortest path, and we have visited all the vertices with distance less
%% than~$d$. Since all edge weights are positive, the distance of~$u$ to
%% the visited set is smaller than that of~$v$.  But this is a
%% contradiction because we have picked~$v$ to be the vertex closest to
%% the visited set.  Thus, we conclude that no such~$w$ exists.

%% We have thus established that by picking~$v$, we have remained
%% consistent with the ordering of the vertices according to their
%% distance to the source.  


%% \begin{teachask}
%% Would we compute~$v$'s distance correctly?
%% \end{teachask}

%% This might seem like a redundancy but let's convince ourselves that we
%% can compute the distance of~$v$ to the source.  Thus far, we have only
%% computed the distance of~$v$ to the visited set. Since we have visited
%% all the vertices at distance less than~$d$ and since~$v$ is the
%% closest next vertex, we know that the computed path is a shortest path
%% from~$v$ to source, by our reasoning with increasing distances as in
%% \exref{dijkstra::monotone}.  In summary, what we have shown is that
%% the distance of~$v$ to the visited set is equal to its shortest path
%% distance to the source.


%% \begin{teachask}
%% Note that there can be many vertices with the same distance to the
%% visited set.  Does it matter which vertex is visited next.
%% \end{teachask}

%% Thus far, we have glossed over one detail: there can be multiple
%% vertices that have the same distance to the visited set.  It does not
%% matter, which one of these we pick because, all we need is that all
%% the vertices with shorter distances have been visited. We can thus
%% visit them in arbitrary order or in fact all in parallel.



%% \begin{teachask}
%% Can you generalize this reasoning to include zero-weight edges?
%% \end{teachask}

%% Thus far, we have assumed that all edges have positive weights.  This
%% is not a necessary assumption but greatly simplifies the reasoning
%% that we went through.  Dijkstra's algorithm actually satisfies an much
%% more elegant and simple property that can proved succinctly without
%% this assumption.  To truly understand this algorithm and appreciate
%% its beauty, you should make sure understand this property.

%% The key property used by Dijkstra's algorithm is that for a set of
%% vertices $X \subset V$ that include $s$ and the rest of the vertices
%% ($\unvisited = V \setminus \visited$), the closest vertex in $T$ from
%% $s$ based on paths that only go through $X$ is also an overall
%% closest vertex in $T$.  This property will allow us to select the next
%% closest vertex by only considering the vertices we have already visited.
%% Defining $\dist_{G,X}(s,v)$ as the shortest
%% path length from $s$ to $v$ in $G$ that only goes through vertices in
%% $X$ (except for $v$), the property can be stated more formally and
%% proved as follows:

%% This lemma implies that if we have already visited a set of vertices
%% $\visited$ we can find a new shortest path to a vertex in $T = V
%% \setminus X$ by just considering the path lengths through $\visited$
%% to a neighbor of $\visited$.  In particular we want to pick a vertex
%% $t$ that minimizes $\delta_{G,X}(s,t)$.  This suggests an algorithm
%% for shortest paths based on priority first search using the priority
%% $P(v) = \delta_{G,\visited}(s,u)$.  Also note that
%% $\delta_{G,\visited}(u) = \min_{v \in V} (\delta_G(v) + w(v,t))$.
%% Indeed this gives us Dijkstra's algorithm, at
%% least in the abstract:

%% \begin{teachask}
%% Can you turn these ideas to an algorithm for computing the shortest
%% distances from a source to all the other vertices?
%% \end{teachask}

%% Based on these ideas, we can solve the single-source shortest paths
%% problem by maintaining a visited set of vertices whose distances have
%% already been computed correctly.  We then consider the vertices in the
%% frontier and calculate their distance by considering their incoming
%% edges.  We then extend the frontier by visiting the vertex with the
%% smallest computed vertex.  As usual we start our visited set with
%% source at distance zero.

%% This is in fact Dijkstra's algorithm.  
%% \end{comment}

%% \input{./media/problems}
%% \flushchapter
\end{section}
\end{chapter}
\end{book}
