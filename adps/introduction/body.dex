\begin{chapter}[Introduction]
\label{ch:intro}

The topic of this \bcswitch{book}{course} might best be described as
{\bf problem solving
  $\underset{\wedge\makebox[.41in]{}}{\overset{\mbox{\em
        parallel\makebox[.3in]{}}}{\mbox{with computers}}}$}. 
%
The idea is you have some problem to solve (e.g. finding the shortest
path from your room to your first class), and you want to use your
computer to solve it for you. 
%
Your primary concern is probably that
your answer is correct (e.g. you would likely be unhappy to find
yourself at the wrong class). However, you also care that
you can get the answer reasonably quickly (e.g. it would not be useful
if your computer had to think about it until next semester).

This course is therefore about different aspects of problem solving
with computers.
%
It is about defining precisely the problem you want to solve.
%
It is about learning the different techniques that can be used to
solve such a problem, and about designing algorithms using these
techniques. 
%
It is about designing abstract data types that can be used in these
algorithms, and data structures that implement these types. 
%
And, it is about analyzing the cost of those algorithms and comparing
them based on their cost.  
%

Unlike traditional courses on algorithms and data structures, which
are concerned with sequential algorithms (ones that are correct and
efficient on sequential computers), in this book we are concerned with
both sequential and parallel algorithms (ones that are correct and
efficient on sequential and parallel computers).  
%
In the approach we take in this course, sequential and parallel
algorithms are not that different.  
%
Indeed the material covers most of what traditional sequential
algorithms course.
%
In the rest of this chapter we discuss why it is important to study
parallelism, why it is important to separate interfaces from
implementations, and outline some algorithm-design techniques.

\section{Parallelism}

The term ``parallelism'' or ``parallel computing'' refers the ability
to run multiple computations (tasks) at the same time.
%
\begin{question}
Why should we care about parallelism?  
%
Do we even have parallel computers today? 
\end{question}
%
%

\paragraph{Parallel systems.}
Today parallelism is available in all computer systems, and at many
different scales starting with parallelism in the nano-circuits that
implement individual instructions, and working the way up to parallel
systems that occupy large data centers.  Since the early 2000s
hardware manufacturers have been placing multiple processing units,
often called ``cores'', onto a single chip.  These cores can be
general purpose processors, or more special purpose processors, such as
those found in \defn{Graphics Processing Units} (GPUs).  Each core can
run in parallel with the others.  At the larger scale many such chips
can be connected by a network and used together to solve large
problems.  For example, when you perform a simple search on the
Internet, you engage a data center with thousands of computers in some
part of the world, likely near your geographic location.
%
\begin{question}
Do you know how many computers are engaged in answering a typical web
search? 
\end{question}
%
Many of these computers (perhaps as many as hundreds, if not
thousands) take up your query and sift through data to give you an
accurate response as quickly as possible.

%% We refer to this form of
%% parallelism as \defn{large-scale parallelism}, because it involves a
%% number of computers over a relatively large area such as a building or
%% a data center.

\begin{question}
What is the advantage of using a parallel algorithm instead of a
sequential one?
\end{question}
There are several reason for why such parallel systems and thus
parallelism has become so important.
%
First, parallelism is simply more powerful than sequential computing,
where only one computation can be run at a time, because it enables
solving more complex problems in shorter time.
%
For example,an Internet search is not as effective if it cannot be
completed at ``interactive speeds'', completing in several
milliseconds.
%
Similarly, a weather-forecast simulation is essentially useless if it
cannot be completed in time.

%
\begin{question}
Do you know how much energy it takes to run a computation twice as
fast using a sequential computer (one line of computation)? 
\end{question}
%
The second reason is efficiency in terms of energy usage.
%
As it turns out, performing a computation twice as fast sequentially
requires eight times as much energy.  
%
Precisely speaking, energy
consumption is a cubic function of clock frequency (speed).  With
parallelism we don't need more energy to speed up a computation, at
least in principle.  For example, to perform a computation in half the
time, we need to divide the computation into two parallel
sub-computations, perform them in parallel and combine their results.
This can require as little as half the time as the sequential
computation while consuming the same amount of energy.
%
In reality, there are some overheads and we will need more energy, for
example, to divide the computation and combine the results.
Such overheads are usually small, e.g., constant fraction over
sequential computation, but can be larger. 
%
These two factors---time and energy---have become increasingly
important in the last decade, catapulting parallelism to the forefront
of computing.

\begin{question}
Can you think of consequences of these developments in hardware?  
\end{question}
\begin{answer}
These developments in hardware make the specification, the design, and
the implementation of parallel algorithms an important topic.
\end{answer}

\begin{example}
As is historically popular in explaining algorithms, we can establish
an analogy between parallel algorithms and cooking.  As in a kitchen
with multiple cooks, in parallel algorithms you can do things in
parallel for faster turnaround time.  For example, if you want to
prepare 3 dishes with a team of cooks you can do so by asking each
cook to prepare one.
%
Doing so will often be faster that using one cook.  But there are some
overheads, for example, the work has to be divided as evenly as
possible.  Obviously, you also need more resources, e.g., each cook
might need their own kitchen utensils.
\end{example}

\paragraph{Parallel software.}
The important advantage of using a parallel instead of a sequential
algorithm is the ability to perform sophisticated computations quickly
enough to make them practical or relevant, without consuming large
amounts of energy.  
%% %
%% For example, without parallelism, computations such as Internet
%% searches, realistic graphics, climate simulations would be
%% prohibitively slow.  
%%

\begin{figure}
\begin{example}
  \label{ex:intro::example-runs}
Example timings (reported in seconds) for some algorithms on 1 and on
32 cores.
  \begin{center}
  \begin{tabular}{l r c  r r}
    \toprule
    &\multicolumn{1}{c}{\textbf{Sequential}} & &\multicolumn{2}{c}{\textbf{Parallel}} \\
    && & 1-core & 32-core\\
    \midrule
    Sorting 10 million strings &        2.9 &&  2.9 &  .095\\
    Remove duplicates for 10 million strings &      .66 &&  1.0 & .038\\
    Minimum spanning tree for 10 million edges    &    1.6 && 2.5  & .14\\
    Breadth first search for 10 million edges  &   .82  && 1.2 &  .046\\
    \bottomrule
  \end{tabular}
  \end{center}
\end{example}
\end{figure}

One way to quantify this advantage is to measure the performance
improvements of parallelism.
%
\exref{intro::example-runs} illustrates the sort of performance
improvements that can achieved today.  These times are on a 32 core
commodity server machine.  In the table, the sequential timings use
sequential algorithms while the parallel timings use parallel
algorithms.  Notice that the \defn{speedup} for the parallel 32 core
version relative to the sequential algorithm ranges from approximately
12 (minimum spanning tree) to approximately 32 (sorting).

%% G.B. This is included below.
%% Currently,
%% obtaining such performance requires developing efficient as well as
%% highly tuned implementations.  In this \bcswitch{book}{course}, we
%% will focus on the first challenge.

\begin{question}
But why after all, do we have to do anything differently to take
advantage of parallelism?  
\end{question}

\paragraph{Challenges of parallel software.}
It would be convenient to use sequential algorithms on parallel
computers, but this does not work well because parallel computing
requires a different way of organizing the computation.
%
The fundamental difference between sequential and parallel computation
is that in the latter certain computations will be performed at the
same time but this is possible only if the computations are actually
\defn{independent}, i.e., do not depend on each other.
%
Thus when designing a parallel algorithm, we have to identify the
underlying dependencies in the computation to be performed and avoid
creating unnecessary dependencies.


%% \begin{example}
%% Suppose that you want to run many searches on a database of student
%% records.  To improve your search time, you decide to sort the records
%% by the student name so that you can use a fast binary search algorithm
%% to find each student.  Since the binary search has to wait for the
%% sort to complete, you cannot sort and search in parallel.  You can
%% however perform all the searches in parallel.   You can also sort in
%% parallel by using a parallel sorting algorithm, as we will describe
%% in this book.
%% \end{example}

\begin{example}
Going back to or cooking example, suppose that we want to make a
frittata in our kitchen with 3 cooks.
%
Making a frittata is quite a bit more involved than just boiling
eggs. We have to be careful about the dependencies between various
tasks.  
%
For example,
%
vegetables cannot be sauteed before they are washed and chopped
%
the eggs cannot be added to the meal before being broken or before the
vegetables are sauteed, etc.
%
\end{example}

An important challenge is therefore to design algorithms that
minimize the dependencies so that more things can run in parallel.
%
This design challenge is a primary focus of this book. 

Another important challenge concerns the coding and usage of a
parallel algorithm in the real world.
%
The many forms of parallelism, ranging from small to large scale, and
from general to special purpose, has led to many different programming
languages and system for coding parallel algorithms.
%
These different programming languages systems often target a
particular kind of hardware, and even a particular kind of problem
domain.  
%
For example, there are separate systems for coding parallel numerical
algorithms on shared memory hardware, for coding graphics algorithms
on Graphical Processing Units (GPUs), and for coding data-analytics
software on a distributed system.
%
Each such system tends to have its own programming interface, its own
cost model, and its own optimizations, making it practically
impossible to take a parallel algorithm and code it once and for all
for all possible applications.
%
As it turns out, one can easily spend weeks or even months optimizing a
parallel sorting algorithm on specific parallel hardware, such as a GPU.
%% For example, it is unlikely that unoptimized code can obtain the
%% speedups discussed in \exref{intro::example-runs}.
%% %


%% The diversity of parallel hardware and software makes it difficult to
%% learn both the high-level ideas of developing parallel algorithms and
%% the optimization techniques required to achieve efficiency on a variety
%% of different machines.
%% %
%% For example, would a particular parallel sorting algorithm be
%% theoretically efficient on a large-scale system? How about on a small
%% scale system?  What would be needed to implement an optimized parallel
%% sorting algorithm for a GPU?


Maximizing speedup by coding and optimizing an algorithm is not the
goal of this book.
%
Instead, our goal is to cover general design principles for parallel
algorithms that can be applied in essentially all parallel systems,
from the data center to the multicore chips on mobile phones.
%
We will learn to think about parallelism at a high-level, learning
general techniques for designing parallel algorithms and data
structures, and learning how to approximately analyze their costs.
%
The focus is on understanding when things can run in parallel, and
when not due to dependencies.  
%
There is much more to learn about parallelism, and we hope you
continue studying this subject.

\section{Work and Span} 
In this \bcswitch{book}{course} we analyze the cost of algorithms in
terms of two measures: \defn{work} and \defn{span}.  Together these
measures capture both the sequential time and the parallelism
available in an algorithm.  We typically analyze both of these
asymptotically, using for example the big-O notation, which will be
described in more detail in Chapter~\ref{chapter:analysis}.

The \defn{work} of an algorithm corresponds to the total number of
primitive operations performed by an algorithm.  If running on a
sequential machine, it corresponds to the sequential time.
%
On a parallel machine, however, work can be split among multiple
processors and thus reduce the time.
%

The interesting question is to what extent can the work be shared.
Ideally we would like the work to be evenly shared.  If we had $W$
work and $P$ processors to work on it in parallel, then even sharing
would imply each processor does $\frac{W}{P}$ work, and hence the
total time is $\frac{W}{P}$.  An algorithm that achieves such ideal
sharing is said to have \defn{perfect speedup}.  Perfect speedup,
however, is not always possible.
%
\begin{question}
Can you come up with an example where perfect speedup is not possible?
\end{question}
%
If our algorithm is fully sequential (each operation depends on prior
operations, leaving no room for parallelism), for example, we can only
take advantage of one processor, and the time would not be improved at
all by adding more.  There is no sharing----at least in parallel.
%
More generally, when executing an algorithm in parallel, we cannot
break dependencies, if a task depends on another task, we have to
complete them in order.
%
\begin{notesonly}
For example, when cooking a frittata, you cannot cook the egg that is
not broken, nor can we add the eggs to the pan until the vegetables
are sauteed.
\end{notesonly}


The second measure, \defn{span}, enables analyzing to what extent the
work of an algorithm can be split among processors.  The \defn{span}
of an algorithm basically corresponds to the longest sequence of
dependences in the computation.  It can be thought of the time an
algorithm would take if we had an unlimited number of processors on an
ideal machine.  
%

As we shall see in \secref{design::scheduling}, even though work and
span, are abstract machine-independent models of cost, they can be
used to predict the run-time on any number of processors.
Specifically, if for an algorithm the work dominates, i.e., is much
larger than, span, then we expect the algorithm to deliver good
speedups.

%
\begin{example}
\label{ex:mergesortcost}
As an example, consider the parallel \cd{mergeSort} algorithm for
sorting a sequence of length $n$.  The work is the same as the
sequential time, which you might know is
\[
W(n) = O(n \lg{n}).
\] 
%
In Chapter~\ref{ch:divide-and-conquer} we will see that the span for
\cd{mergeSort} is
\[
S(n) = O(\lg^2{n}).
\]

Thus, when  sorting a million keys, 
work is $10^6\lg (10^6) > 10^7$, and 
%
span is 
$\lg^2(10^6) < 500.$
%
This means that we would expect to get good (close to perfect)
speedups when using a small to moderate number of processors, e.g.,
couple of tens or hundreds, because the work term will dominate.
%
We should note that in practice, the numbers might be more
conservative due to natural overheads of parallel execution.

\end{example}


\begin{question}
How do we calculate the work and span of an algorithm?
\end{question}

In this book we calculate the work and span of algorithms in a very
simple way that just involves composing costs across subcomputations.
%
Basically we assume that sub-computations are either composed
sequentially (one must be performed after the other) or in parallel
(they can be performed at the same time).
%
We then calculate the work as the sum of the work of the
subcomputations and the span as the sum of the span of sequential
subcomputations or maximum of the work of the parallel
subcomputations.
%
More concretely, given  two subcomputations, we can
calculate the work and the span of their sequential and parallel
composition as follows. 


\begin{center}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{rcc}
\toprule
                          &  \bf $W$ (Work) & \bf $S$ (span)\\
\midrule
\bf Sequential composition & $1 + W_1 + W_2$ & $1 + S_1+ S_2$\\
\midrule
\bf Parallel composition   & $1 + W_1 + W_2$ & $1 + \max(S_1, S_2)$\\
\bottomrule
\end{tabular}
\end{center}

\noindent
In the table,  $W_1$ and $S_1$ are the work and span of the first subcomputation
and $W_2$ and $S_2$ of the second.  The $1$ that is added to each rule
is the cost of composing the subcomputations.

The intuition behind these rules is that work simply adds, whether we
perform computations sequentially or in parallel.  The span, however,
only depends on the span of the maximum of the two parallel
computations.  It might help to think of work as the total energy
consumed by a computation and span as the minimum possible time that
the computation requires.  Regardless of whether computations are
performed serially or in parallel, energy is equally required; time,
however, is determined only by the slowest computation.

\begin{example}
Suppose that we have $30$ eggs to cook using $3$ cooks.  Whether all
$3$ cooks to do the cooking or just one, the total work remains
unchanged: $30$ eggs need to be cooked.
%
Assuming that cooking an egg takes $5$ minutes, the total work
therefore is $150$ minutes.
%
The span of this job corresponds to the longest sequence of
dependences that we must follow.
%
Since we can, in principle, cook all the eggs at the same time, 
span is 5 minutes.
%

Given that we have $3$ cooks, how much time do we actually need?
%
The greedy scheduling principle tells us that we need no more that
$150/3 + 5 = 55$ minutes. That is almost a factor $3$ speedup over the
$150$ that we would need with just one cook. 
%

How do we actually realize the greedy schedule?  In this case, this is
simple, all we have to do is divide the eggs equally between our
cooks.

\end{example}


If algorithm $A$ has less work than algorithm $B$, but has greater
span then which algorithm is better?  In analyzing sequential
algorithms there is only one measure so it is clear when one algorithm
is asymptotically better than another, but now we have two measures.
In general the work is more important than the span.  
%
This is because the work reflects the total cost of the computation
(the processor-time product).  Therefore typically the goal is to
first reduce the work and then reduce the span by designing
asymptotically work-efficient algorithms that perform no work
than the best sequential algorithm for the same problem. 
%
However, sometimes it is worth giving up a little in work to gain a
large improvement in span.
%

\begin{definition}[Work Efficiency]
\label{def:intro::work-efficiency}
We say that a parallel algorithm is \defn{asymptotically work
  efficient} or, simply \defn{work efficient}, if the work is asymptotically
the same as the time for an optimal sequential algorithm that solves
the same problem. 
\end{definition} 

For example, the parallel mergeSort described in \exref{mergesortcost} is work efficient since it does $O(n
\log n)$ work, which optimal time for comparison based sorting.  In
this course we will try to develop work-efficient or close to
work-efficient algorithms.


\section{Specification, Problem, Implementation}

Problem solving in computer science requires reasoning precisely about
problems being studied and the properties of solutions.
%
To facilitate such reasoning, in this book, we define problems by
specifying them and describe the desired properties of solutions at
different levels of abstraction, such as the cost and the
implementation  of the solution.

In this book, we are usually interested in two distinct classes of
problems: algorithms problems and data structures problems.

\paragraph{Algorithm Specification.}

We specify an algorithm by describing what is expected of the
algorithm via an \defn{algorithm specification}.
%
For example, we can specify a sorting algorithm for sequences with
respect to a given comparison function as follows.

\begin{algospec}[Comparison Sorting]
Given a sequence $A$ of $n$ elements taken from a totally ordered set
with comparison operator $\leq$, return a sequence $B$ containing the
same elements but such that $B[i] \leq B[j]$ for $0 \leq i < j < n$.
\end{algospec}

The specification describes \defn{what} the algorithm should do but it
does not describe \defn{how} it achieves what is asked.
%
This is intentional because there can be many algorithms that meet a
specification.
%
A crucial property of any algorithm is its resource requirements or
its \defn{cost}.
%
For example, of the many ways algorithms for sorting a sequence, we
may prefer some over the others.  
%
We specify the cost of class of algorithms with a \defn{cost
  specification}.  For example, the following cost specification
states that a particular class of parallel sorting algorithms performs
$O(n \log{n})$ work and $O(\log^2{n})$ span.

\begin{simpleexample}
Why is this important? If an algorithm does not solve the problem that
we are interested in, it is useless to us.  A specification makes sure
that the algorithm solves the right problem.

For example, imagine yourself waking up on a Sunday morning.  It has
been a long week and you have worked a lot.  All you want to do this
Sunday morning is to sleep a bit more and go out with your friends to
have brunch.  Your stomach has not been feeling great lately, however,
so you want to avoid heavy food, such as a Quiche Lorraine.

So before you hit the bed again to get some more sleep, you call up
your friends and tell them you want to go out to brunch today.  And
that you want to avoid certain types of foods such as a quick loraine
so that they don't start getting excited about a greasy spoon
restaurant that you can't go. 

By describing your requirements you have specified them the problem.
So if they suggest to go somewhere you know that they have understood
your constraints.
\end{simpleexample}

\begin{simpleexample}
Now, happy that you got yourself across you go to take an hour nap
and call up your friends again to see if they have made any
suggestions about where to go.  

Ohh yeah.  They tell you, there is this super cool restaurant that
does all sorts of creative combination by mixing flavors from
different parts of the world and offers them on plate, almost as
elegant compositions.  The newspaper critics say that the food is so
good that it is a must go.  To top it off, there is also a live Jazz
on Sundays.

.Question
**********
What is the first question that comes to your mind?
**********

How much does this cost? Excellent question.  You realize you really
ought to have told that you don't want to spend too much money on the
brunch.  

In computer science as in many other parts of life, cost is a crucial
factor.  In computer science, the kinds of costs that we are
interested include work, span, and memory consumption that we are
willing to accommodate.  


Now going back to the less delicious topic of sorting.  We can give a
cost specification as follows.
\end{simpleexample}


\begin{costspec}[Comparison Sort: Work-Efficient and Parallel]
Assuming the comparison function $<$ does constant work, the cost for
parallel comparison sorting a sequence of length $n$ is $O(n \log n)$
work and $O(\log^2 n)$ span.
\end{costspec}

There can be many cost specifications for sorting.  For example, if we
are not interested in parallelism, we can specify $O(n \log{n})$ work
but no bounds on the span.  Here is another specification that
requires even smaller span but allows for more work.

\begin{costspec}[Comparison Sort: Fast Parallel]
Assuming the comparison function $<$ does constant work, the cost for
parallel comparison sorting a sequence of length $n$ is $O(n^2)$ work
and $O(\log n)$ span.
\end{costspec}
%
As we discussed, we usually care more about work and thus would prefer
the first specification; there might, however, be cases where the
second specification is preferable.


\paragraph{Data Structure Specification.}

We specify a data structure by describing what is expected of the data
structure via an \defn{Abstract Data Type (ADT) specification}.
%
For example, we can specify a priority queue ADT as follows.

\begin{datatype}[Priority Queue]
A priority queue consists of a priority queue type and supports three
operations on values of this type.  The operation \cd{empty} returns
an empty queue.  The operation \cd{insert} inserts a given value with
a priority into the queue and returns the queue.  The operation
\cd{removeMin} removes the value with the smallest priority from the
queue and returns it.
\end{datatype}

As with algorithms, we usually give cost specifications to data
structures.  The following cost specification describes a class of
basic priority queue data structures.
%
\begin{costspec}[Priority Queue: Basic]
The work and span of a priority queue operations are as follows.
\begin{itemize}
\item \cd{create}: $O(1)$, $O(1)$.
\item \cd{insert}: $O(\log{n})$, $O(\log{n})$.
\item \cd{removeMin}: $O(\log{n})$, $O(\log{n})$.
\end{itemize}
\end{costspec}


\paragraph{Problem.}
%
A \defn{problem} requires meeting an algorithm or an ADT specification
and a corresponding cost specification.
%
Since we allow specifying algorithms and data structures, we can
distinguish between algorithms problems and data-structure problems.
%
An \defn{algorithms problem} requires designing an algorithm that
satisfies the given algorithm specification and cost specification if
any.
%
A \defn{data-structures problem} requires meeting an ADT
specification by designing a data structure that can
support the desired operations with the required efficiency specified
by the cost specification.
%
The difference between an algorithms problem and a data-structures
problem is that the latter involves designing a data structure and a
collection of algorithms, one for each operation, that operate on that
data structure.

When we consider problems, it is usually clear from the context
whether we are talking about algorithms or data structures.
%
In such cases, we use the simpler terms \defn{specification} and
\defn{problem} to refer to the algorithm/ADT specification and the
corresponding problem respectively.

\paragraph{Implementation.}
We can solve an algorithms or a data-structures problem by presenting
an \defn{implementation}.  
%
The term \defn{algorithm} refers to an implementation that solves an
algorithms problem and the term \defn{data structure} to
refer to an implementation that solves a data-structures problem.
%
We note that while the distinction between problems and algorithms is
common in the literature, the distinction between abstract data types
and data structures is less so.
%
\begin{todo}
Why do we think this distinction is important?
\end{todo}

We describe an algorithm by using the pseudo-code notation based on
\pml, the language  used in this book.  For example, we can specify
the classic insertion sort algorithm as follows.
%
\begin{algorithm}[Insertion Sort]~
\begin{lstlisting}[numbers=none]
insSort $f$ $s$ = 
  if |$s$| = 0 then $\cseq{}$
  else insert $f$ $s$[0]  (insSort $f$ (s[1,...,n-1]))
\end{lstlisting}
\end{algorithm}

In the algorithms, $f$ is the comparison function and $s$ is the input
sequence.  
%
The algorithm uses a function (\cd{insert $f$ $x$ $s$}) that
takes the comparison function $f$, an element $x$, and a sequence $s$
sorted by $f$, and inserts $x$ in the appropriate place.  
%
Inserting into a sorted sequence is itself an algorithms problem,
since we are not specifying how it is implemented, but just specifying
its functionality.  
%
We might also be given a cost specification for \cd{insert}, e.g., for
a sequence of length $n$ the cost of \sml{insert} should be $O(n)$
work and $O(\log n)$ span.  
%
Given this cost we can determine the overall asymptotic cost of
\sml{sort} using our composition rules described in the last section.
%
Since the code uses \cd{insert} sequentially and since there are $n$
inserts, the algorithm \cd{insSort} has $n \times O(n) =
O(n^2)$ work and $n \times O(\log n) = O(n \log n)$ span.

Similarly, we can specify a data structure by specifying the data type
used by the implementation, and the algorithms for each operation.
%
For example, we can implement a priority queue with a binary heap data
structure and describe each operation as an algorithm that operates on
this data structure.  In other words, a data structure can be viewed
as a collection of algorithms that operate on the same organization
of the data.


\begin{question}
Why is it important to distinguish between specification and implementations?
\end{question}


\begin{remark}[On the importance of specification]
%
Several reasons underline the importance of distinguishing between
specification and implementation.  First, we want to be able to use a
specification without knowing the details of an implementation that
matches that specification.
%
In many cases the specification of a problem is quite simple, but an
efficient algorithm or data structure that solves it, i.e., the
implementation, is complicated.  
%
Specifications allow us abstract from implementation details.
%
Second, we want to be able to change or improve implementations over
time.  As long as each implementation matches the same specification,
and the user relied only on the specification, then he or she can
continue using the new implementation without worrying about their
code breaking. 
%
Third, when we compare the performance of different
algorithms or data structures it is important that we are not
comparing apples with oranges.  We have to make sure the algorithms we
compare are solving the same problem, because subtle differences in
the problem specification can make a significant difference in how
efficiently that problem can be solved.
%%
%% For these reasons, in this book we emphasize defining precise and
%% concise specifications and then implementing those specifications
%% using algorithms and data structures.  When discussing solutions to
%% problems we emphasize general techniques that can be used to design
%% them, such as divide-and-conquer, the greedy method, dynamic
%% programming, and balance trees. 
\end{remark}


%% It is important that in this course you learn how to design your own
%% algorithms and data structures given a specification, and even how to
%% specify your own problems and ADTs given a task at hand.



\newpage
\input{introduction/problems}

\flushchapter



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Umut: Did a major rewrite of this section.  Keeping the old copy
%% here for reference.

%% \paragraph{Specification and cost specification.}
%% We define an algorithmic problem, or simply a problem, by giving a
%% \defn{specification} for the problem.  
%% %
%% A specification describes what the problem requires from us. For
%% %
%% A crucial property of any solution to a problem is its resource
%% requirements or its \defn{cost}.
%% %
%% For example, of the many ways to sort a sequence, we may prefer some
%% over the others.  We specify the cost of the solution with a
%% \defn{cost specification}.  For example, the following cost
%% specification states that a particular class of parallel sortings
%% algorithm perform $O(n \log{n})$ work and $O(\log^2{n})$ span.

%% \begin{costspec}[Comparison Sort: Work-Efficient and Parallel]
%% Assuming the comparison function $<$ does constant work, the cost for
%% parallel sorting on a sequence of length $n$ is $O(n \log n)$
%% work and $O(\log^2 n)$ span.
%% \end{costspec}

%% There can be many cost specification for sorting.  For example, if we
%% are not interested in parallelism, we can specify $O(n \log{n})$ work
%% but no bounds on the span.  Here is another specification that
%% requires even smaller span but allows for more work.

%% \begin{costspec}[Comparison Sort: Fast Parallel]
%% Assuming the comparison function $<$ does constant work, the cost for
%% parallel sorting a sequence of length $n$ is $O(n^2)$ work and
%% $O(\log n)$ span.
%% \end{costspec}

%% As we discussed, we usually care more about work and thus would prefer
%% the first specification; there might, however, be cases where the
%% second specification is preferable.

%% Another common class of problems require constructing some data and
%% operating on it by using some operations. 
%% %
%% We specify such problems by describing what each operation does as
%% part of an \defn{abstract data type} definition.  We use the
%% abbreviation \defn{ADT} to refer to an abstract data type.  We can
%% specify a priority queue ADT as follows.

%% \begin{datatype}[Priority Queue]
%% A priority queue consists of a priority queue type and supports three
%% operations on values of this type.  The operation \cd{empty} returns
%% an empty queue.  The operation \cd{insert} inserts a given value with
%% a priority into the queue and returns the queue.  The operation
%% \cd{removeMin} removes the value with the smallest priority from the
%% queue and returns it.
%% \end{datatype}

%% As with a problem, we can give a cost specification to a priority
%% queue ADT.  The following cost specification describes a basic
%% priority queue.
%% %
%% \begin{costspec}[Priority Queue: Basic]
%% The work and span of a priority queue operations are as follows.
%% \begin{itemize}
%% \item \cd{create}: $O(1)$, $O(1)$.
%% \item \cd{insert}: $O(\log{n})$, $O(\log{n})$.
%% \item \cd{removeMin}: $O(\log{n})$, $O(\log{n})$.
%% \end{itemize}
%% \end{costspec}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Umut: Did a major rewrite of this section.  Keeping the old copy
%% here for reference.
%%
%% \section{Specification and Implementation}
%%
%% In this course we try to carefully distinguish between specifications
%% and implementation.  
%% %
%% We do this both for individual functions (e.g. a sorting function),
%% and for data structures, which can be viewed as a collection of
%% functions that act on a common data type (e.g. a set of operations on
%% a priority queue).  
%% %
%% A \defn{specification} (sometimes called \defn{interface}) defines
%% precisely what we want of a function or of the data.
%% %
%% An \defn{implementation} describes how to meet the specification.  
%% %
%% In other words specifications and implementations refer to the {\em
%%   what} and the \defn{how}: what we want a function or data type to
%% achieve and how to do that.  
%% %
%% The goal of the specification is to abstract away from the specific
%% implementation and capture just what is needed to use a function.
%% There may be many ways to implement an abstract
%% specificantion---i.e. many hows for a given what.


%% What do we need to include in a specification?  Certainly we need to
%% specify what a function does, i.e., its behavior.
%% %
%% For example, we can specify that a function squares an integer and
%% another sorts a sequence of numbers.  
%% %

%% In addition to the behavior of functions, we also care about their
%% cost.
%% %
%% For example, we might want to specify the work and span of sorting
%% $n$~integers (asymptotically).
%% %
%% We refer to a specification that states cost bounds as a \defn{cost
%%   specification}.
%% %
%% It might appear, that cost specification is specific to an
%% implementation. 
%% %
%% This is not quite true, because there can be many ways of meeting the
%% same cost specification.  For example, we can sort a set of natural
%% numbers is $O(n \log n)$ work and $O(\log^2 n)$ span in many ways.
%% %
%% As a user of a sort function, we don't care about exactly which
%% implementation is used as long as the cost bounds are met.  
%% %
%% Cost specifications thus allow us to specify an important property of
%% the implementation---its cost, without having to control its details.

%% To sum up, we have considered three levels of abstraction---the
%% functional or behavioral specification, the cost specification, and
%% matching implementations.  
%% %
%% There might be multiple cost specifications for a given functional
%% specification, and then multiple implementations that match each cost
%% specification.  
%% %

%% Different levels of abstraction can be applied to functions or to
%% data, more precisely, to collections of functions or operations on a
%% common data type.
%% %
%% In the case of a function, we refer to the functional specification as
%% an \defn{algorithmic problem} or simply as a \defn{problem} and an
%% implementation as an \defn{algorithm}.
%% %
%% In the case data, we refer to the functional specification as a
%% \defn{abstract data type} and an implementatio

%% %% \begin{center}
%% %%   \centering
%% %%   \begin{tabular}{l  c c c}
%% %%     \toprule
%% %%     & \multicolumn{2}{c}{\textbf{Specification}} & \textbf{Implementation}\\
%% %%     & \textbf{Functionality} & \textbf{Cost} &  \\
%% %%     \midrule
%% %%     \textbf{Functions} & Problem & Function Cost & Algorithm \\
%% %%     \textbf{Data} & Data Type & Operation Costs & Data Structure \\
%% %%     \bottomrule
%% %%   \end{tabular}
%% %% \label{tab:specify-design}
%% %% \end{center}

%% We now consider some examples to clarify these ideas.  The first
%% example concerns sorting. We can specify the sorting problem  and its
%% cost specification as follows.

%% \begin{problem}[Comparison Sorting]
%% Given a sequence $S$ of $n$ elements taken from a totally ordered set
%% with comparison $\leq$, return a sequence $R$ containing the same
%% elements but such that $R[i] \leq R[j]$ for $0 \leq i < j < n$.
%% \end{problem}



%% \begin{costspec}[Parallel Sort]
%% The cost for parallel sorting on a sequence of length $n$ and assuming
%% the comparison function $<$ does constant work, is $O(n \log n)$ work
%% and $O(\log^2 n)$ span.
%% \end{costspec}

%% Note that in the cost specification we had to be clear about the cost
%% of the comparison operation.  In this book we make significant use of
%% functions that take other functions as arguments.  We therefore will
%% often have to be careful about the cost of those functions.  We might
%% provide another cost specificaiton for the sorting problem.

%% \begin{costspec}[Shallow Sorting]
%% The cost for parallel sorting on a sequence of length $n$ and assuming
%% the comparison function $<$ does constant work, is $O(n^2)$ work
%% and $O(\log n)$ span.
%% \end{costspec}
%% We note that this specification has asymptotically larger work but
%% smaller span.  As discussed in the last section, we usually care more
%% about work, we thus would prefer the first specification; there might,
%% however, be cases where the second specification is preferable.

%% We can now specify an algorithm, such as the following insertion sort.
%% \begin{algorithm}[Insertion Sort]~
%% \begin{lstlisting}[numbers=none]
%% sort$(f,S)$ = 
%%   if $|S| = 0$ then $\cseq{}$
%%   else insert$(f,S[0],\sml{sort}(S[1,\ldots,n-1])$
%% \end{lstlisting}
%% \end{algorithm}

%% where $f$ is the comparison function and $S$ is the input sequence.
%% The algorithm uses a function \sml{insert$(f,e,S)$} that takes the
%% comparison function $f$, an element $e$, and a sequence $S$ sorted by
%% $f$, and inserts $e$ in the appropriate place.  This is itself an
%% algorithmic problem, since we are not specifying how it is
%% implemented, but just specifying its functionality.  We might also be
%% given a cost specification, and let's say for a sequence of length $n$
%% the cost of \sml{insert} is $O(n)$ work and $O(\log n)$ span.  Given
%% this cost we can determine the overall asymptotic cost of \sml{sort}
%% using our composition rules described in the last section.  Since the
%% code does the inserts one after the other sequentially, we need to add
%% both the work and span.  Since there are $n$ inserts the algorithm
%% will have $n \times O(\times n) = O(n^2)$ work and $n \times O(\log n)
%% = O(n \log n)$ span.

%% Similarly an \defn{abstract data type} (ADT) specifies precisely an
%% interface for operating on data in an abstract form.  The
%% specification will typically consist of a set of functions for
%% accessing or manipulating the particular data type along with a
%% definition of what each function does.  The specification, however,
%% does not specify how the data is structured or how the functions are
%% implemented.  This is hidden by the ADT.  A \defn{data structure}, on
%% the other hand, implements the specification by organizing the data in
%% a particular form, typically in a way that allows an efficient
%% implementation of the functions.
%% \begin{example}
%% For example, a priority queue is an ADT with functions that might
%% include \texttt{insert}, \texttt{findMin}, and \texttt{isEmpty}.
%% Various data structures can be used to implement a priority queue,
%% including binary heaps, arrays, and balanced binary trees. 
%% \end{example}
%% Some ADTs we will cover in this \bcswitch{book}{course} include:
%% {sequences, sets, ordered sets, tables, priority queues, and graphs}.

%% The terminology ADTs versus data structures is not as widely used as
%% problems versus algorithms.  In some other textbooks the term data
%% structure is used to refer to both the specification and the
%% implementation.  We will try to avoid such ambiguous usage in this
%% book.

%% \begin{question}
%% Why is it important to distinguish between specification and implementations?
%% \end{question}

%% Several reasons underline the importance of distinguishing between
%% specification and implementation.  First, we want to be able to use a
%% specification without knowing the details of an implementation that
%% matches that specification.
%% %
%% \begin{teachingonly}
%% An analogy: this is like being able to obtain healthcare without
%% having to study (understand) medicine.
%% \end{teachingonly}
%% %
%% In many cases the specification of a problem is quite simple, but an
%% efficient algorithm or data structure that solves it, i.e., the
%% implementation, is complicated.  
%% %
%% Specifications allow us abstract from implementation details.
%% %
%% Second, we want to be able to change or improve implementations over
%% time.  As long as each implementation matches the same specification,
%% and the user relied only on the specification, then he or she can
%% continue using the new implementation without worrying about their
%% code breaking. 
%% %
%% Third, when we compare the performance of different
%% algorithms or data structures it is important that we are not
%% comparing apples with oranges.  We have to make sure the algorithms we
%% compare are solving the same problem, because subtle differences in
%% the problem specification can make a significant difference in how
%% efficiently that problem can be solved.

%% For these reasons, in this \bcswitch{book}{course} we will put a
%% strong emphasis on defining precise and concise specifications and
%% then implementing those specifications using algorithms and data
%% structures.  When discussing solutions to problems we will emphasize
%% general techniques that can be used to design them, such as
%% divide-and-conquer, the greedy method, dynamic programming, and
%% balance trees.  \bcswitch{}{It is important that in this course you
%%   learn how to design your own algorithms and data structures given a
%%   specification, and even how to specify your own problems and ADTs given
%%   a task at hand.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
