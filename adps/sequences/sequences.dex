\documentclass{course}
\title{Parallel and Sequential Algorithms}

% Course number must be unique in the database
\coursenumber{15210}

\semester{Spring 2018}
\picture{/210/course/air-pavilion.jpg}
\website{http://www.cs.cmu.edu/~15210}

% Provides book
% This must be provided
% The name should be relative to course number.
\providesbook{S18}

% Start counting chapters from 
% This is optional. Will start counting at 1.
\provideschapter{8}
\providessection{1}

15-210 aims to teach methods for designing, analyzing, and programming
sequential and parallel algorithms and data structures. The emphasis
is on teaching fundamental concepts applicable across a wide variety
of problem domains, and transferable across a reasonably broad set of
programming languages and computer architectures. This course also
includes a significant programming component in which students will
program concrete examples from domains such as engineering, scientific
computing, graphics, data mining, and information retrieval (web
search).

Unlike a traditional introduction to algorithms and data structures,
this course puts an emphasis on parallel thinking â€” i.e., thinking
about how algorithms can do multiple things at once instead of one at
a time. The course follows up on material learned in 15-122 and 15-150
but goes into significantly more depth on algorithmic issues. Concepts
covered in this class include:


\begin{book}
\title{Algorithm Design: Parallel and Sequential}
\authors{Umut A. Acar and Guy Blelloch}

\begin{chapter}[Sequences]
\label{ch:sequences}

\picture{./media/water-town-suzhou-embroidery.jpg}

A sequence is an ordered set, i.e., is a collection of elements that
are totally ordered. 
%
Computer scientists use sequence data grams such as arrays and
lists to represent many different sorts of data.
%


This chapter defines the syntax and the semantics of sequences and
presents cost specifications and implementation strategies for
matching them.

\begin{section}[Semantics]
\label{sec:sequences}

Thi section specifies the syntax and the semantics for a
sequence abstract data type.

\begin{unit}[Defining Sequences]

\begin{gram}
Mathematically, a sequence is an enumerated collection.  As with a
set, a sequence has~\defn{elements}.  The~\defn{length} of the
sequence is the number of elements in the sequence.  A sequence can be
finite or infinite; we only consider finite ones.
%

Sequences allow for repetition: an element can appear at multiple
positions.
%
The position of an element is called its~\defn{rank} or its~\defn{index}.
%
Traditionally, the first element of the sequence is given rank $1$,
but, being computer scientists, we start at $0$.
% 
Mathematically, we can define a sequence as a function whose domain is
a contiguous set of natural numbers starting at zero.
%
\end{gram}

\begin{group}
\begin{definition}[Sequences]

\label{def:seq::sequences}

An~\defn{$\alpha$ sequence} is a mapping (function) from $\nats$ to
$\alpha$ with domain $\cset{0, \ldots, n-1}$ for some $n \in \nats$.
%
\end{definition}

\begin{example}
\label{ex:seq::sequencesdef}

\parent{def:seq::bayesian-inference}

Let $A = \cset{0,1,2,3}$ and $B = \cset{\cstr{a},\cstr{b},\cstr{c}}$.
The relation
\[R = \cset{(0,\cstr{a}),(1,\cstr{b}),(3,\cstr{a})}\]
is a function from $A$ to $B$ with domain $\{0,1,3\}$ since each
element only appears once on the left. It is, however, not a sequence
since there is a gap in the domain.

The relation
\[Z = \{(1,\cstr{b}), (3,\cstr{a}), (2,\cstr{a}), (0,\cstr{a})\}\]
from~$A$ to $B$ is a sequence.
%
The first element of the sequence is $\cstr{a}$ and thus has rank $0$.
The second element is $\cstr{b}$ and has rank $1$.  
%
The length of the sequence is $4$.
%
\end{example}
\end{group}

\begin{note}
This mathematical definition might seem pedantic but it is useful for
at least a couple reasons: it allows for a concise but yet precise
definition of the semantics of the functions on sequences, and we will
see, relating sequences to mappings creates a symmetry with the
abstract data types such as tables or dictionaries for representing
mappings.
%
An important point to notice in the definition is that sequences are
parametrized by the type (i.e., set of possible values) of their
elements.
\end{note}


%
\begin{group}
\begin{syntax}[Sequences and Indexing]
As in mathematics, we use a special notation for writing sequences.
%
The notation
%
$\cseq{a_0,a_1,\ldots,a_{n-1}}$
%
is shorthand for the sequence
$\cset{(0,a_0),(1,a_1),\ldots,((n-1),a_{n-1})}$.
%

For a sequence $a$
\begin{itemize}
\item  $a[i]$ refers to the element of $a$ at
position $i$,

\item $a\cirange{l}{h}$ refers to the subsequence of $a$ restricted to
  the position between $l$ and $h$.
\end{itemize}

Since they occur frequently, we use special notation and terminology
for sequences with two elements and sequences of characters.
%
\begin{itemize}
\item
An~\defn{ordered pair $(x,y)$} is a pair of elements in which the
element on the left, $x$, is identified as the~\defn{first} entry, and
the one on the right, $y$, as the~\defn{second} entry.
\item
We refer to a sequence of characters as a~\defn{string}, and use the
standard syntax for them, e.g., $\mstr{c_0c_1c_2 \ldots c_{n-1}}$ is a
string consisting of the $n$ characters $c_0, \ldots, c_{n-1}$.
\end{itemize}
\end{syntax}
%

\begin{example}[Example sequences]

Some example sequences follow.


\begin{itemize}
\item For the sequence $a =  \cseq{2,3,5,7,11,13,17,19,23,29}$, we have


\begin{itemize}

\item $a[0] = 2$, 
\item $a[2] = 5$, and 
\item
$a\cirange{1}{4} = \cseq{3,5,7,11}$.
\end{itemize}
%
\item 
A character sequence, or a string:
$\cseq{\chr{s},\chr{e},\chr{q}} \equiv \str{seq}~.$
%
\item

An integer-and-string sequence:
$\cseq{(10,\str{ten}),(1,\str{one}),(2,\str{two})}.$
%
\item 
A string-and-string-sequence sequence: $\cseq{\cseq{\str{a}},\cseq{\str{nested}, \str{sequence}}}.$
%
\item
A $\tyint \rightarrow \tyint$ function sequence:
\[
\begin{array}{ll}
\cseqb & \cfn{x}{x^2},
\\
& \cfn{y}{y+2},
\\
& \cfn{x}{x-4}
\\
\cseqe.
\end{array}
\]
\end{itemize}
\end{example}
\end{group}

%\begin{quiz}[Basic Quiz]

%% \begin{problemmc}[Pick one]
%% \parent{deq:seq}
%% \label{p:easy}


%% \points 
%% 10

%% \prompt
%% This is the text of the problem.  Pick all that apply:

%% \explain
%% This is some more explanation.

%% \hint
%% This is a hint.

%% \solution
%% Choice 1 and 3 are correct, because $1 + 1 = 2$ and therefore, choice
%% 2 cannot be right (otherwise this question would have been to
%% easy). Since at least one of these must be correct (otherwise the quiz
%% software would crash), we have between 1 and 3 to choose from.  But
%% why choose, when you can have both.  So we conclude 1 and 3.

%% \begin{choice}[10]
%% This is choice 1. 
%% This has three sentences on different lines.
%% It has a hint and an explanation

%% \explain
%% Of course.

%% \hint
%% Go to lectures.

%% \end{choice}

%% \begin{choice}
%% This is choice 2 with no points. This has four sentences.  
%% It has no hint and no explanation
%% This here is the last sentence on a second line. 
%% \end{choice}

%% \begin{choice}[10]
%% This is choice 3. It has a hint but no explanation.

%% \hint
%% This as a hint. There can be many sentences.
%% Multiple lines.
%% \end{choice}

%% \end{problemmc}



%% \begin{problemmc}[Pick another]
%% \label{p:easy}
%% \parent{deq:seq}

%% \points 
%% 12

%% \prompt
%% This is the text of the problem.  Pick all that apply:


%% \hint
%% This as a hint. There can be many sentences.
%% Multiple lines.

%% \solution 
%% We apply exactly the same logic as in problem 1. But we notice that
%% that was a hard problem.  To counterbalance that this time we keep
%% things simple: Since  $1 + 1 = 2$, choice 2 must be correct.

%% \begin{choice}
%% This is choice 1. 
%% This has two sentences on different lines.
%% \end{choice}

%% \begin{choice}
%% This is choice 2. This has three sentences.  
%% This here is the last sentence on a second line.
%% \end{choice}

%% \begin{choice}
%% This is choice 3.
%% \end{choice}

%% \end{problemmc}

%% \begin{problemfr}[free response problem]
%% \label{this}
%% \parent{deq:seq}

%% \points 
%% 10

%% \prompt
%% Describe the limitations of the PRAM model of computation for
%% specifying parallel algorithms.

%% \solution
%% There are several limitations but perhaps the most important
%% one is that the number of processors is fixed.  This is probably
%% because the theoreticians that proposed the model did want to deal
%% with the facts of life, such as the Moore's law, which might have
%% sounded like a marketing ploy at the time.


%% There are several problems.

%% \begin{enumerate}
%% \item problem 1
%% \item problem 2
%% \item problem 3
%% \end{enumerate}

%% \hint
%% Think about what happens when you need to specify an algorithm in the
%% PRAM model.

%% \end{problemfr}

%\end{quiz}

\end{unit}

\begin{unit}[The Sequence Abstract Data Type]
\label{sec:seq::adt}

\begin{gram}
%
%
%, and later is
%\secref{seq::spec} specify these operations more carefully
%
Sequences are one of the most prevalent ADT's used in this book, and
more generally in computing.  
%
In this section, we present the interface of an abstract data type for
sequences, describe the semantics of the operations in the interface,
and define a notation for sequences, called~\defn{sequence
  comprehensions}.
%
Throughout, we use $e$ and its variants to for \PML expression.
%
When specifying the semantics of operation, we rely on the
mathematical definition of a sequence as a partial function whose
domain is natural numbers.
\end{gram}

\begin{gram}
The sequence ADT, shown below, can be broadly divided into several
categories.
\begin{itemize}
\item 
Operations such as  $\cd{nth}$ and  $\cd{length}$  that return an
element of the sequence or a particular property of it.
%
\item Constructors such as $\cd{empty}$, $\cd{singleton}$, and $\cd{tabulate}$ that create
sequences.
%
\item 
Operations such as $\cd{map}$, $\cd{filter}$ that operate on each
element of a sequence independently in parallel.
%
\item 
Operations such as $\cd{append}$ and $\cd{flatten}$ that operate on
sequences as a whole.
%
\item 
Operations such as $\cd{update}$ and $\cd{inject}$ that updates the
elements of a sequence.
%
\item 
Operations such as $\cd{iterate}$, $\cd{reduce}$, and $\cd{scan}$ that~\defn{aggregate} information over the elements of the sequence.
\end{itemize}
\end{gram}



\begin{datatype}[Sequences]
\label{adt:seq::sequences}
For a value type $\alpha$, the~\defn{sequence data type} is the type
$\sseq{\alpha}$ consisting of the set of all $\alpha$ sequences, and
the following values and functions on $\sseq{\alpha}$.
%
In the specifiction, we define booleans as $\tybool = \{\ctrue{},
\cfalse{}\}$ and orders as $\mathcal{O} = \{\cd{less}, \cd{greater},
\cd{equal}\}$.
%
\[
\begin{array}{lcl}
\cd{length}
& : &
\sseq{\alpha} \rightarrow \tynat
\\
\cd{nth}
& : & 
\sseq{\alpha} \rightarrow \tynat \rightarrow \alpha 
\\
\cd{empty}
& : & 
\sseq{\alpha} 
\\
\cd{singleton}
& : & 
\alpha \rightarrow \sseq{\alpha}
\\
\cd{tabulate}
& : & (\tynat \rightarrow \alpha) \rightarrow \tynat \rightarrow \sseq{\alpha}
\\
\cd{map}
& : & (\alpha \rightarrow \beta) \rightarrow \sseq{\alpha} \rightarrow \sseq{\beta}
\\
\cd{subseq}
& : & 
\sseq{\alpha} \rightarrow \tynat \rightarrow \tynat \rightarrow \sseq{\alpha}
\\
\cd{append}
& : & 
\sseq{\alpha} \rightarrow \sseq{\alpha} \rightarrow \sseq{\alpha}
\\
\cd{filter}
& : & (\alpha \rightarrow \tybool) \rightarrow \sseq{\alpha} \rightarrow \sseq{\alpha}
\\
\cd{flatten}
& : & \sseq{\sseq{\alpha}} \rightarrow \sseq{\alpha}
\\
\cd{update}
& : &  \sseq{\alpha} \rightarrow (\tynat \times \alpha) \rightarrow \sseq{\alpha}
\\
\cd{inject}
& : &  \sseq{\alpha} \rightarrow \sseq{\tynat \times \alpha} \rightarrow \sseq{\alpha}
\\
\cd{isEmpty}
& : & \sseq{\alpha} \rightarrow \tybool
\\
\cd{isSingleton}
& : & \sseq{\alpha} \rightarrow \tybool
\\
\cd{iterate}
& : &  
(\alpha \times \beta \rightarrow \alpha)
\rightarrow
\alpha
\rightarrow
\sseq{\beta}
\rightarrow
\alpha
\\
\cd{reduce}
& : &  
(\alpha \times \alpha \rightarrow \alpha)
\rightarrow
\alpha
\rightarrow
\sseq{\alpha}
\rightarrow
\alpha
\\
\cd{scan}
& : &  
(\alpha \times \alpha \rightarrow \alpha)
\rightarrow
\alpha
\rightarrow
\sseq{\alpha}
\rightarrow
(\sseq{\alpha} \times \alpha)
\\
\cd{collect}
& : &  
(\alpha \times \alpha \rightarrow \mathcal{O})
\rightarrow
\sseq{\alpha \times \beta}
\rightarrow
\sseq{\alpha \times \sseq{\beta}}
\end{array}
\]
\end{datatype}

\begin{gram}
Inspired by mathematical notation for sequences, we use a
``sequence comprehensions'' notation, which we  define below.
\end{gram}


\begin{syntax}[Syntax for Sequences]
\label{syn:seq::seq}
The table below defines the syntax for the sequence ADT.
%
In the definition
%
$i$~is a variable ranging over natural numbers,
%
$x$~is a variable ranging over the elements of a sequence,
%
~$e$~is a \PML expression, 
%
$e_n$ and $e_n'$ are \PML expressions whose values are natural numbers, 
%
$e_s$~is a \PML expression whose value is a sequence, 
%
$p$ is a \PML pattern that binds one or more variables.

\medskip

\begin{tabular}{lcl}
$|e_s|$ & $\equiv$ & $\cd{length}$ $e_s$
\\
$e_s[i]$ & $\equiv$ & $\cd{nth}$ $e_s$
\\
$\cseq{}$ & $\equiv$ & $\cd{empty}$
\\
$\cseq{e}$ & $\equiv$ & $\cd{singleton}$ $e$
\\
 $\cseq{e : 0 \leq i < e_n}$  & $\equiv$ & $\cd{tabulate}$ $(\cfn{i}{e})$ ${e_n}$
\\
$\cseq{e : p \in e_s}$ & $\equiv$ & $\cd{map}$ $(\cfn{p}{e})$ $e_s$
\\

$\cseqf{x \in e_s}{e}$ & $\equiv$ & $\cd{filter}$ $(\cfn{x}{e})$ $e_s$

\\
$e_s[e_l,\cdots,e_n']$ & $\equiv$ & $\cd{subseq}$ $(e_s, e_l, e_n'-e_l+1)$
\\
%
$\kwappend{e_s}{e_s'}$ & $\equiv$ & $\cd{append}~e_s~e_s'$
\end{tabular}
\end{syntax}

\begin{gram}[Length and indexing]
Given a sequence $a$, $\cd{length}~a$, also written $\cseqlen{a}$,
returns the length of $a$.
%
The function $\cd{nth}$ returns the element of a sequence at a specified
index, e.g. $\cd{nth}~a~2$, written $a[2]$, returns the element of $a$
with rank $2$.
%
If the element demanded is out of range, the behavior is undefined and
leads to an error.
%
% if for example the index is
% less than $0$ or greater than the rank of the last element, then the
% function returns the special value $\bot$ (bottom), which indicates an
% error or an exception.
%
%% More precisely, we can specify the function \cd{nth} as follows
%% \begin{code}
%% \[
%% \cd{nth}~a~i =
%% \left\{
%% \begin{array}{lc}
%% a[i] & 0 \le i < \cseqlen{a} 
%% \\
%% \bot & \mbox{otherwise.}
%% \end{array} \right.
%% %% 
%% \]
%% \medskip
%% \end{code}
\end{gram}



\begin{gram}[Empty and singleton]
%
The value $\cd{empty}$ is the empty sequence, $\cseq{}$.  The function
$\cd{singleton}$ takes an element and returns a sequence containing that
element, e.g., $\cd{singleton}~1$ evaluates to $\cseq{1}$.
\end{gram}

\begin{gram}[Tabulate]
The function $\cd{tabulate}$ takes a function $f$ and an natural number
$n$ and produces a sequence of length $n$ by applying $f$ at each
position.
%
The function $f$ can be applied to each element in parallel.
%
We specify $\cd{tabulate}$ as follows
%
\[
\begin{alignat}{1}
\cd{tabulate}&~(f: \tynat \rightarrow \alpha)~(n: \tynat) : \sseq{\alpha}
\\
&= \cseq{f(0), f(1), \ldots, f(n-1)}.
\end{alignat}
\]
%
We use the following syntax for $\cd{tabulate}$ operation
%
\[
\equivs
{\cseq{e : 0 \leq i < e_n}}
{\cd{tabulate}~(\cfn{i}{e})~{e_n},}
\]
%
where $e$ and $e_n$ are expressions, the second evaluating
to an integer,  and $i$ is a variable.
%
More generally, we can also start at any other, as in:
\[\cseq{e : e_j \leq i < e_n}. \]
\end{gram}


\begin{gram}[Map]
A common operation on sequences is to apply some computation to each
element of a sequence.  
%
For example we might want to add five to each
element of a sequence.  
%
For this purpose, we can use the operation $\cd{map}$, which takes a
function $f$ and a sequence $a$ and applies the function $f$ to each
element of $a$ returning a sequence of equal length with the results.
%
As with $\cd{tabulate}$, in $\cd{map}$, the function~$f$ can be
applied to all the elements of the sequence in parallel.  
%
We specify the behavior of $\cd{map}$ as follows
\[
\begin{alignat}{1}
\cd{map} &~(f: \alpha  \rightarrow \beta)~(a: \sseq{\alpha}) : \sseq{\beta}
\\
&~= \cset{(i,f(x)) : (i,x) \in a}
\end{alignat}
\]
\smallskip
or equivalently as
\smallskip
\[
\cd{map}~(f: \alpha  \rightarrow \beta)~\cseq{a_1, \ldots, a_{n-1}}: \sseq{\alpha}) : \sseq{\beta}
 = 
\cseq{f(a_1), \ldots, f(a_{n-1})}.
\]

We use the following syntax for the $\cd{map}$ function
%
\[
\equivs
{\cseq{e : p \in e_s}}
{\cd{map}~(\cfn{p}{e})~e_s,}
\]
%
where $e$ and $e_s$ are expressions, the second evaluating to a sequence, and $p$ is a a pattern of
variables (e.g., $x$ or $(x,y)$).
\end{gram}


\begin{gram}[Filter]
To filter out elements from a given sequence, we can use the function
$\cd{filter}$.  The function takes a Boolean function~$f$ and a
sequence~$a$ as arguments and applies~$f$ to each element of~$a$,
returning the sequence consisting exactly of those elements of 
$s \in a$ for which $f(s)$ returns true, and maintaining the order of the
elements returned. 
%
We can specify the behavior of $\cd{filter}$ as follows
%
\[
\begin{alignat}{1}
\cd{filter} & ~(f: \alpha \rightarrow \tybool)~(a: \sseq{\alpha}) : \sseq{\alpha}
=
\\
 & ~\left\{\left(\cseqlen{\csetf{(j,y) \in a}{j < i \land f(y)}}, x\right): (i,x) \in a \mid f(x)\right\}.
\end{alignat}
\]
%
As with $\cd{map}$ and $\cd{tabulate}$, the function $f$ in
$\cd{filter}$ can be applied to the elements in parallel.
%

We use the following syntax for the $\cd{filter}$ function
%
\[
\equivs
{\cseqf{x \in e_s}{e}}
{\cd{filter}~(\cfn{x}{e})~e_s,}
\]
%
where $e$ and $e_s$ are expressions.
%
In the syntax, note the distinction between the colon ($:$) and the
bar ($\mid$).
%
We use the colon to draw elements from a sequence for mapping and we
use the bar to select the elements that we wish to filter.
%
We can use them together, as in: 
%
\[
\begin{alignat}{2}
  \cseqf{e : x \in  e_s}{e_f} & \quad \equiv \quad
   & \cd{map} &~(\cfn{x}{e})
\\
   & & &~(\cd{filter}~(\cfn{x}{e_f})~e_s).
\\
\end{alignat}
\]
%
What appears before the colon (if any) is an expression to apply each
element of the sequence to generate the result; what appears after the
bar (if there is any) is an expression to apply to each element to
decide whether to keep it.
\end{gram}

\begin{example}[Fibonacci]
Given the function $\cd{fib}~i$, which returns the $i^{th}$ Fibonacci
number, the expression:
\[ 
a = \cseq{\cd{fib}~i :  0 \leq i < 9}
\]
%
is equivalent to
%
\[
a = \cd{tabulate}~~\cd{fib}~~9.
\]
%
When evaluated, it returns the sequence
\[
a = \cseq{0, 1, 1, 2, 3, 5, 8, 13, 21, 34}.
\]
%
The expression
\[
\cseq{x^2 : x \in a}
\]
%
is equivalent to
\[
\cd{map}~(\cfn{x}{x^2})~a.
\]
%
When evaluated it returns the sequence:
\[
\cseq{0, 1, 1, 4, 25, 64, 169, 441, 1156}.
\]
%
Given the function $\cd{isPrime}~x$
which checks if $x$ is prime, the expression
\[
\cseq{x : x \in a \sucht \cd{isPrime}~x}
\]
%
is equivalent to
%
\[
\cd{filter}~\cd{isPrime}~a.
\]
When evaluated, it returns the sequence
$
\cseq{2,5,13}.
$
\end{example}


\begin{gram}[Subsequences]
The 
%
$\cd{subseq}(a,i,j)$ 
%
function extracts a contiguous subsequence of $a$
starting at location~$i$ and with length~$j$.  If the subsequence is
out of bounds of~$a$, only the part within~$a$ is returned.
%
We can specify $\cd{subseq}$ as follows
\[
\begin{alignat}{1}
\cd{subseq} &~(a:\sseq{\alpha})~(i:\tynat)~(j:\tynat): \sseq{\alpha}
\\
&~= \cset{(k-i,x) : (k,x) \in a \mid i \leq k < i + j}.
\end{alignat}
\]

%
We use the following  syntax for denoting subsequences
%
\[
\equivs
{a\cirange{e_i}{e_j}}
{\cd{subseq}~(a,e_i,e_j-e_i+1)}.
\]
\end{gram}


\begin{gram}[Splitting sequences]
As we shall see in the rest of this book, many algorithms operate
inductively on a sequence by splitting the sequence into parts,
consisting for example, of the first element and the rest, a.k.a., the~\defn{head} and the~\defn{tail}, or the first half or the second half.
%
We could define functions such as $\cd{splitHead}$, $\cd{splitMid}$,
$\cd{take}$, and $\cd{drop}$ for these purposes. Since all of these are
trivially expressible in terms of subsequences, we omit their
discussion for simplicity.
\end{gram}



\begin{gram}[Append and flatten]
\label{gr:seq::append}
For constructing large sequences from smaller ones, the sequence ADT
provides the  functions $\cd{append}$ and $\cd{flatten}$.
%
The function $\cd{append}~(a,b)$ appends the sequence~$b$
after the sequence~$a$.
%
More precisely, we can specify $\cd{append}$ as follows
\[
\begin{alignat}{1}
\cd{append} & ~(a: \sseq{\alpha})~(b: \sseq{\alpha}) : \sseq{\alpha}
\\
& ~= a \cup \cset{(i+\cseqlen{a}, x) : (i,x) \in b}
\end{alignat}
\]
%
We write $\kwappend{a}{b}$ as a short form for $\cd{append}~a~b$.
%
\end{gram}

\begin{gram}[Flatten]
To append more than two sequences the $\cd{flatten}~a$ function
takes a sequence of sequences and flattens them. If the input is
a sequence $a = \cseq{a_1,a_2,\ldots,a_n}$ it appends all the $a_i$'s.
%
We can specify $\cd{flatten}$ more precisely as follows
\[
\begin{alignat}{1}
\cd{flatten} & ~(a: \sseq{\sseq{\alpha}}) : \sseq{\alpha}
\\
& ~= \left\{\left(i +\sum_{(k,c) \in a, k < j}\cseqlen{c}, x\right) 
%
~:~ (i,x) \in b, (j,b) \in a \right\}.
\end{alignat}
\]
\end{gram}

\begin{example}[Append and flatten] 
\parent{gr:seq::append}
The $\cd{append}$ operation $\kwappend{\cseq{1,2,3}}{\cseq{4,5}}$ yields the sequence $\cseq{1,2,3,4,5}.$

The $\cd{flatten}$ operation 
\[
\cd{flatten}~\cseq{\cseq{1,2,3},\cseq{4},\cseq{5,6}}
\]
 yields the sequence $\cseq{1,2,3,4,5,6}$.
\end{example}

\begin{gram}[Updates and injections]
The function $\cd{update}~(a,(i,x))$, updates location $i$ of
sequence $a$ to contain the value $x$.  If the location is out of
range for the sequence, the function returns the input sequence
unchanged.  
%
We can specify $\cd{update}$ as follows
\[
\begin{alignat}{1}
\cd{update} & ~(a:\sseq{\alpha})~(i:\tynat,x:\alpha): \sseq{\alpha}
\\[1mm]
& ~=  
\left\{
\begin{array}{ll}
\left\{(j, y) : (j, y) \in a \mid j \not= i\right\} \cup \left\{(i, x)\right\} & \mbox{if}~0
\le i < \csetsize{a}
\\
a & \mbox{otherwise}.
\end{array}
\right.
\end{alignat}
\]
\medskip
\end{gram}

\begin{gram}[Inject]
\label{gr:seq::inject}
To update multiple positions at once, we can use $\cd{inject}$.
%
The function $\cd{inject}~(a,b)$ takes a sequence $b$ of
location-value pairs and updates each location with its associated
value.  
%
If any locations are out of range, that pair does nothing.  If
multiple locations are the same,  one of the updates take effect.
%

In the case of duplicates in the update sequence $b$, i.e., multiple
updates to the same position, we leave it unspecified which update
takes effect.
%
The operation $\cd{inject}$ may thus treat duplicate updates
non-deterministically.
\end{gram}

\begin{example}[Inject]
\parent{gr:seq::inject}

Given the string sequence 
\[ 
a = \cseq{\cstr{the},\cstr{cat},\cstr{in},\cstr{the},\cstr{hat}},\]
\[
\cd{update}~a~(1,\cstr{rabbit})
\]
magically yields
\[
\cseq{\cstr{the},\cstr{rabbit},\cstr{in},\cstr{the},\cstr{hat}}
\]
since location $1$ is updated with $\cstr{rabbit}$.  The expression
\[
\cd{inject}~a~\cseq{(4,\cstr{log}),(1,\cstr{dog}),(6,\cstr{hog}),(4,\cstr{bog}),(0,\cstr{a})}
\]
could yield
\[ 
\cseq{\cstr{a},\cstr{dog},\cstr{in},\cstr{the},\cstr{bog}}
\]
since location $0$ is updated with $\cstr{a}$, location $1$ with
$\cstr{dog}$, and location $4$ with $\cstr{bog}$.
%
It could also yield
\[ 
\cseq{\cstr{a},\cstr{dog},\cstr{in},\cstr{the},\cstr{log}}
\]
%
The entry with location $6$ is ignored
since it is out of range for $a$.
\end{example}

\begin{gram}[Collect]
\label{gr:seq::collect}

The primitive $\cd{collect}$ is useful when elements of a sequence
are ``keyed'', making it possible to associate data with some key.
Such pairs consisting of a key and a value are sometimes called~\defn{key-value} pairs.
%
Given a sequence of key-value pairs, we might want to~\defn{collect}
together all the values for a given key.
%
Collecting values together based on a key is very common in processing
databases.  In relational database languages such as SQL it is
referred to as ``Group by''.  More generally it has many applications.


We will use the function $\cd{collect}$ for this purpose, and it is part
of the sequence library.   Its type signature is
\[
\cd{collect}
: 
(\cd{cmp}: \alpha \times \alpha \rightarrow \mathcal{O})
\rightarrow
(a: \sseq{\alpha \times \beta})
\rightarrow
\sseq{\alpha \times \sseq{\beta}}.
\]
%
Here the "order set" $\mathcal{O} = \{\cd{less}, \cd{equal}, \cd{greater}\}$. 

The first argument $cmp$ is a function for comparing keys of type $\alpha$,
and must define a total order over the keys.  
%
The second argument $a$ is a sequence of key-value pairs.  
%
The $\cd{collect}$ function collects all values in $a$ that share the
same key together into a sequence, ordering the values in the same
order as their appearance in the original sequence.
\end{gram}

\begin{example}[Collect]
\parent{gr:seq::collect}

The following sequence shows a sequence of key-value pairs consisting
of our students from last semester and the classes they take.
%
\[
\begin{alignat}{1}
kv  = \langle 
& (\cstr{jack}, \cstr{15210}), (\cstr{jack}, \cstr{15213})
\\
& (\cstr{mary}, \cstr{15210}), (\cstr{mary}, \cstr{15213}), (\cstr{mary}, \cstr{15251}),
\\
& (\cstr{peter}, \cstr{15150}), (\cstr{peter}, \cstr{15251}), 
\\
& \ldots
\\
\rangle & .           
\end{alignat}
\]
%
We can determine the classes taken by each student by using
$\cd{collect}~\cd{cmp}$, where $\cd{cmp}$ is a comparison function for
strings
\[
\begin{alignat}{1}
\cd{collect}~\cd{cmp}~kv = \langle
~&~ (\cstr{jack}, \cseq{\cstr{15210}, \cstr{15213}, \ldots})
\\
~&~ (\cstr{mary}, \cseq{\cstr{15210}, \cstr{15213},  \cstr{15251}, \ldots}),
\\
~&~ (\cstr{peter}, \cseq{\cstr{15150}, \cstr{15251}, \ldots}), 
\\
~&~ \ldots
\\
\rangle ~&~ .           
\end{alignat}
\]
%
Note that the output sequence is ordered based on the first instance
of their key in the input sequences.  
%
Similarly, the order of the classes taken by each student are the same
as in the input sequence.
%
\end{example}


\begin{gram}[Checking for empty and singularity sequences]
\label{gr:seq::check-small}

To identify trivial sequences such as empty sequences and singular
sequences, which contain only one element, the interface provides the
functions $\cd{isEmpty}$ and $\cd{isSingular}$, which return
respectively return $\cd{true}$ if the sequence is empty or singular
and return $\cd{false}$ otherwise.
%
\end{gram}
\end{unit}

\begin{unit}[Aggregation by Iteration]

\begin{definition}[The $\cd{iterate}$ function]
\label{gr:seq::iterate}
Iteration is a key concept in computing, and specifically in algorithm
design.  
%
Iteration involves a sequence of steps, taken one after another, where
each step transforms the state from the previous step.  
%
Iteration is therefore an inherently sequential process.
%
The function $\cd{iterate}$ can be used to create a computation that
iterates over a sequence while accumulating information.
%
It starts with an initial state and a sequence, and on each step
updates the state based on the next element of the sequence.
%


The function $\cd{iterate}$ has the type signature
\[
\cd{iterate}~(f: \alpha \times \beta \ra \alpha)~(x: \alpha)~(a:
\sseq{\beta}) : \alpha
\]

where $f$ is a function mapping a state and an element of $a$ to a new
state, $x$ is the initial state,  $a$ is a sequence.
%
%

The semantics of $\cd{iterate}$ is defined as follows.
\[
\cd{iterate}~f~x~a
=  
\left\{
\begin{array}{ll}
x & \mbox{if}~ \cseqlen{a}= 0\\
f(x, a[0]) & \mbox{if}~\cseqlen{a}= 1\\
\cd{iterate}~f~(f(x, a[0])) (a\cirange{1}{\cseqlen{a}-1}) & \mbox{otherwise.}
\end{array}
\right.
\]
\end{definition}

\begin{gram}[Iteration with prefixes]
As a variant of iteration, we define $\cd{iteratePrefixes}$, which
takes the same arguments but returns all the intermediate values
computed as a sequence.
%
\end{gram}

\begin{example}
The function $\cd{iterate}$ computes its final result by computing a
result for each element of the sequence.
%
Concretely, $\cd{iterate}~f~x~a$ computes the results $x_i$, $0 \le i \le n
= \cseqlen{a}$.
\[
\begin{array}{lcl}
x_0 & = & x
\\
x_1 & = & f(x_0,a[0])
\\
x_2 & = & f(x_1, a[1])
\\
& \vdots &
\\
x_{n} & = & f(x_{n-1},a[n-1]).
\end{array}
\]
%
As the result
%
\[
\cd{iterate}~f~x~a
\]
%
returns  $x_n$.
%

The expression
%
\[
\cd{iteratePrefixes}~f~x~a
\]
%
performs the same computation and
returns  $\cseq{x_0, \ldots, x_n}$.
\end{example}



\begin{example}[Iteration]
\parent{gr:seq::iterate}
For a sequence of length $5$, iteration computes its final result  as
\[
\cd{iterate}~f~x~a  = f(f(f(f(f(v,a[0]),a[1]),a[2]),a[3]),a[4]).
\]
%

For example,
\[
\cd{iterate}~\cstr{+}~0~\cseq{2, 5, 1, 6}
\]
returns $14$ since it starts with the integer state $0$ and then one
by one adds the integer elements $2$, $5$, $1$ and $6$ of the sequence
to the state. 

Similarly
\[
\cd{iterate}~\cstr{-}~0~\cseq{2, 5, 1, 6}
\]
returns $(((0 - 2) - 5) - 1) - 6 = -14$.
%

The function 
\[
\cd{iterate}~\cstr{+}~0~(\cd{map}~\cd{zeroWhenEven}~a),
\]
which uses the function $\cd{zeroWhenEven}$ to map even numbers to zero,
sums up only the odd numbers in sequence $a,$~returning $6$
\end{example}


\begin{teachask}
You are likely familiar with what a properly matched strings are but
how can we define such strings?
\end{teachask}


\begin{group}[Parantheses matching]
\begin{definition}[Parantheses matching]
\parent{def:seq::iterate}
%
%
We say a string is matched if it can be described recursively
as
\[
p = \cseq{} \mid p~p \mid \chr{(}~p~\chr{)}, 
\]
where $\cseq{}$ is the empty sequence, $p~p$ indicates appending two
strings of matched parentheses (recursively defined), and
$\chr{(}~p~\chr{)}$ indicates the string starting with $\chr{(}$
followed by a matched string $p$ followed by $\chr{)}$.
\end{definition}

\begin{example}[Parenthesis matching]
The string $\cstr{(())()}$ is matched.
%
The $\cstr{())(()}$ is not matched.
\end{example}
\end{group}

\begin{problem}[Parentheses matching]
\label{pr:seq::matching}
The \emph{parentheses matching} problem requires determining whether a
given a string of parentheses is matched.
\end{problem}

\begin{teachask}[Parethesis matching via iteration]
\parent{gr:seq::iterate}
\parent{pr:seq::matching}
Can you solve this problem  using $\cd{iterate}$?
\end{teachask}

\begin{gram}[Parethesis matching via iteration]
As an application of iteration, let's try to solve the parenthesis
matching problem.
%
%% There are a several algorithms for solving this problem.  Here we
%% consider a linear-work sequential algorithm based on $\cd{iterate}$.  In
%% Chapter~\ref{ch:divide-and-conquer}, we present a divide-and-conquer
%% algorithm that requires no more work asymptotically, but has a low
%% span.
%
We can solve this problem by starting at the beginning of the sequence
with a counter set to zero and iterating through the elements one by
one.  If we ever see a left parenthesis we increment the counter and
whenever we see a right parenthesis we decrement the counter.
%
A sequence of parentheses can only be matched if the count ends at~$0$
since being matched requires that there are an equal number of right
and left parentheses.  However ending with a count of~$0$ is not
adequate since the string~$\cstr{))((}$ has count~$0$ but is obviously
not matched.  It also has to be the case that the count can never go
below~$0$ during the iterations.  

This observation leads to the algorithm shown below.
%
The algorithm starts with the state  $\csome{0}$ and
increments or decrements the counter on a left and right parenthesis,
respectively.  If the iterations ever encounter a right parenthesis
when the count is zero, this indicates the count will go below zero,
and at this point the state is changed to $\cd{None}$, which is
propagated through the rest of the iterations to the result.
Therefore at the end if the state is $\csome{0}$ then the counter
never went below zero and ended up at zero so the parentheses must be matched.
\end{gram}


\begin{algorithm}[Parenthesis matching via iterate]
\label{alg:seq::iterate::pmatch}
\parent{gr:seq::iterate}
\[
\begin{array}{l}
\cd{matchParens}~a = \\ 
~~\cd{let} \\
~~~~\cd{count}~(s, x) = \\
~~~~~~\cd{case}~(s, x) \\ 
~~~~~~~~|~(\cd{None}, \cignore) \dra \cd{None}\\
~~~~~~~~|~(\cd{Some}~n, \cd{')'}) \dra \cd{if}~(n = 0)~\cd{then}~\cd{None}~\cd{else}~\cd{Some}~(n - 1)\\
~~~~~~~~|~(\cd{Some}~n, \cd{'('}) \dra \cd{Some}~(n + 1)\\
~~\cd{in}\\
~~~~(\cd{iterate}~\cd{count}~(\cd{Some}~0)~a) = \cd{Some}~0 \\
~~\cd{end}
\end{array}
\]
\end{algorithm}
%

\begin{teachask}[Order of additions]
Consider the algorithm for summing the numbers in a sequence of
natural numbers $a$ using $\cd{iterate}$:
%
\[
\cd{iterate}~\cstr{+}  0 a
\]

This algorithm performs the plus operations in a particular order.  Is
this necessary? 
\end{teachask}

\begin{gram}[Iteration and order of operations]
Iteration is a powerful technique but can be too big of a hammer,
especially when used unnecessarily.  
%
For example, when summing the elements in a sequence, we don't need to
perform the addition operations in a particular order because addition
operations are associative and thus they can be performed in any order
desired.  
%
The iteration-based algorithm for computing the sum does not take
advantage of this property, computing instead the sum in a
left-to-right order.
%
As we will see next, we can use the associativity of the addition
operations to sum up the elements of a sequence in parallel.
\end{gram}
\end{unit}

\begin{unit}[Aggregation by Reduction]

\begin{gram}[Reduction]
\label{gr:seq::reduce-intro}

The term~\defn{reduction} refers to a computation that repeatedly
applies an associative binary operation to a collection of elements
until the result is reduced to a single value.
%
Recall that  associative operations are defined as operations that
allow commuting the order of operations.
\end{gram}

\begin{group}[Associativity]
\begin{definition}[Assocative Function]
\label{def:seq::assoc}
A function~$f: \alpha \times \alpha \rightarrow \alpha$ is associative
if $f(f(x,y),z) = f(x,f(y,z))$ for all $x,y$ and $z$ of type $\alpha$.
\end{definition}
%

\begin{note}
Associativity implies that when applying $f$ to some values, the order
in which the applications are performed does not matter.
%
Associativity does not mean that you can reorder the arguments to a
function (that would be commutativity).
\end{note}

\begin{example}
Many functions are associative. 

\begin{itemize}
\item Addition and
multiplication on natural numbers are associative, with 0 and 1 as
their identities, respectively.  

\item Minimum and maximum are also
associative with identities $\infty$ and $-\infty$ respectively.
%


\item The $\cd{append}$ function on sequences is associative, with
identity being the empty sequence.


\item The union operation on sets is associative, with the empty set as the
identity.
%

\end{itemize}
\end{example}

\begin{important}[Associativity of Floating Point Operations]
 An important class of operations that are not associative is
  floating-point operations.
%
These operations are typically not associative because performing a
set of operations in different orders can lead to different results
because of loss of precision.
\end{important}
\end{group}

\begin{definition}[The $\cd{reduce}$ operation]
\label{gr:seq::reduce}
In the sequence ADT, we use the function $\cd{reduce}$ to perform a
reduction over a sequence by applying an associative binary operation
to the elements of the sequence until the result is reduced to a
single value.
%
The operation function has the type signature
\[
\cd{reduce}~(f: \alpha \times \alpha \ra \alpha)~(id: \alpha)~(a: \sseq{\alpha}) : \alpha
\]
where $f$ is an associative function, $a$ is the sequence, and $id$ is
the~\defn{left identity} of $f$, i.e., $f(id,x) = x$ for all $x \in
\alpha$.
%

When applied to an input sequence with a function $f$, $\cd{reduce}$
returns the ``sum'' with respect to~$f$ of the input sequence.  In
fact if~$f$ is associative this sum in equal to iteration.
%
We can define the behavior of $\cd{reduce}$ inductively as follows
%
\[
\cd{reduce}~f~id~a
= 
\left\{
\begin{array}{ll}
id & \mbox{if}~\cseqlen{a}= 0
\\
a[0] & \mbox{if}~\cseqlen{a}= 1
\\[2ex]
f\left(\cd{reduce}~f~id~(a\cirange{0}{\lfloor \frac{\cseqlen{a}}{2}
    \rfloor - 1}),
\right.
\\
~~~~\left.\cd{reduce}~f~id~(a\cirange{\lfloor \frac{\cseqlen{a}}{2} \rfloor}{\cseqlen{a}-1}\right) & \mbox{otherwise.}
\end{array}
\right.
\]
\end{definition}
%

\begin{important}
%
The function $\cd{reduce}$ is more restrictive than $\cd{iterate}$ because
it is the same function but with extra restrictions on its input
(i.e. that~$f$ be associative, and $id$ is a left identity).
%
If the function $f$ is associative, then we have
\[
\cd{reduce}~f~id~a = \cd{iterate}~f~id~a.
\]
\end{important}

\begin{example}[Reduce and append]
\parent{gr:seq::reduce}
The expression 
\[
\cd{reduce}~\cd{append}~\cseq{}~\cseq{\cstr{another},\cstr{way},\cstr{to},\cstr{flatten}}
\] 
evaluates to
\\
\[
\cstr{anotherwaytoflatten}.
\]
\end{example}


\begin{exercise}
Give an example function $f$, a left identity $x$, and an input
sequence $a$ such that
$\cd{iterate}~f~x~a$ and $\cd{reduce}~f~x~a$ return different results.
\end{exercise}

\begin{important}
Although we will use $\cd{reduce}$ only with associative functions, we
define it for all well-typed functions.
%
To deal properly with functions that are non-associative, the
specification of $\cd{reduce}$ makes precise the order in which the
argument function~$\cd{f}$ is applied.
%
For instance, when reducing with floating point addition or
multiplication, we will need to take the order of operations into
account.
%
Because the specification defines the order in which the operations
are applied, every (correct) implementation of $\cd{reduce}$ must
return the same result: the result is deterministic regardless of the
specifics of the algorithm used in the implementation.
\end{important}

\begin{group}
\begin{exercise}
Given that $\cd{reduce}$ and $\cd{iterate}$ are equivalent for
assocative functions, why would we use $\cd{reduce}$?
\end{exercise}
\begin{solution}
Even though the input-output behavior of $\cd{reduce}$ and $\cd{iterate}$
may match, their cost specifications differ: unlike $\cd{iterate}$,
which is strictly sequential, $\cd{reduce}$ is parallel. 
%
In fact, as we will see later,
% in Section~\ref{sec:seq::cost} , 
the span of $\cd{iterate}$ is linear in the size of the input, whereas
the span of $\cd{reduce}$ is logarithmic.
\end{solution}
\end{group}
%
\end{unit}

\begin{unit}[Aggregation with Scan]

\begin{gram}[The $\cd{scan}$ operation]
\label{gr:seq::scan-intro}
When we restrict ourselves to associative functions, the input-output
behavior of the function $\cd{reduce}$ can be defined in terms of the
$\cd{iterate}$. 
%
But the reverse is not true: $\cd{iterate}$ cannot always be defined
in terms of $\cd{reduce}$, because $\cd{iterate}$ can use the results
of intermediate states computed on the prefixes of the sequence,
whereas $\cd{reduce}$ cannot because such intermediate states are not
available.  
%
For example, in our parenthesis matching algorithm
%(Algorithm~\ref{alg:seq::iterate::pmatch}),
we used this property by defining our function to propagate

a mismatched parenthesis forward in the computation.
%
We now describe a function called $\cd{scan}$ that
allows using the results of intermediate computations and also does so
in parallel.
\end{gram}

\begin{teachnote}[Operations in scan and iteration]
Note that scan still requires the function to be a binary operation
returning the same type as its argument, whereas iteration does not.
For example, the function for computing parenthesis matching does not
work as a scan operator.
\end{teachnote}

\begin{definition}[The function $\cd{scan}$]
\label{gr:seq::scan-intro}
The term ``scan'' refers to a computation that reduces every prefix of
a given sequence by repeatedly applying an associative binary
operation.  
%
The $\cd{scan}$ function has the type signature
\[
\cd{scan}~(f: \alpha * \alpha \ra \alpha)~(id: \alpha)~(a: \sseq{\alpha})~:~(\sseq{\alpha} * \alpha),
\]
where $f$ is an associative function, $a$ is the sequence, and $id$ is
the left identity element of $f$.
%
The expression $\cd{scan}~f~a$ returns the ``sum'' with respect
to~$f$ of all prefixes of the input sequence~$a$.
%
For this reason, the $\cd{scan}$ operation is sometimes called the~\defn{prefix sums} operation.
%


We define the semantics of $\cd{scan}$ in terms of $\cd{reduce}$ as follows.
\[
\begin{alignat}{2}
\cd{scan}~f~id~a
& ~=~ &
(&\cseq{\cd{reduce}~f~id~a\cirange{0}{(i-1)} : 0 \le i < \cseqlen{a}},
\\
& & &~\cd{reduce}~f~id~a)
\end{alignat}
\]

For the definition, we assume that $a\cirange{0}{-1} = \cseq{}$. 

\end{definition}

\begin{definition}[Inclusive Scan]
When computing the result for position~$i$, $\cd{scan}$ does not
include the element of the input sequence at that position.  It is
sometimes useful to do so.  To this end, we define $\cd{scanI}$ (``I''
stands for ``inclusive'').

We define the semantics of $\cd{scanI}$ in terms of $\cd{reduce}$ as follows.
\[
\begin{alignat}{1}
\cd{scanI}~f~id~a
& ~=~ 
\cseq{\cd{reduce}~f~id~a\cirange{0}{i} : 0 \le i < \cseqlen{a}}
\\
\end{alignat}
\]

\end{definition}

\begin{example}[Scan]
\parent{gr:seq::scan}
Consider the sequence $a = \cseq{0,1,2}$.
%
The prefixes of $a$ are  
\begin{itemize}
\item $\cseq{}$
\item $\cseq{0}$
\item $\cseq{0,1}$
\item $\cseq{0,1,2}.$
\end{itemize}
%
The prefixes of a sequence are all the subsequences of the sequence
that starts at its beginning. Empty sequence is a prefix of any
sequence.
%
The computation $\cd{scan}~\cd{+}~0~\cseq{0,1,2}$ can be written as
%
\[
\begin{alignat}{2}
\cd{scan}~\cd{+}~0~\cseq{0,1,2}
& =
& 
\left( \right. 
& \cseqb ~\cd{reduce}~\cd{+}~0~\cseq{}, 
\\
& & & ~~\cd{reduce}~\cd{+}~0~\cseq{0}, 
\\
& & & ~~\cd{reduce}~\cd{+}~0~\cseq{0, 1} 
\\ 
& & & \cseqe~, 
\\
& & & \cd{reduce}~\cd{+}~0~\cseq{0, 1, 2} 
\\
&  & \left.\right) &
\\
& = & \left(\right. & \cseq{0, 0, 1}, 3 \left.\right).
\end{alignat}
\]


%
The computation $\cd{scanI}~\cd{+}~0~\cseq{0,1,2}$ can be written as
\[
\begin{alignat}{2}
\cd{scanI}~\cd{+}~0~\cseq{0,1,2}
& ~=~ & 
\cseqb ~&~ \cd{reduce}~\cd{+}~0~\cseq{0}, 
\\
& & ~&~ \cd{reduce}~\cd{+}~0~\cseq{0, 1}, 
\\
& & ~&~ \cd{reduce}~\cd{+}~0~\cseq{0, 1, 2,} 
\\
& & \cseqe 
\\
& ~=~ & & \!\!\!\cseq{0, 1, 3}.
\end{alignat}
\]
\end{example} 
%

\begin{teachask}
Given that scan can be expressed just in terms of the operations that
we have already seen, why do we need it?
\end{teachask}

\begin{gram}[Scan versus reduce]
%
Since $\cd{scan}$ can be specified in terms of reduce, one might be
tempted to argue that it is redundant.
%
In fact, it is not: as we shall see, performing $\cd{reduce}$ repeatedly
on every prefix is not work efficient.  
%
Remarkably $\cd{scan}$ can be implemented by performing essentially the
same work and span of $\cd{reduce}$.
\end{gram}



\begin{example}[Copy scan]
\parent{gr:seq::scan}
  Scan is useful when we want pass information along the
  sequence. For example, suppose you have some ``marked'' elements
  that you would like to copy across to their right until they reach
  another marked element.
%
For example, suppose that we are given a sequence of type $\sseq{\tynat}$
consisting only of natural numbers and asked to return a sequence
of the same length where each element receives the previous positive
value.  For the example, for input $\cseq{0,~7,~0,~0,~3,~0},$ the
result should be $\cseq{0,~0,~7,~7,~7,~3}$.

Using a sequential loop or $\cd{iterate}$ would be easy.  To solve
this problem using $\cd{scan}$ we need a combining function $f$.  
%
Consider the function
%
\[
\cd{skipZero}~(x,y) = \cd{if}~y > 0~\cd{then}~y~\cd{else}~x.
\]
%
%
The function returns its right (second) argument if it is positive,
otherwise it returns its the left (first) argument.
%

To be used in a scan, $\cd{skipZero}$ must be associative.  In
particular we need to show that for all $x$, $y$ and $z$, we have
\[
\cd{skipZero}(x,\cd{skipZero}(y,z)) =
\cd{skipZero}(\cd{skipZero}(x,y),z).
\]
%
There are eight possibilities corresponding to each of $x$, $y$ and
$z$ being either positive or not.  
%
For the cases where $z$ is
positive, it is easy to verify that either ordering returns $z$.
%
For the cases that $z = 0$ and $y$ is positive, it is likewise easy to
verify that both orderings give $y$.
%
Finally, for the cases that both $y = z = 0$ and $x$ is positive they
both return $x$, and for all being zero, the ordering returns zero.

To use $\cd{skipZero}$ as part of the scan operation, we need to find
its left identity.  We can see that for any natural number $y$
\[
\cd{skipZero}~(0, y) =  y, 
\]
and that for any natural number $x$
\[
\cd{skipZero}~(x, 0) = x.
\]
%
Thus $0$ is the left identity for $\cd{skipZero}.$
\end{example}

\begin{remark}[Reduce and scan]
Experience in parallel computing shows that $\cd{reduce}$ and $\cd{scan}$
are powerful primitives that suffice to express many parallel
algorithms on sequences. 
%
In some ways this is not surprising, because the operations allow
using two important algorithm-design techniques: $\cd{reduce}$ operation
allows expressing divide-and-conquer algorithms and
%
the $\cd{scan}$ operation allows expressing an iterative algorithms.
\end{remark}
\end{unit}
\end{section}

\begin{section}[Comprehensions]
\label{sec:sequences}

We describe the sequence comprehensions notation that we shall be
using heavily in this book.



\begin{unit}[Sequence Comprehensions]
\label{unit:seq:sequence-comprehensions}

\begin{gram}
\label{gr:seq:comprehend-intro}
Notation such as $\csetf{x^2 : x \in a}{\cd{isPrime}~a}$ in which
one set is defined in terms of the elements of other sets, and
conditions on them is referred to as a~\defn{set comprehensions}.  
%
The example can be read as: the set of squares of the primes in the
set $a$.  
%
Comprehensions are commonly used in mathematics, because of
the economy of expression and ``comprehension'' that they offer.
%
In this book we use the comprehension syntax for a similar reason:
they allow expressing algorithms in clear, concise notation.
%
%% For example, the syntax for sequences shown in Syntax~\ref{syn:seq::seq}, as
%% well as sets which will see later in \chref{sets-and-tables}, are
%% based on set comprehensions.
%% %
In the examples shown thus far, we have mostly used sequences in
``flat'' fashion, without nesting sequence primitives within each
other.
%
In this section, we describe the sequence comprehensions in more
detail and how they can be used to express more complex algorithms
concisely.
%
\end{gram}

\begin{example}[Points in 2D]
\parent{gr:seq:comprehend-intro} Suppose that we wish to create a
sequence consisting of points in two dimensional space $(x,y)$ whose
coordinates are natural numbers that satisfy the conditions that $0
\le x \le n-1$ and $1 \le y \le n$.
%
For example, for $n = 3$, we would like to construct the sequence
$\cseq{(0,1), (0,2), (1,1), (1,2), (2,1), (2,2)}$.
%

The code below shows one way to generate such a sequence.
%
\[
\begin{array}{l}
\cd{points2D}~n = \\
~~\cd{flatten}~(\cd{tabulate}~(\cd{lambda}~x\,.\,\cd{tabulate}~(\cd{lambda}~y\,.\,(x,y+1))
\\
\phantom{
~~\cd{flatten}~(\cd{tabulate}~(\cd{lambda}~x\,.\,\cd{tabulate}}~n)
\\
\phantom{
~~\cd{flatten}~(\cd{tabulate}}~n)
\end{array}
\]

The algorithm first generates a sequence of the form 
\[
\begin{alignat}{2}
\cseqbb 
& 
\cseq{(0,1), (0,2), \ldots, (0,n)}
\\
& \cseq{(1,1), (1,2), \ldots, (1,n)}
\\
& \vdots
\\
& \cseq{(n-1,1), (n-1,2), \ldots, (n-1,n)}
\\
\cseqee
\end{alignat}
\]
and then concatenates the inner sequences by using $\cd{flatten}$.

Using our sequence comprehension notation, we can express the same
code more succinctly as
\[
\begin{array}{l}
\cd{points2D}~n = \cd{flatten}~\cseq{\cseq{(x, y): 1 \le y \le n }: 0 \le x \le n-1}.
\end{array}
\]
%
We simplify this notation a bit more and write the same algorithm as
\[
\begin{array}{l}
\cd{points2D}~n = \cseq{(x, y): 1 \le y \le n, 0 \le x \le n-1}.
\end{array}
\]
%
In this notation, we allow the expression to define the members of a
sequence by using multiple variables.
%
%
\end{example}


\begin{important}
Note that there is an implicit $\cd{flatten}$ in sequence
comprehensions that involve multiple sequences as shown in the example
above.
%
We will have to remember this point, as we analyze algorithms that
range over multiple sequences.
\end{important}

\begin{gram}[Deeper nesting levels]
\label{gr:seq:comprehend-nesting}
 
The notation generalizes to arbitrary levels of nesting.
%
For example, we may want to add one more dimension to point sequences
by considering points in three dimensional space, $(x,y,z)$, whose
coordinates are natural numbers that satisfy the conditions that $0
\le x \le n-1$, $1 \le y \le n$, $2 \le z \le n+1$.
%
We can write the code for this by nesting the sequences in three
levels as follows.
\[
\begin{array}{l}
\cd{points3D}~n = \\ 
~~\cd{flatten} \cseq{\cseq{\cseq{(x,y,z): 2 \le z \le n+1 }: 1 \le y \le  n}: 0 \le x \le n-1}
\end{array}
\]
%
or more succinctly as 
%
\[
\begin{array}{l}
\cd{points3D}~n = \\ 
~~\cseq{(x,y,z): 2 \le z \le n+1, 1 \le y \le  n, 0 \le x \le n-1}, 
\end{array}
\]
or as
\[
\begin{array}{l}
\cd{points3D}~n = \\ 
~~\cseq{(x,y,z): 0 \le x \le n-1, 1 \le y \le  n, 2 \le z \le n+1}. 
\end{array}
\]

\end{gram}

\begin{gram}[Nested sequences, more generally]
\label{gr:seq:comprehend-nesting-2}
We can also nest other sequence operations.
%
For example, suppose that we wish to compute the Cartesian product of
two sequences.
%
Given
%
$a = \cseq{1,2}$, and 
\\
$b = \cseq{3.0,4.0,5.0}$,
the
Cartesian product of $a$ and $b$ is 
\[
a \times b = \cseq{(1,3.0), (1,4.0), (1,5.0), (2,3.0),
  (2,4.0), (2,5.0)}.
\]
%
We can write the code for a function for computing as  
\[
\begin{array}{l}
\cd{CartesianProduct}~(a,b) = \\
~~\cd{flatten}~(\cd{map}~(\cfn{x}{\cd{map}~(\cfn{y}{(x,y)})~b})~a).
\end{array}
\]
%
or equivalently as
%
\[
\begin{array}{l}
\cd{CartesianProduct}~(a,b) = \\ 
~~\cseq{(x,y): x \in a, y \in b}.
\end{array}
\]
%
Note that the resulting sequence is ordered by the natural
lexicographic generalization of the ordering of all the sequences
involved.
\end{gram}

\begin{example}[Multiple sequences]
\parent{gr:seq:comprehend-nesting}
\parent{gr:seq:comprehend-nesting-2}
Some simple examples with multiple sequences follow.   
\begin{itemize}
\item 
Let $a = \cseq{0,1}$ and $b = \cseq{\chr{a},\chr{b}}$.  
\[
\cseq{(x,y) : x \in a, y \in x} = \cseq{(0,\cchr{a}), (0,\cchr{b}), (1,\cchr{a}), (1,\cchr{b})}. 
\]
%
\item
The expression
\[
\cseq{x * y : x \in \cseq{1,2,3} , y \in \cseq{4,5}}
\]
multiplies all pairs and evaluates to
\[
\cseq{4,5,8,10,12,15}.
\]
\end{itemize}

\end{example}


%

%In general, we can \cd{tabulate}, \cd{map} and \cd{filter} over any
%number of sequences.

\begin{syntax}[Comprehensions for multiple sequences]
\label{syn:seq::comp-mult}
We can sample from any finitely many sequences and compute an
expression in terms of their elements, while also filtering the
elements using finetely many expressions:
\[
\cseq{e : x_1 \in e_1, x_2 \in e_2 \ldots, x_n \in e_n \mid e_1', e_2'
  \ldots e_m'}.
\] 
%

We can also allow variable binding involving ranges of natural
numbers, as for example, can be used by $\cd{tabulate}$. 
%
Specifically, $x_i \in e_i$ could be replaced by $e_j \le i \le e_k$,
where $e_j$ and $e_k$ are expressions whose values are natural numbers
and $i$ is a variable.
\end{syntax}


\begin{example}[Using multiple sequences]
\parent{gr:seq:comprehend-nesting}
\parent{gr:seq:comprehend-nesting-2}
\parent{syn:seq::comp-mult}

  Given sequences $a$ of natural numbers and $b$ of letters of the
  alphabet, we wish to compute the sequence that pairs each even
  element of $a$ with all elements of $b$ that are vowels.
%
We can writes this simply by adding the filtering predicates
$\cd{isEven}$, which holds for even numbers, and $\cd{isVowel}$, which
holds for vowels.
%
\[
\begin{array}{l}
\cseq{(x,y): x \in a, y \in b~\sucht~\cd{isEven}~x, \cd{isVowel}~y}
\end{array}
\]
\end{example}

\begin{example}[All contiguous subsequences]
\label{ex:seq::allsubseqs}
\parent{gr:seq:comprehend-nesting}
\parent{gr:seq:comprehend-nesting-2}
\parent{gr:seq:comprehend-nesting-2}
\parent{syn:seq::comp-mult}


Let's say we want to generate all contiguous subsequences of a 
sequence $a$.   Each sequence can start at any position $0 \leq i <
\cseqlen{a}$, and end at any position  $i \leq j < \cseqlen{a}$.    
%
We can do this with the following pseudocode
\[
\cseq{a\cseq{i,\ldots,j} : 0 \leq i < \cseqlen{a}, i \leq j < \cseqlen{a}}\, ,
\]
which is equivalent to
%
\[
\begin{alignat}{1}
\cd{flatten}~(\cd{tabulate} & ~(\cd{lambda}~i\,.\,\cd{tabulate}~(\cd{lambda}~j.~a\cirange{i}{i+j})
\\
                   & \phantom{~(\cd{lambda}~i\,.\,\cd{tabulate}}~(\cseqlen{a}-i-1)
\\
&~\cseqlen{a}).

\end{alignat}
\]

%
This example shows again that comprehensions can be quite convenient.
\end{example}

\begin{remark}[Comprehensions]
Syntax based on set comprehensions is included in many programming
languages either directly for sets (e.g., SETL), or for other
collections of values such as lists, sequences, or mappings
(e.g. Python, Haskell and Javascript).  We should note, however, that
the syntax is not uniform among the languages.  Indeed even among
texts on set theory in mathematics the syntax for set comprehensions
varies significantly.  In our usage, we try to be self consistent, but
necessarily we are not always consistent with usage found elsewhere.
To be precise we always view comprehensions as syntactic sugar for
specific \pml code, and always define the translation between the two.
%,
%as we do in Syntax~\ref{syn:seq::seq}.
\end{remark}

\end{unit}
\end{section}
\begin{section}[Cost]
\label{sec:seq::cost}

So far in this chapter, we have only specified the behavior of
the operations in the sequence ADT.
%
In this section, we consider several different cost specifications for
the sequence ADT that are implemented by using arrays, trees, and
lists.
%


\begin{unit}[Cost Specification]

\begin{gram}
Cost specifications indicate the cost for the class of implementations
that can achieve the specified cost bounds.
%
There can be many specific implementations that match a cost
specification.
%
For example, for the tree-sequence specification, an implementations
can use one of many balanced binary tree data structures available.
%
To apply the cost bounds, we don't need to know the details of how
these implementations work.  
%
Cost specifications can thus be viewed as an abstraction over
implementation details that do not matter for the purposes of the
algorithm.
\end{gram}

\begin{gram}[Domination of cost specifications]
Since there usually are many ways to implement an ADT specification,
there can be multiple cost specifications for an ADT.
%
We say that one cost specification~\defn{dominates} another if for
every function, its asymptotic costs are no higher.
%
Of the three specifications we consider, none dominates another but
there are trade-offs: while some operations may be cheaper in one
specification, other operations may be more expensive.
%
\end{gram}

\begin{gram}[Choosing cost specifications]
Such trade-offs are common and should be considered when selecting
which cost specification to use.  
%
When designing an algorithm, our goal would be to choose the
specification that minimizes the cost for the algorithm.
%
For example, as we will see soon, if an algorithm makes many calls to
$\cd{nth}$ but no calls to $\cd{append}$, then we would use the
array-sequence specification rather than the tree-sequence
specification.  
%
Conversely, if the algorithm mostly uses $\cd{append}$ and
$\cd{update}$, then tree-sequence specification would be better.
%
After we decide the specification to use, what remains is to select
the implementation that matches the specification, which can include
additional considerations.
\end{gram}


% We will justify this cost in the implementation section.

\begin{note}[Cost of aggregation]
When presenting the cost bounds, we consider the aggregation
operations separately, because the cost of such operations depend on
the nature of the aggregation performed.
\end{note}
\end{unit}

\begin{unit}[Array Sequences]

\begin{costspec}[Array Sequences]
\label{cost:seq::arraySeq}
The table below specifies the~\defn{array-sequence} costs.
%
The notation $\Rset(-)$ refer to the trace of the corresponding
operation.
%
The specification for $\cd{scan}$ assumes that $f$ has constant work and span.

\[
\begin{array}{lcc}
\mbox{Operation} & \mbox{Work} & \mbox{Span}
\\
\cd{length}~a
&
\bigo{1}
& 
\bigo{1}
\\
\cd{length}~a
&
\bigo{1}
& 
\bigo{1}
\\
\cd{singleton}~x
&
\bigo{1}
& 
\bigo{1}
\\
\cd{isSingleton}~x
&
\bigo{1}
& 
\bigo{1}
\\
\cd{isEmpty}~x
&
\bigo{1}
& 
\bigo{1}
\\ 
\cd{nth}~a~i
& \bigo{1}
& \bigo{1}
\\ 
\cd{tabulate}~f~n
& \bigo{1 + \displaystyle\sum_{i=0}^n \cwork{f(i)}}
& \bigo{1 + \displaystyle\max_{i=0}^n \cspan{f(i)}} 
\\ 
\cd{map}~f~a
& \bigo{1 + \displaystyle\sum_{x \in a}  \cwork{f(x)}}
& \bigo{1 + \displaystyle\max_{x \in a}  \cspan{f(x)}} 
\\ 
\cd{filter}~f~a
& \bigo{1 + \displaystyle\sum_{x \in a} \cwork{p(x)}}
& \bigo{\log \cseqlen{a} + \displaystyle\max_{x \in a} \cspan{f(x)}} 
\\ 
\cd{subseq}~a~(i,j)
& \bigo{1}
& \bigo{1} 
\\ 
\cd{append}~a~b
& \bigo{1 + \cseqlen{a}+\cseqlen{b}}
& \bigo{1} 
\\ 
\cd{flatten}~a
& \bigo{1 + \cseqlen{a} + \sum_{x \in a} |x|}
& \bigo{1 + \log \cseqlen{a}} 
\\ 
\cd{update}~a~(i,x) 
& \bigo{1 + \cseqlen{a}}
& \bigo{1} 
\\ 
\cd{inject}~a~b 
& \bigo{1 + \cseqlen{a} + \cseqlen{b}}
& \bigo{1} 
\\ 
\cd{collect}~f~a
& \bigo{1 + \cwork{f} \cdot \cseqlen{a} \log \cseqlen{a}}
& \bigo{1 + \cspan{f} \cdot \log^2 \cseqlen{a}}
\\ 
\cd{iterate}~f~x~a
&
\bigo{1 + \displaystyle\sum_{f(y,z) \in \Rset(-)} \cwork{f(y,z)}}
&
\bigo{1 + \displaystyle\sum_{f(y,z) \in \Rset(-)} \cspan{f(y,z)}}
\\ 
\cd{reduce}~f~x~a 
& 
\bigo{1 + \displaystyle\sum_{f(y,z) \in \Rset(-)} \cwork{f(y,z)}}
&
\bigo{\log \cseqlen{a} \cdot  \displaystyle\max_{f(y,z) \in \Rset(-)} \cspan{f(y,z)}}
\\ 
\cd{scan}~f~x~a
& \bigo{\cseqlen{a}}
& \bigo{\log \cseqlen{a}}
\\
\end{array}
\]
\end{costspec}
%%%%

\begin{gram}[Cost specification for Array Sequences]
%

\begin{itemize}

\item Simple operations such as $\cd{length}$ as well as $\cd{isEmpty}$ and
$\cd{singleton}$ and $\cd{isSingleton}$ all require constant work and span.
%

\item 
Since arrays support random access to any element in constant time,
the function $\cd{nth}$ takes constant work and span. 
%

\item
For the three operations $\cd{tabulate}$, $\cd{map}$, and $\cd{filter}$ the
work includes the sum of the work of applying $f$ at each position, as
well as an additional unit cost, for the operation itself.
%
In all three operations it is possible to apply the function $f$ in
parallel since there is no dependency among the positions.  Therefore
the span of the functions is the maximum of the span of applying $f$
at each position.
%
\item
The operations $\cd{map}$ and $\cd{tabulate}$ incur an additional unit
overhead in the span, but for $\cd{filter}$ the overhead is
logarithmic, because $\cd{filter}$ requires \defn{compaction}, i.e.,
packing the chosen elements contiguously into the result array.

\item
The operation $\cd{subseq}$ has constant work and span.  It can be
implemented by maintaining the relevant parts of an array.
%

\item
The operation $\cd{append}$ requires work proportional to the length
of the sequences given as input, can be implemented in constant span.
%

\item
The operation $\cd{flatten}$ generalizes $\cd{append}$, requiring work
proportional to the total length of the sequences flattened, and
can be implemented in parallel in logarithmic span in the number of
sequences flattened.
%

\item 
The operations $\cd{update}$ and $\cd{inject}$ both require work
proportional to the length of the sequences they are given as input,
and can be implemented in constant span.
%
It might seem surprising that $\cd{update}$ takes work proportional to
the size of the input sequence $a$, since updating a single element
should require constant work.  The reason is that the interface is purely
functional so that the input sequence needs to be copied--we are not
allowed to update the old copy.  
%
In the last section of this chapter,
%~\ref{sec:seq::starrays}, 
we describe single-threaded array sequences that allows updating under
a sequence in constant work, but under  certain restrictions.

\item
The primary cost in implementing $\cd{collect}$ is a sorting step that
sorts the sequence based on the keys.  The work and span of collect is
therefore determined by the work and span of (comparison) sorting with
the specified comparison function~$f$. 
\end{itemize}
\end{gram}


\begin{example}[Tabulate and map with array costs]
As an example of $\cd{tabulate}$ and $\cd{map}$ 
\[
\begin{array}{lllll} 
\cwork{\cseq{i^2 : 0 \leq i < n}}  
& = &  
\bigoh{1 + \sum_{0=1}^{n-1} \bigoh{1}}  
& = & \bigoh{n} 
\\ 
\cspan{\cseq{i^2 : 0 \leq i < n}}  
& = &  
\bigoh{1 + \max_{i=0}^{n-1} \bigoh{1}}  
& = & \bigoh{1} 
\end{array} 
\]
because the work and span for $i^2$ is $\bigoh{1}$.   
 
As an example of $\cd{filter}$, we have 
\[
\begin{array}{lllll} 
\cwork{\cseqf{x: x \in a}{x < 27}}  
& = &  
\bigoh{1 + \sum_{i=0}^{|a|-1} \bigoh{1}}  
& = & 
\bigoh{|a|} 
\\ 
\cspan{\cseqf{x: x \in a}{x < 27}}  
& = & 
\bigoh{\lg{|a|} + \max_{i=0}^{|a|-1} \bigoh{1}}  
& = & 
 O(\lg{|a|}). 
\end{array} 
\]
\end{example}


\begin{example}[Bounding cost for all subsequences]
\label{ex:subseqcosts}
Consider the code 
%from~Example\ref{ex:seq::allsubseqs}:
\[
\cseq{a\cirange{i}{j} : 0 \leq i < \cseqlen{a}, i \leq j <  \cseqlen{a}},
\]
which extracts all contiguous subsequences from the sequence $a$.
Recall that the notation is equivalent to a nested $\cd{tabulate}$
first over the indices $i$, and then inside over the indices $j$.  The
results are then $\cd{flatten}$'ed.  
%
The nesting of $\cd{tabulate}$'s allows all the calls to
$a\cirange{i}{j}$ (i.e., $\cd{subseq}$) to run in parallel.  
%
Let $n =
\cseqlen{a}$.  
%
There are a total of
\[
\sum_{i=1}^n i = n(n+1)/2 = O(n^2)
\] contiguous subsequences and hence that many calls to $\cd{subseq}$,
each of which has constant work and span according to the cost
specifications.
%
The work of the nested $\cd{tabulate}$'s and the $\cd{subseq}$'s is
therefore $O(n^2)$.
%
The span of the inner $\cd{tabulate}$ is maximum over 
the span of the inner $\cd{subseq}$'s, which is $\bigoh{1}$.
%
The span of the outer $\cd{tabulate}$ is the maximum over the inner
$\cd{tabulate}$'s, which is again $\bigoh{1}$.
%
The $\cd{flatten}$ at the end requires $O(n^2)$ work and $O(\lg{n})$
span, because the total length of all subsequences is
$\frac{n(n+1)}{2} = O(n^2)$, and $\cseqlen{a} = n$.
%
The total work and span are therefore
\begin{align*}
\cwork{e} & =   O(\cseqlen{a}^2),~\mbox{and}\\
\cspan{e}  & =  O(\lg{\cseqlen{a}}).\\
\end{align*}
\end{example}


\begin{gram}[Cost of aggregation]

The cost of aggregation operations, $\cd{iterate}$, $\cd{reduce}$, and
$\cd{scan}$ are somewhat more difficult to specify because they depend
on the functions supplied as arguments and more specifically on the
intermediate values computed during evaluation.
%
For example, the cost of $\cd{iterate}$ depends not only on the
arguments but also the intermediate values computed during evaluation.
%
The next example
%Example~\ref{ex:seq::iterate-cost} 
shows a case where the length of the sequence being iterated over and
that of the results are the same but the costs differ due to
intermediate values.
%
\end{gram}
%% \begin{teachask}[Intermediate values]
%% But which intermediate values exactly? 
%% \end{teachask}




\begin{example}[Cost of iterated append]
\label{ex:seq::iterate-cost}
Consider appending the following sequence of strings using $\cd{iterate}$:
\[
\cd{iterate}~\cd{append}~\cstr{}~\cseq{\cstr{abc}, \cstr{d}, \cstr{e}, \cstr{f}}.
\]
If we only count the work of $\cd{append}$ operations performed during
evaluation, we obtain a total work of $22$, because the following
$\cd{append}$ operations are performed
\begin{enumerate}
\item $\cd{append}~\cstr{}~\cstr{abc}$ (work $4$), 
\item $\cd{append}~\cstr{abc}~\cstr{d}$ (work $5$), 
\item $\cd{append}~\cstr{abcd}~\cstr{e}$ (work $6$), and
\item $\cd{append}~\cstr{abcde}~\cstr{f}$ (work $7$).
\end{enumerate}
%

Consider now appending the following sequence of strings, which is a
permutation of the previous, using $\cd{iterate}$:
\[
\cd{iterate}~\cd{append}~\cstr{}~\cseq{\cstr{d}, \cstr{e}, \cstr{f}, \cstr{abc}}
\]
If we only count the work of $\cd{append}$ operations using the
array-sequence specification, we obtain a total work of $16$, because
the following $\cd{append}$ operations are performed 
\begin{enumerate}
\item $\cd{append}~\cstr{}~\cstr{d}$ (work $2$), 
\item $\cd{append}~\cstr{d}~\cstr{e}$ (work $3$), 
\item $\cd{append}~\cstr{de}~\cstr{f}$, (work $4$) and
\item $\cd{append}~\cstr{def}~\cstr{abc}$ (work $7$).
\end{enumerate}

In summary, we have used iteration over two sequences both with $4$
elements and obtained different costs even though the sequences are
permutations of each other (their elements have the same length).
%
This is because the total cost depends on the intermediate values
generated during computation.

\end{example}

\begin{gram}[Defining cost of iteration]
To specify its cost, we will consider the intermediate
values from the specification of $\cd{iterate}$, reproduced here for 
convenience.
%
%%%%
%%\input{sequences/fig-iterate-spec}
\begin{code}
\[
\cd{iterate}~f~x~a
=  
\left\{
\begin{array}{ll}
x & \mbox{if}~ \cseqlen{a}= 0\\
f(x, a[0]) & \mbox{if}~\cseqlen{a}= 1\\
\cd{iterate}~f~(f(x, a[0])) (a\cirange{1}{\cseqlen{a}-1}) & \mbox{otherwise.}
\end{array}
\right.
\]
\medskip
\end{code}


%%%%

%
Consider evaluation of
$\cd{iterate}~f~v~a)$ and let ${\mathcal{T}}(\cd{iterate}~f~v~a)$ denote the
set of calls to $f(\cdot,\cdot)$ performed along with the arguments,
as defined by the specification above.
%
We refer to this set of function calls as the ~\defn{trace} of
$\cd{iterate}$.
% 
We define the cost of $\cd{iterate}$ as the sum of these calls.
\end{gram}



\begin{costspec}[Cost for $\cd{iterate}$]
Consider evaluation of $\cd{iterate}~f~v~a$ and let
${\mathcal{T}}(\cd{iterate}~f~v~a)$ denote the set of calls (trace) to
$f(\cdot,\cdot)$ performed along with the arguments.
%
The work and span are as follows.
\[
\begin{array}{lll}
\cwork{\cd{iterate}~f~x~a} 
& = & 
O\left(
1 +  \sum_{f(y,z) \in {\mathcal{T}}(\cd{iterate}~f~x~a)} \cwork{f(y,z)}
\right)
\\
\cspan{\cd{iterate}~f~x~a} 
& = & 
O\left(
1 +  \sum_{f(y,z) \in {\mathcal{T}}(\cd{iterate}~f~x~a)} \cspan{f(y,z)}
\right)
\end{array}
\]
\end{costspec}



\begin{teachask}
Can you sort a sequence by using such a function?
\end{teachask}

\begin{example}[Cost of merging]
As an interesting example, consider the function $\cd{mergeOne}~a~x$
for merging a sequence~$a$ with the singleton sequence $\cseq{x}$ by
using an assumed comparison function.
%
The function performs $O(n)$ work in $O(\lg{n})$ span, where $n$ is
the total number of elements in the output sequence.
%
We can use the $\cd{mergeOne}$ function to sort a sequence via iteration as follows
%
\[
\cd{iterSort}~a = \cd{iterate}~\cd{mergeOne}~\cseq{}~a.
\]
%
For example, on input $a = \cseq{2, 1, 0}$, $\cd{iterSort}$ first
merges $\cseq{}$ and $\cseq{2}$, then merges the result $\cseq{2}$
with $\cseq{1}$, then merges the resulting sequence $\cseq{1, 2}$ with
$\cseq{0}$ to obtain the final result $\cseq{0, 1, 2}$.

The trace for $\cd{iterSort}$ with an input sequence of length $n$
consists of a set of calls to $\cd{mergeOne}$, where the first argument
is a sequence of sizes varying from $1$ to $n-1$, while its right
argument is always a singleton sequence.
%
For example, the final $\cd{mergeOne}$ merges the first $(n-1)$ elements
with the last element, the second to last $\cd{mergeOne}$ merges the
first $(n-2)$ elements with the second to last element, and so on.
%
Therefore, the total work for an input sequence $a$ of length $n$ is
\[
\cwork{\cd{iterSort}~a} 
\leq 
\sum_{i=1}^{n-1} c\cdot (1 + i)
= O(n^2).
\]

Using the trace, we can also analyze the span of $\cd{iterSort}$.  
%
Since we iterate adding in each element after the previous, there is
no parallelism between merges, but there is parallelism within a
$\cd{mergeOne}$, whose span is is logarithmic.
%
We can calculate the total span as
\[
\cspan{\cd{iterSort}~a} 
\leq
\sum_{i=1}^{n-1} c\cdot \lg{(1 + i)}
= O(n \lg{n}).
\]
%
Since average parallelism, $\cwork{n}/\cspan{n} = O(n / \lg{n})$, we
see that the algorithm has a reasonable amount of parallelism.
%
Unfortunately, it does much too much work.
\end{example}


\begin{teachask}[Algorithm for $\cd{iterSort}$]
Can you see what algorithm $\cd{iterSort}$ implements?
\end{teachask}

\begin{gram}[Algorithm for $\cd{iterSort}$]
%
Using this reduction order the algorithm is effectively working from
the front to the rear, using $\cd{mergeOne}$ to ``insert'' each element
into a sorted prefix where it is placed at the correct location to
maintain the sorted order.  
%
The algorithm thus implements the well-known insertion sort.
\end{gram}

\begin{gram}[Cost of $\cd{reduce}$]
Recall that with $\cd{reduce}$, we noted that the result
of the computation is not affected by the order in which the
associative function is applied and in fact is the same as that of
performing the same computation with $\cd{iterate}$.
%
As discussed below, however, the cost of $\cd{reduce}$, however,
depends on the order in which the operations are performed.
%, as shown by Example~\ref{ex:seq::cost-reduce}.
\end{gram}

%
\begin{example}[Cost of reduce append]
\label{ex:seq::cost-reduce}
Consider appending the following code
\[
\cd{reduce}~\cd{append}~\cstr{~}~\cseq{\cstr{abc}, \cstr{d}, \cstr{e}, \cstr{f}}.
\]
Suppose performing append operations in left-to-right order and count
their work using the array-sequence specification. 
%
The total work is $19$, because the following $\cd{append}$
operations are performed
\begin{enumerate}
\item $\cd{append} \cstr{abc} \cstr{d}$ (work $5$),
\item $\cd{append} \cstr{abcd} \cstr{e}$ (work $6$), and
\item $\cd{append} \cstr{abcde} \cstr{f}$ (work $7$).
\end{enumerate}

Consider now performing the $\cd{append}$ operations from right to
left order. 
%
We obtain a total cost of $15$, because the following $\cd{append}$
operations are performed
\begin{enumerate}
\item $\cd{append}~\cstr{e}~\cstr{f}$ (work $3$),
\item $\cd{append}~\cstr{d}~\cstr{ef}$, (work $4$) and
\item $\cd{append}~\cstr{abc}~\cstr{def}$ (work $7$).
\end{enumerate}
\end{example}



\begin{gram}
To specify the cost of reduce, we therefore consider its trace based
on its specification,
% as given in Section~\ref{sec:seq::adt},
reproduced below
for convenience.
%%%%
%%\input{sequences/fig-reduce-spec}
\[ 
\cd{reduce}~f~id~a
= 
\left\{
\begin{array}{ll}
id & \mbox{if}~\cseqlen{a}= 0
\\
a[0] & \mbox{if}~\cseqlen{a}= 1
\\[2ex]
f\left(\cd{reduce}~f~id~(a\cirange{0}{\lfloor \frac{\cseqlen{a}}{2}
    \rfloor - 1}),
\right.
\\
~~~~\left.\cd{reduce}~f~id~(a\cirange{\lfloor \frac{\cseqlen{a}}{2} \rfloor}{\cseqlen{a}-1}\right) & \mbox{otherwise.}
\end{array}
\right.
\]
\end{gram}
%%%%%

\begin{costspec}[Cost for $\cd{reduce}$]
Consider evaluation of $\cd{reduce}~f~x~a$ and let
${\mathcal{T}}(\cd{reduce}~f~x~a)$ denote the set of calls to
$f(\cdot,\cdot)$ performed along with the arguments. 
%
The work and span are defined as
\[
\begin{array}{lll}
\cwork{\cd{reduce}~f~x~a} 
& = & 
O\left(
1 +  \displaystyle\sum_{f(y,z) \in {\mathcal{T}}(\cd{reduce}~f~x~a)} \cwork{f(y,z)}
\right),~\mbox{and}
\\[2ex]
\cspan{\cd{reduce}~f~x~a} 
& = & 
O\left(
\lg{\cseqlen{a}} \cdot  \displaystyle\max_{f(y,z) \in {\mathcal{T}}(\cd{reduce}~f~x~a)} \cspan{f(y,z)}
\right).
\end{array}
\]
\end{costspec}

\begin{gram}[Work and span of reduce]
The work bound is simply the total work performed, which we obtain by
summing across all combine functions, plus one for the
$\cd{reduce}$. 
%
The span bound is more interesting. The $\lg{\cseqlen{a}}$ term
expresses the fact that the recursion tree in the specification of
$\cd{reduce}$
%
% (ADT~\ref{adt:seq::sequences}) 
%
is at most $O(\lg{\cseqlen{a}})$
deep. Since each node in the recursion tree has span at most
$\max_{f(y,z)} \cspan{f(y,z)}$, any root-to-leaf path, has at most
$O(\lg{\cseqlen{a}} \cdot \max_{f(a,b)} \cspan{f(a,b)})$ span.
\end{gram}


%% This can be used, for example, to prove the following lemma:

%% \begin{lemma}
%%   For any combine function $f\!: \alpha \times \alpha \to \alpha$ and 
%%   size function $s\!: \alpha \to \R_+$, if for any $x, y$,
%% \begin{enumerate}[topsep=0pt,itemsep=1pt]
%% \item $s(f(x,y)) \leq s(x) + s(y)$ and
%% \item $W(f(x,y)) \leq c\pparen{s(x) + s(y)}$ for some constant $c$,
%% \end{enumerate}
%% then \[W(\creduce\; f\; \mathbb{I}\; S) = O\pparen{\log |S| \sum_{x \in S} (1 + s(x))}.\]
%% \end{lemma}



%% \begin{quote}
%% \begin{tabular}{lp{4in}}
%% $S_i$ & The $i^{th}$ element of sequence $S$\\
%% $|S|$ & The length of sequence $S$ \\
%% $\cseq{}$ & The empty sequence\\
%% $\cseq{v}$ & A sequence with a single element $v$\\
%% $\cirange{i}{j}$ & A sequence of integers starting at $i$ and ending at
%% $j \geq i$.\\
%% $\cseq{e : p \in S}$ & {\bf Map} the expression $e$ to each element $p$ of
%% sequence $S$.   The same as ``$\cd{map}~(\cfn{p}{e})~S$'' in ML.\\
%% $\cseqf{p \in S}{e}$ & {\bf Filter} out the elements $p$ in $S$ that satisfy
%% the
%% predicate $e$.   The same as ``$\cd{filter}~(\cfn{p}{e})~S$'' in ML.\\
%% \end{tabular}
%% \end{quote}
%% More examples are given in the ``Syntax and Costs'' document.
%\end{document}

\begin{gram}[Cost of $\cd{scan}$]
As in $\cd{iterate}$ and $\cd{reduce}$ the cost specification of
$\cd{scan}$ depends on the intermediate results.  
%
But the dependency is more complex than can be represented by our ADT
specification.  
%
For $\cd{scan}$, we will stop at giving a cost specification by assuming
that the function that we are scanning with performs $\bigoh{1}$ work and
span.
\end{gram}


\begin{costspec}[Cost for $\cd{scan}$]
Consider evaluation of $\cd{scan}~f~x~a$.  For both the
array-sequence and tree-sequence specification
\[
\begin{array}{lll}
\cwork{\cd{scan}~f~x~a}
& = & 
O(\cseqlen{a})
\\[2ex]
\cspan{\cd{scan}~f~x~a} 
& = & 
O(\lg{\cseqlen{a}}).
\end{array}
\]
\end{costspec}

%%%%
%%\input{./sequences/fig-cost-tree}

%%%%

\end{unit}

\begin{unit}[Tree Sequences]

\begin{gram}
The costs for tree sequences is given in Cost~Specification below. %~\ref{cost:seq::treeSeq}. 
%
The specification represents the cost for a class of implementations
that use a balanced tree to represent the sequence.
%
The cost of each operation is similar to the array-based
specification, and many are exactly the same, i.e., $\cd{length}$,
$\cd{singleton}$, $\cd{isSingleton}$, $\cd{isEmpty}$, $\cd{collect}$,
$\cd{iterate}$, $\cd{reduce}$, and $\cd{scan}$.


There are also differences.
%
The work and span of the operation $\cd{nth}$ is logarithmic, as
opposed to being constant.  
%
This is because in balanced-tree based implementation, the operation
must follow a path from the root to a leaf to find the desired element
element.  
%
For a sequence $a$, such a path has length $O(\lg{\cseqlen{a}})$.
%
Although $\cd{nth}$ does more work with tree sequences, $\cd{append}$ does
not. Instead of requiring linear work, the work of $\cd{append}$ with
tree sequences is proportional to the logarithm of the ratio of the
size of the larger sequence to the size of the smaller one smaller
one.
%
For example if the two sequences are the same size, then
$\cd{append}$ takes $\bigoh{1}$ work.  On the other hand if one is length
$n$ and the other $1$, then the work is $O(\lg{n})$.  
%
The work of $\cd{update}$ is also less with tree sequences than
within array sequences. 
%

The work for operations $\cd{map}$ and $\cd{tabulate}$ are the same as
those for array sequences; their span incurs an extra logarithmic
overhead.
%
The work and span of $\cd{filter}$ are the same for both.
\end{gram}




\begin{costspec}[Tree~Sequences]
\label{cost:seq::treeSeq}

We specify the~\defn{tree-sequence} costs as follows.
%
The notation $\Rset(-)$ refer to the trace of the corresponding
operation.
%
The specification for $\cd{scan}$ assumes that $f$ has constant work and span.

\[
\begin{array}{lcc}
\mbox{Operation} & \mbox{Work} & \mbox{Span}
\\ 
\cd{length}~a
&
\bigo{1}
& 
\bigo{1}
\\
\cd{singleton}~x
&
\bigo{1}
& 
\bigo{1}
\\
\cd{isSingleton}~x
&
\bigo{1}
& 
\bigo{1}
\\
\cd{isEmpty}~x
&
\bigo{1}
& 
\bigo{1}
\\
\cd{nth}~a~i
& \bigo{\log \cseqlen{a}}
& \bigo{\log \cseqlen{a}}
\\ 
\cd{tabulate}~f~n
& \bigo{1 + \displaystyle\sum_{i=0}^n \cwork{f(i)}}
& \bigo{1 + \log n + \displaystyle\max_{i=0}^n \cspan{f(i)}} 
\\ 
\cd{map}~f~a
& \bigo{1 + \displaystyle\sum_{x \in a} \cwork{f(x)}}
&\bigo{1 + \log \cseqlen{a} + \displaystyle\max_{x \in a} \cspan{f(x)}} 
\\
\cd{filter}~f~a
& \bigo{1+ \displaystyle\sum_{x \in a} \cwork{f(x)}}
& \bigo{1 + \log \cseqlen{a} + \displaystyle\max_{x \in a} \cspan{f(x)}} 
\\
\cd{subseq}(a,i,j)
& \bigo{1 + \log(\cseqlen{a})}
& \bigo{1 + \log(\cseqlen{a})}
\\
\cd{append}~a~b
& \bigo{1 +|\log(\cseqlen{a}/\cseqlen{b})|}
& \bigo{1 +|\log(\cseqlen{a}/\cseqlen{b})|}
\\
\cd{flatten}~a
& \bigo{1 + \cseqlen{a}\log\left(\sum_{x \in a} |x|\right)}
& \bigo{1 + \log(\cseqlen{a} + \sum_{x \in a} |x|)} 
\\
\cd{inject}~a~b
& \bigo{1 + (\cseqlen{a}+\cseqlen{b})\log\cseqlen{a}}
& \bigo{1 + \log(\cseqlen{a}+\cseqlen{b})}
\\
\cd{collect}~f~a
& \bigo{1 + \cwork{f} \cdot \cseqlen{a} \log \cseqlen{a}}
& \bigo{1 + \cspan{f} \cdot \log^2 \cseqlen{a}}
\\
\cd{iterate}~f~x~a
&
\bigo{
1 + \sum\limits_{f(y,z) \in \Rset(-)} \cwork{f(y,z)}
}
&
\bigo{
1 + \sum\limits_{f(y,z) \in \Rset(-)} \cspan{f(y,z)}
}
\\
\cd{reduce}~f~x~a 
& 
\bigo{
1 + \sum\limits_{f(y,z) \in \Rset(-)} \cwork{f(y,z)}
}
&
\bigo{
\log \cseqlen{a} \cdot  \max\limits_{f(y,z) \in \Rset(-)} \cspan{f(y,z)}
}
\\
\cd{scan}~f~x~a
& \bigo{\cseqlen{a}}
& \bigo{\log \cseqlen{a}}
\end{array}
\]

\end{costspec}
\end{unit}

\begin{unit}[List Sequences]

\begin{gram}
%The costs for list sequences is given in \costref{seq::listSeq}. 
%
The Cost Specification below defines the cost for list sequences.
% 
The specification represents the cost for a class of implementations
that use (linked) lists to represent the sequence.
%
The determining cost in list-based implementations is the sequential
nature of the representation: accessing the element at position~$i$
requires traversing the list from the head to~$i$, which leads to
$\bigoh{i}$ work and span.
%
List-based implementations therefore expose hardly any parallelism.
%
Their main advantage is that they require quick access to the~\defn{head} and the~\defn{tail} of the sequence, which are defined as
the first element and the suffix of the sequence that starts at the
second element respectively.
\end{gram}

\begin{gram}
The work of each operation is similar to the array-based
specification.
%
Since the data structure mostly serial, the span of each operation is
essentially the same as that of its work, except that the total is
taken over the spans of its components.
%
The work and span of $\cd{subseq}$ operation depends on the beginning
position of the subsequence,  because list-based representation can
share their suffixes.
\end{gram}

\begin{costspec}[List Sequences]
\label{cost:seq::listSeq}
{ 
We specify the~\defn{list-sequence} costs as follows.
%
The notation $\Rset(-)$ refer to the trace of the corresponding
operation.
%
The specification for $\cd{scan}$ assumes that $f$ has constant work and span.


\[
\begin{array}{lcc}
\mbox{Operation} & \mbox{Work} & \mbox{Span}
\\ 
\cd{length}~a
&
\bigo{1}
& 
\bigo{1}
\\
\cd{singleton}~x
&
\bigo{1}
& 
\bigo{1}
\\
\cd{isSingleton}~x
&
\bigo{1}
& 
\bigo{1}
\\
\cd{isEmpty}~x
&
\bigo{1}
& 
\bigo{1}
\\
\cd{nth}~a~i
& \bigo{i}
& \bigo{i}
\\
\cd{tabulate}~f~n
& \bigo{1 + \displaystyle\sum_{i=0}^n \cwork{f(i)}}
& \bigo{1 + \displaystyle\sum_{i=0}^n \cspan{f(i)}} 
\\
\cd{map}~f~a
& \bigo{1 + \displaystyle\sum_{x \in a}  \cwork{f(x)}}
& \bigo{1 + \displaystyle\sum_{x \in a}  \cspan{f(x)}} 
\\
\cd{filter}~f~a
& \bigo{1 + \displaystyle\sum_{x \in a} \cwork{p(x)}}
& \bigo{1 + \displaystyle\sum_{x \in a} \cspan{p(x)}}
\\
\cd{subseq}~a~(i,j)
& \bigo{1+i}
& \bigo{1+i} 
\\
\cd{append}~a~b
& \bigo{1 + \cseqlen{a}}
& \bigo{1 + \cseqlen{a}}
\\
\cd{flatten}~a
& \bigo{1 + \cseqlen{a} + \sum_{x \in a} |x|}
& \bigo{1 + \cseqlen{a} + \sum_{x \in a} |x|}
\\
\cd{update}~a~(i,x) 
& \bigo{1 + \cseqlen{a}}
& \bigo{1 + \cseqlen{a}}
\\
\cd{inject}~a~b 
& \bigo{1 + \cseqlen{a} + \cseqlen{b}}
& \bigo{1 + \cseqlen{a} + \cseqlen{b}}
\\
\cd{collect}~f~a
& \bigo{1 + \cwork{f} \cdot \cseqlen{a} \lg{\cseqlen{a}}}
& \bigo{1 + \cspan{f} \cdot \cseqlen{a} \lg{\cseqlen{a}}}
\\
\cd{iterate}~f~x~a
&
\bigo{
1 + \displaystyle\sum_{f(y,z) \in \Rset(-)} \cwork{f(y,z)}
}
&
\bigo{
1 + \displaystyle\sum_{f(y,z) \in \Rset(-)} \cspan{f(y,z)}
}
\\
\cd{reduce}~f~x~a 
& 
\bigo{
1 + \displaystyle\sum_{f(y,z) \in \Rset(-)} \cwork{f(y,z)}
}
&
\bigo{
1 + \displaystyle\sum_{f(y,z) \in \Rset(-)} \cspan{f(y,z)}
}
\\
\cd{scan}~f~a
& \bigo{\cseqlen{a}}
& \bigo{\cseqlen{a}}
\\
\end{array}
\]
} 
\end{costspec}


\begin{remark}
Since they are serial, list-based sequences are usually ineffective
for parallel algorithm design.
\end{remark}
\end{unit}


\begin{unit}[Single-Threaded Sequences]
\label{sec:seq::starrays}

\begin{gram}
In this course we will be using purely functional code because it is
safe for parallelism and enables higher-order design of algorithms by
use of higher-order functions.  It is also easier to reason about
formally, and is just cool.  For many algorithms using the purely
functional version makes no difference in the asymptotic work
bounds---for example quickSort and mergeSort use $\Theta(n \log n)$
work (expected case for quickSort) whether purely functional or
imperative.  However, in some cases purely functional implementations
lead to up to a $O(\log n)$ factor of additional work.  To avoid this
we will slightly cheat in this class and allow for benign ``effect''
under the hood in exactly one ADT, described in this section.  These
effects do not affect the observable values (you can't observe them by
looking at results), but they do affect cost analysis---and if you
sneak a peak at our implementation, you will see some side effects.

The issue has to do with updating positions in a sequence.  In
an imperative language updating a single position can be done in ``constant
time''.  In the functional setting we are not allowed to
change the existing sequence, everything is persistent.  This means
that for a sequence of length $n$ an update can either be done in
$\Theta(n)$ work with an arraySequence (the whole sequence has to be copied
before the update) or $\Theta(\log n)$ work with a treeSequence (an update
involves traversing the path of a tree to a leaf).    In fact you
might have noticed that our sequence interface does not even supply a
function for updating a single position.   The reason is both to
discourage sequential computation, but also because it would be
expensive.

Consider a function $\cd{update}~(i,v)~S$ that updates sequence $S$
at location $i$ with value $v$ returning the new sequence.  This
function would have cost $\Theta(|S|)$ in the arraySequence cost
specification.  Someone might be tempted to write a sequential loop 
using this function.     For example for a function $f : \alpha \ra
\alpha$,  a $\cd{map}$ function can be implemented as follows:
\[
\begin{array}{ll}
\cd{map}~f~S = 
\\
~~\cd{iterate} & (\lambda~((i,S'),v).~(i+1,~\cd{update}~(i,f(v))~S'))
\\
& (0,S)
\\
&  S
\end{array}
\]
This code iterates over $S$ with $i$ going from $0$ to $n-1$ and at
each position $i$ updates the value $S_i$ with $f(S_i)$.  The problem
with this code is that even if $f$ has constant work, with an
$\cd{arraySequence}$ this will do $\Theta(|S|^2)$ total work since every
update will do $\Theta(|S|)$ work.  By using a $\cd{treeSequence}$
implementation we can reduce the work to $\Theta(|S| \log |S|)$ but that is
still a factor of $\Theta(\log |S|)$ off of what we would like.

In the class we sometimes do need to update either a single element or
a small number of elements of a sequence.  We therefore introduce an
ADT we refer to as a \emph{Single Threaded Sequence} ($\cd{stseq}$).
Although the interface for this ADT is quite straightforward, the cost
specification is somewhat tricky.  To define the cost specification we
need to distinguish between the latest ``copy'' of an instance of an
$\cd{stseq}$, and earlier copies.  Basically whenever we update a
sequence we create a new ``copy'', and the old ``copy'' is still
around due to the persistence in functional languages.  The cost
specification is going to give different costs for updating the latest
copy and old copies.  Here we will only define the cost for updating
and accessing the latest copy, since this is the only way we will be
using an $\cd{stseq}$.  The interface and costs is as follows:
\end{gram}

\begin{gram}[Interface and cost for single-threaded sequences]
\begin{tabular}{lcc}
& Work & Span
\\
%
$\cd{fromSeq}$ $S$ : $\alpha$ seq $\rightarrow$ $\alpha$ stseq &
$O(|S|)$ & $\bigoh{1}$ 
\\
%
\hspace* {.3in}Converts from a regular sequence to a stseq.
\\
$\cd{toSeq}$ $ST$ : $\alpha$ stseq $\rightarrow$ $\alpha$ seq &
$O(|S|)$ & $\bigoh{1}$ 
\\
\hspace*{.3in}Converts from a stseq to a regular sequence.
\\
$\cd{nth}$ $ST$ $i$ : $\alpha$ stseq $\rightarrow$ int $\rightarrow$
$\alpha$ &
$\bigoh{1}$ & $\bigoh{1}$ 
\\
\hspace*{.3in}Returns the $i^{th}$ element of ST.  Same as for seq. 
\\
$\cd{update}$ $ST$ $(i,v)$ :$\alpha$ stseq $\rightarrow$ (int $\times\ \alpha$)
 $\rightarrow$ $\alpha$ stseq &
$\bigoh{1}$ & $\bigoh{1}$ 
\\
\hspace*{.3in}Replaces the $i^{th}$ element of $ST$ with $v$.
\\
$\cd{inject}$ $ST$ $I$ : $\alpha$ stseq $\rightarrow$  (int $\times$ $\alpha$) seq $\rightarrow$ $\alpha$ stseq &
$O(|I|)$ & $\bigoh{1}$ 
\\
\hspace*{.3in}For each $(i,v) \in I$ replaces the $i^{th}$ element of $ST$ with
$v$.
\\
%\small\tt inject? $I$ ST : (int $\times \alpha$) stseq $\rightarrow$
%$\alpha$ option seq $\rightarrow$ $\alpha$ option seq
%& for each $(i,v) \in I$ replaces the $i^{th}$ element of ST with
%$v$ if the location contains NONE.
\end{tabular}
\end{gram}

\begin{gram}[Operations on single threaded sequences]

An $\cd{stseq}$ is basically a sequence but with very little
functionality.  Other than converting to and from sequences, the only
functions are to read from a position of the sequence ($\cd{nth}$),
update a position of the sequence ($\cd{update}$) or update multiple
positions in the sequence ($\cd{inject}$).  To use other functions
from the sequence library, one needs to covert an $\cd{stseq}$ back
to a sequence (using $\cd{toSeq}$).  

In the cost specification the work for both $\cd{nth}$ and
$\cd{update}$ is $\bigoh{1}$, which is about as good as we can get.
Again, however, this is only when $S$ is the latest version of a
sequence (i.e. noone else has updated it).   The work for
$\cd{inject}$ is proportional to the number of updates.  It can be
viewed as a parallel version of $\cd{update}$.
\end{gram}

\begin{example}[Map with single-threaded sequences]
Now with an $\cd{stseq}$ we can implement our map as follows:
\[
\begin{array}{l}
\cd{map}~f~S = 
\\
~~\cd{let}
\\
~~~~S' = \cd{StSeq.fromSeq}
\\
~~~~R = \cd{iterate}~(\lambda~((i,S''),v).~(i+1,~\cd{StSeq.update}~S''~(i,f(v))))
\\
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~(0,S')
\\
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~S
\\
~~\cd{in}
~~~~\cd{StSeq.toSeq} R
~~\cd{end}
\end{array}
\]

This implementation first converts the input sequence to an
$\cd{stseq}$, then updates each element of the $\cd{stseq}$, and
finally converts back to a sequence.  Since each update takes constant
work, and assuming the function $f$ takes constant work, the overall
work is $O(n)$.  The span is also $O(n)$ since $\cd{iter}$ is
completely sequential.  This is therefore not a good way to implement
$\cd{map}$ but it does illustrate that the work of multiple updates can
be reduced from $\Theta(n^2)$ on array sequences or $O(n \log n)$ on tree
sequences to $O(n)$ using an $\cd{stseq}$.

\end{example}


\begin{gram}[Implementing Single Threaded Sequences]



You might be
curious about how single threaded sequences can be implemented so they
act purely functional but match the cost specification.  Here we will
just briefly outline the idea.

The trick is to keep two copies of the sequence (the original and the
current copy) and additionally to keep a ``change log''.  The change
log is a linked list storing all the updates made to the original
sequence.  When converting from a sequence to an $\cd{stseq}$ the
sequence is copied to make a second identical copy (the current copy),
and an empty change log is created.  A different representation is
now
used for the latest version and old versions of an $\cd{stseq}$.  In
the latest version we keep both copies (original and current) as well
as the change log.  In the old versions we only keep the original copy
and the change log.  Let's consider what is needed to update either the
current or an old version.  To update the current version we modify
the current copy in place with a side effect (non functionally), and
add the change to the change log.  We also take the previous version
and mark it as an old version by removing its current copy.  When
updating an old version we just add the update to its change log.
Updating the current version requires side effects since it needs to
update the current copy in place, and also has to modify the old
version to mark it as old and remove its current copy.

Either updating the current version or an old version takes constant
work.  The problem is the cost of $\cd{nth}$.  When operating on the
current version we can just look up the value in the current copy,
which is up to date.  When operating on an old version, however, we
have to go back to the original copy and then check all the changes in
the change log to see if any have modified the location we are asking
about.  This can be expensive.  This is why updating and reading the
current version is cheap ($\bigoh{1}$ work) while working with an old
version is expensive.

In this course we will use $\cd{stseq}$'s for some graph algorithms,
including breadth-first search (BFS) and depth-first search (DFS), and
for hash tables.
\end{gram}


\end{unit}
\end{section}

\begin{section}[An Example: Primes]
\label{sec:seq::primes}

We now give some more involved examples of how to use sequences and
analyze work and span.  
%
As usual, the ratio of work and span gives us the parallelism of the
algorithm.  
%
As our example, we consider the  problem of finding prime numbers,
more precisely defined as follows.
%

\begin{unit}[Computing Primes]
\begin{problem}[Primes]
The~\defn{primes} problem requires finding all prime numbers less than
a given natural number $n$.
\end{problem}

\begin{gram}
Recall that a natural number $n$ is a prime if it has exactly two
distinct divisors $1$ and itself.
%
For example, the number $1$ is not prime, but $2$, $3$, $7$, and
$9967$ are.
%
If $n$ is not prime, then it has a divisor that is at most $\sqrt{n}$
since for any $i \times j = n$, either $i$ or $j$ has to be less than
or equal to $\sqrt{n}$.
%
We can therefore test the primality of a number $n$ by checking
whether any $i$, $2 \leq i \leq \sqrt{n}$ is a divisor of $n$.
%
We can write such an algorithm using sequences as follows.
%
For simplicity we assume throughout that $n \ge 2$.
%
\end{gram}

\begin{algorithm}[Brute Force Primality Test]
\label{alg:seq::bfprime}
\[
\begin{array}{l}
\cd{isPrime}~n =
\\ 
~~\cd{let}
\\ 
~~~~\cd{all} = \cseq{n \bmod i : 1 \le i \le \fsqrt{n}}
\\
~~~~\cd{divisors} = \cseq{x: x \in \cd{all} \sucht x = 0}
\\
~~\cd{in}
\\ 
~~~~|\cd{divisors}| = 1
\\
~~\cd{end}
\end{array}
\]
\end{algorithm}
%
%%%% Old version
%% \begin{algorithm}~
%% \label{alg:seq::bfprime}
%% \begin{lstlisting}
%% isPrime $m$ = $(|\cseqf{1 \leq i \leq \isqrt{m}}{i \bmod m = 0}| = 1)$
%% \end{lstlisting}
%% \end{algorithm}
%%
%%%% The algorithm below is not quite correct, does not work for 2
%%%% Also, mod notation is used incorrectly.
%%%%
%% \begin{algorithm}~
%% \label{alg:seq::bfprime}
%% \begin{lstlisting}
%% isPrime $m$ = $(|\cseqf{2 \leq i \leq \isqrt{m}}{m \bmod i = 0}| = 0)$
%% \end{lstlisting}
%% \end{algorithm}
%% The length of the sequence is simply the number of $j$ that divide
%% $n$.  If that is zero, then $n$ is a prime.  

\begin{gram}[Cost of Brute Force Primality Test]
Let's calculate the work and span of this algorithm based on the array
sequence cost specification.  
%
The algorithm constructs a sequence of length $\fsqrt{n}$ and then
filters it.
%
Since the work for computing $i \bmod n$ and checking that a value is
zero $x = 0$ is constant, based on the array-sequence costs, we can
write work as
\[
\cworkof{\cd{isPrime}}{n} 
=  
\bigoh{1 + \sum_{i=1}^{\fsqrt{n}} \bigoh{1}} 
= 
\bigoh{\sqrt{n}}.
\]
%
Similarly we
can write span as:
\[
\cspanof{\cd{isPrime}}{n} 
=  
\bigoh{\lg{\sqrt{n}} + \max_{i=1}^{\fsqrt{n}} \bigoh{1}}
=
\bigoh{\lg{n}}.
\]
%
The $\lg{\sqrt{n}}$  additive terms come from the cost specification for
$\cd{filter}$.

Since parallelism is the ratio of work to span,  it is
\[
\bigoh{\frac{\sqrt{n}}{\lg{\sqrt{n}}}}.
\]
%
This is not an abundant amount of parallelism but adequate especially,
because work is small.
%
\end{gram}

\begin{algorithm}[Brute Force Solution to the Primes Problem]
\label{alg:seq::bfprimes}
Now that we can test for primality of a number, we can solve the
primes problem by testing the numbers up to $n$.
%
We can write the code for such a brute-force algorithm as follows.
% 
\[
\begin{array}{l}
\cd{primesBF}~n =
\\ 
~~\cd{let}
\\ 
~~~~\cd{all} = \cseq{i : 1 < i < n}
\\
~~~~\cd{primes} = \cseq{x : x \in \cd{all} \sucht \cd{isPrime}(x)}
\\
~~\cd{in} 
\\
~~~~\cd{primes}
\\
~~\cd{end}
\end{array}
\]
\end{algorithm}

%% \begin{algorithm}~
%% \label{alg:seq::bfprimes}
%% \cd{primes} $n$ = $\cseqf{1 \leq i \leq n}{\cd{isPrime}(i)}$ 
%% \end{algorithm}

\begin{gram}
Let's analyze work and span, again  using array sequences.
%
Constructing the sequence $\cd{all}$ using $\cd{tabulate}$ requires linear
work.
%
Filtering through $\cd{all}$ requires work that is the sum of the work
of the calls to $\cd{isPrime}$; thus we have
%
\begin{align*}
\cworkof{\cd{primesBF}}{n} 
 & =  
\bigoh{\sum_{i=2}^{n-1}{1+\cworkof{\cd{isPrime}}{i}}}
\\
& =  \bigoh{\sum_{i=2}^{n-1}{1 +\sqrt{i}}}
\\
& =  \bigoh{n^{3/2}}.
\end{align*}

Similarly, the span is dominated by the maximum of the span of calls
to $\cd{isPrime}$ and a logarithmic additive term.
%
\begin{align*}
\cspanof{\cd{primesBF}}{n} = 
& = 
\bigoh{\lg{n} + \max_{i=2}^{n}{\cspanof{\cd{isPrime}}{i}}}
\\
& = 
\bigoh{\lg{n} + \max_{i=2}^{n}{\lg{i}}}
\\
& = 
\bigoh{\lg{n}}                               
\end{align*}

The parallelism is hence 
\[
\frac
{\cworkof{\cd{primesBF}}{n}}
{\cspanof{\cd{primesBF}}{n}} 
=
\frac{n^{3/2}}{\lg{n}}.
\]  
%
This is plenty of parallelism but comes at the expense of a large amount of work.
%

%
\end{gram}

\begin{teachask}
Can you see why the algorithm does a lot of work.
\end{teachask}
%

\begin{gram}
We can improve the work for the algorithm, because the algorithm does
a lot of redundant work.
%
Intuitively, we can see that the algorithm does redundant work,
because it repeatedly performs checks with the same numbers.  To test
whether a number $m$ is prime, the algorithm checks its divisors, it
then checks essentially the same divisors for multiples of $m$, such
as $2m, 3m, \ldots$, which largely overlap, because if a number
divides $m$, it also divides its multiples.

We can eliminate this redundancy by more actively eliminating numbers
that are \defn{composites}, i.e., not primes.
%
The basic idea is to create a collection of composite numbers up to $n$ and use
this as a \defn{sieve.}.
%
Generating such a sieve is easy: we just have to include for any
number $i \le \sqrt{n}$, its multiples of up to $\frac{n}{i}$.
%
Having generated the sieve, what remains is to run the numbers up to
$n$ through the sieve. 
%
To do this in parallel, we  can use $\cd{inject}$.
\end{gram}
%

\begin{algorithm}[Prime Sieve]
The pseudo-code below presents the prime-sieve algorithm.
%
The idea is to do construct the sieve as a length-$n$ sequence of the
Boolean value $\cd{true}$, and then update the sequence by writing
$\cd{false}$ into all positions that correspond to composite numbers.
%
The remaining $\cd{true}$ values indicate the prime numbers.
%

\[
\begin{array}{l}
\cd{primeSieve}~n = 
\\
~~\cd{let}
\\ 
~~~~\cd{(*  Composite numbers. *)}
\\
~~~~\cd{cs} = \cseq{i * j : 2 \le i \le \fsqrt{n}, 2 \le j  \le n/i}
\\
~~~~\cd{sieve} = \cseq{(x,\cd{false}) : x \in cs}
\\
~~~~\cd{all} = \cseq{\cd{true}: i \le 0 < n}
\\
~~~~\cd{isPrime} = \cd{inject}~\cd{all}~\cd{sieve}
\\
~~~~\cd{primes} = \cseq{i : 2 \le i < n \sucht  \cd{isPrime}[i] = \cd{true}}
\\
~~\cd{in}
\\ 
~~~~\cd{primes}
\\
~~\cd{end}
\end{array}
\]
\end{algorithm}

%%%% Algorithm with flatten.
%% \begin{algorithm}~
%% \begin{lstlisting}
%% primeSieve $n$ = 
%%   let 
%%     $composites$ = flatten $\cseq{\cseq{i * j : 1 \le j  \le n/i } : 0 \le i \le \fsqrt{n}}$
%%     $sieve$ = $\cseqb$ ($x$,true) $:$ $x \in composites$ $\cseqe$
%%     $all$ = $\cseqb$ true $:$ $i \le 0 < n$ $\cseqe$
%%     $isPrime$ = inject $all$ $sieve$
%%     $primes$ = $\cseqb$ $i : 2 \le i < n$ $\sucht$ $isPrime[i] = true$ $\cseqe$
%%   in 
%%     $primes$
%%   end 
%% \end{lstlisting}
%% \end{algorithm}

%
%%%%% There seems to some issues with the edge cases in the algorithm below.
%%%%% Otherwise it is essentially identical to the one above.
%% \begin{algorithm}~
%% \begin{lstlisting}
%% primes(n) = 
%%   let 
%%     sieves = $\cseq{(i \times j,\sml{false}) : 2 \leq i \leq \isqrt{n}, 1 \leq j \leq \lceil n/i \rceil}$
%%     $R$ = inject($\cset{\sml{true} : 0 \leq i \leq n}$,sieves) 
%%   in 
%%     $\cseqf{i : 2 \leq i \leq n}{R[i]}$@\label{line:primesFilter}@
%%   end 
%% \end{lstlisting}
%% \end{algorithm}


\begin{gram}[Cost of the Sieve Algorithm]
The work and span for calculating $\cd{primeSieve}$ is similar to the
analysis for finding all subsequences.
% in \exref{subseqcosts}.
%
We shall consider the phases of the algorithm and show that the work
and span are functions of the total number of composites which we
denote by $m$.
%
\begin{itemize}
\item 
Generating each composite takes constant work and because it is just a
multiplication.
%
The work for generating the sequence of composites is linear in
the total number of composites, $m$.
%
The span is $O(\lg{m})$ because of the nested sequence and the implied
$\cd{tabulate}$.
%
Constructing the sieve requires linear work in its length, which is
$m$, and constant span.

\item 
 The work of $\cd{inject}$ is also proportional to the length of
 $\cd{sieve}$, $m$,  and its span is constant.  

\item 
The work for computing $\cd{primes}$, using $\cd{tabulate}$ and
$\cd{filter}$ is proportional to $n$, and the span is $O(\lg{n})$.
%
\end{itemize}

Therefore the total work is proportional to the number composites $m$,
which is larger than $n$, and the total span is $O(\lg{n+m})$.
%
To calculate $m$, we can add up the number of multiples each $i$ from
$2$ to $\floor{\sqrt{n}}$ have, i.e.,
%

\begin{align*}
m & =  \sum_{i=2}^{\fsqrt{n}} \left\lceil \frac{n}{i} \right\rceil\\
                           & \leq  (n+1) \sum_{i=2}^{\fsqrt{n}}\frac{1}{i} \\
                           & =  (n+1) H(\fsqrt{n}) \\
                           & \leq  (n+2) \ln n^{1/2}\\
                           & = \frac{n+2}{2} \ln n.\\
\end{align*}
%
Here $H(n)$ is the $n^{th}$ harmonic number, which is known to be
bounded below by $\ln n$ and above by $\ln n + 1$.   
We therefore have    
\begin{align*}
\cworkof{\cd{primeSieve}}{n} & =  O(n \lg{n}),~\mbox{and}
\\
\cspanof{\cd{primeSieve}}{n} & =  O(\lg{n}).
\end{align*}

%
We have thus reduced the work from $\bigoh{\frac{n^{3/2}}{\lg{n}}}$
to something much more reasonable $\bigoh{n\lg{n}}$.
\end{gram}

\begin{teachnote}
The question is how do we generate just the primes less than
$\isqrt{n}$ for computing the sieves. This is easy to recursively,
giving the following algorithm.

%\begin{algorithm}
\begin{lstlisting}
primeSieve $n$ = 
  let 
    $P$ = primeSieve $\fsqrt{n}$ 
    $cs$ = $\cseq{i * j :  i \in P, 2 \le j  \le n/i}$
    $sieve$ = $\cseqb$ ($x$,true) $:$ $x \in cs$ $\cseqe$
    $all$ = $\cseqb$ true $:$ $0 \le i < n$ $\cseqe$
    $isPrime$ = inject $all$ $sieve$
    $primes$ = $\cseqb$ $i : 2 \le i < n$ $\sucht$ $isPrime[i] = true$ $\cseqe$
  in 
    $primes$
  end 
\end{lstlisting}
%\end{algorithm}

%% \begin{algorithm}~
%% \begin{lstlisting}
%% primes(n) = 
%% if (n < 2) then $\cseq{}$
%% else 
%%   let $P$ = primes($\isqrt{n}$) 
%%     sieves = $\cseq{(p \times i,\sml{false}) : p \in P, 1 \leq i \leq \lceil n/p \rceil}$
%%     $R$ = inject($\cset{\sml{true} : 0 \leq i \leq n}$,sieves) 
%%   in 
%%    $\cseqf{i : 2 \leq i \leq n}{R[i]}$
%%   end 
%% \end{lstlisting}
%% \end{algorithm}

We leave the analysis of this algorithm as an exercise, but we state
without justification that it has $\bigoh{n \lg{\lg{n}}}$ work and
$\bigoh{\lg{n}}$ span.
\end{teachnote}


\begin{remark}
The algorithm for computing primes described here dates back to
antiquity and attributed to Eratosthenes of Cyrene, a Greek
mathematician.
\end{remark}
\begin{checkpoint}

\begin{questionfr}
\points 10 

\prompt 
Describe an algorithm for computing prime numbers up to $n$ in
$\bigoh{n\lg{\lg{n}}}$ work and $\bigoh{\lg{n}}$ span.
%
Show that the cost bounds hold.

\hint
The work of the sieve algorithm can be further improved by
noticing that when computing the composites, we only need to consider
the multiples of prime numbers.
%
% This is because one of
% its divisors will include all its multiples.  
%
For example we don't need to consider the multiples of $6$ since all
multiples of 6 are also multiples of $2$ and of $3$. 
%

\begin{answer}


%% Answer below. hidden  
%% \begin{lstlisting}[numbers=none]
%% primeSieve $n$ = 
%%   let 
%%     $P$ = primeSieve $\fsqrt{n}$ 
%%     $cs$ = $\cseq{i * j :  i \in P, 2 \le j  \le n/i}$
%%     $sieve$ = $\cseqb$ ($x$,true) $:$ $x \in cs$ $\cseqe$
%%     $all$ = $\cseqb$ true $:$ $0 \le i < n$ $\cseqe$
%%     $isPrime$ = inject $all$ $sieve$
%%     $primes$ = $\cseqb$ $i : 2 \le i < n$ $\sucht$ $isPrime[i] = true$ $\cseqe$
%%   in 
%%     $primes$
%%   end 
%% \end{lstlisting}


%%%% The algorithm below seems to have some issues regarding edge cases.
%% \begin{algorithm}~
%% \begin{lstlisting}[numbers=none]
%% primes(n) = 
%% if (n < 2) then $\cseq{}$
%% else 
%%   let $P$ = primes($\isqrt{n}$) 
%%     sieves = $\cseq{(p \times i,\sml{false}) : p \in P, 1 \leq i \leq \lceil n/p \rceil}$
%%     $R$ = inject($\cset{\sml{true} : 0 \leq i \leq n}$,sieves) 
%%   in 
%%    $\cseqf{i : 2 \leq i \leq n}{R[i]}$
%%   end 
%% \end{lstlisting}
%% \end{algorithm}

We leave the analysis of this algorithm as an exercise, but we state
without justification that it has $\bigoh{n \lg{\lg{n}}}$ work and
$\bigoh{\lg{n}}$ span.
\end{answer}

\end{questionfr}

\end{checkpoint}

\end{unit}
\end{section}

\end{chapter}
\end{book}
