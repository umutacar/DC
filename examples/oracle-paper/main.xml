<?xml version="1.0" encoding="UTF-8"?>
<segment name='chapter'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Oracle
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Oracle
]]>
</field> <!-- title_src -->
<field name='label'>
ch:oracle
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::classic
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::classic
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[

]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\begin{abstract}
A classic problem in parallel computing is determining whether to
execute a task in parallel or sequentially. If small tasks are
executed in parallel, the task-creation overheads can be overwhelming.
If large tasks are executed sequentially, processors may spin
idle. This granularity problem, however well known, is not well
understood: broadly applicable solutions remain elusive.  

We propose techniques for controlling granularity in implicitly
parallel programming languages.  Using a cost semantics for a
general-purpose language in the style of the lambda calculus with
support for parallelism, we show that task-creation overheads can
indeed slow down parallel execution by a multiplicative factor.  We
then propose {\em oracle scheduling}, a technique for reducing these
overheads, which bases granularity decisions on estimates of
task-execution times.  We prove that, for a class of computations,
oracle scheduling can reduce task creation overheads to a small
fraction of the work without adversely affecting available
parallelism, thereby leading to efficient parallel executions.

We realize oracle scheduling in practice by a combination of static
and dynamic techniques.  We require the programmer to provide the
asymptotic complexity of every function and use run-time profiling to
determine the implicit, architecture-specific constant factors. In our
experiments, we were able to reduce overheads of parallelism down to
between 3 and 13 percent, while achieving 6- to 10-fold speedups.






\end{abstract}
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::programming
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::programming
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[

]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\category{D.1.3}{Programming Techniques}{Concurrent Programming}
\terms{Algorithms, Experimentation, Languages}
\keywords{Scheduling, Granularity Control, Work Stealing}
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::parallel
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::parallel
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[

]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\begin{comment}
\begin{abstract}

A classic problem in parallel computing is determining whether to
execute a task in parallel or sequentially. If many small tasks are
executed in parallel, the overhead of creating these parallel tasks
can be overwhelming.
If many large tasks are executed
sequentially, processors may spin idle, resulting again in suboptimal
speedups. Although identified as an important problem with practical
importance, the theoretical underpinnings of this problem are not well
understood and broadly applicable solutions are lacking.

In this paper, we propose techniques for controlling the scheduling
overheads of task creation to achieve parallel efficiency.
Specifically, we extent Brent's theorem~\cite{Brent74} (a.k.a.'s
work-time principle) to include task creation overheads.  Using a cost
semantics for a general-purpose language in the style of lambda
calculus with parallel tuples, we then show that task-creation
overheads can slowdown parallel execution by a multiplicative factor.
To reduce the overheads, we introduce {\em oracle scheduling}, a
semantics guided by estimates of the sizes of parallel tasks.  We show
that if the oracle provides in constant time estimates that are
accurate within a constant multiplicative factor then oracle
scheduling provable reduces the task-creation overheads for a broad
range of parallel computations.










\end{abstract}
\end{comment}
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->


<segment name='section'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Introduction
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Introduction
]]>
</field> <!-- title_src -->
<field name='label'>
sec:introduction
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::explicit
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::explicit
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p><span><em>Explicit parallel programming</em></span> provides full control over parallel resources by offering primitives for creating and managing parallel tasks, which are small, independent threads of control. As a result, the programmer can, at least in principle, write efficient parallel programs by performing a careful cost-benefit analysis to determine which tasks should be executed in parallel and under what conditions. This approach, however, often requires reasoning about low-level execution details, such as data races or concurrent effects, which is known to be notoriously hard; it can also result in code that performs well in a particular hardware setting but not in others.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
{\em Explicit parallel programming} provides full control over
parallel resources by offering primitives for creating and managing
parallel tasks, which are small, independent threads of control.   
As a result, the programmer can, at least in principle, write efficient
parallel programs by performing a careful cost-benefit analysis to
determine which tasks should be executed in parallel and under what
conditions.  This approach, however, often requires reasoning about
low-level execution details, such as data races or concurrent effects,
which is known to be notoriously hard; it can also result in code that
performs well in a particular hardware setting but not in others.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::complexities
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::complexities
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>The complexities of parallel programming with explicit languages have motivated interest in <span><em>implicitly parallel languages</em></span>, such as Cilk <span class="citation" data-cites="Blumofe95"></span>, Manticore <span class="citation" data-cites="Fluet:2008:SFG:1411204.1411239 FluetRaReSh11"></span>, Multilisp <span class="citation" data-cites="Halstead85"></span>, and NESL <span class="citation" data-cites="nesl-implement"></span>. These languages enable the programmer to express opportunities for parallelism via language constructs, <em>e.g.</em>, parallel sequences, parallel arrays, and parallel tuples. This implicit approach enables a declarative programming style by delegating the task of utilizing the parallelism exposed by the program to the compiler and the run-time system. As an implicit parallel program executes, it exposes opportunities for parallelism (as indicated by the parallel constructs) and the language run-time system creates parallel tasks as needed. To execute parallel tasks efficiently, implicit programming languages use a scheduler to load balance, <em>i.e.</em>, distribute parallel tasks among processors. Various scheduling techniques and practical schedulers have been developed, including work-stealing schedulers <span class="citation" data-cites="BlumofeWorkStealing AroraBlPl98 AcarBlBl02"></span> and depth-first-search schedulers <span class="citation" data-cites="BlellochGr96"></span>.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
The complexities of parallel programming with explicit languages have
motivated interest in {\em implicitly parallel languages}, such as
Cilk~\cite{Blumofe95},
Manticore~\cite{Fluet:2008:SFG:1411204.1411239,FluetRaReSh11},
Multilisp~\cite{Halstead85}, and NESL~\cite{nesl-implement}.  These
languages enable the programmer to express opportunities for
parallelism via language constructs, \textit{e.g.}, parallel
sequences, parallel arrays, and parallel tuples.  This implicit
approach enables a declarative programming style by delegating the
task of utilizing the parallelism exposed by the program to the
compiler and the run-time system.  As an implicit parallel program
executes, it exposes opportunities for parallelism (as indicated by
the parallel constructs) and the language run-time system creates
parallel tasks as needed.  To execute parallel tasks efficiently,
implicit programming languages use a scheduler to load balance,
\textit{i.e.}, distribute parallel tasks among processors.  Various
scheduling techniques and practical schedulers have been developed,
including work-stealing
schedulers~\cite{BlumofeWorkStealing,AroraBlPl98,AcarBlBl02} and
depth-first-search schedulers~\cite{BlellochGr96}.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::experience
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::experience
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Experience with implicitly parallel programs shows that one of the most important decisions that any implicit parallel language must make is determining whether or not to exploit an opportunity for parallelism by creating a parallel task. Put another way, the question is to determine which tasks to execute in parallel and which tasks to execute sequentially. This problem, often referred to as the <span><em>granularity problem</em></span>, is important because creating a parallel task requires additional overhead. If the task granularity is not handled effectively, task-creation overheads can easily obliterate the benefit of parallelism.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Experience with implicitly parallel programs shows that one of the
most important decisions that any implicit parallel language must make
is determining whether or not to exploit an opportunity for
parallelism by creating a parallel task.  Put another way, the
question is to determine which tasks to execute in parallel and which
tasks to execute sequentially.  This problem, often referred to as the
{\em granularity problem}, is important because creating a parallel
task requires additional overhead. If the task granularity is not
handled effectively, task-creation overheads can easily obliterate the
benefit of parallelism.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::programs
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::programs
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Many parallel programs are characterized by parallel slackness <span class="citation" data-cites="Valiant90"></span>, a property which indicates that the program exposes many more opportunities for parallelism than the number of available processors. In such programs, effective granularity control is crucial because the program typically creates many small tasks, thereby ensuring significant scheduling overhead.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Many parallel programs are characterized by parallel
slackness~\cite{Valiant90}, a property which indicates that the
program exposes many more opportunities for parallelism than the
number of available processors. In such programs, effective
granularity control is crucial because the program typically creates
many small tasks, thereby ensuring significant scheduling overhead.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::known
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::known
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>No known broadly applicable solution to the granularity problem exists. Theoretical analyses often ignore task-creation overheads, yielding no significant clues about how these overheads may affect efficiency. Practical implementations often focus on reducing task-creation overheads instead of attempting to control granularity. As a result, practitioners often deal with this issue by trying to estimate the right granularity of work that would be sufficiently large to execute in parallel. Since the running time of a task depends on the hardware, such manual control of granularity is difficult and bound to yield suboptimal results and/or non-portable code <span class="citation" data-cites="lazy-binary-splitting"></span>.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
No known broadly applicable solution to the granularity problem
exists. Theoretical analyses often ignore task-creation overheads,
yielding no significant clues about how these overheads may affect
efficiency.  Practical implementations often focus on reducing
task-creation overheads instead of attempting to control granularity.
As a result, practitioners often deal with this issue by trying to
estimate the right granularity of work that would be sufficiently
large to execute in parallel.  
Since the running time of a task depends on the hardware, such manual
control of granularity is difficult
and bound to yield suboptimal results and/or non-portable
code~\cite{lazy-binary-splitting}.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::paper
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::paper
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>In this paper, we propose theoretical and practical techniques for the granularity problem in implicit parallel-programming languages. Our results include theorems that characterize how parallel run time is affected by task-creation overheads, which we show to be significant (Section <a href="#sec:theorems" data-reference-type="ref" data-reference="sec:theorems">[sec:theorems]</a>). To reduce these overheads, we consider a granularity control technique that relies on an oracle for determining the run-time of parallel tasks (Section <a href="#sec:cost-semantics" data-reference-type="ref" data-reference="sec:cost-semantics">[sec:cost-semantics]</a>). We show that if the oracle can be implemented efficiently and accurately, it can be used to improve efficiency for a relatively large class of computations (Section <a href="#sec:theorems" data-reference-type="ref" data-reference="sec:theorems">[sec:theorems]</a>). Based on this result, we describe how oracles can be realized in practice; we call this technique <span><em>oracle scheduling</em></span> because it relies on an oracle to estimate task sizes and because it can be used in conjunction with practically any other scheduler (Section <a href="#sec:schedule" data-reference-type="ref" data-reference="sec:schedule">[sec:schedule]</a>). Finally, we propose an implementation of oracle scheduling that uses complexity functions defined by the user to approximate accurately run-time of parallel tasks (Section <a href="#sec:schedule" data-reference-type="ref" data-reference="sec:schedule">[sec:schedule]</a>). We present an implementation and evaluation of the proposed approach by extending a subset of the Caml language (Sections <a href="#sec:imp" data-reference-type="ref" data-reference="sec:imp">[sec:imp]</a> and <a href="#sec:exp" data-reference-type="ref" data-reference="sec:exp">[sec:exp]</a>).</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
In this paper, we propose theoretical and practical techniques for the
granularity problem in implicit parallel-programming languages.  Our
results include theorems that characterize how parallel run time is
affected by task-creation overheads, which we show to be significant
(\secref{theorems}).  To reduce these overheads, we consider
a granularity control technique that relies on an oracle for
determining the run-time of parallel tasks (\secref{cost-semantics}).  We
show that if the oracle can be implemented efficiently and accurately,
it can be used to improve efficiency for a relatively large class of
computations (\secref{theorems}). 
Based on this result, we describe how oracles can be
realized in practice; we call this technique {\em oracle scheduling}
because it relies on an oracle to estimate task sizes and because it
can be used in conjunction with practically any other scheduler
(\secref{schedule}).  Finally, we propose an implementation of oracle
scheduling that uses complexity functions defined by the user to
approximate accurately run-time of parallel tasks (\secref{schedule}).
We present an implementation and evaluation of the proposed approach
by extending a subset of the Caml language (\secreftwo{imp}{exp}).
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::brent
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::brent
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Brent’s theorem <span class="citation" data-cites="Brent74"></span>, commonly called the work-time principle, characterizes what is arguably the most important benefit of parallel programs, which is that a parallel program can be executed on a multiprocessor to obtain near linear speedups. For a computation, let <span><em>raw work</em></span>, written w, refer to the total number of executed instructions, and let <span><em>raw depth</em></span>, written d, refer to the length longest dependent chain of executed instructions. Brent’s theorem shows that we can execute a computation with <span class="math inline">\(w\)</span> raw work and <span class="math inline">\(d\)</span> raw depth in no more than <span class="math inline">\(w/P + d\)</span> steps on <span class="math inline">\(P\)</span> processors using any <em>greedy scheduler</em>. <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> A greedy scheduler is a scheduler that can find available work immediately. This assumption is reasonably realistic, as practical multiprocessor scheduling algorithms, such as work-stealing, can match Brent’s bound asymptotically for certain relatively large classes of computations, <em>e.g.</em>, fork-join and nested data-parallel computations.</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Note that the bound is tight within a factor of two.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩</a></p></li>
</ol>
</section>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Brent's theorem~\cite{Brent74}, commonly called the work-time
principle, characterizes what is arguably the most important benefit
of parallel programs, which is that a parallel program can be executed
on a multiprocessor to obtain near linear speedups.  For a
computation, let {\em raw work}, written \sw, refer to the total
number of executed instructions, and let {\em raw depth}, written \sd,
refer to the length longest dependent chain of executed instructions.
Brent's theorem shows that we can execute a computation with $\sw$ raw
work and $\sd$ raw depth in no more than $\sw/P + \sd$ steps on $P$
processors using any \emph{greedy scheduler}.~\footnote{Note that the
  bound is tight within a factor of two.}  A greedy scheduler is a
scheduler that can find available work immediately.  This assumption
is reasonably realistic, as practical multiprocessor scheduling
algorithms, such as work-stealing, can match Brent's bound
asymptotically for certain relatively large classes of computations,
\textit{e.g.}, fork-join and nested data-parallel computations.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::execution
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::execution
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>In the execution model with raw work and raw depth, each instruction implicitly is assigned unit cost. Unfortunately, this model does not direcly account for task-creation overheads. To assess the significance of these overheads in implicitly parallel programs, we consider a lambda calculus with parallel tuples and present a cost-semantics for evaluating expression of this language (Section <a href="#sec:cost-semantics" data-reference-type="ref" data-reference="sec:cost-semantics">[sec:cost-semantics]</a>). The cost semantics accounts for task-creation overheads by assigning non-unit costs to the operations generating such overheads. In addition to raw work and raw depth, the cost semantics yield total work, total depth of each evaluated expression. We define <span><em>total work</em></span>, written , as the total cost of the evaluated instructions, and <span><em>total depth</em></span>, written , as the total cost of the most expensive dependent chain of evaluated instructions—total work and total depth include task-creation overheads.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
In the execution model with raw work and raw depth, each instruction
implicitly is assigned unit cost.  Unfortunately, this model does not
direcly account for task-creation overheads.  To assess the
significance of these overheads in implicitly parallel programs, we
consider a lambda calculus with parallel tuples and present a
cost-semantics for evaluating expression of this language
(\secref{cost-semantics}).  The cost semantics accounts for
task-creation overheads by assigning non-unit costs to the operations
generating such overheads.  In addition to raw work and raw depth, the
cost semantics yield total work, total depth of each evaluated
expression.  We define {\em total work}, written \sws, as the total
cost of the evaluated instructions, and {\em total depth}, written
\sds, as the total cost of the most expensive dependent chain of
evaluated instructions---total work and total depth include
task-creation overheads.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::using
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::using
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Using this cost semantics, we show that task creation overheads can be a significant multiplicative factor of the raw work. To understand the understand the impact of the overheads, we adapt Brent’s theorem to take them into account (Section <a href="#sec:cost-semantics" data-reference-type="ref" data-reference="sec:cost-semantics">[sec:cost-semantics]</a>). Specifically, we show that parallel computations with total work <span class="math inline">\(\mathcal{W}\)</span> and total depth <span class="math inline">\(\mathcal{D}\)</span> can be executed in no more than <span class="math inline">\(\mathcal{W}/P + \mathcal{D}\)</span> steps. Intuitively, this bound shows that task-creation overheads contribute directly to the parallel run time just like any other work. Combined with the result that task-creation overheads can increase total work by a multiplicative factor, the generalized Brent’s theorem implies that the overheads slow down parallel run time by a multiplicative factor.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Using this cost semantics, we show that task creation overheads can be
a significant multiplicative factor of the raw work.  To understand
the understand the impact of the overheads, we adapt Brent's theorem
to take them into account (\secref{cost-semantics}).  Specifically, we
show that parallel computations with total work $\sws$ and total
depth $\sds$ can be executed in no more than $\sws/P + \sds$ steps.
Intuitively, this bound shows that task-creation overheads contribute
directly to the parallel run time just like any other work.  Combined
with the result that task-creation overheads can increase total work
by a multiplicative factor, the generalized Brent's theorem implies
that the overheads slow down parallel run time by a multiplicative
factor.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::reduce
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::reduce
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>To reduce task-creation overheads, we propose an alternative <span> <em>oracle semantics</em></span> that capture a well-known principle for avoiding the task-creation overheads. We evaluate a task in parallel only if its is sufficiently large, <em>i.e.</em>, greater than some <span> <em>cutoff</em></span> constant <span class="math inline">\(\kappa\)</span>. We show that the oracle semantics can decrease the overheads of task-creation by any desired constant factor <span class="math inline">\(\kappa\)</span>, but only at the cost of increasing the total depth (Sections <a href="#sec:cost-semantics" data-reference-type="ref" data-reference="sec:cost-semantics">[sec:cost-semantics]</a> and <a href="#sec:analysis" data-reference-type="ref" data-reference="sec:analysis">[sec:analysis]</a>). These bounds suggest that we can reduce the task-creation overheads significantly, if we can realize the semantics in practice. This unfortunately is impossible because it requires determining a priori task-creation overheads. We show, however, that a realistic oracle that can give constant-factor approximations to the task run times can still result in similar reductions in the overheads. We show that if we have prior knowledge of the raw work and the raw depth of a computation, then we can pick the optimal cutoff constant that yields the fastest parallel run time for a class of computations. We also show that, under some assumptions, there exists a constant that reduces the task creation overheads to a small constant ratio of the raw work, without increasing the depth of the computation in a way that would significantly affect the run time.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
To reduce task-creation overheads, we propose an alternative {\em
  oracle semantics} that capture a well-known principle for avoiding
the task-creation overheads. We evaluate a task in parallel only if
its is sufficiently large, \textit{i.e.}, greater than some {\em
  cutoff} constant $\coff$.  We show that the oracle semantics can
decrease the overheads of task-creation by any desired constant
factor $\coff$, but only at the cost of increasing the total depth
(\secreftwo{cost-semantics}{analysis}).
These bounds suggest that we can reduce the
task-creation overheads significantly, if we can realize the semantics
in practice.  This  unfortunately is impossible because it requires
determining a priori task-creation overheads.  We show, however, that
a realistic oracle that can give constant-factor approximations to the
task run times can still result in similar reductions in the
overheads. We show that if we have prior knowledge of the
raw work and the raw depth of a computation, then we can pick the
optimal cutoff constant \coff that yields the fastest parallel run
time for a class of computations.  We also show that, under some
assumptions, there exists a constant \coff that reduces the task
creation overheads to a small constant ratio of the raw work, without
increasing the depth of the computation in a way that would 
significantly affect the run time.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::realize
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::realize
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>To realize the oracle semantics in practice, we describe a scheduling technique that we call <span><em>oracle scheduling</em></span> (Section <a href="#sec:schedule" data-reference-type="ref" data-reference="sec:schedule">[sec:schedule]</a>). Oracle scheduling relies on a <span><em>task-size estimator</em></span> that can estimate the actual run time of parallel tasks in constant-time within a constant factor of accuracy, and a conventional greedy scheduling algorithm, <em>e.g.</em>, work-stealing, or a parallel depth-first scheduler. Oracle schedulers perform efficient parallel task creation by selectively executing in parallel only those tasks that have a large parallel run-time. We describe an instance of the oracle scheduler that relies on an estimator that uses asymptotic cost functions (asymptotic complexity bounds) and judicious use of run-time profiling techniques to estimate actual run-times accurately and efficiently. This approach combines an interesting property of asymptotic complexity bounds, which are expressed without hardware-dependent constants, and profiling techniques, which can be used to determine these constants precisely.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
To realize the oracle semantics in practice, we describe a scheduling
technique that we call {\em oracle scheduling} (\secref{schedule}).
Oracle scheduling relies on a {\em task-size estimator} that can
estimate the actual run time of parallel tasks in constant-time within
a constant factor of accuracy, and a conventional greedy scheduling
algorithm, \textit{e.g.}, work-stealing, or a parallel depth-first
scheduler.  Oracle schedulers perform efficient parallel task creation
by selectively executing in parallel only those tasks that have a
large parallel run-time. We describe an instance of the oracle
scheduler that relies on an estimator that uses asymptotic cost
functions (asymptotic complexity bounds) and judicious use of run-time
profiling techniques to estimate actual run-times accurately and
efficiently.  This approach combines an interesting property of
asymptotic complexity bounds, which are expressed without
hardware-dependent constants, and profiling techniques, which can be
used to determine these constants precisely.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::present
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::present
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>We present a prototype implementation of the proposed approach (Section <a href="#sec:imp" data-reference-type="ref" data-reference="sec:imp">[sec:imp]</a>) by extending the OCAML language to support parallel tuples and complexity functions. The implementation translates programs written in this extended language to the PML (Parallel ML) language <span class="citation" data-cites="FluetRaReSh11"></span>. Although our implementation requires the programmer to enter the complexity information, this information could also be inferred in some cases via static analysis (<em>e.g.</em>, <span class="citation" data-cites="JostHaLoHo10"></span> and references therein). In our implementation, for simplicity we only consider programs for which the execution time is (with high probability) proportional to the value obtained by evaluating the asymptotic complexity expression. We extend the Manticore compiler for PML to support oracle scheduling and use it to compile generated PML programs. Our experiments (Section <a href="#sec:exp" data-reference-type="ref" data-reference="sec:exp">[sec:exp]</a>) show that oracle implementation can reduce the overheads of a single processor parallel execution to between 3 and 13 percent of the sequential time. When using 16 processors, we achieve 7- to 15-fold speedups on an AMD machine and 6- to 10-fold speedups on an Intel machine.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
We present a prototype implementation of the proposed approach
(\secref{imp}) by extending the OCAML language to support parallel
tuples and complexity functions.  The implementation translates
programs written in this extended language to the PML (Parallel ML)
language~\cite{FluetRaReSh11}.  Although our implementation requires
the programmer to enter the complexity information, this information
could also be inferred in some cases via static analysis
(\textit{e.g.},~\cite{JostHaLoHo10} and references therein).  In our
implementation, for simplicity we only consider programs for which the
execution time is (with high probability) proportional to the value
obtained by evaluating the asymptotic complexity expression.  We
extend the Manticore compiler for PML to support oracle scheduling and
use it to compile generated PML programs.  Our experiments
(\secref{exp}) show that oracle implementation can reduce the
overheads of a single processor parallel execution to between 3 and 13
percent of the sequential time.
When using 16 processors, we achieve 7- to 15-fold speedups
on an AMD machine and 6- to 10-fold speedups on an Intel machine.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::generalizing
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::generalizing
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[

]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\begin{comment}

\section{Generalizing Brent's theorem}
\label{sec:brent}

\begin{figure}
\centering
\ifx\arthur\false
\includegraphics[width=0.4\columnwidth]{pictures/computation-dag}
\fi
\caption{An example computation DAG.}
\label{fig:dag}
\end{figure}




We represent a parallel computation with a directed acyclic graph, 
called {\em computation DAG}. Nodes in the graph represent
atomic computations.
Edges between nodes represent precedence relations,
in the sense that an edge from $a$ to $b$ indicates that the execution of $a$ must
be completed before the execution of $b$ can start.
Every computation DAG includes a {\em source} node and a {\em sink} node,
representing the starting and the end points of the computation, respectively.  
Those nodes are such that all nodes of a computation DAG are reachable from
the source node, and the sink node is reachable from all nodes.
An example computation DAG appears in \figref{dag}.

In the traditional computational model, every atomic computation is
considered to take a single unit of time. In other words, every
node has weight $1$. In this setting, we can
define the standard notion of work and depth, which we here
call {\em raw work} and {\em raw depth}.
The {\em raw work} of a computation graph is equal to the total number of 
nodes that it contains. The {\em raw depth} of the computation graph
is equal to the total number of nodes along the longest path.
Brent proved the following bound.

\begin{theorem}[Brent's theorem]
\label{lem:brent}
Let $G$ be a computation DAG with $\sw$ raw work and $\sd$ raw depth.
Any greedy scheduler can execute the computation in $G$ 
in time $O(\frac{\sw}{P} + d)$ on a $P$-processor parallel machine
\end{theorem}
\begin{proof}
We recall Brent's proof since our aim is to generalize it.
Consider the nodes at depth $i$ in the DAG, and assume there are
$\sw_i$ of them. A greedy scheduler can spend no more than time
$\ceil{\frac{\sw_i}{P}}$ for executing those nodes. 
Summing up over the various depths, one can thus deduce that
the total execution time does not exceed:
$$\sum_{i=1}^{\sd}{\ceil{\frac{\sw_i}{P}}}
\Sc\leq \sum_{i=1}^{\sd}{(\frac{\sw_i}{P}+1)}
\Sc\leq \frac{\sum_{i=1}^{\sd}{\sw_i}}{P} + \sd
\Sc\leq \frac{\sw}{p} + \sd$$
\qed
\end{proof}
Observe that the bound provided by Brent's theorem is tight,
because the execution time is at least $\kwmax{ \frac{\sw}{P} }{ \sd }$.

This theorem does not take into account the overheads associated with task creation.
So, we want to refine the model and generalize Brent's theorem.
To that end, we consider that if a node creates parallel tasks
then an extra computation cost \csp needs to be paid for. In other words, 
any node that has an out-degree two or greater now has weight $1+\csp$
instead of just~$1$.
We then define the {\em total work} as the sum of the weights of all 
the nodes in this revised computation graph.
Similarly, we define the {\em total depth} as the maximum weight of 
a path from the source to the sink in the revised graph.
A first attempt at generalizing Brendt's theorem is as follows.

\begin{theorem}[Naive generalization of Brent's theorem]
Let $G$ be a computation DAG with $\sws$ total work and $\sd$ raw depth.
Any greedy scheduler can execute these computations
in time $O(\frac{\sws}{P} + (1+\csp) \sd)$ on $P$ processors.
\end{theorem}
\begin{proof}
Consider layers like in Brent's theorem, with the difference that
at every layer there might be tasks of weight $1$ and tasks
of weight $1+\csp$. Observe that there is still exactly $d$ levels.
Hereafter, let $\myr$ be a shorthand for $1 +\csp$.
Let $\sws_i$ denote the sum of the weights of the nodes at level $i$.
A greedy scheduler executes this work in a time less 
than $\myr \ceil{\frac{\sws_i}{\myr P}}$.
Thus, the total time execution is bounded by:
\aremark{Can you detail where the $\myr \ceil{\frac{\sws_i}{\myr P}}$ bound comes from?}
$$\sum_{i=1}^{\sd}{\myr \ceil{\frac{\sws_i}{\myr P}}}
\Sc\leq \sum_{i=1}^{\sd}{\myr(\frac{\sws_i}{\myr P}+1)}
\Sc\leq \frac{\sws}{P} + \myr \sd  \quad \qed$$
\end{proof}

In the above theorem, Brent's original theorem generalizes with
respect to total work, in the sense that the ratio $\frac{\sw}{P}$
gets replaced by $\frac{\sws}{P}$, however it does not generalize as well
with respect to the depth, because the component $d$ is replaced
by $(1+\csp)d$ and not $\sds$.
For computations that involve task creation all along their critical
path, $D$ can be equal to $(1+\csp)d$, so in this case the naive generalization
of Brent's theorem already gives a tight bound.
However, there are computations for which $(1+\csp)d$ can be 
significantly bigger than the total depth $\sds$.
Typically, $\sds$ might be of the form $d+n\csp$ for some $n$.
In this case, the bound obtained is extremely loose. 
We remedy to this situation by establishing a tight bound 
that nicely generalizes the statement of Brent's theorem.

\begin{theorem}[Generalized version of Brent's theorem]
\label{thm:generalize-brent}
\label{thm:generalized-brent}
Let $G$ be a computation DAG with $W$ total work and $D$ total depth.
Any greedy scheduler can execute these computations
in time $O(\frac{W}{P} + D)$ on $P$ processors. 
\end{theorem}
\begin{proof}
The problem shares similarities with the classic problem known 
as $P|prec|C_{max}$ in scheduling theory.
This problem consists in scheduling tasks on $P$ machines
in a way that minimize the total makespan,
while satisfying a set of precedence constraints 
Our problem, however, differs in a significant way: 
we do not want to establish a bound for a particular scheduler,
but instead we want to establish a result for a entire class of
scheduler, covering all the schedulers that are greedy
(they never wait if there is work to do) and on-line
(they are not aware of the existence of a task until is
becomes available).
Our proof reuses a particular aspect of the proof
of 2-optimality of the greedy ``earliest-job-first'' 
approximation algorithm for the problem $P|prec|C_{max}$.
More specifically, we build a particular sequence of
tasks iteratively, starting from the last one.
The structure of our proof is, however, significantly different.
In particular, the invariants are more complex
because we are making fewer assumptions about the scheduler's policy.

Consider a scheduling of tasks by a greedy scheduler.
Our goal is to prove a bound on the total execution time $T$.
Let the tasks be labelled using integers from $1$ to $M$.
The duration of task $i$ is written $w_i$, and the time at
which it starts is written $t_i$. We call $\GD_i$
the time interval $\myint{t_i}{t_i+w_i}$ during which the
task $i$ is executed.
To capture the dependencies, we consider a set of
precedence constraints: $i\prec j$ indicates that the task $j$
depends directly on indirectly on the result of the task $i$.
For the sake of the proof, we assume that the set of tasks
includes a task of duration zero such that all other tasks depend on it.
This task is scheduled at time $0$.
Similarly, we assume
the existence of a task of duration zero 
such that this task depend on all other tasks.
This task is scheduled at time $T$.
Hereafter, let $\Gp$ denote a sequence of tasks of the form
$\Gp_1 \prec \Gp_2 \prec \ldots \prec \Gp_N$.
We write $\PL{\Gp}$ the number of tasks in the path $\Gp$
and $\PW{\Gp}$ the sum of the duration of the tasks in that path, that is, 
the value $\mysum{n}{1}{N}{\Gp_n}$.
By definition of work and depth, we have
$\sws=\mysum{1}{i}{M}{w_i}$ and $\sds=\mymax{\Gp}{\PW{\Gp}}$.
Our goal is to show $T \leq \frac{\sws}{P} + D$.

$\bullet$ Let $(\myint{u_k}{v_k})_{k\in[1,K-1]}$ be the 
set of nonempty time intervals during which not all processors 
are working. We define $u_K = v_K = T$. 
The total time $T$ decomposes into the total time during
which all processors are busy, call it $T_{full}$, and the
total time during which not all processors are busy, call it $T_{partial}$.
We thus have $T = T_{full} + T_{partial}$.
Technically, we have
$T_{full} = \mysum{k}{1}{K-1}{(u_{k+1}-v_{k})}$
and $T_{partial} = \mysum{k}{1}{K}{(v_k-u_{k})}$.
During the time when processors are fully busy, they execute an amount
of work equal to $P\cdot T_{full}$ .This amount cannot exceed the
total amount of work available, which is $\sws$. So, we have $T_{full} \leq \sws / P$.
In order to establish that $T\leq \sws/P + \sds$, it therefore
remains to show that $T_{partial}\leq \sds$.

$\bullet$ {\bf Observation A:}
If a task $i$ starts after the time $v_k$, for some $k$, then
there exists a task $j$ that executes at time $v_k$ and such that $i$ depends on $j$.
Formally, $$\Tfor{ik}\; t_i > v_k \Sc\impl \Texi{j}\;\, {j \prec i}\Sc\land v_k \in \GD_j$$
To prove this, consider the set of tasks that $i$ depends on, and add $i$ itself to that set.
Select from this set the subset of tasks that starts after $v_k$. 
Call $j'$ the task among these that has the minimal starting time (\textit{i.e.} $t_{j'}$ minimal).   
Now, consider all the tasks that $j'$ depends on. 
Due to the minimality of $t_{j'}$, all those tasks must start before $v_k$.
If none of those task is executing at the time $v_k$, then it means that
the task $j'$ could have been scheduled to start just before $v_k$.
Indeed, there was a free scheduler at this point because the interval
$\myint{u_k}{v_k}$ corresponds to a nonempty period of time where not
all processors are busy. So, there must exists at least one task $j$
that executes at time $v_k$ and such that $j \prec j'$.
We therefore have $j\prec i$ and $v_k \in \GD_j$.

$\bullet$ {\bf Observation B:}
If we have a task $i_1$ that executes at time $v_k$ then we can find a 
sequence of tasks $i_N \prec \ldots \prec i_1$ such that these tasks
entirely cover the interval $\myint{u_k}{v_k}$.
Formally, 
$$\begin{lines}
\Tfor{i_1 k}\;\, v_k\in\GD_{i_1} \Sc\impl  \Texi{i_2 \ldots i_N}\;\, \\
\qquad \begin{ands}
 i_N \prec \ldots \prec i_1  \\
 u_k \in \GD_{i_N} \\
 v_k-u_k = \mysum{n}{1}{N}{ \PN{ \GD_{i_n} \cap \myint{u_k}{v_k} } }
\end{ands}
\end{lines} $$
Above, the expression $\PN{ \GD_{i_n} \cap \myint{u_k}{v_k} } $
corresponds to the aomunt of time that the task $i$ spent
being executed inside the time interval $\myint{u_k}{v_k}$.
We construct the sequence $(i_n)$ iteratively
in such a way that the tasks are adjacents to each others.
Technically, we have $t_{i_n} = t_{i_{n+1}} + w_{i_{n+1}}$ for $n \in \myint{1}{N-1}$.
Initially, we only have $i_1$. 
At a given point in the construction, the sequence built up to
index $n$. There are two cases.
If $u_k \in \GD_{i_n}$, then we are done ($N=n$). Otherwise,
the task $i_n$ must depend on a task that ends at time $t_{i_n}$
If this was not the case then the task $i_n$ 
could have been scheduled earlier.
Indeed, we have $u_k < t_{i_n} \leq v_k$ so
there is at least one processor available just before time $t_{i_n}$).
We call $i_{n+1}$ the task that preceeds $i_n$, and we then repeat the process. 
Since there are
only a finite number of tasks, the process must end after finitely many iterations. 
Note that we must reach the date $u_k$ at some point, because the last task that
can be considered is the task that is scheduled at time $0$, which is earlier than $u_k$.

$\bullet$ {\bf Main induction:} we construct a sequence of tasks
that belong to a same precedence path and
that covers all the periods of time where not all processors are busy.
To build this sequence, we exploit observation A to traverse periods of full activity
and exploit observation B to cover periods of partial activity.
More precisely, we prove by induction that, for any $L \in \myint{1}{K}$, there exists
a path $\Gp$ such that the task at the head of the path $\Gp$ is running
at time $v_L$ and such that the sum of the execution time of the tasks involved
in the path $\Gp$, counting only the execution occuring in the interval
$\myint{u_L}{u_K}$, is greater than the sum of the width of the intervals
of the form $\myint{u_k}{v_k}$ for $k\geq L$. Formally,
$$\Tfor{L}\;\Texi{\Gp}\, u_L \in \GD_{\F{hd}(\Gp)} \Sc\land
\mysum{k}{1}{L}{ (v_k - u_k) } \leq
\mysum{n}{1}{\PL{\Gp}}{ \PN{ \GD_{\Gp_n} \cap \myint{u_L}{u_K} } }$$

The base case is $L=K$. In this case, we define $\Gp$ as the singleton
path made of the tasks that depends on all the others. This task is 
executed at time $u_K$ (which is equal to $T$), so we have $u_K \in \GD_{\F{hd}(\Gp)}$.
Since $v_K = u_K$, the two sums involved are both equal to zero, so we are done for the base case.

Now, assume the result true for $L$, and let us establish it for $L+1$.
By induction hypothesis, there exists a path $\Gp$
such that $u_L \in \GD_i$, where $i$ denotes the head of the path $\Gp$,
and such that
$\mysum{k}{1}{L}{ (v_k - u_k) } \leq \mysum{n}{1}{\PL{\Gp}}{ \PN{ \GD_{\Gp_n} \cap \myint{u_L}{u_K} } } $.
The first step consists in extending the path $\Gp$ into a path $\Gp'$ whose 
head task, call it $j_1$, is executing at time $v_{L+1}$. There are two cases, if 
$t_i \leq v_{L+1}$, then we can simply define $\Gp' = \Gp$ and we have $j_1 = i$.
Otherwise, $t_i > v_{L+1}$, so we can apply observation A to get a task $j_1$
such that $j_1 \prec i$ and $v_{L+1}\in \GD_{j_1}$, and we then define $\Gp' = j_1 \cdot \Gp$.
Now, we apply observation B, which asserts the existence of a sequence of tasks
$j_N \prec \ldots \prec j_2 \prec j_1$ such that $u_{K+1} \in \GD_{j_N}$
and $v_{L+1}-u_{L+1} = \mysum{n}{1}{N}{ \PN{ \GD_{j_n} \cap \myint{u_k}{v_k} } }$.
The path $\Gp''$ defined as $j_N \cdot \ldots \cdot j_2 \cdot j_1 \cdot \Gp'$
covers the time interval $\myint{u_{L+1}}{u_K}$. This path can be used to conclude.
First, the head of the path $\Gp''$ is the task $j_N$, which satisfies $u_{K+1} \in \GD_{j_N}$
as required.
Second, the required inequality is as shown next.
$$\begin{array}{lll}
& \mysum{n}{1}{\PL{\Gp''}}{ \PN{ \GD_{\Gp''_n} \cap \myint{u_{L+1}}{u_K} } }  \vspace{3pt}\\
\geq & \phantom{+\;} \mysum{n}{1}{N}{ \PN{ \GD_{j_n} \cap \myint{u_{L+1}}{u_{L+1}} } } \\
& +\; \mysum{n}{1}{\PL{\Gp}}{ \PN{ \GD_{\Gp_n} \cap \myint{u_{L}}{u_K} } } \vspace{3pt}\\
\geq & (v_{L+1} - u_{L+1}) + \mysum{k}{1}{L}{ (v_k - u_k) } \\
\geq & \mysum{k}{1}{L+1}{ (v_k - u_k) }
\end{array}$$
The case where $j_1 = i$ is a bit delicate.
This case occurs when the execution of task $i$ intersects
with several periods of time during which not all processors are working.
In this case, we also have $\Gp''_N = i$, so a part of the task $i$ appears as
$\PN{\GD_{\Gp''_N} \cap \myint{u_{L+1}}{u_K} }$ 
and another part appears as $\PN{ \GD_{j_1} \cap \myint{u_{L+1}}{u_{L+1}} }$.

$\bullet$ {\bf Conclusion:} 
We construct a path $\Gp$ by applying the result from the main induction with $L=1$.
We can then establish the inequality $T_{partial}\leq \sds$ as follows.
$$\begin{lines}
 T_{partial}
\Sc{=} \mysum{k}{1}{K}{ (v_k - u_k) }  
\Sc\leq \mysum{n}{1}{\PL{\Gp}}{ \PN{ \GD_{\Gp_n} \cap \myint{u_1}{u_K} } } \\
\Sc\leq \mysum{n}{1}{\PL{\Gp}}{ \GD_{\Gp_n} } 
\Sc\leq \PW{\Gp}
\Sc\leq \mymax{\Gp'}{\PW{\Gp'}} 
\Sc{=} \sds 
\end{lines}$$
\qed
\end{proof}

\end{comment}
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->


</segment> <!-- section -->

<segment name='section'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Source Language
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Source Language
]]>
</field> <!-- title_src -->
<field name='label'>
sec:cost-semantics
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::give
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::give
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>To give an accurate account of the cost of task creation, and to specify precisely our compilation strategy, we consider a source language in the style of the <span class="math inline">\(\lambda\)</span>-calculus and present a dynamic cost semantics for it. The semantics and the costs are parameterized by <span class="math inline">\(\tau\)</span> and <span class="math inline">\(\phi\)</span>, which represent the cost of creating a parallel task and the cost of consulting an external oracle for predicting the sizes of its two branches respectively. By using a known proof technique, we generalize Brent’s theorem to take task-creation overheads into account.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
To give an accurate account of the cost of task creation, and to
specify precisely our compilation strategy, we consider a source
language in the style of the $\lambda$-calculus and present a dynamic
cost semantics for it.  The semantics and the costs are parameterized
by $\csp$ and $\corc$, which represent the cost of creating a parallel
task and the cost of consulting an external oracle for predicting the
sizes of its two branches respectively.  By using a known proof
technique, we generalize Brent's theorem to take task-creation
overheads into account.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::abstract
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='figure'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
fig:src::syntax
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p><span class="math display">\[\begin{array}{rcl}  
v       
&amp; {\bf ::=}
&amp; x {\bf \,\,\mid\,\,}\mathtt{{n}}{\bf \,\,\mid\,\,}\left({v},{v}\right) {\bf \,\,\mid\,\,}\mathtt{{inl}}~{v} {\bf \,\,\mid\,\,}
\mathtt{{inr}}~{v} {\bf \,\,\mid\,\,}\mathtt{{fun}}~{f}.{x}.{e}  
\\[2ex]  
e   
&amp; {\bf ::=}
&amp;   v {\bf \,\,\mid\,\,}\mathtt{{let}}~{x}~=~{e_1}~\mathtt{{in}}~{e_2} {\bf \,\,\mid\,\,}({v}~{v}) {\bf \,\,\mid\,\,}\mathtt{{fst}}~{v} {\bf \,\,\mid\,\,}\mathtt{{snd}}~{v} {\bf \,\,\mid\,\,}
\\  
&amp; &amp; \mathtt{{case}}~{v}~\mathtt{{of}}~\{\mathtt{{inl}}~{x}.{e}, \mathtt{{inr}}~{x}.{e}\} {\bf \,\,\mid\,\,}\left({e},{e}\right) {\bf \,\,\mid\,\,}{\left(|{e},{e}|\right)}    \\  
\end{array}\]</span>  <br />
<span>Abstract syntax of the source language</span> </p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\centering
\[
\begin{array}{rcl}
v     
& \bnfdef       
& x \bnfalt \kwn \bnfalt \kwt{v}{v} \bnfalt \kwinl{v} \bnfalt
\kwinr{v} \bnfalt \kwfun{f}{x}{e}
\\[2ex]
e 
& \bnfdef       
&   v \bnfalt  \kwletin{x}{e_1}{e_2} \bnfalt (\kwapp{v}{v}) \bnfalt \kwfst{v} \bnfalt \kwsnd{v} \bnfalt  
\\
& & \kwcase{v}{x}{e}{x}{e} \bnfalt  \kwt{e}{e} \bnfalt \kwpt{e}{e}    \\
\end{array}
\]
\caption{Abstract syntax of the source language} 
\label{fig:src::syntax}
]]>
</field> <!-- body_src -->

</atom> <!-- figure -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::value
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='figure'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
fig:src-dyn
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p><span class="math display">\[\begin{array}{c}  

\cfrac{  
\begin{array}{l@{\qquad}}  
v \Downarrow^{\alpha} v, (1,1), (1,1)  
\end{array}  
}{  
\begin{array}{l@{\qquad}}  
\strut  
\end{array}  
}
%\genfrac{}{}{}{0}{#2}{#1}
\quad\mbox{(value)}  
\\[2mm]  
\\  

\cfrac{  
\begin{array}{l@{\qquad}}  
 (\mathtt{{let}}~{x}~=~{e_1}~\mathtt{{in}}~{e_2})   \Downarrow^{\alpha} v, (w_1+w_2+1,d_1+d_2+1), (\mathcal{W}_1+\mathcal{W}_2+1,\mathcal{D}_1+\mathcal{D}_2+1)  
\end{array}  
}{  
\begin{array}{l@{\qquad}}  
 e_1  \Downarrow^{\alpha} v_1, (w_1,d_1), (\mathcal{W}_1,\mathcal{D}_1)   
\\  
 e_2[v_1/x]  \Downarrow^{\alpha} v, (w_2,d_2), (\mathcal{W}_2,\mathcal{D}_2)   
\end{array}  
}
%\genfrac{}{}{}{0}{#2}{#1}
\quad\mbox{(let)}  
\\[2mm]  
\\  

\cfrac{  
\begin{array}{l@{\qquad}}  
 ({v_1}~{v_2})  \Downarrow^{\alpha} v, (w+1,d+1), (\mathcal{W}+1,\mathcal{D}+1)  
\end{array}  
}{  
\begin{array}{l@{\qquad}}  
(v_1 = \mathtt{{fun}}~{f}.{x}.{e})  
\\  
 e[v_2/x, v_1/f]  \Downarrow^{\alpha} v, (w,d), (\mathcal{W},\mathcal{D})  
\end{array}  
}
%\genfrac{}{}{}{0}{#2}{#1}
\quad\mbox{(app)}  
\\[2mm]  
\\  

\cfrac{  
\begin{array}{l@{\qquad}}  
 (\mathtt{{fst}}~{\left({v_1},{v_2}\right)})  \Downarrow^{\alpha} v_1, (1,1), (1,1)  
\end{array}  
}{  
\begin{array}{l@{\qquad}}  
\end{array}  
}
%\genfrac{}{}{}{0}{#2}{#1}
\quad\mbox{(first)}  
\quad  

\cfrac{  
\begin{array}{l@{\qquad}}  
 (\mathtt{{snd}}~{\left({v_1},{v_2}\right)})  \Downarrow^{\alpha} v_2, (1,1), (1,1)  
\end{array}  
}{  
\begin{array}{l@{\qquad}}  
\end{array}  
}
%\genfrac{}{}{}{0}{#2}{#1}
\quad\mbox{(second)}  
\\[2mm]  
\\  

\cfrac{  
\begin{array}{l@{\qquad}}  
 \mathtt{{case}}~{(\mathtt{{inl}}~{v_1})}~\mathtt{{of}}~\{\mathtt{{inl}}~{x_1}.{e_1}, \mathtt{{inr}}~{x_2}.{e_2}\}  \Downarrow^{\alpha} v, (w+1,d+1), (\mathcal{W}+1,\mathcal{D}+1)  
\end{array}  
}{  
\begin{array}{l@{\qquad}}  
 e_1[v_1/x_1]  \Downarrow^{\alpha} v, (w,d), (\mathcal{W},\mathcal{D})  
\end{array}  
}
%\genfrac{}{}{}{0}{#2}{#1}
\quad\mbox{(case-left)}  
\\[2mm]  
\\  

\cfrac{  
\begin{array}{l@{\qquad}}  
 \mathtt{{case}}~{(\mathtt{{inr}}~{v_2})}~\mathtt{{of}}~\{\mathtt{{inl}}~{x_1}.{e_1}, \mathtt{{inr}}~{x_2}.{e_2}\}  \Downarrow^{\alpha} v, (w+1,d+1), (\mathcal{W}+1,\mathcal{D}+1)  
\end{array}  
}{  
\begin{array}{l@{\qquad}}  
 e_2[v_2/x_2]  \Downarrow^{\alpha} v, (w,d), (\mathcal{W},\mathcal{D})  
\end{array}  
}
%\genfrac{}{}{}{0}{#2}{#1}
\quad\mbox{(case-right)}  
\\[2mm]  
\\  

\cfrac{  
\begin{array}{l@{\qquad}}  
 \left({e_1},{e_2}\right)  \Downarrow^{\alpha}  \left({v_1},{v_2}\right) , (w_1+w_2+1,d_1+d_2+1), (\mathcal{W}_1+\mathcal{W}_2+1,\mathcal{D}_1+\mathcal{D}_2+1)  
\end{array}  
}{  
\begin{array}{l@{\qquad}}  
 e_1  \Downarrow^{\alpha} v_1, (w_1,d_1), (\mathcal{W}_1,\mathcal{D}_1)   
\\  
 e_2  \Downarrow^{\alpha} v_2, (w_2,d_2), (\mathcal{W}_2,\mathcal{D}_2)   
\end{array}  
}
%\genfrac{}{}{}{0}{#2}{#1}
\quad\mbox{(tuple)}  
\quad  
\\[2mm]  
\\  

\cfrac{  
\begin{array}{l@{\qquad}}  
 {\left(|{e_1},{e_2}|\right)}  \Downarrow^{\textsf{seq}}  \left({v_1},{v_2}\right) , (w_1+w_2+1,d_1+d_2+1), (\mathcal{W}_1+\mathcal{W}_2+1,\mathcal{D}_1+\mathcal{D}_2+1)  
\end{array}  
}{  
\begin{array}{l@{\qquad}}  
 e_1  \Downarrow^{\textsf{seq}} v_1, (w_1,d_1), (\mathcal{W}_1,\mathcal{D}_1)   
\\  
 e_2  \Downarrow^{\textsf{seq}} v_2, (w_2,d_2), (\mathcal{W}_2,\mathcal{D}_2)   
\end{array}  
}
%\genfrac{}{}{}{0}{#2}{#1}
\quad\mbox{(ptuple-seq)}  
\\[2mm]  
\\  

\cfrac{  
\begin{array}{l@{\qquad}}  
 {\left(|{e_1},{e_2}|\right)}  \Downarrow^{\textsf{par}}  \left({v_1},{v_2}\right) , (w_1+w_2+1,\mathtt{{max}}\left(d_1,d_2\right)+1), (\mathcal{W}_1+\mathcal{W}_2+1+\tau,\mathtt{{max}}\left(\mathcal{D}_1,\mathcal{D}_2\right)+1+\tau)  
\end{array}  
}{  
\begin{array}{l@{\qquad}}  
 e_1  \Downarrow^{\textsf{par}} v_1, (w_1,d_1), (\mathcal{W}_1,\mathcal{D}_1)   
\\  
 e_2  \Downarrow^{\textsf{par}} v_2, (w_2,d_2), (\mathcal{W}_2,\mathcal{D}_2)   
\end{array}  
}
%\genfrac{}{}{}{0}{#2}{#1}
\quad\mbox{(ptuple-par)}  
\\[2mm]  
\\  

\cfrac{  
\begin{array}{l@{\qquad}}  
 {\left(|{e_1},{e_2}|\right)}  \Downarrow^{\textsf{orc}}  \left({v_1},{v_2}\right) , (w_1+w_2+1,\mathtt{{max}}\left(d_1,d_2\right)+1), (\mathcal{W}_1+\mathcal{W}_2+1+\tau+\phi,\mathtt{{max}}\left(\mathcal{D}_1,\mathcal{D}_2\right)+1+\tau+\phi)  
\end{array}  
}{  
\begin{array}{l@{\qquad}}  
w_1 \geq \kappa\, \land  \,w_2 \geq \kappa
\\  
 e_1  \Downarrow^{\textsf{orc}} v_1, (w_1,d_1), (\mathcal{W}_1,\mathcal{D}_1)   
\\  
 e_2  \Downarrow^{\textsf{orc}} v_2, (w_2,d_2), (\mathcal{W}_2,\mathcal{D}_2)   
\end{array}  
}
%\genfrac{}{}{}{0}{#2}{#1}
\quad\mbox{(ptuple-orc-parallelize)}  
\\[2mm]  
\\  

\cfrac{  
\begin{array}{l@{\qquad}}  
 {\left(|{e_1},{e_2}|\right)}  \Downarrow^{\textsf{orc}}  \left({v_1},{v_2}\right) , (w_1+w_2+1,d_1+d_2+1), (\mathcal{W}_1+\mathcal{W}_2+1+\phi,\mathcal{D}_1+\mathcal{D}_2+1+\phi)  
\end{array}  
}{  
\begin{array}{l@{\qquad}}  
w_1 &lt; \kappa\, \lor  \,w_2 &lt; \kappa
\\  
 e_1  \Downarrow^{(\textsf{if}\, w_1 &lt; \kappa \,\textsf{then}\, \textsf{seq} \,\textsf{else}\, \textsf{orc})} v_1, (w_1,d_1), (\mathcal{W}_1,\mathcal{D}_1)   
\quad  
 e_2  \Downarrow^{(\textsf{if}\, w_2 &lt; \kappa \,\textsf{then}\, \textsf{seq} \,\textsf{else}\, \textsf{orc})} v_2, (w_2,d_2), (\mathcal{W}_2,\mathcal{D}_2)   
\end{array}  
}
%\genfrac{}{}{}{0}{#2}{#1}
\quad\mbox{(ptuple-orc-sequentialize)}  
  
\end{array}\]</span>  <br />
<span>Dynamic cost semantics</span>  </p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\small
\centering
\begin{minipage}{0.9\textwidth}
\[
\begin{array}{c}
\infer{
\begin{array}{l@{\qquad}}
\strut
\end{array}
}{
\begin{array}{l@{\qquad}}
\Jcostof{\Ga}{v}{v}{1}{1}{1}{1}
\end{array}
}\quad\mbox{\rlabel{value}}
\\[2mm]
\\
\infer{
\begin{array}{l@{\qquad}}
\Jcostof{\Ga}{ e_1 }{v_1}{\sw_1}{\sd_1}{\sws_1}{\sds_1} 
\\
\Jcostof{\Ga}{ e_2[v_1/x] }{v}{\sw_2}{\sd_2}{\sws_2}{\sds_2} 
\end{array}
}{
\begin{array}{l@{\qquad}}
\Jcostof{\Ga}{ (\kwletin{x}{e_1}{e_2})  }{v}{\sw_1+\sw_2+1}{\sd_1+\sd_2+1}{\sws_1+\sws_2+1}{\sds_1+\sds_2+1}
\end{array}
}\quad\mbox{\rlabel{let}}
\\[2mm]
\\
\infer{
\begin{array}{l@{\qquad}}
(v_1 = \kwfun{f}{x}{e})
\\
\Jcostof{\Ga}{ e[v_2/x, v_1/f] }{v}{\sw}{\sd}{\sws}{\sds}
\end{array}
}{
\begin{array}{l@{\qquad}}
\Jcostof{\Ga}{ (\kwapp{v_1}{v_2}) }{v}{\sw+1}{\sd+1}{\sws+1}{\sds+1}
\end{array}
}\quad\mbox{\rlabel{app}}
\\[2mm]
\\
\infer{
\begin{array}{l@{\qquad}}
\end{array}
}{
\begin{array}{l@{\qquad}}
\Jcostof{\Ga}{ (\kwfst{\kwt{v_1}{v_2}}) }{v_1}{1}{1}{1}{1}
\end{array}
}\quad\mbox{\rlabel{first}}
\quad
\infer{
\begin{array}{l@{\qquad}}
\end{array}
}{
\begin{array}{l@{\qquad}}
\Jcostof{\Ga}{ (\kwsnd{\kwt{v_1}{v_2}}) }{v_2}{1}{1}{1}{1}
\end{array}
}\quad\mbox{\rlabel{second}}
\\[2mm]
\\
\infer{
\begin{array}{l@{\qquad}}
\Jcostof{\Ga}{ e_1[v_1/x_1] }{v}{\sw}{\sd}{\sws}{\sds}
\end{array}
}{
\begin{array}{l@{\qquad}}
\Jcostof{\Ga}{ \kwcase{(\kwinl{v_1})}{x_1}{e_1}{x_2}{e_2} }{v}{\sw+1}{\sd+1}{\sws+1}{\sds+1}
\end{array}
}\quad\mbox{\rlabel{case-left}}
\\[2mm]
\\
\infer{
\begin{array}{l@{\qquad}}
\Jcostof{\Ga}{ e_2[v_2/x_2] }{v}{\sw}{\sd}{\sws}{\sds}
\end{array}
}{
\begin{array}{l@{\qquad}}
\Jcostof{\Ga}{ \kwcase{(\kwinr{v_2})}{x_1}{e_1}{x_2}{e_2} }{v}{\sw+1}{\sd+1}{\sws+1}{\sds+1}
\end{array}
}\quad\mbox{\rlabel{case-right}}
\\[2mm]
\\
\infer{
\begin{array}{l@{\qquad}}
\Jcostof{\Ga}{ e_1 }{v_1}{\sw_1}{\sd_1}{\sws_1}{\sds_1} 
\\
\Jcostof{\Ga}{ e_2 }{v_2}{\sw_2}{\sd_2}{\sws_2}{\sds_2} 
\end{array}
}{
\begin{array}{l@{\qquad}}
\Jcostof{\Ga}{ \kwt{e_1}{e_2} }{ \kwt{v_1}{v_2} }{\sw_1+\sw_2+1}{\sd_1+\sd_2+1}{\sws_1+\sws_2+1}{\sds_1+\sds_2+1}
\end{array}
}\quad\mbox{\rlabel{tuple}}
\quad
\\[2mm]
\\
\infer{
\begin{array}{l@{\qquad}}
\Jcostof{\sseq}{ e_1 }{v_1}{\sw_1}{\sd_1}{\sws_1}{\sds_1} 
\\
\Jcostof{\sseq}{ e_2 }{v_2}{\sw_2}{\sd_2}{\sws_2}{\sds_2} 
\end{array}
}{
\begin{array}{l@{\qquad}}
\Jcostof{\sseq}{ \kwpt{e_1}{e_2} }{ \kwt{v_1}{v_2} }{\sw_1+\sw_2+1}{\sd_1+\sd_2+1}{\sws_1+\sws_2+1}{\sds_1+\sds_2+1}
\end{array}
}\quad\mbox{\rlabel{ptuple-seq}}
\\[2mm]
\\
\infer{
\begin{array}{l@{\qquad}}
\Jcostof{\spar}{ e_1 }{v_1}{\sw_1}{\sd_1}{\sws_1}{\sds_1} 
\\
\Jcostof{\spar}{ e_2 }{v_2}{\sw_2}{\sd_2}{\sws_2}{\sds_2} 
\end{array}
}{
\begin{array}{l@{\qquad}}
\Jcostof{\spar}{ \kwpt{e_1}{e_2} }{ \kwt{v_1}{v_2} }{\sw_1+\sw_2+1}{\kwmax{\sd_1}{\sd_2}+1}{\sws_1+\sws_2+1+\csp}{\kwmax{\sds_1}{\sds_2}+1+\csp}
\end{array}
}\quad\mbox{\rlabel{ptuple-par}}
\\[2mm]
\\
\infer{
\begin{array}{l@{\qquad}}
\sw_1 \geq \coff \Sc\land \sw_2 \geq \coff
\\
\Jcostof{\sorc}{ e_1 }{v_1}{\sw_1}{\sd_1}{\sws_1}{\sds_1} 
\\
\Jcostof{\sorc}{ e_2 }{v_2}{\sw_2}{\sd_2}{\sws_2}{\sds_2} 
\end{array}
}{
\begin{array}{l@{\qquad}}
\Jcostof{\sorc}{ \kwpt{e_1}{e_2} }{ \kwt{v_1}{v_2} }{\sw_1+\sw_2+1}{\kwmax{\sd_1}{\sd_2}+1}{\sws_1+\sws_2+1+\csp+\corc}{\kwmax{\sds_1}{\sds_2}+1+\csp+\corc}
\end{array}
}\quad\mbox{\rlabel{ptuple-orc-parallelize}}
\\[2mm]
\\
\infer{
\begin{array}{l@{\qquad}}
\sw_1 < \coff \Sc\lor \sw_2 < \coff 
\\
\Jcostof{(\Lifthenelse{\sw_1 < \coff}{\sseq}{\sorc})}{ e_1 }{v_1}{\sw_1}{\sd_1}{\sws_1}{\sds_1} 
\quad
\Jcostof{(\Lifthenelse{\sw_2 < \coff}{\sseq}{\sorc})}{ e_2 }{v_2}{\sw_2}{\sd_2}{\sws_2}{\sds_2} 
\end{array}
}{
\begin{array}{l@{\qquad}}
\Jcostof{\sorc}{ \kwpt{e_1}{e_2} }{ \kwt{v_1}{v_2} }{\sw_1+\sw_2+1}{\sd_1+\sd_2+1}{\sws_1+\sws_2+1+\corc}{\sds_1+\sds_2+1+\corc}
\end{array}
}\quad\mbox{\rlabel{ptuple-orc-sequentialize}}

\end{array}
\]
\caption{Dynamic cost semantics}
\label{fig:src-dyn}
\label{fig:src-sem}
\end{minipage}
]]>
</field> <!-- body_src -->

</atom> <!-- figure -->

</segment> <!-- cluster -->


<segment name='subsection'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Cost semantics
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Cost semantics
]]>
</field> <!-- title_src -->
<field name='label'>
sec:oracle::cost-semantics
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::source
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::source
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>The source language includes recursive functions, pairs, sum types, and parallel tuples. Parallel tuples enable expressing computations that can be performed in parallel, similar to the fork-join or nested data parallel computations. For simplicity of exposition, we consider parallel tuples of arity two only. Parallel tuples of higher arity can be easily represented with those of arity two.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
The source language includes recursive functions, pairs, sum types,
and parallel tuples.  Parallel tuples enable expressing computations
that can be performed in parallel, similar to the fork-join or nested
data parallel computations.  For simplicity of exposition, we consider
parallel tuples of arity two only.  Parallel tuples of higher arity
can be easily represented with those of arity two.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::streamline
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::streamline
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>To streamline the presentation, we assume programs to be in A-normal form, with the exception of pairs and parallel pairs, which we treat symmetrically because our compilation strategy involves translating parallel pairs to sequential pairs. Figure <a href="#fig:src::syntax" data-reference-type="ref" data-reference="fig:src::syntax">[fig:src::syntax]</a> illustrates the abstract syntax of the source language. We note that, even though the presentation is only concerned with a purely-functional language, it is easy to include references; for the purposes of this paper, however, they add no additional insight and thus are omitted for clarity.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
To streamline the presentation, we assume programs to be in A-normal
form, with the exception of pairs and parallel pairs, which we treat
symmetrically because our compilation strategy involves translating
parallel pairs to sequential pairs.  \figref{src::syntax} illustrates
the abstract syntax of the source language. We note that, even though
the presentation is only concerned with a purely-functional language,
it is easy to include references; for the purposes of this paper,
however, they add no additional insight and thus are omitted for
clarity.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::define
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::define
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>We define a dynamic semantics where parallel tuples are evaluated selectively either in parallel or sequentially, as determined by their relative size compared with some constant <span class="math inline">\(\kappa\)</span>, called the cutoff value and such that <span class="math inline">\(\kappa \geq 1\)</span>. To model this behavior, we present an evaluation semantics that is parameterized by an identifier that determines the <span><em>mode</em></span> of execution, <em>i.e.</em>, sequential or not. For the purpose of comparison, we also define a <span><em>(fully) parallel</em></span> semantics where parallel tuples are always evaluated in parallel regardless of their size. The <span><em>mode</em></span> of an evaluation is sequential (written <span class="math inline">\(\textsf{seq}\)</span>), parallel (written <span class="math inline">\(\textsf{par}\)</span>), or oracle (written <span class="math inline">\(\textsf{orc}\)</span>). We let <span class="math inline">\(\alpha\)</span> range over modes: <span class="math display">\[\alpha\quad \bf ::= \quad\textsf{seq}\, \bf \,\,\mid\,\, \,\textsf{par}\, \bf \,\,\mid\,\, \,\textsf{orc}.\]</span></p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
We define a dynamic semantics where parallel tuples are evaluated
selectively either in parallel or sequentially, as determined by their
relative size compared with some constant $\kappa$, called the cutoff
value and such that $\kappa \geq 1$. To model this behavior, we
present an evaluation semantics that is parameterized by an identifier
that determines the {\em mode} of execution, \textit{i.e.}, sequential or
not. For the purpose of comparison, we also define a {\em (fully)
  parallel} semantics where parallel tuples are always evaluated in
parallel regardless of their size.  The {\em mode} of an evaluation is
sequential (written $\sseq$), parallel (written $\spar$), or oracle
(written $\sorc$).  We let $\Ga$ range over modes:
$$\Ga \Sq\bnfdef \sseq \Sc\bnfalt \spar \Sc\bnfalt \sorc.$$
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::addition
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::addition
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>In addition to an evaluating expression, the dynamic semantics also returns cost measures including <span><em>raw work</em></span> and <span><em>raw depth</em></span> denoted by <span class="math inline">\(w\)</span> and <span class="math inline">\(d\)</span> (and variants), and <span><em>total work</em></span> and <span> <em>total depth</em></span>, denoted by <span class="math inline">\(\mathcal{W}\)</span> and <span class="math inline">\(\mathcal{D}\)</span> (and variants). Dynamic semantics is presented in the style of a natural (big-step) semantics and consists of evaluation judgments of the form <span class="math display">\[e  \Downarrow^{\alpha} v, (w,d), (\mathcal{W},\mathcal{D}).\]</span> This judgment states that evaluating expression <span class="math inline">\(e\)</span> in mode <span class="math inline">\(\alpha\)</span> yields value <span class="math inline">\(v\)</span> resulting in raw work of <span class="math inline">\(w\)</span> and raw depth of <span class="math inline">\(d\)</span> and total work of <span class="math inline">\(\mathcal{W}\)</span> and total depth of <span class="math inline">\(\mathcal{D}\)</span>.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
In addition to an evaluating expression, the dynamic semantics also
returns cost measures including {\em raw work} and {\em raw depth}
denoted by $\sw$ and $\sd$ (and variants), and {\em total work} and {\em
  total depth}, denoted by $\sws$ and $\sds$ (and variants).  Dynamic
semantics is presented in the style of a natural (big-step) semantics
and consists of evaluation judgments of the form
$$\Jcostof{\Ga}{ e }{v}{\sw}{\sd}{\sws}{\sds}.$$ 
This judgment states
that evaluating expression $e$ in mode $\Ga$ yields value $v$
resulting in raw work of $\sw$ and raw depth of~$\sd$ and total work
of $\sws$ and total depth of $\sds$.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::src-sem
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::src-sem
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Figure <a href="#fig:src-sem" data-reference-type="ref" data-reference="fig:src-sem">[fig:src-sem]</a> shows the complete inductive definition of the dynamic cost semantics judgment <span class="math inline">\(e  
 \Downarrow^{\alpha} v, (w,d), (\mathcal{W},\mathcal{D})\)</span>. When evaluating any expression that is not a parallel tuple, we calculate the (raw or total) work and the (raw or total) depth by summing up those of the premises (subexpressions) and adding one unit to include the cost of the judgment. For all expressions, including parallel tuples, each evaluation step contributes <span class="math inline">\(1\)</span> to the raw work or raw depth. When calculating total work and total depth, we take into account the cost of creating a parallel task <span class="math inline">\(\tau\)</span> and the cost of making an oracle decision <span class="math inline">\(\phi\)</span>.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\figref{src-sem} shows the complete inductive definition of the
dynamic cost semantics judgment $\Jcostof{\Ga}{ e
}{v}{\sw}{\sd}{\sws}{\sds}$.  When evaluating any expression that is
not a parallel tuple, we calculate the (raw or total) work and the
(raw or total) depth by summing up those of the premises
(subexpressions) and adding one unit to include the cost of the
judgment.  For all expressions, including parallel tuples, each
evaluation step contributes~$1$ to the raw work or raw depth.  When
calculating total work and total depth, we take into account the cost
of creating a parallel task~$\csp$ and the cost of making an oracle
decision~$\corc$.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::evaluation
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::evaluation
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Evaluation of parallel tuples vary depending on the mode.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Evaluation of parallel tuples vary depending on the mode.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::sequential
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::sequential
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<ul>
<li><p><strong>Sequential mode.</strong> Parallel tuples are treated exactly like sequential tuples: evaluating a parallel tuple simply contributes <span class="math inline">\(1\)</span> to the raw and the total work (depth), which are computed as the sum of the work (depth) of the two branches plus <span class="math inline">\(1\)</span>. In the sequential mode, raw and total work (depth) are the same.</p></li>
<li><p><strong>Parallel mode.</strong> The evaluation of parallel tuples induces an additional constant cost <span class="math inline">\(\tau\)</span>. The depth is computed as the maximum of the depths of the two branches of the parallel tuple plus <span class="math inline">\(1\)</span>, and work is computed as the sum of the work of the two branches plus <span class="math inline">\(\tau\)</span>. In the oracle mode, there are two cases. If the parallel tuple is scheduled sequentially, then its costs <span class="math inline">\(1\)</span> unit. Raw/total work and depth are both calculated as the sum of the depth of the branches plus one. If the parallel tuple is evaluated in parallel, then an extra cost <span class="math inline">\(\tau\)</span> is included in the total work and depth and the depth is computed as the maximum of the depth of the two branches.</p></li>
<li><p><strong>Oracle mode.</strong> The scheduling of a parallel tuple depends on the amount of raw work involved in the two branches. If the raw work of each branch is more than <span class="math inline">\(\kappa\)</span>, then the tuple is evaluated in parallel in the oracle mode. Otherwise, the raw work of at least one branch is less than <span class="math inline">\(\kappa\)</span>, and the tuple is executed sequentially. When evaluating a parallel tuple sequentially, the mode in which each branch is evaluated depends on the work involved in the branch. If a branch contains more than <span class="math inline">\(\kappa\)</span> units of raw work, then it is evaluated in oracle mode, otherwise it is evaluated in sequential mode. This switching to sequential mode on small tasks is needed for ensuring that the oracle is not called too often during the evaluation of a program.</p></li>
</ul>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\begin{itemize}

\item \textbf{Sequential mode.} Parallel tuples are treated exactly
  like sequential tuples: evaluating a parallel tuple simply
  contributes~$1$ to the raw and the total work (depth), which are
  computed as the sum of the work (depth) of the two branches
  plus~$1$.  In the sequential mode, raw and total work (depth) are
  the same.


\item \textbf{Parallel mode.} The evaluation of parallel tuples induces
  an additional constant cost~$\csp$. The depth is computed as the
  maximum of the depths of the two branches of the parallel tuple
  plus~$1$, and work is computed as the sum of the work of the two
  branches plus~$\tau$. In the oracle mode, there are two cases.  If
  the parallel tuple is scheduled sequentially, then its costs $1$
  unit.  Raw/total work and depth are both calculated as the sum of
  the depth of the branches plus one.  If the parallel tuple is
  evaluated in parallel, then an extra cost $\csp$ is included in the
  total work and depth and the depth is computed as the maximum of the
  depth of the two branches.

\item \textbf{Oracle mode.} The scheduling of a parallel tuple depends
  on the amount of raw work involved in the two branches.  If the raw
  work of each branch is more than~$\kappa$, then the tuple is
  evaluated in parallel in the oracle mode.  Otherwise, the raw work
  of at least one branch is less than~$\kappa$, and the tuple is
  executed sequentially.  When evaluating a parallel tuple
  sequentially, the mode in which each branch is evaluated depends on
  the work involved in the branch.  If a branch contains more
  than~$\coff$ units of raw work, then it is evaluated in  oracle
  mode, otherwise it is evaluated in  sequential mode.  This
  switching to sequential mode on small tasks is needed for ensuring
  that the oracle is not called too often during the evaluation of a
  program.
\end{itemize}
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->


</segment> <!-- subsection -->

<segment name='subsection'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Generalized Brent’s theorem
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Generalized Brent's theorem
]]>
</field> <!-- title_src -->
<field name='label'>
sec:oracle::generalized-brent-theorem
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::order
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::order
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>In order to relate the total work and total depth of a program with its execution time, we rely on Brent’s theorem. This theorem is usually formulated in terms of <span><em>computation DAGs</em></span>. A computation DAG is a directed acyclic graph that represents a parallel computation. Nodes in the graph represent atomic computations. Edges between nodes represent precedence relations, in the sense that an edge from <span class="math inline">\(a\)</span> to <span class="math inline">\(b\)</span> indicates that the execution of <span class="math inline">\(a\)</span> must be completed before the execution of <span class="math inline">\(b\)</span> can start. Every computation DAG includes a <span><em>source</em></span> node and a <span><em>sink</em></span> node, representing the starting and the end points of the computation, respectively. Those nodes are such that all nodes of a computation DAG are reachable from the source node, and the sink node is reachable from all nodes. An example computation DAG appears in Figure <a href="#fig:dag" data-reference-type="ref" data-reference="fig:dag">[fig:dag]</a>. A node is said to be <span><em>ready</em></span> if all the nodes that points to it have already been executed.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
In order to relate the total work and total depth of a program with
its execution time, we rely on Brent's theorem. This theorem is
usually formulated in terms of {\em computation DAGs}. 
A computation DAG is a directed acyclic graph that represents a
parallel computation. Nodes in the graph represent atomic
computations.  Edges between nodes represent precedence relations, in
the sense that an edge from $a$ to $b$ indicates that the execution of
$a$ must be completed before the execution of $b$ can start.  Every
computation DAG includes a {\em source} node and a {\em sink} node,
representing the starting and the end points of the computation,
respectively.  Those nodes are such that all nodes of a computation
DAG are reachable from the source node, and the sink node is reachable
from all nodes.  An example computation DAG appears in \figref{dag}.
A node is said to be {\em ready} if all the nodes that points to it
have already been executed.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::gives
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::gives
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Brent’s theorem gives a bound on the time required for executing all the tasks in a computation DAG with a greedy scheduler, assuming that each node takes a unit of time to execute. A scheduler is said to be <span><em>greedy</em></span> if it never stays idle unnecessarily, i.e., when there exists a ready node the scheduler finds it at no cost and executes it. Typical proofs of Brent’s theorem assume a unit cost model where each instruction costs a unit cost to execute and construct a “level-by-level” execution schedule.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Brent's theorem gives a bound on the time required for executing all
the tasks in a computation DAG with a greedy scheduler, assuming that
each node takes a unit of time to execute. A scheduler is said to be
{\em greedy} if it never stays idle unnecessarily, i.e., when there
exists a ready node the scheduler finds it at no cost and executes
it.  Typical proofs of Brent's theorem assume a unit cost model where
each instruction costs a unit cost to execute and construct a
``level-by-level'' execution schedule.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::extend
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::extend
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>One way to extend the Brent’s theorem to include task-creation overheads is to assign a weight to each node. Proving such a generalization directly, however, turns out to be highly nontrivial and in our attempts resulted in relatively complex proofs. Another approach is to represent non-unit cost tasks with a sequence of unit tasks, e.g., we can replace a task with weight three with a sequence of three unit-cost tasks. Since overheads are non-divisible work, we would require that such tasks execute on the same processors back to back without interleaving with other tasks. With this approach, typical proofs of Brent’s theorem, which assume a “level-by-level” execution schedule, do not work because they break up sequences. Fortunately, we have found that Arora et al’s proof <span class="citation" data-cites="AroraBlPl98"></span> can be adapted easily for this purpose, because it makes no assumption about ordering of ready nodes, directly allowing us to generalize Brent’s theorem to include task-creation overheads.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
One way to extend the Brent's theorem to include task-creation
overheads is to assign a weight to each node. Proving such a
generalization directly, however, turns out to be highly nontrivial
and in our attempts resulted in relatively complex proofs.  Another
approach is to represent non-unit cost tasks with a sequence of unit
tasks, e.g., we can replace a task with weight three with a sequence
of three unit-cost tasks.  Since overheads are non-divisible work, we
would require that such tasks execute on the same processors back to
back without interleaving with other tasks.  With this approach,
typical proofs of Brent's theorem, which assume a ``level-by-level''
execution schedule, do not work because they break up
sequences. Fortunately, we have found that Arora et al's
proof~\cite{AroraBlPl98} can be adapted easily for this purpose,
because it makes no assumption about ordering of ready nodes, directly
allowing us to generalize Brent's theorem to include task-creation
overheads.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::width
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='figure'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
fig:dag
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p><img src="pictures/computation-dag" alt="image" />  <br />
<span>An example computation DAG.</span> </p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\centering
\ifx\arthur\false
\includegraphics[width=0.4\columnwidth]{pictures/computation-dag}
\fi
\caption{An example computation DAG.}
\label{fig:dag}
]]>
</field> <!-- body_src -->

</atom> <!-- figure -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::computation
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::computation
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p> Let <span class="math inline">\(G\)</span> be a computation DAG made of <span class="math inline">\(W\)</span> nodes and whose longest path has length <span class="math inline">\(D\)</span>. Any greedy scheduler can execute this computation DAG in no more than <span class="math inline">\(\frac{W}{P} + D\)</span> steps on <span class="math inline">\(P\)</span> processors.</p>
<p>t each execution step, each processor places a token in the <span><em>work bucket</em></span> if it is busy at this step, otherwise it places a token in the <span><em>idle bucket</em></span>. The work bucket contains exactly <span class="math inline">\(W\)</span> tokens at the end of the execution. Let <span class="math inline">\(I\)</span> be the number of tokens contained in the idle bucket at the end of the execution, and let <span class="math inline">\(T\)</span> denote the total number of steps in the execution. Because a total <span class="math inline">\(T P\)</span> tokens are created, we have <span class="math inline">\(T P = W + I\)</span>. In order to establish the result <span class="math inline">\(T \leq \frac{W}{P} + D\)</span>, it thus suffices to establish the inequality <span class="math inline">\(I \leq P D\)</span>.</p>
<p>Consider a given time step. If all processors are executing then the idle bucket receives zero tokens. Otherwise, a number of processors are idle. In this case, the idle bucket receives between one and <span class="math inline">\(P-1\)</span> tokens. We can bound the number of time steps at which this situation happens, as follows. If one or more processors are idle, it means that those processors cannot find a ready task to execute. Because the scheduler is assumed to be greedy, it must be the case that all the ready tasks are currently executing. Therefore, at such a time step, the maximal length of a path in the computation DAG starting from a ready node decreases by one unit. Because the maximal length of a path in the computation DAG is initially <span class="math inline">\(D\)</span>, there can be at most <span class="math inline">\(D\)</span> time steps at which not all processors are executing. It follows that the final number of tokens in the idle bucket does not exceeed <span class="math inline">\((P-1) D\)</span>. This result entails the inequality <span class="math inline">\(I \leq P D\)</span>.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\begin{theorem}[Brent's theorem for computation DAGs]
\label{thm:basic-brent}
Let $G$ be a computation DAG made of $W$ nodes and whose
longest path has length $D$.
Any greedy scheduler can 
execute this computation DAG in no more than $\frac{W}{P} + D$ 
steps on $P$ processors. 
\end{theorem}
\begin{proof}
At each execution step, each processor places a token in the {\em work bucket}
if it is busy at this step, otherwise it places a token in the {\em idle bucket}.
The work bucket contains exactly $W$ tokens at the end of the execution.
Let $I$ be the number of tokens contained in the idle bucket at the end of the
execution, and let $T$ denote the total number of steps in the execution.
Because a total $T P$ tokens are created, we have $T P = W + I$.
In order to establish the result $T \leq \frac{W}{P} + D$,
it thus suffices to establish the inequality $I \leq P D$.

Consider a given time step.
If all processors are executing then the idle bucket
receives zero tokens. Otherwise, a number of processors are idle.
In this case, the idle bucket receives between one and $P-1$ tokens.
We can bound the number of time steps at which this situation happens,
as follows. 
If one or more processors are idle, it means that those processors cannot
find a ready task to execute. Because the scheduler is assumed to be greedy,
it must be the case that all the ready tasks are currently executing.
Therefore, at such a time step, the maximal length of a path 
in the computation DAG starting from a ready node decreases by one unit. 
Because the maximal length of a path in the computation DAG is initially $D$,
there can be at most $D$ time steps at which not all processors are executing.
It follows that the final number of tokens in the idle bucket
does not exceeed $(P-1) D$. This result entails the inequality $I \leq P D$. \qed
\end{proof}
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::observe
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::observe
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Observe that the proof does not impose any constraint on the order in which the ready tasks should be executed by the processors. So, if one processor starts working on a sequence of several nodes, then it can execute all the nodes in the sequence before looking for other ready tasks. Therefore, the proof accepts computation DAGs that encode non-unit tasks as sequences of unit tasks. We will make use of such an encoding in the proof of our next theorem, which relates our cost semantics to the computation DAG model.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Observe that the proof does not impose any constraint on the order in
which the ready tasks should be executed by the processors.  So, if one
processor starts working on a sequence of several nodes, then it can
execute all the nodes in the sequence before looking for other ready
tasks.  Therefore, the proof accepts computation DAGs that encode
non-unit tasks as sequences of unit tasks.  We will make use of such
an encoding in the proof of our next theorem, which relates our cost
semantics to the computation DAG model.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::cost
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::cost
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>  <br />
Assume <span class="math inline">\(e  \Downarrow^{\textsf{orc}}  v , (w,d), (\mathcal{W},\mathcal{D})\)</span> to hold for some <span class="math inline">\(v\)</span>, <span class="math inline">\(w\)</span> and <span class="math inline">\(d\)</span>. Any greedy scheduler can execute the expression <span class="math inline">\(e\)</span> in no more than <span class="math inline">\(\frac{\mathcal{W}}{P} + \mathcal{D}\)</span> computations steps on <span class="math inline">\(P\)</span> processors.</p>
<p>n order to invoke the version of Brent’s theorem that applies to computation DAGs, we build the computation DAG associated with the execution of the expression <span class="math inline">\(e\)</span>, including nodes that represent the cost of scheduling. To that end, we describe a recursive algorithm for turning an expression <span class="math inline">\(e\)</span> with total work <span class="math inline">\(\mathcal{W}\)</span> and total depth <span class="math inline">\(\mathcal{D}\)</span> into a corresponding computation DAG containing <span class="math inline">\(\mathcal{W}\)</span> nodes and whose longest path has length <span class="math inline">\(\mathcal{D}\)</span>. The algorithm follows the structure of the derivation that <span class="math inline">\(e\)</span> has total work <span class="math inline">\(\mathcal{W}\)</span> and total depth <span class="math inline">\(\mathcal{D}\)</span>.</p>
<ul>
<li><p>If the last rule has zero premises, then <span class="math inline">\(e\)</span> is an atomic expression and <span class="math inline">\(\mathcal{W}= \mathcal{D}= 1\)</span>. We build the corresponding DAG as a single node.</p></li>
<li><p>If the last rule has one premise, then <span class="math inline">\(\mathcal{W}\)</span> takes the form <span class="math inline">\(\mathcal{W}_1 + 1\)</span> and <span class="math inline">\(\mathcal{D}\)</span> takes the form <span class="math inline">\(\mathcal{D}_1 + 1\)</span>. Let <span class="math inline">\(G_1\)</span> be the DAG corresponding to the sub-expression described in the premise. We build <span class="math inline">\(G\)</span> by extending <span class="math inline">\(G_1\)</span> with one node at the bottom, that is, by sequentially composing <span class="math inline">\(G_1\)</span> with a DAG made of a single node.</p></li>
<li><p>Otherwise the last rule has two premises. First, consider the case where <span class="math inline">\(e\)</span> is a let-expression. <span class="math inline">\(\mathcal{W}\)</span> takes the form <span class="math inline">\(\mathcal{W}_1 + \mathcal{W}_2 + 1\)</span> and <span class="math inline">\(\mathcal{D}\)</span> takes the form <span class="math inline">\(\mathcal{D}_1 + \mathcal{D}_2 + 1\)</span>. Let <span class="math inline">\(G_1\)</span> and <span class="math inline">\(G_2\)</span> be the DAGs corresponding to the two sub-expressions. We build <span class="math inline">\(G\)</span> by sequentially composing <span class="math inline">\(G_1\)</span> with a single node and then with <span class="math inline">\(G_2\)</span>.</p></li>
<li><p>Consider now the case of a parallel tuple that is sequentialized. <span class="math inline">\(\mathcal{W}\)</span> takes the form <span class="math inline">\(\mathcal{W}_1 + \mathcal{W}_2 + 1 + \phi\)</span> and <span class="math inline">\(\mathcal{D}\)</span> takes the form <span class="math inline">\(\mathcal{D}_1 + \mathcal{D}_2 + 1 + \phi\)</span>. Let <span class="math inline">\(G_1\)</span> and <span class="math inline">\(G_2\)</span> be the DAGs corresponding to the two branches. We build <span class="math inline">\(G\)</span> by sequentially composing <span class="math inline">\(1 + \phi\)</span> unit-cost nodes with the sequential composition of <span class="math inline">\(G_1\)</span> and <span class="math inline">\(G_2\)</span>.</p></li>
<li><p>Finally, consider the case of a parallel tuple that is parallelized. <span class="math inline">\(\mathcal{W}\)</span> takes the form <span class="math inline">\(\mathcal{W}_1 + \mathcal{W}_2 + 1 + \tau+ \phi\)</span> and <span class="math inline">\(\mathcal{D}\)</span> takes the form <span class="math inline">\(\mathtt{{max}}\left(\mathcal{D}_1,\mathcal{D}_2\right) + 1 + \tau+ \phi\)</span>. Let <span class="math inline">\(G_1\)</span> and <span class="math inline">\(G_2\)</span> be the DAGs corresponding to the two branches. We build <span class="math inline">\(G\)</span> by sequentially composing <span class="math inline">\(1 + \tau+ \phi\)</span> unit-cost nodes with the parallel composition of <span class="math inline">\(G_1\)</span> and <span class="math inline">\(G_2\)</span>.</p></li>
</ul>
<p>It is straightforward to check that, in each case, <span class="math inline">\(\mathcal{W}\)</span> and <span class="math inline">\(\mathcal{D}\)</span> match the number of nodes and the total depth of the DAG being produced.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\begin{theorem}[Brent's theorem for the cost semantics] 
\label{thm:generalize-brent}
\label{thm:generalized-brent}~\\
Assume 
$\Jcostof{\sorc}{ e }{ v }{\sw}{\sd}{\sws}{\sds}$
to hold for some $v$, $\sw$ and $\sd$.
Any greedy scheduler can execute the expression $e$
in no more than $\frac{\sws}{P} + \sds$ computations steps on $P$ processors. 
\end{theorem}
\begin{proof}
In order to invoke the version of Brent's theorem that applies 
to computation DAGs, we build the computation DAG associated 
with the execution of the expression $e$, including nodes that
represent the cost of scheduling.
To that end, we describe a recursive algorithm for turning 
an expression $e$ with total work $\sws$ and total depth $\sds$
into a corresponding computation DAG containing $\sws$ nodes and
whose longest path has length $\sds$.
The algorithm follows the structure of the derivation that
$e$ has total work $\sws$ and total depth~$\sds$.
\begin{itemize}
\item If the last rule has zero premises, then $e$ is an atomic expression
and $\sws = \sds = 1$. We build the corresponding DAG as a single node.
\item If the last rule has one premise, then $\sws$ takes the form $\sws_1 + 1$
and $\sds$ takes the form $\sds_1 + 1$. Let $G_1$ be the DAG corresponding
to the sub-expression described in the premise. We build $G$ by 
extending $G_1$ with one node at the bottom, that is, by
sequentially composing $G_1$ with a DAG made of a single node. 
\item Otherwise the last rule has two premises.
First, consider the case where $e$ is a let-expression. 
$\sws$ takes the form $\sws_1 + \sws_2 + 1$
and $\sds$ takes the form $\sds_1 + \sds_2 + 1$.
Let $G_1$ and $G_2$ be the DAGs corresponding to the two sub-expressions.
We build $G$ by sequentially composing $G_1$ with a single node and then 
with $G_2$.
\item Consider now the case of a parallel tuple that is sequentialized.
$\sws$ takes the form $\sws_1 + \sws_2 + 1 + \corc$
and $\sds$ takes the form $\sds_1 + \sds_2 + 1 + \corc$.
Let $G_1$ and $G_2$ be the DAGs corresponding to the two branches.
We build $G$ by sequentially composing $1 + \corc$ unit-cost nodes
with the sequential composition of $G_1$ and $G_2$.
\item Finally, consider the case of a parallel tuple that is parallelized.
$\sws$ takes the form $\sws_1 + \sws_2 + 1 + \csp + \corc$
and $\sds$ takes the form $\kwmax{\sds_1}{\sds_2} + 1 + \csp + \corc$.
Let $G_1$ and $G_2$ be the DAGs corresponding to the two branches.
We build $G$ by sequentially composing $1 + \csp + \corc$ unit-cost nodes
with the parallel composition of $G_1$ and $G_2$. 
\end{itemize}
It is straightforward to check that, in each case, $\sws$ and $\sds$ 
match the number of nodes and the total depth of the DAG being produced. \qed
\end{proof}
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->


</segment> <!-- subsection -->

</segment> <!-- section -->

<segment name='section'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Analysis
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Analysis
]]>
</field> <!-- title_src -->
<field name='label'>
sec:analysis
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::_2_
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::_1_
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p> </p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\label{sec:analysis}
\label{sec:theorems}
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::analyze
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::analyze
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>We analyze the impact of task creation overheads on parallel execution time and show how these costs can be reduced dramatically by using our oracle semantics. For our analysis, we first consider an <span><em>ideal oracle</em></span> that always makes perfectly accurate predictions (about the raw work of expressions) without any overhead (<em>i.e.</em>, <span class="math inline">\(\phi= 0\)</span>). Such an ideal oracle is unrealistic, because it is practically impossible to determine perfectly accurately the raw work of computations. We therefore consider a realistic oracle that approximates the raw work of computations by performing constant work. Our main result is a theorem that shows that the ideal oracle can reduce the task-creation overheads to any desired constant fraction of the raw work with some increase in depth, which we show to be small for a reasonably broad class of computations.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
We analyze the impact of task creation overheads on parallel execution
time and show how these costs can be reduced dramatically by using our
oracle semantics.  For our analysis, we first consider an {\em ideal
  oracle} that always makes perfectly accurate predictions (about the
raw work of expressions) without any overhead (\textit{i.e.}, $\corc = 0$).
Such an ideal oracle is unrealistic, because it is practically
impossible to determine perfectly accurately the raw work of
computations.  We therefore consider a realistic oracle that
approximates the raw work of computations by performing constant work.
Our main result is a theorem that shows that the ideal oracle can
reduce the task-creation overheads to any desired constant fraction of
the raw work with some increase in depth, which we show to be small
for a reasonably broad class of computations.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->


<segment name='subsection'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Ideal oracle
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Ideal oracle
]]>
</field> <!-- title_src -->
<field name='label'>
sec:oracle::ideal-oracle
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::quantify
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::quantify
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>We quantify the relationships between raw work, raw depth and total work, total depth for each mode.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
We quantify the relationships between raw work, raw depth and
total work, total depth for each mode.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::work-and-depth
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='theorem'>
<field name='title'>
<![CDATA[
Work and depth
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Work and depth
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
thm:work-depth-all
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p> Consider an expression <span class="math inline">\(e\)</span> such that <span class="math inline">\(e  \Downarrow^{\alpha} v, (w,d), (\mathcal{W},\mathcal{D})\)</span>. Assume <span class="math inline">\(\phi= 0\)</span>. The following tight bounds can be obtained for total work and total depth, on a machine with <span class="math inline">\(P\)</span> processors where the cost of creating parallel tasks is <span class="math inline">\(\tau\)</span>. <span class="math display">\[\begin{array}{c|c|c}   
\textrm{$\alpha$} &amp;   
\textrm{Bound on total work} &amp;  
\textrm{Bound on total depth}   
  
\\\hline  
\textsf{seq}&amp;   
\mathcal{W}= w&amp;  
\mathcal{D}= d= w
  
\\\hline  
\textsf{par}&amp;   
\mathcal{W}\leq (1+\frac{\tau}{2})\,w&amp;  
\mathcal{D}\leq (1+\tau)\,d
  
\\\hline  
\textsf{orc}&amp;   
\mathcal{W}\leq (1 + \frac{\tau}{\kappa+1})\,w&amp;  
\mathcal{D}\leq (1+\mathtt{{max}}\left(\tau,\kappa\right))\,d
\end{array}\]</span></p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\label{thm:work-depth-all}
Consider an expression $e$ such that $\Jcostof{\Ga}{ e }{v}{\sw}{\sd}{\sws}{\sds}$.
Assume $\corc = 0$.
The following tight bounds can be obtained for total work and total depth,
on a machine with $P$ processors where the cost of creating parallel tasks is $\csp$.
$$\begin{array}{c|c|c} 
\X{$\Ga$} & 
\X{Bound on total work} &
\X{Bound on total depth} 

\\\hline
\sseq & 
\sws = \sw &
\sds = \sd = \sw 

\\\hline
\spar & 
\sws \leq (1+\frac{\csp}{2})\,\sw &
\sds \leq (1+\csp)\,\sd 

\\\hline
\sorc & 
\sws \leq (1 + \frac{\csp}{\coff+1})\,\sw &
\sds \leq (1+\kwmax{\csp}{\coff})\,\sd 
\end{array}$$
]]>
</field> <!-- body_src -->

</atom> <!-- theorem -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::equations
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='proof'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
prf:oracle::equations
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>The equations concerning the sequential semantics follow by inspection of the semantics of the source language (Figure <a href="#fig:src-sem" data-reference-type="ref" data-reference="fig:src-sem">[fig:src-sem]</a>). The inequalities for the parallel and the oracle modes follow directly by our more general bounds presented later (Theorems <a href="#thm:orc-cost-depth" data-reference-type="ref" data-reference="thm:orc-cost-depth">[thm:orc-cost-depth]</a> and <a href="#thm:orc-cost-work" data-reference-type="ref" data-reference="thm:orc-cost-work">[thm:orc-cost-work]</a>). To prove that the inequalities for the parallel and the oracles modes are tight, we give example computation that achieve the bounds.</p>
<ul>
<li><p><strong>Parallel mode.</strong> Consider an expression consisting only of parallel tuples with <span class="math inline">\(n\)</span> leaves, and thus <span class="math inline">\(n-1\)</span> “internal nodes”. The raw work <span class="math inline">\(w\)</span> is equal to <span class="math inline">\(n+(n-1)\)</span> while the total work <span class="math inline">\(\mathcal{W}\)</span> is equal to <span class="math inline">\(n+(n-1)(1+\tau)\)</span>. We therefore have <span class="math inline">\(\mathcal{W}
  = (1+\frac{n\tau}{2n+1})w\le \left(1+\frac{\tau}{2}\right)  
  w\)</span>. As <span class="math inline">\(n\)</span> increases, the bound approaches <span class="math inline">\(\left(1+\frac{\tau}{2}\right) w\)</span> and thus the bound on the total work is tight. To see that the depth bound is also tight, note that each parallel tuple adds <span class="math inline">\(1\)</span> to the raw depth and <span class="math inline">\(1+\tau\)</span> to the total depth. The total depth therefore can be as much as <span class="math inline">\(1+\tau\)</span> times greater than the raw depth.</p></li>
<li><p><strong>Oracle mode.</strong> Consider an expression with <span class="math inline">\(n\)</span> nested parallel tuples, where tuples are always nested in the right branch of their parent tuple. The tuples are built on top of expressions that involve <span class="math inline">\(\kappa\)</span> units of work. In the oracle semantics, all the tuples are executed in parallel. Thus the raw work <span class="math inline">\(w\)</span> is <span class="math inline">\(n +  
(n+1)\kappa\)</span>, the total work <span class="math inline">\(\mathcal{W}\)</span> is <span class="math inline">\(n (1+\tau) +  
(n+1)\kappa\)</span>, and <span class="math inline">\(\mathcal{W}= w\left( 1 + \frac{n\tau}{n(\kappa+1)+\kappa}  
\right) \le w\left( 1+\frac{\tau}{\kappa+1} \right)\)</span>. As <span class="math inline">\(n\)</span> increases, the bound approaches <span class="math inline">\(\left(1+\frac{\tau}{\kappa+ 1}\right) w\)</span> and thus the bound on the total work is tight.</p>
<p>For the depth bound, we consider two cases. In the first case, we have <span class="math inline">\(\tau\geq \kappa\)</span>. Using the same example, the raw depth is <span class="math inline">\(d= n+1\)</span>, the total depth is <span class="math inline">\(\mathcal{D}= n(1+\tau) + \kappa\)</span>, and <span class="math inline">\(\mathcal{D}=  
\left( 1+\frac{n\tau+\kappa-1}{n+1} \right) d\le \left( 1+\tau
\right) d\)</span>. As <span class="math inline">\(n\)</span> increases, <span class="math inline">\(\mathcal{D}\)</span> approaches <span class="math inline">\(\left( 1+\tau
\right) d\)</span> and thus the bound is tight.</p>
<p>For the second case when <span class="math inline">\(\kappa\geq\tau\)</span>, we change the example slightly by reducing the amount of raw work in each leaf to just under <span class="math inline">\(\kappa\)</span>. This will cause all the parallel tuples to be evaluated sequentially; the raw depth is <span class="math inline">\(d= n+\kappa\)</span> and the total depth is equal to the total work, <em>i.e.</em>, <span class="math inline">\(\mathcal{D}= n+(n+1)\kappa\le  
\left( 1+\frac{n\kappa}{n+\kappa} \right) d\)</span>. As <span class="math inline">\(n\)</span> increases, <span class="math inline">\(\mathcal{D}\)</span> approaches <span class="math inline">\(\left( 1+\kappa\right) d\)</span> and thus the bound is tight.</p></li>
</ul>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
The equations concerning the sequential semantics follow by inspection
of the semantics of the source language (\figref{src-sem}).  The
inequalities for the parallel and the oracle modes follow directly by
our more general bounds presented later
(\thmreftwo{orc-cost-depth}{orc-cost-work}).  To prove that the
inequalities for the parallel and the oracles modes are tight, we give
example computation that achieve the bounds.

\begin{itemize}
\item \textbf{Parallel mode.}  Consider an expression consisting only
  of parallel tuples with $n$ leaves, and thus $n-1$ ``internal
  nodes''.  The raw work $\sw$ is equal to $n+(n-1)$ while the total
  work $\sws$ is equal to $n+(n-1)(1+\csp)$.  We therefore have $\sws
  = (1+\frac{n\csp}{2n+1})\sw \le \left(1+\frac{\tau}{2}\right)
  \sw$. As $n$ increases, the bound approaches
  $\left(1+\frac{\tau}{2}\right) \sw$ and thus the bound on the total
  work is tight. To see that the depth bound is also tight, note that
  each parallel tuple adds~$1$ to the raw depth and~$1+\csp$ to the
  total depth.  The total depth therefore can be as much as $1+\csp$
  times greater than the raw depth.

\item \textbf{Oracle mode.}  
Consider an expression with~$n$ nested parallel tuples, where tuples
are always nested in the right branch of their parent tuple.  The
tuples are built on top of expressions that involve~$\coff$ 
units of work.  In the oracle semantics, all the tuples
are executed in parallel.  Thus the raw work~$\sw$ is $n +
(n+1)\coff$, the total work~$\sws$ is $n (1+\csp) +
(n+1)\coff$, and $\sws = \sw \left( 1 + \frac{n\csp}{n(\coff+1)+\coff}
\right) \le \sw \left( 1+\frac{\csp}{\coff+1} \right)$.  As~$n$
increases, the bound approaches $\left(1+\frac{\csp}{\coff + 1}\right) \sw$
and thus the bound on the total work is tight.  

For the depth bound, we  consider two cases.  In the first case,
we have~$\csp \geq \coff$.  Using the same example, the raw depth is~$
\sd = n+1$, the total depth is~$\sds = n(1+\csp) + \coff$, and $\sds =
\left( 1+\frac{n\csp+\coff-1}{n+1} \right) \sd \le \left( 1+\csp
\right) \sd$.  As~$n$ increases,~$\sds$ approaches~$\left( 1+\csp
\right) \sd$ and thus the bound is tight.

For the second case when $\coff\geq\csp$, we change the example
slightly by reducing the amount of raw work in each leaf to just
under $\coff$.  This will cause all the parallel tuples to be
evaluated sequentially; the raw depth is $\sd = n+\coff$ and the
total depth is equal to the total work, \textit{i.e.}, $\sds = n+(n+1)\coff \le
\left( 1+\frac{n\coff}{n+\coff} \right) \sd$.  As $n$ increases, $\sds$
approaches $\left( 1+\coff \right) \sd$ and thus the
bound is tight. \qed
\end{itemize}
]]>
</field> <!-- body_src -->

</atom> <!-- proof -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::leads
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::leads
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>This theorem leads to some important conclusions. First, the theorem shows that task creation (scheduling) costs matter a great deal. In a parallel evaluation, the total work and total depth can be as much as <span class="math inline">\(\tau\)</span> times larger than the raw depth and raw work. This essentially implies that a parallel program can be significantly slower than its sequential counterpart. If <span class="math inline">\(\tau\)</span> is large compared to the number of processors, then even in the ideal setting, where the number of parallel processors is small relative to <span class="math inline">\(\tau\)</span>, we may observe no speedups. In fact, it is not uncommon to hear anecdotal evidence of this kind of slowdown in modern computer systems.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
This theorem leads to some important conclusions.  First, the theorem
shows that task creation (scheduling) costs matter a great deal.  In a
parallel evaluation, the total work and total depth can be as much as
$\csp$~times larger than the raw depth and raw work.  This essentially
implies that a parallel program can be significantly slower than its
sequential counterpart.  If $\csp$ is large compared to the number of
processors, then even in the ideal setting, where the number of
parallel processors is small relative to $\csp$, we may observe no
speedups.  In fact, it is not uncommon to hear anecdotal evidence of
this kind of slowdown in modern computer systems.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::shows
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::shows
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Second, the theorem shows that evaluation of a program with an ideal oracle can require as much as <span class="math inline">\(\frac{\kappa}{2}\)</span> less work than in the parallel mode. This comes at a cost of increasing the depth by a factor of <span class="math inline">\(\frac{\kappa}{\tau}\)</span>. Increasing the depth of a computation can hurt parallel execution times because many parallel schedulers rely on the availability of large degree of parallelism to achieve optimal speedups. Unless done carefully, increasing the depth can dramatically reduce parallel slackness. In the common case, however, where there is plenty of parallelism, i.e., when <span class="math inline">\(\frac{w}{P}\)</span> is far greater than <span class="math inline">\(d\)</span>, we can safely increase depth by a factor of <span class="math inline">\(\frac{\kappa}{\tau}\)</span> to reduce the task-creation overheads. Concretely, if parallel slackness is high and <span class="math inline">\(\kappa\)</span> is not too large, then <span class="math inline">\(\kappad\)</span> remains small compared to <span class="math inline">\(\frac{w}{P}\)</span>, and <span class="math inline">\(\frac{\tau}{\kappa}\frac{w}{P}\)</span> becomes much smaller than <span class="math inline">\(\frac{\tau}{2}\frac{w}{P}\)</span>, dramatically reducing task-creation overheads without harming parallel speedups.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Second, the theorem shows that evaluation of a program with an ideal
oracle can require as much as $\frac{\coff}{2}$ less work than in the
parallel mode.  This comes at a cost of increasing the depth by a
factor of $\frac{\coff}{\csp}$.  Increasing the depth of a computation can
hurt parallel execution times because many parallel schedulers rely on
the availability of large degree of parallelism to achieve optimal
speedups.  Unless done carefully, increasing the depth can dramatically
reduce parallel slackness.  In the common case, however, where there
is plenty of parallelism, i.e., when $\frac{\sw}{P}$ is far greater
than $\sd$, we can safely increase depth by a factor of
$\frac{\coff}{\csp}$ to reduce the task-creation overheads.
Concretely, if parallel slackness is high and $\coff$ is not too
large, then $\coff\sd$ remains small compared to $\frac{\sw}{P}$, and
$\frac{\csp}{\coff}\frac{\sw}{P}$ becomes much smaller than
$\frac{\csp}{2}\frac{\sw}{P}$,  dramatically reducing task-creation overheads
without harming parallel speedups.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->


</segment> <!-- subsection -->

<segment name='subsection'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Realistic oracles
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Realistic oracles
]]>
</field> <!-- title_src -->
<field name='label'>
sec:realistic-oracles
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::_4_
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::_3_
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p> </p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\label{sec:realistic-oracles}
\label{sec:estimate}
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::analysis
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::analysis
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>The analysis that we present above makes two unrealistic assumptions about oracles: 1) that they can accurately predict the raw work for a task, and 2) that the oracle can make predictions in zero time. Realizing a very accurate oracle in practice is difficult, because it requires determining a priori the execution time of a task. We therefore generalize the analysis by considering an approximate or realistic oracle that can make errors up to a multiplicative factor <span class="math inline">\(\mu\)</span> when estimating raw work. For example, an oracle can approximate raw work up to a constant factor of <span class="math inline">\(\mu=3\)</span>, <em>i.e.</em>, a task with raw work <span class="math inline">\(w\)</span> would be estimated to perform raw work between <span class="math inline">\(\frac{w}{3}\)</span> and <span class="math inline">\(3w\)</span>. Additionally, we allow the oracle to take some (fixed) constant time, written <span class="math inline">\(\phi\)</span>, to provide its answer.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
The analysis that we present above makes two unrealistic assumptions
about oracles: 1) that they can accurately predict the raw work for a
task, and 2) that the oracle can make predictions in zero time.
Realizing a very accurate oracle in practice is difficult, because it
requires determining a priori the execution time of a task.  We
therefore generalize the analysis by considering an approximate or
realistic oracle that can make errors up to a multiplicative factor
$\cerr$ when estimating raw work.  For example, an oracle can
approximate raw work up to a constant factor of~$\cerr=3$,
\textit{i.e.}, a task with raw work $\sw$ would be estimated to
perform raw work between $\frac{\sw}{3}$ and $3\sw$.  Additionally, we
allow the oracle to take some (fixed) constant time, written $\corc$,
to provide its answer.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::show
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::show
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>We show that even with a realistic oracle, we can reduce task creation overheads. We start with bounding the depth; the result implies that the total depth is no larger than <span class="math inline">\(\mu\kappa\)</span> times the raw depth when <span class="math inline">\(\kappa\)</span> is large compared to <span class="math inline">\(\tau\)</span> and <span class="math inline">\(\phi\)</span>. Since with the ideal oracle this factor was <span class="math inline">\(\kappa\)</span>, the bound implies that the imprecision of the oracle can be influenced by changing the constant multiplicative factor.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
We show that even with a realistic oracle, we can reduce task creation
overheads.  We start with bounding the depth; the result implies that
the total depth is no larger than $\cerr\coff$ times the raw depth
when $\coff$ is large compared to $\csp$ and $\corc$.  Since with the
ideal oracle this factor was $\coff$, the bound implies that the
imprecision of the oracle can be influenced by changing the
constant multiplicative factor.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::depth
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::depth
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>  <span class="math display">\[e  \Downarrow^{\textsf{orc}} v, (w,d), (\mathcal{W},\mathcal{D}) \quad \Rightarrow \quad\mathcal{D}\leq (1+\mathtt{{max}}\left(\tau,\mu\kappa\right)+\phi)\,d\]</span></p>
<p>et <span class="math inline">\(\rho\)</span> denote <span class="math inline">\(1+\mathtt{{max}}\left(\tau,\mu\kappa\right)+\phi\)</span>; we want to prove that <span class="math inline">\(\mathcal{D}\leq  
\rhod\)</span>. The proof is by induction on the derivation <span class="math inline">\(e  \Downarrow^{\textsf{orc}} v, (w,d), (\mathcal{W},\mathcal{D})\)</span>.</p>
<p><span class="math inline">\(\bullet\)</span> For a rule with zero premises, we have <span class="math inline">\(\mathcal{D}= d= 1\)</span>. Because <span class="math inline">\(\rho\geq 1\)</span>, it follows that <span class="math inline">\(\mathcal{D}\leq \rhod\)</span>.</p>
<p><span class="math inline">\(\bullet\)</span> For a rule with one premise, we know by induction hypothesis that <span class="math inline">\(\mathcal{D}\leq \rhod\)</span>. Using again the fact that <span class="math inline">\(\rho\geq 1\)</span>, we can deduce the inequality <span class="math inline">\(\mathcal{D}+1 \leq \rho(d+1)\)</span>.</p>
<p><span class="math inline">\(\bullet\)</span> For a rule with two premises, we can similarly establish the conclusion <span class="math inline">\(\mathcal{D}_1+\mathcal{D}_2+1 \leq \rho(d_1+d_2+1)\)</span> using the induction hypotheses <span class="math inline">\(\mathcal{D}_1 \leq \rhod_1\)</span> and <span class="math inline">\(\mathcal{D}_2 \leq \rhod_2\)</span>.</p>
<p><span class="math inline">\(\bullet\)</span> Now, consider the case of a parallel tuple. First, assume that the two branches of this tuple are predicted to be large. In this case, the tuple is executed in parallel and the branches are executed in oracle mode. We exploit the induction hypotheses <span class="math inline">\(\mathcal{D}_1 \leq \rhod_1\)</span> and <span class="math inline">\(\mathcal{D}_2 \leq \rhod_2\)</span> to conclude as follows: <span class="math display">\[\begin{array}{l@{}l}  
\mathcal{D}&amp; \, = \, \mathtt{{max}}\left(\mathcal{D}_1,\mathcal{D}_2\right) + 1 + \tau+ \phi\\  
&amp; \, \leq  \,\mathtt{{max}}\left( \rho d_1 , \rho d_2 \right) + 1+\mathtt{{max}}\left(\tau,\mu\kappa\right)+\phi\\  
&amp; \, \leq  \,\mathtt{{max}}\left( \rho d_1 , \rho d_2 \right) + \rho\\  
&amp; \, \leq   \,\rho\, (\mathtt{{max}}\left( d_1 , d_2 \right) + 1) \\  
&amp; \, \leq \,  \rhod
\end{array}\]</span></p>
<p><span class="math inline">\(\bullet\)</span> Consider now the case where both branches are predicted to be small. In this case, the tuple is executed sequentially. Because the oracle predicts the branches to be smaller than <span class="math inline">\(\kappa\)</span>, they must be actually smaller than <span class="math inline">\(\mu\kappa\)</span>. So, we have <span class="math inline">\(w_1 \leq \mu\kappa\)</span> and <span class="math inline">\(w_2 \leq \mu\kappa\)</span>. Moreover, both branches are executed according to the sequential mode, so we have <span class="math inline">\(\mathcal{D}_1 = w_1\)</span> and <span class="math inline">\(\mathcal{D}_2 = w_2\)</span>. It follows that <span class="math inline">\(\mathcal{D}_1 \leq \mu\kappa\)</span> and <span class="math inline">\(\mathcal{D}_2 &lt; \mu\kappa\)</span>. Below, we also exploit the fact that <span class="math inline">\(\mathtt{{max}}\left( d_1 , d_2 \right) \geq 1\)</span>, which comes from the fact that raw depth is at least one unit. We conclude as follows: <span class="math display">\[\begin{array}{l@{}l}  
\mathcal{D}&amp; \, = \, \mathcal{D}_1 + \mathcal{D}_2 + 1 + \phi\\  
&amp; \, \leq  \,\mu\kappa+ \mu\kappa+ 1 + \phi\\  
&amp; \, \leq   \,(1+\mu\kappa+\phi)*2 \\  
&amp; \, \leq   \,(1+\mathtt{{max}}\left(\tau,\mu\kappa\right)+\phi)\cdot(\mathtt{{max}}\left( d_1 , d_2 \right) + 1) \\  
&amp; \, \leq   \,\rhod
\end{array}\]</span></p>
<p><span class="math inline">\(\bullet\)</span> It remains to consider the case where one branch is predicted to be smaller than the cutoff while the other branch is predicted to be larger than the cutoff. In this case again, both branches are executed sequentially. Without loss of generality, assume that the second branch is predicted to be small. In this case, we have <span class="math inline">\(w_2 \leq \mu\kappa\)</span>. This first branch is thus executed according to the sequential mode, so we have <span class="math inline">\(\mathcal{D}_2 = d_2 = w_2\)</span>. It follows that <span class="math inline">\(\mathcal{D}_2 \leq \mu\kappa\)</span>. For the first branch, which is executed according to the oracle mode, we can exploit the induction hypothesis which is <span class="math inline">\(\mathcal{D}_1 \leq \rhod_1\)</span>. We conclude as follows: <span class="math display">\[\begin{array}{l@{}l}  
\mathcal{D}&amp; \, = \, \mathcal{D}_1 + \mathcal{D}_2 + 1 + \phi\\  
&amp; \, \leq  \,\rhod_1 + \mu\kappa+ 1 + \phi\\  
&amp; \, \leq   \,\rhod_1 + (1+\mathtt{{max}}\left(\tau,\mu\kappa\right)+\phi) \\  
&amp; \, \leq   \,\rho\, (d_1 + 1) \\  
&amp; \, \leq   \,\rho\,(\mathtt{{max}}\left( d_1 , d_2 \right) + 1) \\  
&amp; \, \leq   \,\rhod
\end{array}\]</span></p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\begin{theorem}[Depth with a realistic oracle]
\label{thm:orc-cost-depth}
\label{thm:real-orc-depth}
$$\Jcostof{\sorc}{ e }{v}{\sw}{\sd}{\sws}{\sds} \Sq\impl \sds \leq (1+\kwmax{\csp}{\cerr\coff}+\corc)\,\sd$$
\end{theorem}
\begin{proof}


Let $\cspp$ denote
$1+\kwmax{\csp}{\cerr\coff}+\corc$; we want to  prove that $\sds \leq
\cspp\sd$.  The proof is by induction on the derivation 
$\Jcostof{\sorc}{ e }{v}{\sw}{\sd}{\sws}{\sds}$.

$\bullet$  For a rule with zero premises, we have $\sds = \sd = 1$.
Because $\cspp \geq 1$, it follows that $\sds \leq \cspp\sd$.

$\bullet$  For a rule with one premise, we know by induction hypothesis that
$\sds \leq \cspp\sd$.
Using again the fact that $\cspp \geq 1$, 
we can deduce the inequality $\sds+1 \leq \cspp(\sd+1)$.

$\bullet$ For a rule with two premises, we can similarly establish
the conclusion $\sds_1+\sds_2+1 \leq \cspp(\sd_1+\sd_2+1)$ using 
the induction hypotheses 
$\sds_1 \leq \cspp \sd_1$ and $\sds_2 \leq \cspp \sd_2$.

$\bullet$  Now, consider the case of a parallel tuple.
First, assume that the two branches of this tuple
are predicted to be large.
In this case, the tuple is executed in parallel and the
branches are executed in oracle mode.
We exploit the induction hypotheses
$\sds_1 \leq \cspp \sd_1$ and $\sds_2 \leq \cspp \sd_2$
to conclude as follows:
$$\begin{array}{l@{}l}
\sds & \Sc{=} \kwmax{\sds_1}{\sds_2} + 1 + \csp + \corc \\
& \Sc\leq \kwmax{ \cspp\sd_1 }{ \cspp\sd_2 } + 1+\kwmax{\csp}{\cerr\coff}+\corc \\
& \Sc\leq \kwmax{ \cspp\sd_1 }{ \cspp\sd_2 } + \cspp \\
& \Sc\leq  \cspp\, (\kwmax{ \sd_1 }{ \sd_2 } + 1) \\
& \Sc{\leq}  \cspp\sd
\end{array}$$

$\bullet$  Consider now the case where both branches are predicted
to be small. In this case, the tuple is executed sequentially.
Because the oracle predicts the branches to be smaller
than $\coff$, they must be actually smaller than $\cerr\coff$.
So, we have $\sw_1 \leq \cerr\coff$ and $\sw_2 \leq \cerr\coff$.
Moreover, both branches are executed according to the sequential mode,
so we have $\sds_1 = \sw_1$ and $\sds_2 = \sw_2$. 
It follows that $\sds_1 \leq \cerr\coff$ and $\sds_2 < \cerr\coff$.
Below, we also exploit the fact that 
$\kwmax{ \sd_1 }{ \sd_2 } \geq 1$, which comes from the
fact that raw depth is at least one unit.
We conclude as follows:
$$\begin{array}{l@{}l}
\sds & \Sc{=} \sds_1 + \sds_2 + 1 + \corc \\
& \Sc\leq \cerr\coff + \cerr\coff + 1 + \corc \\
& \Sc\leq  (1+\cerr\coff+\corc)*2 \\
& \Sc\leq  (1+\kwmax{\csp}{\cerr\coff}+\corc)\cdot(\kwmax{ \sd_1 }{ \sd_2 } + 1) \\
& \Sc\leq  \cspp\sd
\end{array}$$

$\bullet$  It remains to consider the case where one branch is predicted
to be smaller than the cutoff while the other branch is 
predicted to be larger than the cutoff.
In this case again, both branches are executed sequentially.
Without loss of generality, assume that the second branch is predicted
to be small. In this case, we have $\sw_2 \leq \cerr\coff$.
This first branch is thus executed according
to the sequential mode, so we have $\sds_2 = \sd_2 = \sw_2$. 
It follows that $\sds_2 \leq \cerr\coff$.
For the first branch, which is executed according to the oracle mode,
we can exploit the induction hypothesis which is $\sds_1 \leq \cspp\sd_1$.
We conclude as follows:
$$\begin{array}{l@{}l}
\sds & \Sc{=} \sds_1 + \sds_2 + 1 + \corc \\
& \Sc\leq \cspp\sd_1 + \cerr\coff + 1 + \corc\\
& \Sc\leq  \cspp\sd_1 + (1+\kwmax{\csp}{\cerr\coff}+\corc) \\
& \Sc\leq  \cspp \, (\sd_1 + 1) \\
& \Sc\leq  \cspp \,(\kwmax{ \sd_1 }{ \sd_2 } + 1) \\
& \Sc\leq  \cspp \sd
\end{array}$$
\qed
\end{proof}
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::ends
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::ends
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>This ends our analysis of the depth. Now, let us look at the work. The fact that every call to the oracle can induce a cost <span class="math inline">\(\phi\)</span> can lead the work to be multiplied by <span class="math inline">\(\phi\)</span>. For example, consider a program made of a complete tree built using <span class="math inline">\(n-1\)</span> sequential tuples, and leading to <span class="math inline">\(n\)</span> parallel tuples generating <span class="math inline">\(2n\)</span> values as leaves. The raw work is equal to <span class="math inline">\((n-1)+n+2n\)</span>, and the total work is <span class="math inline">\((n-1)+n\phi+2n\)</span>. Thus, <span class="math inline">\(\mathcal{W}\le \frac{\phi}{4} w\)</span> and this is tight for large values of <span class="math inline">\(n\)</span>. This means that a program executed according to the oracle semantics can slow down by as much as <span class="math inline">\(\phi/4\)</span>.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
This ends our analysis of the depth. Now, let us look at the work.
The fact that every call to the oracle can induce a cost~$\corc$ can
lead the work to be multiplied by~$\corc$.  For example, consider a
program made of a complete tree built using~$n-1$ sequential tuples,
and leading to $n$ parallel tuples generating $2n$ values as leaves.
The raw work is equal to $(n-1)+n+2n$, and the total work is
$(n-1)+n\corc+2n$.  Thus, $\sws \le \frac{\corc}{4} \sw$ and this is
tight for large values of $n$.  This means that a program executed
according to the oracle semantics can slow down by as much as
$\corc/4$.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::oracle
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::oracle
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>The problem with the above example is that the oracle is called infrequently—only at the leaves of the computation—preventing us from amortizing the cost of the oracle towards larger pieces of computations. Fortunately, most programs do not exhibit this pathological behavior, because parallel tuples are often performed close to the root of the computation, allowing us to detect smaller pieces of work early.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
The problem with the above example is that the oracle is called
infrequently---only at the leaves of the computation---preventing us
from amortizing the cost of the oracle towards larger pieces of
computations.  Fortunately, most programs do not exhibit this
pathological behavior, because parallel tuples are often performed
close to the root of the computation, allowing us to detect 
smaller pieces of work early.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::prevent
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::prevent
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>One way to prevent the oracle from being called on smaller pieces of work is to make sure that it is called at regular intervals. For proving a strong bound on the work, we will simply assume that the oracle is not called on small tasks by restricting our attention to balanced programs. To this end, we define balanced programs as programs that call the oracle only on expressions that are no smaller than some constant <span class="math inline">\(\gamma\)</span> off from the value <span class="math inline">\(\frac{\kappa}{\mu}\)</span>, for some <span class="math inline">\(\gamma\geq 1\)</span>. Note that we use <span class="math inline">\(\frac{\kappa}{\mu}\)</span> as a target and not <span class="math inline">\(\kappa\)</span> so as to accomodate possible over-estimations in the estimations of raw work. The formal definition follows.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
One way to prevent the oracle from being
called on smaller pieces of work is to make sure that it is called at
regular intervals.  For proving a strong bound on the
work, we will simply assume that the oracle is not called on small
tasks by restricting our attention to balanced programs. 
To this end, we define balanced programs as programs 
that call the oracle only on expressions that are no smaller 
than some constant $\cbal$ off from the value $\frac{\coff}{\cerr}$,
for some $\cbal \geq 1$. Note that we use $\frac{\coff}{\cerr}$ as
a target and not $\coff$ so as to accomodate possible over-estimations
in the estimations of raw work.
The formal definition follows.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::balanced
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::balanced
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>For <span class="math inline">\(\gamma\ge 1\)</span>, a program or expression <span class="math inline">\(e\)</span> is <span class="math inline">\(\gamma\)</span>-balanced if evaluating <span class="math inline">\(e\)</span> in the oracle mode invokes the oracle only for subexpressions whose raw work is no less than <span class="math inline">\(\frac{\kappa}{\mu\gamma}\)</span>.</p>
<p>Note that if a program is <span class="math inline">\(\gamma\)</span>-balanced and if <span class="math inline">\(\gamma&lt; \gamma&#39;\)</span>, then this program is also <span class="math inline">\(\gamma&#39;\)</span>-balanced. We will later give a sufficient condition for proving that particular programs are balanced (§<a href="#sec:balanced" data-reference-type="ref" data-reference="sec:balanced">[sec:balanced]</a>).</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\begin{definition}[Balanced programs]
For $\cbal \ge 1$, a program or expression $e$ is $\cbal$-balanced if
evaluating $e$ in the oracle mode invokes the oracle only for
subexpressions whose raw work is no less than
$\frac{\coff}{\cerr\cbal}$.
\end{definition}
Note that if a program is $\cbal$-balanced and 
if $\cbal < \cbal'$, then this program is also $\cbal'$-balanced.
We will later give a sufficient condition for
proving that particular programs are balanced (\sref {sec:balanced}).
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::work
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::work
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>  Assume <span class="math inline">\(e  \Downarrow^{\textsf{orc}} v, (w,d), (\mathcal{W},\mathcal{D})\)</span> where <span class="math inline">\(e\)</span> is a <span class="math inline">\(\gamma\)</span>-balanced program. <span class="math display">\[\mathcal{W}\quad \leq  \quad\left(1+\frac{\mu(\tau+\gamma\phi)}{\kappa+1} \right)\,{w}.\]</span></p>
<p>e establish the following slightly tighter inequality. <span class="math display">\[\mathcal{W}\quad \leq  \quad\left(1 + \frac{\tau}{\kappa/\mu\; + \;1} + \frac{\phi}{\kappa/(\mu\gamma)\; + \;1} \right)\,w.\]</span> The bound is indeed tighter because <span class="math inline">\(\gamma\geq 1\)</span> and <span class="math inline">\(\mu\geq 1\)</span>. Define <span class="math inline">\(\kappa&#39;\)</span> as a shorthand for <span class="math inline">\(\kappa/\mu\)</span> and <span class="math inline">\(\kappa&#39;&#39;\)</span> as a shorthand for <span class="math inline">\(\kappa/(\mu\gamma)\)</span>. Note that, because <span class="math inline">\(\gamma\geq 1\)</span>, we have <span class="math inline">\(\kappa&#39;&#39;\leq \kappa&#39;\)</span>. Let <span class="math inline">\({x}^{+}\)</span> be defined as the value <span class="math inline">\(x\)</span> when <span class="math inline">\(x\)</span> is nonnegative and as zero otherwise. We prove by induction that: <span class="math display">\[\begin{array}{l}   
\mathcal{W}\, \leq  \,w+ \tau\left\lfloor \frac{ { \left(w-\kappa\right) }^{+}}{\kappa&#39;+ 1} \right\rfloor + \phi\left\lfloor \frac{ { \left(w-\kappa&#39;&#39;\right) }^{+}}{\kappa&#39;&#39;+1} \right\rfloor  
 \end{array}\]</span> This is indeed a strengthened result because we have: <span class="math display">\[\begin{array}{ll}  
&amp;  \begin{array}{l}   
\tau\left\lfloor \frac{ { \left(w-\kappa&#39;\right) }^{+}}{\kappa&#39;+1} \right\rfloor  
\, \leq  \,\tau\frac{w}{\kappa&#39;+1}   
\, \leq  \,\frac{\tau}{\kappa/\mu+1}\,w
 \end{array}  \vspace{3pt} \\  
\textrm{and} &amp;  
 \begin{array}{l}   
\phi\left\lfloor \frac{ { \left(w-\kappa&#39;&#39;\right) }^{+}}{\kappa&#39;&#39;+1} \right\rfloor  
\, \leq  \,\phi\frac{w}{\kappa&#39;&#39;+1}   
\, \leq  \,\frac{\phi}{\kappa/(\mu\gamma)\; + \;1}\,w
 \end{array}   
\end{array}\]</span></p>
<p>The proof is conducted by induction on the derivation of the reduction hypothesis.</p>
<p><span class="math inline">\(\bullet\)</span> For a rule with zero premises, which describe an atomic operation, we have <span class="math inline">\(\mathcal{W}= w= 1\)</span>, so the conclusion is satisfied.</p>
<p><span class="math inline">\(\bullet\)</span> For a rule with a single premise, the induction hypothesis is: <span class="math display">\[\begin{array}{@{}l}  
\mathcal{W}\, \leq  \,w+ \tau\left\lfloor \frac{ { \left(w-\kappa&#39;\right) }^{+}}{\kappa&#39;+1} \right\rfloor + \phi\left\lfloor \frac{ { \left(w-\kappa&#39;&#39;\right) }^{+}}{\kappa&#39;&#39;+1} \right\rfloor  
\end{array}\]</span> So, we can easily derive the conclusion: <span class="math display">\[\begin{array}{@{}l}  
\mathcal{W}+1 \, \leq  \,(w+1) + \tau\left\lfloor \frac{ { \left((w+1)-\kappa&#39;\right) }^{+}}{\kappa&#39;+1} \right\rfloor + \phi\left\lfloor \frac{ { \left((w+1)-\kappa&#39;&#39;\right) }^{+}}{\kappa&#39;&#39;+1} \right\rfloor  
\end{array}\]</span></p>
<p><span class="math inline">\(\bullet\)</span> For a rule with two premises, we exploit the mathematical inequality <span class="math inline">\(\left\lfloor \frac{n}{q} \right\rfloor+\left\lfloor \frac{m}{q} \right\rfloor \leq \left\lfloor \frac{n+m}{q} \right\rfloor\)</span>. We have: <span class="math display">\[\begin{array}{l@{}ll}  
\mathcal{W}&amp; \, = \, \mathcal{W}_1 + \mathcal{W}_2 + 1 \\  
&amp; \, \leq  \,w_1 + \tau\left\lfloor \frac{ { \left(w_1-\kappa&#39;\right) }^{+}}{\kappa&#39;+1} \right\rfloor + \phi\left\lfloor \frac{ { \left(w_1-\kappa&#39;&#39;\right) }^{+}}{\kappa&#39;&#39;+1} \right\rfloor \\  
&amp; \quad +\; w_2 + \tau\left\lfloor \frac{ { \left(w_2-\kappa&#39;\right) }^{+}}{\kappa&#39;+1} \right\rfloor + \phi\left\lfloor \frac{ { \left(w_2-\kappa&#39;&#39;\right) }^{+}}{\kappa&#39;&#39;+1} \right\rfloor + 1 \\  
&amp; \, \leq  \,w+ \tau\left\lfloor \frac{ { \left(w_1-\kappa&#39;\right) }^{+}+ { \left(w_2-\kappa&#39;\right) }^{+}}{\kappa&#39;+1} \right\rfloor  \\   
&amp; \quad +\; \phi\left\lfloor \frac{ { \left(w_1-\kappa&#39;&#39;\right) }^{+}+ { \left(w_2-\kappa&#39;&#39;\right) }^{+}}{\kappa&#39;&#39;+1} \right\rfloor \\  
\end{array}\]</span> To conclude, we need to establish the following two mathematical inequalities: <span class="math display">\[\begin{array}{l}   
 { \left(w_1-\kappa&#39;\right) }^{+}+ { \left(w_2-\kappa&#39;\right) }^{+} \, \leq  \, { \left((w_1+w_2+1)-\kappa&#39;\right) }^{+}\\  
 { \left(w_1-\kappa&#39;&#39;\right) }^{+}+ { \left(w_2-\kappa&#39;&#39;\right) }^{+} \, \leq  \, { \left((w_1+w_2+1)-\kappa&#39;&#39;\right) }^{+}  
 \end{array}\]</span> The two equalities can be proved in a similar way. Let us establish the first one. There are four cases to consider. First, if both <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span> are less than <span class="math inline">\(\kappa&#39;\)</span>, then the right-hand side is zero, so we are done. Second, if both <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span> are greater than <span class="math inline">\(\kappa&#39;\)</span>, then all the expressions are nonnegative, and we are left to check the inequality <span class="math inline">\(w_1-\kappa&#39;+w_2-\kappa&#39;\leq w_1+w_2+1-\kappa&#39;\)</span>. Third, if <span class="math inline">\(w_1\)</span> is greater than <span class="math inline">\(\kappa&#39;\)</span> and <span class="math inline">\(w_2\)</span> is smaller than <span class="math inline">\(\kappa&#39;\)</span>, then the inequality becomes <span class="math inline">\({ \left(w_1-\kappa&#39;\right) }^{+} \, \leq  \, { \left((w_1-\kappa&#39;)+(w_2+1)\right) }^{+}\)</span>, which is clearly true. The case <span class="math inline">\(w_1 \geq \kappa&#39;\)</span> and <span class="math inline">\(w_2 &lt; \kappa&#39;\)</span> is symmetrical. This concludes the proof.</p>
<p><span class="math inline">\(\bullet\)</span> Consider now the case of a parallel tuple where both branches are predicted to involve more than <span class="math inline">\(\kappa\)</span> units of work. This implies <span class="math inline">\(w_1 \geq \kappa&#39;\)</span> and <span class="math inline">\(w_2 \geq \kappa&#39;\)</span>. In this case, a parallel task is created. Note that, because <span class="math inline">\(\kappa&#39;&#39;\leq \kappa&#39;\)</span>, we also have <span class="math inline">\(w_1 \geq \kappa&#39;&#39;\)</span> and <span class="math inline">\(w_2 \geq \kappa&#39;&#39;\)</span>. So, all the values involved in the following computations are nonnegative. Using the induction hypotheses, we have: <span class="math display">\[\begin{array}{l@{}ll}  
\mathcal{W}
&amp; \, = \, \mathcal{W}_1 + \mathcal{W}_2 + 1 + \tau+ \phi\\   
&amp; \, \leq  \,w_1 + \tau\left\lfloor \frac{w_1-\kappa&#39;}{\kappa&#39;+1} \right\rfloor + \phi\left\lfloor \frac{w_1-\kappa&#39;&#39;}{\kappa&#39;&#39;+1} \right\rfloor \\  
&amp; \quad +\; w_2 + \tau\left\lfloor \frac{w_2-\kappa&#39;}{\kappa&#39;+1} \right\rfloor + \phi\left\lfloor \frac{w_2-\kappa&#39;&#39;}{\kappa&#39;&#39;+1} \right\rfloor + 1 + \tau+ \phi\\   
&amp; \, \leq  \,(w_1 + w_2 + 1) + \tau(\left\lfloor \frac{w_1-\kappa&#39;}{\kappa&#39;+1} \right\rfloor + \left\lfloor \frac{w_2-\kappa&#39;}{\kappa&#39;+1} \right\rfloor + 1) \\  
&amp; \quad +\;\phi(\left\lfloor \frac{w_1-\kappa&#39;&#39;}{\kappa&#39;&#39;+1} \right\rfloor + \left\lfloor \frac{w_2-\kappa&#39;&#39;}{\kappa&#39;&#39;+1} \right\rfloor + 1)  \\  
&amp; \, \leq  \,w+ \tau\left\lfloor \frac{(w_1-\kappa&#39;)+(w_2-\kappa&#39;)+(\kappa&#39;+1)}{\kappa&#39;+1} \right\rfloor  \\  
&amp; \quad +\; \phi\left\lfloor \frac{(w_1-\kappa&#39;&#39;)+(w_2-\kappa&#39;&#39;)+(\kappa&#39;&#39;+1)}{\kappa&#39;&#39;+1} \right\rfloor \\  
&amp; \, \leq  \,w+ \tau\left\lfloor \frac{(w_1+w_2+1)-\kappa&#39;}{\kappa&#39;+1} \right\rfloor + \phi\left\lfloor \frac{(w_1+w_2+1)-\kappa&#39;&#39;}{\kappa&#39;&#39;+1} \right\rfloor \\  
&amp; \, \leq  \,w+ \tau\left\lfloor \frac{w-\kappa&#39;}{\kappa&#39;+1} \right\rfloor + \phi\left\lfloor \frac{w-\kappa&#39;&#39;}{\kappa&#39;&#39;+1} \right\rfloor  \\  
\end{array}\]</span></p>
<p><span class="math inline">\(\bullet\)</span> Assume now that the two branches are predicted to be less than the cutoff. This implies <span class="math inline">\(w_1 \leq \kappa&#39;\)</span> and <span class="math inline">\(w_2 \leq \kappa&#39;\)</span>. Both these tasks are executed sequentially, so <span class="math inline">\(\mathcal{W}_1 = w_1\)</span> and <span class="math inline">\(\mathcal{W}_2 = w_2\)</span>. Since the program is <span class="math inline">\(\gamma\)</span>-balanced, we have <span class="math inline">\(w_1 \geq \kappa&#39;&#39;\)</span> and <span class="math inline">\(w_2 \geq \kappa&#39;&#39;\)</span>. Those inequalities ensure that we are able to pay for the cost of calling the oracle, that is, the cost <span class="math inline">\(\phi\)</span>. Indeed, since we have <span class="math inline">\(w_1 + w_2 + 1 - \kappa&#39;&#39;\geq \kappa&#39;&#39;+1\)</span>, we know that <span class="math inline">\(\left\lfloor \frac{w_1 + w_2 + 1-\kappa&#39;&#39;}{\kappa&#39;&#39;+1} \right\rfloor \geq 1\)</span>. Therefore: <span class="math display">\[\begin{array}{l@{}ll}  
\mathcal{W}
&amp; \, = \, \mathcal{W}_1 + \mathcal{W}_2 + 1 + \phi\\   
&amp; \, \leq  \,w_1 + w_2 + 1 + \phi\\  
&amp; \, \leq  \,(w_1 + w_2 + 1) + \phi\left\lfloor \frac{w_1 + w_2 + 1-\kappa&#39;&#39;}{\kappa&#39;&#39;+1} \right\rfloor  \\  
&amp; \, \leq  \,w+ \tau\left\lfloor \frac{ { \left(w-\kappa&#39;\right) }^{+}}{\kappa&#39;+1} \right\rfloor + \phi\left\lfloor \frac{w-\kappa&#39;&#39;}{\kappa&#39;&#39;+1} \right\rfloor \\  
\end{array}\]</span></p>
<p><span class="math inline">\(\bullet\)</span> It remains to consider the case where one branch is predicted to be bigger than the cutoff while the other is predicted to be smaller than the cutoff. For example, assume <span class="math inline">\(w_1 \geq \kappa&#39;\)</span> and <span class="math inline">\(w_2 \leq \kappa&#39;\)</span>. The parallel tuple is thus executed as a sequential tuple. The first task is executed in oracle mode, whereas the second task is executed in the sequential mode. For the first task, we can invoke the induction hypothesis <span class="math inline">\(\mathcal{W}_1 \leq w_1 + \tau\left\lfloor \frac{w_1-\kappa&#39;}{\kappa&#39;+1} \right\rfloor + \phi\left\lfloor \frac{w_1-\kappa&#39;&#39;}{\kappa&#39;&#39;+1} \right\rfloor\)</span>. For the second task, which is executed sequentially, we have <span class="math inline">\(\mathcal{W}_2 = w_2\)</span>. Moreover, the regularity hypothesis gives us <span class="math inline">\(w_2 \geq \kappa&#39;&#39;\)</span>. Hence, we have <span class="math inline">\(\left\lfloor \frac{w_2+1}{\kappa&#39;&#39;+1} \right\rfloor \geq 1\)</span>. We conclude as follows: <span class="math display">\[\begin{array}{@{}l@{}ll}  
\mathcal{W}
&amp; \, = \, \mathcal{W}_1 + \mathcal{W}_2 + 1 + \phi\\   
&amp; \, \leq  \,w_1 + \tau\left\lfloor \frac{w_1-\kappa&#39;}{\kappa&#39;+1} \right\rfloor   
+ \phi\left\lfloor \frac{w_1-\kappa&#39;&#39;}{\kappa&#39;&#39;+1} \right\rfloor + w_2 + 1 + \phi\vspace{1pt}\\  
&amp; \, \leq  \,w_1 + \tau\left\lfloor \frac{w_1-\kappa&#39;}{\kappa&#39;+1} \right\rfloor   
+ \phi\left\lfloor \frac{w_1-\kappa&#39;&#39;}{\kappa&#39;&#39;+1} \right\rfloor + w_2 + 1 + \phi\left\lfloor \frac{w_2+1}{\kappa&#39;+1} \right\rfloor \vspace{1pt}\\  
&amp; \, \leq  \,w+ \tau\left\lfloor \frac{w_1+w_2+1-\kappa&#39;}{\kappa&#39;+1} \right\rfloor + \phi\left\lfloor \frac{w_1+w_2+1-\kappa&#39;&#39;}{\kappa&#39;&#39;+1} \right\rfloor \vspace{1pt}\\  
&amp; \, \leq  \,w+ \tau\left\lfloor \frac{w-\kappa&#39;}{\kappa&#39;+1} \right\rfloor + \phi\left\lfloor \frac{w-\kappa&#39;&#39;}{\kappa&#39;&#39;+1} \right\rfloor \\  
\end{array}\]</span></p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\begin{theorem}[Work with a realistic oracle]
\label{thm:orc-cost-work}
\label{thm:real-orc-work}
Assume $\Jcostof{\sorc}{ e }{v}{\sw}{\sd}{\sws}{\sds}$ where $e$ is
a $\cbal$-balanced program.
\[
\sws \Sq\leq \left(1+\frac{\cerr(\csp+\cbal\corc)}{\coff+1} \right)\,{\sw}.
\]

\end{theorem}
\begin{proof}
We establish the following slightly tighter inequality.
$$\sws \Sq\leq \left(1 + \frac{\csp}{\coff/\cerr\Ss{+}1} + \frac{\corc}{\coff/(\cerr\creg)\Ss{+}1} \right)\,\sw.$$
The bound is indeed tighter because $\creg \geq 1$ and $\cerr \geq 1$.
Define $\coffe$ as a shorthand for $\coff/\cerr$
and $\coffr$ as a shorthand for $\coff/(\cerr\creg)$.
Note that, because $\creg \geq 1$, we have $\coffr \leq \coffe$.
Let $\kpos{x}$ be defined as the value $x$ when $x$ is nonnegative and as zero otherwise.
We prove by induction that:
$$\begin{lines}
\sws \Sc\leq \sw + \csp \floorfracposn{\sw-\coff}{\coffe + 1} + \corc \floorfracposn{\sw-\coffr}{\coffr +1}
\end{lines}$$
This is indeed a strengthened result because we have:
$$\begin{array}{ll}
& \begin{lines}
\csp \floorfracposn{\sw-\coffe}{\coffe+1}
\Sc\leq \csp \frac{\sw}{\coffe+1} 
\Sc\leq \frac{\csp}{\coff/\cerr+1}\,\sw
\end{lines} \vspace{3pt} \\
\X{and} &
\begin{lines}
\corc \floorfracposn{\sw-\coffr}{\coffr+1}
\Sc\leq \corc \frac{\sw}{\coffr+1} 
\Sc\leq \frac{\corc}{\coff/(\cerr\creg)\Ss{+}1}\,\sw
\end{lines}
\end{array}$$

The proof is conducted by induction on the derivation of the reduction hypothesis.

$\bullet$ For a rule with zero premises, which describe an atomic
operation, we have $\sws = \sw = 1$, so the conclusion is satisfied.

$\bullet$  For a rule with a single premise,
the induction hypothesis is:
$$\begin{array}{@{}l}
\sws \Sc\leq \sw + \csp \floorfracposn{\sw-\coffe}{\coffe+1} + \corc \floorfracposn{\sw-\coffr}{\coffr+1}
\end{array}$$
So, we can easily derive the conclusion:
$$\begin{array}{@{}l}
\sws+1 \Sc\leq (\sw+1) + \csp \floorfracposn{(\sw+1)-\coffe}{\coffe+1} + \corc \floorfracposn{(\sw+1)-\coffr}{\coffr+1}
\end{array}$$

$\bullet$  For a rule with two premises, we exploit the mathematical inequality
$\floorfrac{n}{q}+\floorfrac{m}{q} \leq \floorfrac{n+m}{q}$. We have:
$$\begin{array}{l@{}ll}
\sws & \Sc{=} \sws_1 + \sws_2 + 1 \\
& \Sc\leq \sw_1 + \csp \floorfracposn{\sw_1-\coffe}{\coffe+1} + \corc \floorfracposn{\sw_1-\coffr}{\coffr+1} \\
& \quad +\; \sw_2 + \csp \floorfracposn{\sw_2-\coffe}{\coffe+1} + \corc \floorfracposn{\sw_2-\coffr}{\coffr+1} + 1 \\
& \Sc\leq \sw + \csp \floorfrac{\kposOf{\sw_1-\coffe}+\kposOf{\sw_2-\coffe}}{\coffe+1}  \\ 
& \quad +\; \corc \floorfrac{\kposOf{\sw_1-\coffr}+\kposOf{\sw_2-\coffr}}{\coffr+1} \\
\end{array}$$
To conclude, we need to establish the following two mathematical inequalities:
$$\begin{lines}
\kposOf{\sw_1-\coffe}+\kposOf{\sw_2-\coffe} \Sc\leq \kposOf{(\sw_1+\sw_2+1)-\coffe}\\
\kposOf{\sw_1-\coffr}+\kposOf{\sw_2-\coffr} \Sc\leq \kposOf{(\sw_1+\sw_2+1)-\coffr}
\end{lines}$$
The two equalities can be proved in a similar way. Let us establish the first one.
There are four cases to consider.
First, if both $\sw_1$ and $\sw_2$ are less than $\coffe$, then the right-hand side is zero,
so we are done. Second, if both $\sw_1$ and $\sw_2$ are greater than $\coffe$,
then all the expressions are nonnegative, and we are left to check the inequality
$\sw_1-\coffe+\sw_2-\coffe \leq \sw_1+\sw_2+1-\coffe$.
Third, if $\sw_1$ is greater than $\coffe$ and $\sw_2$ is smaller than $\coffe$, then
the inequality becomes $\kposOf{\sw_1-\coffe} \Sc\leq \kposOf{(\sw_1-\coffe)+(\sw_2+1)}$,
which is clearly true.
The case $\sw_1 \geq \coffe$ and $\sw_2 < \coffe$ is symmetrical.
This concludes the proof.

$\bullet$  Consider now the case of a parallel tuple
where both branches are predicted to involve more than $\coff$ units of work.
This implies $\sw_1 \geq \coffe$ and $\sw_2 \geq \coffe$.
In this case, a parallel task is created. 
Note that, because $\coffr \leq \coffe$, we also have 
$\sw_1 \geq \coffr$ and $\sw_2 \geq \coffr$. So, all the values
involved in the following computations are nonnegative.
Using the induction hypotheses, we have:
$$\begin{array}{l@{}ll}
\sws 
& \Sc{=} \sws_1 + \sws_2 + 1 + \csp + \corc  \\ 
& \Sc\leq \sw_1 + \csp \floorfrac{\sw_1-\coffe}{\coffe+1} + \corc \floorfrac{\sw_1-\coffr}{\coffr+1} \\
& \quad +\; \sw_2 + \csp \floorfrac{\sw_2-\coffe}{\coffe+1} + \corc \floorfrac{\sw_2-\coffr}{\coffr+1} + 1 + \csp + \corc \\ 
& \Sc\leq (\sw_1 + \sw_2 + 1) + \csp (\floorfrac{\sw_1-\coffe}{\coffe+1} + \floorfrac{\sw_2-\coffe}{\coffe+1} + 1) \\
& \quad +\;\corc (\floorfrac{\sw_1-\coffr}{\coffr+1} + \floorfrac{\sw_2-\coffr}{\coffr+1} + 1)  \\
& \Sc\leq \sw + \csp \floorfrac{(\sw_1-\coffe)+(\sw_2-\coffe)+(\coffe+1)}{\coffe+1}  \\
& \quad +\; \corc \floorfrac{(\sw_1-\coffr)+(\sw_2-\coffr)+(\coffr+1)}{\coffr+1} \\
& \Sc\leq \sw + \csp \floorfrac{(\sw_1+\sw_2+1)-\coffe}{\coffe+1} + \corc \floorfrac{(\sw_1+\sw_2+1)-\coffr}{\coffr+1} \\
& \Sc\leq \sw + \csp \floorfrac{\sw-\coffe}{\coffe+1} + \corc \floorfrac{\sw-\coffr}{\coffr+1}  \\
\end{array}$$

$\bullet$  Assume now that the two branches are predicted to be less than the cutoff.
This implies $\sw_1 \leq \coffe$ and $\sw_2 \leq \coffe$. Both these tasks
are executed sequentially, so $\sws_1 = \sw_1$ and $\sws_2 = \sw_2$. 
Since the program is $\cbal$-balanced, 
we have $\sw_1 \geq \coffr$
and $\sw_2 \geq \coffr$. 
Those inequalities ensure that we are able to pay for the cost of calling the oracle,
that is, the cost $\corc$.
Indeed, since we have $\sw_1 + \sw_2 + 1 - \coffr \geq \coffr+1$, 
we know that $\floorfrac{\sw_1 + \sw_2 + 1-\coffr}{\coffr+1} \geq 1$.
Therefore:
$$\begin{array}{l@{}ll}
\sws 
& \Sc{=} \sws_1 + \sws_2 + 1 + \corc \\ 
& \Sc\leq \sw_1 + \sw_2 + 1 + \corc \\
& \Sc\leq (\sw_1 + \sw_2 + 1) + \corc \floorfrac{\sw_1 + \sw_2 + 1-\coffr}{\coffr+1}  \\
& \Sc\leq \sw + \csp \floorfrac{\kposOf{\sw-\coffe}}{\coffe+1} + \corc \floorfrac{\sw-\coffr}{\coffr+1} \\
\end{array}$$

$\bullet$  It remains to consider the case where one branch
is predicted to be bigger than the cutoff while the other
is predicted to be smaller than the cutoff. For example,
assume $\sw_1 \geq \coffe$ and $\sw_2 \leq \coffe$.
The parallel tuple is thus executed as a sequential tuple.
The first task is executed in oracle mode, whereas the
second task is executed in the sequential mode.
For the first task, we can invoke the induction hypothesis
$\sws_1 \leq \sw_1 + \csp \floorfrac{\sw_1-\coffe}{\coffe+1} + \corc \floorfrac{\sw_1-\coffr}{\coffr+1} $.
For the second task, which is executed sequentially, we have $\sws_2 = \sw_2$.
Moreover, the regularity hypothesis gives us $\sw_2 \geq \coffr$.
Hence, we have $\floorfrac{\sw_2+1}{\coffr+1} \geq 1$.
We conclude as follows:
$$\begin{array}{@{}l@{}ll}
\sws 
& \Sc{=} \sws_1 + \sws_2 + 1 + \corc \\ 
& \Sc\leq \sw_1 + \csp \floorfrac{\sw_1-\coffe}{\coffe+1} 
+ \corc \floorfrac{\sw_1-\coffr}{\coffr+1} + \sw_2 + 1 + \corc \vspace{1pt}\\
& \Sc\leq \sw_1 + \csp \floorfrac{\sw_1-\coffe}{\coffe+1} 
+ \corc \floorfrac{\sw_1-\coffr}{\coffr+1} + \sw_2 + 1 + \corc \floorfrac{\sw_2+1}{\coffe+1} \vspace{1pt}\\
& \Sc\leq \sw + \csp \floorfrac{\sw_1+\sw_2+1-\coffe}{\coffe+1} + \corc \floorfrac{\sw_1+\sw_2+1-\coffr}{\coffr+1} \vspace{1pt}\\
& \Sc\leq \sw + \csp \floorfrac{\sw-\coffe}{\coffe+1} + \corc \floorfrac{\sw-\coffr}{\coffr+1} \\
\end{array}$$
\qed
\end{proof}
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::ready
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::ready
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>We are now ready to combine the version of Brent’s theorem adapated to our cost semantics with the bounds that we have established for the total work and depth in <span class="math inline">\(\gamma\)</span>-balanced parallel programs executed under the oracle semantics.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
We are now ready to combine the version of Brent's theorem
adapated to our cost semantics with the bounds that we
have established for the total work and depth 
in $\cbal$-balanced parallel programs executed
under the oracle semantics.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::time
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::time
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>  <br />
Assume an oracle that costs <span class="math inline">\(\phi\)</span> and makes an error by a factor not exceeding <span class="math inline">\(\mu\)</span>. Assume <span class="math inline">\(\kappa&gt; \tau\)</span>, which is always the case in practice. The execution time of a parallel <span class="math inline">\(\gamma\)</span>-balanced program on a machine with <span class="math inline">\(P\)</span> processors under the oracle semantics with a greedy scheduler does not exceed the value <span class="math display">\[\left( 1+\frac{\mu(\tau+\gamma\phi)}{\kappa} \right)\,\frac{w}{P} \, + \, \left( \kappa\mu+ \phi+ 1\right)d.\]</span></p>
<p>he bound follows by the version of Brent’s theorem adpated to our cost semantics (Theorem <a href="#thm:generalize-brent" data-reference-type="ref" data-reference="thm:generalize-brent">[thm:generalize-brent]</a>), and by the bounds established in Theorem <a href="#thm:orc-cost-work" data-reference-type="ref" data-reference="thm:orc-cost-work">[thm:orc-cost-work]</a> and Theorem <a href="#thm:orc-cost-depth" data-reference-type="ref" data-reference="thm:orc-cost-depth">[thm:orc-cost-depth]</a>. For simplicity, we have replaced the denominator <span class="math inline">\(\kappa+1\)</span> with <span class="math inline">\(\kappa\)</span>. This change does not loosen the bound significantly because <span class="math inline">\(\kappa\)</span> is usually very large in front of a unit cost.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\begin{theorem}[Execution time with a realistic oracle]
\label{thm:orc-cost-bound}
\label{thm:orc-time-bound}~\\
Assume an oracle that costs~$\corc$ and makes an error by a factor not
exceeding~$\cerr$. Assume $\coff > \csp$, which is always the case
in practice.
The execution time of a parallel $\cbal$-balanced
program on a machine with $P$ processors under the oracle semantics
with a greedy scheduler does not exceed the value
\[
\left( 1+\frac{\cerr(\csp+\cbal\corc)}{\coff} \right)\,\frac{\sw}{P} \Sc{+} \left( \coff\cerr + \corc + 1\right)d.
\]
\end{theorem}
\begin{proof}
The bound follows by the version of Brent's theorem
adpated to our cost semantics
(\thmref{generalize-brent}), and by the bounds 
established in
\thmref{orc-cost-work} and \thmref{orc-cost-depth}. 
For simplicity, we have replaced the denominator $\coff+1$
with $\coff$. This change does not loosen the bound significantly
because $\coff$ is usually very large in front of a unit cost.
 \qed
\end{proof}
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->


</segment> <!-- subsection -->

<segment name='subsection'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Choice of the cutoff
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Choice of the cutoff
]]>
</field> <!-- title_src -->
<field name='label'>
sec:oracle::choice-of-the-cutoff
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::pictures
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='figure'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
fig:run-time-versus-kappa
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p><img src="pictures/plot-run-time-versus-k-multi" alt="image" style="width:3in" /></p>
<p> <br />
<span>An illustration of the run-time function <span class="math inline">\(\left(  
  1+\frac{\mu(\tau+\gamma\phi)}{\kappa} \right)\,\frac{w}{P} \, + \,  
  (\kappa\mu+\phi+1)\,d\)</span> on <span class="math inline">\(P = 4\)</span> processors with constants <span class="math inline">\(\mu=1\)</span>, <span class="math inline">\(\tau= 5\)</span>, <span class="math inline">\(\gamma= 1\)</span>, and <span class="math inline">\(\phi= 2\)</span>, and different work and depth values.</span></p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\includegraphics[width=3in]{pictures/plot-run-time-versus-k-multi}

\caption{An illustration of the run-time function $\left(
  1+\frac{\cerr(\csp+\cbal\corc)}{\coff} \right)\,\frac{\sw}{P} \Sc{+}
  (\coff\cerr+\corc+1)\,d$ on $P = 4$ processors with constants $\cerr =1$,
   $\csp = 5$, $\cbal = 1$, and $\corc = 2$, and different work and
  depth values.}

\label{fig:run-time-versus-kappa}
]]>
</field> <!-- body_src -->

</atom> <!-- figure -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::orc-time-bound
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::orc-time-bound
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Theorem <a href="#thm:orc-time-bound" data-reference-type="ref" data-reference="thm:orc-time-bound">[thm:orc-time-bound]</a> shows that the running time of a parallel program can be controlled by changing the constant <span class="math inline">\(\kappa\)</span>; the formula, however, reveals an interesting tradeoff: we can reduce task-creation overheads but this comes at the cost of increasing the depth. To see this connection better, consider the bound that appears in the statement of Theorem <a href="#thm:orc-time-bound" data-reference-type="ref" data-reference="thm:orc-time-bound">[thm:orc-time-bound]</a> and notice that as increases the work (first) term decreases but the depth (second) term increases. Figure <a href="#fig:run-time-versus-kappa" data-reference-type="ref" data-reference="fig:run-time-versus-kappa">[fig:run-time-versus-kappa]</a> illustrates a concrete instance of the bound for a hypothetical computation for fixed constants but different raw work and raw depth. The exact values of the constant and the raw work and depth are not relevant to our discussion; constants are fixed at some reasonable values consistent with our experimental observations. The work and depth are consistent with a program whose raw work is linear in the input size and whose raw depth is logarithmic in the input size.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\thmref{orc-time-bound} shows that the running time of a parallel
program can be controlled by changing the constant $\coff$; the
formula, however, reveals an interesting tradeoff: we can reduce
task-creation overheads but this comes at the cost of increasing the
depth.  To see this connection better, consider the bound
that appears in the statement of \thmref{orc-time-bound}
and notice that as \coff increases the work (first) term decreases
but the depth (second) term increases.  \figref{run-time-versus-kappa}
illustrates a concrete instance of the bound for a hypothetical
computation for fixed constants but different raw work and raw depth.
The exact values of the constant and the raw work and depth are not
relevant to our discussion; constants are fixed at some reasonable
values consistent with our experimental observations.  The work and
depth are consistent with a program whose raw work is linear
in the input size
and whose raw depth is logarithmic in the input size.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::run-time-versus-kappa
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::run-time-versus-kappa
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>As Figure <a href="#fig:run-time-versus-kappa" data-reference-type="ref" data-reference="fig:run-time-versus-kappa">[fig:run-time-versus-kappa]</a> illustrates, the parallel run time decreases as we increase up to some inflection point and then starts increasing. We compute the optimal value for by solving for the root of the derivative. We obtain: <span class="math display">\[\kappa^* = \sqrt{\tau+\gamma\phi} \cdot  
\sqrt{ \frac{w}{Pd} }.\]</span> Thus, with prior knowledge of the raw work and raw depth of a computation, we can pick to ensure efficiency of parallel programs.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
As \figref{run-time-versus-kappa} illustrates, the parallel run time
decreases as we increase \coff up to some inflection point and then
starts increasing.  
We compute the optimal value for \coff by solving for the root of the  
derivative. We obtain:
\[
\coff^* = \sqrt{\csp+\creg\corc} \cdot
\sqrt{ \frac{\sw}{P\sd} }.
\]
Thus, with prior knowledge of the raw work and raw depth of a
computation, we can pick \coff to ensure efficiency of parallel
programs.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::knowledge
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::knowledge
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Such knowledge, however, is often unavailable. As we now show, we can improve efficiency of parallel programs by selecting a fixed <span class="math inline">\(\kappa\)</span> that guarantees that the task creation overheads can be bounded by any constant fraction of the raw work, without increasing the depth of the computation significantly.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Such knowledge, however, is often unavailable.  As we now
show, we can improve efficiency of parallel programs by selecting a
fixed $\coff$ that guarantees that the task creation overheads can be
bounded by any constant fraction of the raw work, without increasing
the depth of the computation significantly.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::fixed
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::fixed
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p> Consider an oracle with <span class="math inline">\(\phi\)</span> cost and <span class="math inline">\(\mu\)</span> error. For any <span class="math inline">\(\gamma\geq 1\)</span> and for any constant <span class="math inline">\(r\)</span> such that <span class="math inline">\(0 &lt; r &lt; 1\)</span>, there exists a constant <span class="math inline">\(\kappa\)</span> and a constant <span class="math inline">\(c\)</span> such that the evaluation with the oracle semantics of a <span class="math inline">\(\gamma\)</span>-balanced program reduces task creation overheads to a fraction <span class="math inline">\(r\)</span> of the raw work, while in the same time increasing the total depth by no more than a factor <span class="math inline">\(\frac{c}{r}\)</span>. With a greedy scheduler, the total parallel run time on <span class="math inline">\(P\)</span> processors of such a program therefore does not exceed <span class="math inline">\((1 + r) \frac{w}{P} + \frac{c}{r}\, d\)</span>.</p>
<p>onsider a particular <span class="math inline">\(\gamma\)</span>-balanced program with raw work <span class="math inline">\(w\)</span> and raw depth <span class="math inline">\(d\)</span>, and consider its evaluation under the oracle semantics. By Theorem <a href="#thm:real-orc-work" data-reference-type="ref" data-reference="thm:real-orc-work">[thm:real-orc-work]</a> we know that total work does not exceed <span class="math display">\[\left(1+\frac{\mu(\tau+\gamma\phi)}{\kappa} \right)\,{w}.\]</span> To achieve the desired bound on execution time, we take <span class="math inline">\(\kappa= \frac{\mu(\tau+ \gamma\phi)}{r}\)</span>. Plugging this value of <span class="math inline">\(\kappa\)</span> into the formula yields <span class="math inline">\((1+r)\,{w}\)</span> for total work, showing that task creation overheads are reduced to a fraction <span class="math inline">\(r\)</span> of the raw work.</p>
<p>Furthermore, by Theorem <a href="#thm:real-orc-depth" data-reference-type="ref" data-reference="thm:real-orc-depth">[thm:real-orc-depth]</a> we know that the total depth is bounded by <span class="math inline">\((\mathtt{{max}}\left(\tau,\mu\kappa\right)+\phi+1)\,d\)</span>. Plugging in the same value for <span class="math inline">\(\kappa\)</span> yields the following bound on total depth: <span class="math display">\[\mathcal{D}\, \leq  \,\left(\mathtt{{max}}\left(\tau,\frac{\mu^2(\tau+  
    \gamma\phi)}{r}\right)+\phi+ 1\right)\,d.\]</span> Using <span class="math inline">\(\mu\ge 1\)</span> and <span class="math inline">\(r &lt; 1\)</span>, we can derive the inequality <span class="math display">\[\mathcal{D}\, \leq  \,\left(\frac{\mu^2(\tau+ \gamma\phi)}{r}+\frac{\phi+1}{r}\right)\,d.\]</span> Choosing <span class="math inline">\(c = \mu^2(\tau+ \gamma\phi)+\phi+1\)</span> therefore ensures that the total depth does not exceed the desired bound <span class="math inline">\(\frac{c}{r}\,d\)</span>. The run-time bound follows by an application of Brent’s theorem (Theorem <a href="#thm:generalized-brent" data-reference-type="ref" data-reference="thm:generalized-brent">[thm:generalized-brent]</a>).</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\begin{theorem}[Run time with fixed $\coff$]
\label{thm:task-creation-overheads}
Consider an oracle with $\corc$ cost and $\cerr$ error.
For any $\cbal\geq 1$ and for any constant $r$ such that $0 < r < 1$,
there exists a constant $\coff$ and a constant $c$
such that the evaluation with 
the oracle semantics of a $\cbal$-balanced program 
reduces task creation overheads to a fraction $r$ of the
raw work, while in the same time increasing the total depth 
by no more than a factor $\frac{c}{r}$.
With a greedy scheduler, the total parallel
run time on $P$ processors of such a program
therefore does not exceed
$(1 + r) \frac{\sw}{P} + \frac{c}{r}\, \sd$.  
\end{theorem}
\begin{proof}
Consider a particular $\cbal$-balanced program 
with raw work $\sw$ and raw depth $\sd$,
and consider its evaluation under the oracle semantics.  
By \thmref{real-orc-work} we know that total work does not exceed
\[
\left(1+\frac{\cerr(\csp+\cbal\corc)}{\coff} \right)\,{\sw}.
\]
To achieve the desired bound on execution time, we take 
 $\coff = \frac{\cerr(\csp + \cbal\corc)}{r}$.
Plugging this value of $\coff$ into the formula
yields~$(1+r)\,{\sw}$ for total work, showing
that task creation overheads are reduced to a fraction $r$ of
the raw work.

Furthermore, by \thmref{real-orc-depth} we know that the total depth 
is bounded by $(\kwmax{\csp}{\cerr\coff}+\corc+1)\,\sd$. Plugging in 
the same value for $\coff$ yields the following bound on total depth:
\[
\sds \Sc\leq \left(\kwmax{\csp}{\frac{\cerr^2(\csp +
    \cbal\corc)}{r}}+\corc + 1\right)\,\sd.
\]
Using $\cerr \ge 1$ and $r < 1$,
we can derive the inequality
\[
\sds \Sc\leq \left(\frac{\cerr^2(\csp + \cbal\corc)}{r}+\frac{\corc+1}{r}\right)\,\sd.
\]
Choosing $c = \cerr^2(\csp + \cbal\corc)+\corc+1$ 
therefore ensures that the total depth does not exceed the desired bound
$\frac{c}{r}\,\sd$.
The run-time bound follows by an application of 
Brent's theorem (\thmref{generalized-brent}). \qed
\end{proof}
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::final
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::final
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>This final theorem enables us to reduce task creation overheads to any desired constant fraction of the raw work by choosing a <span class="math inline">\(\kappa\)</span> that is independent of the specific inputs. This comes at the cost of increasing the depth, but only by a constant factor of <span class="math inline">\(\frac{c}{r}\)</span>. In the common case, when the work is asymptotically greater than depth, <em>e.g.</em>, <span class="math inline">\(\Theta(n)\)</span> versus <span class="math inline">\(O(\log{n})\)</span>, the resulting run-time guarantees that the increase in depth remain small: specifically, the depth term itself is a fraction of the work term for all but a constant number of small inputs.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
This final theorem enables us to reduce task creation overheads to any
desired constant fraction of the raw work by choosing a $\coff$ that
is independent of the specific inputs.  This comes at the cost
of increasing the depth, but only by a constant factor of
$\frac{c}{r}$.  In the common case, when the work is
asymptotically greater than depth, \textit{e.g.}, $\Theta(n)$ versus
$O(\log{n})$, the resulting run-time guarantees that the increase in
depth remain small: specifically, the depth term itself is a fraction
of the work term for all but a constant number of small inputs.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->


</segment> <!-- subsection -->

<segment name='subsection'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Balanced programs
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Balanced programs
]]>
</field> <!-- title_src -->
<field name='label'>
sec:balanced
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::bounds
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::bounds
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Our bounds with the realistic oracle hold only for what we called <span class="math inline">\(\gamma\)</span>-balanced programs, where the oracle is not called on small tasks. This assumption can be satisfied by calling the oracle “regularly.” It seems likely that this assumption would hold for many programs without requiring any changes to the program code. In this section, we show that recursive, divide-and-conquer programs are <span class="math inline">\(\gamma\)</span>-balanced.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Our bounds with the realistic oracle hold only for what we called
$\cbal$-balanced programs, where the oracle is not called on small
tasks.  This assumption can be satisfied by calling the oracle
``regularly.''  It seems likely that this assumption would hold for
many programs without requiring any changes to the program code.  In
this section, we show that recursive, divide-and-conquer programs are
$\cbal$-balanced.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::introduce
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::introduce
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>To that end, we introduce the notion of <span class="math inline">\(\gamma\)</span>-regularity. Intuitively, a program is <span class="math inline">\(\gamma\)</span>-regular if, between any two calls to the oracle involved in the execution of this program, the amount of work does not reduce by more than a factor <span class="math inline">\(\gamma\)</span>. We will then establish that any <span class="math inline">\(\gamma\)</span>-regular program is a <span class="math inline">\(\gamma\)</span>-balanced program. Before giving the formal definition of <span class="math inline">\(\gamma\)</span>-regularity, we need to formally define what it means for a parallel tuple to be dominated by another parallel tuple.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
To that end, we introduce the notion of $\cbal$-regularity.
Intuitively, a program is $\cbal$-regular if, between any two calls
to the oracle involved in the execution of this program,
the amount of work does not reduce by more than a factor $\cbal$.
We will then establish that any $\cbal$-regular program is 
a $\cbal$-balanced program.
Before giving the formal definition of $\cbal$-regularity,
we need to formally define what it means for a parallel tuple
to be dominated by another parallel tuple.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::arthur
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::arthur
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[

]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\begin{comment}
\uremark{Arthur, I am leaving this to you to complete. Old text follows.}
Fortunately, most programs do not exhibit this pathological behavior.
However, to prove a better bound we need to make further assumptions
about the structure of the program. It turns out that a sufficient
condition for establishing an interesting bound is to ensure
that the oracle is never called on too small tasks.
This property is hard to capture in a hardware-independent way.
However, we can devise a sufficient condition, called {\em regularity},
which is hardware-independent and ensures the desired property. 
Intuitively, a program is $\creg$-regular if the ratio between 
the work involved in a recursive call and the work involved
in the next recursive call does not exceed $\creg$.
Divide-and-conquer algorithm typically satisfy the regularity 
condition. We next formalize the definition of regularity.
\end{comment}
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::domination-of-a-parallel-branch
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='definition'>
<field name='title'>
<![CDATA[
Domination of a parallel branch
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Domination of a parallel branch
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
def:oracle::domination-of-a-parallel-branch
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>A branch <span class="math inline">\(e\)</span> of a parallel tuple is said to be <span><em>dominated</em></span> by the branch <span class="math inline">\(e_i\)</span> of another parallel tuple <span class="math inline">\({\left(|{e_1},{e_2}|\right)}\)</span> if the expression <span class="math inline">\(e\)</span> is involved in the execution of the branch <span class="math inline">\(e_i\)</span>.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
A branch $e$ of a parallel tuple is said to be {\em dominated} by the
branch $e_i$ of another parallel tuple $\kwpt{e_1}{e_2}$ if
the expression $e$ is involved in the execution of the branch $e_i$.
]]>
</field> <!-- body_src -->

</atom> <!-- definition -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::regularity
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::regularity
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>A program is said to be <span class="math inline">\(\gamma\)</span>-regular if, for any parallel branch involving, say, <span class="math inline">\(w\)</span> units of raw work, either <span class="math inline">\(w\)</span> is very large compared with <span class="math inline">\(\kappa/(\mu\gamma)\)</span> or this branch is dominated by another parallel branch that involves less than <span class="math inline">\(\gammaw\)</span> units of work.</p>
<p>The condition “<span class="math inline">\(w\)</span> is very large compared with <span class="math inline">\(\kappa/(\mu\gamma)\)</span>” is used to handle the outermost parallel tuples, which are not dominated by any other tuple.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\begin{definition}[Regularity of a parallel program]
A program is said to be $\creg$-regular if,
for any parallel branch involving, say, $\sw$ units of raw work, 
either $\sw$ is very large compared with $\coff/(\cerr\creg)$
or this branch is dominated by another parallel branch
that involves less than $\creg\sw$ units of work.
\end{definition}
The condition ``$\sw$ is very large compared with $\coff/(\cerr\creg)$''
is used to handle the outermost parallel tuples, which are not
dominated by any other tuple.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::program
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::program
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Note that the regularity of a program is always greater than <span class="math inline">\(2\)</span>. Indeed, if one of the branch of a parallel tuple is more than half of the size of the entire tuple, then the other branch must be smaller than half of that size. On the one hand, algorithms that divide their work in equal parts are <span class="math inline">\(\gamma\)</span>-regularity with <span class="math inline">\(\gamma\)</span> very close to <span class="math inline">\(2\)</span>. On the other hand, ill-balanced programs can have a very high degree of regularity. Observe that every program is <span class="math inline">\(\infty\)</span>-regular.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Note that the regularity of a program is always greater than $2$.
Indeed, if one of the branch of a parallel tuple is more
than half of the size of the entire tuple, then the other 
branch must be smaller than half of that size.
On the one hand, algorithms that divide their work in 
equal parts are $\creg$-regularity with $\creg$ very close to $2$.
On the other hand, ill-balanced programs can have a very
high degree of regularity. Observe that every program is $\infty$-regular.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::consider
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::consider
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>For example, consider a program that traverses a complete binary tree in linear time. A call on a tree of size <span class="math inline">\(n\)</span> has raw work nc, for some constant <span class="math inline">\(c\)</span>. If the tree is not a leaf, its size <span class="math inline">\(n\)</span> has to be at least <span class="math inline">\(3\)</span>. The next recursive call involves raw work c, The ratio between those two values is equal <span class="math inline">\(n / \left\lfloor \frac{n-1}{2} \right\rfloor\)</span>. This value is always less than <span class="math inline">\(3\)</span> when <span class="math inline">\(n\geq 3\)</span>. So, the traversal of a complete binary tree is a 3-regular algorithm.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
For example, consider a program that traverses a complete binary tree
in linear time. A call on a tree of size $n$ 
has raw work \W{nc}, for some constant $c$.
If the tree is not a leaf, its size $n$ has to be at least $3$.
The next recursive call involves raw work \W{\floorfrac{n-1}{2} c},
The ratio between those two values is equal $n / \floorfrac{n-1}{2}$.
This value is always less than $3$ when $n\geq 3$.
So, the traversal of a complete binary tree is a 3-regular algorithm.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::explains
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::explains
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>The following lemma explains how the regularity assumption can be exploited to ensure that the oracle is never invoked on tasks of size less than <span class="math inline">\(\kappa/(\mu\gamma)\)</span>. This suggests that, for the purpose of amortizing well the costs of the oracle, a smaller regularity is better.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
The following lemma explains how the regularity assumption
can be exploited to ensure that the oracle is never
invoked on tasks of size less than $\coff/(\cerr\creg)$.
This suggests that, for the purpose of amortizing well the 
costs of the oracle, a smaller regularity is better.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::ee
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::ee
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p> <br />
If a program is <span class="math inline">\(\gamma\)</span>-regular then it is <span class="math inline">\(\gamma\)</span>-balanced.</p>
<p>e have to show that, during the execution of a <span class="math inline">\(\gamma\)</span>-regular program according to oracle semantics, the oracle is never invoked on subexpressions involving less than <span class="math inline">\(\kappa/(\mu\gamma)\)</span> raw work. Consider a particular subexpression <span class="math inline">\(e\)</span> involving <span class="math inline">\(w\)</span> units of raw work, and assume that the oracle is invoked on this subexpression. Because the oracle is being invoked, <span class="math inline">\(e\)</span> must correspond to the branch of a parallel tuple. By the regularity assumption, either <span class="math inline">\(w\)</span> is very large compared with <span class="math inline">\(\kappa/(\mu\gamma)\)</span>, in which case the conclusion holds immediately, or the branch <span class="math inline">\(e\)</span> is dominated by a branch <span class="math inline">\(e_i\)</span> that involves that involves <span class="math inline">\(w&#39;\)</span> units of work, with <span class="math inline">\(w&#39; \leq \gammaw\)</span>. For the latter case, we need to establish <span class="math inline">\(w\geq \kappa/(\mu\gamma)\)</span>. To that end, it suffices to prove that <span class="math inline">\(w&#39; \geq \kappa/\mu\)</span>, which amounts to showing that the amount of raw work associated with the dominating branch <span class="math inline">\(e_i\)</span> contains at least <span class="math inline">\(\kappa/\mu\)</span> raw work.</p>
<p>We conclude the proof by establishing the inequality <span class="math inline">\(w&#39; \geq \kappa/\mu\)</span>. Because the oracle is being invoked on the subexpression <span class="math inline">\(e\)</span>, it means that <span class="math inline">\(e\)</span> is being evaluated in the mode <span class="math inline">\(\textsf{orc}\)</span>. Therefore, the call to the oracle on the dominating branch <span class="math inline">\(e_i\)</span> must have predicted <span class="math inline">\(e_i\)</span> to contain more than <span class="math inline">\(\kappa\)</span> raw work. (Otherwise <span class="math inline">\(e_i\)</span> and its subexpression <span class="math inline">\(e\)</span> would have both been executed in the sequential mode.) Given that the oracle makes error by no more than a factor <span class="math inline">\(\mu\)</span>, if <span class="math inline">\(e_i\)</span> is predicted to contain more than <span class="math inline">\(\kappa\)</span> units of raw work, then <span class="math inline">\(e_i\)</span> must contain at least <span class="math inline">\(\kappa/\mu\)</span> units of raw work. So, <span class="math inline">\(w&#39; \geq \kappa/\mu\)</span>.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\begin{lemma}[From regularity to balanced]
\label{lem:regularity-elim}~\\
If a program is $\cbal$-regular then it is $\cbal$-balanced.
\end{lemma}
\begin{proof}
We have to show that, during the execution of a $\creg$-regular 
program according to oracle semantics, the oracle is
never invoked on subexpressions involving less than $\coff/(\cerr\creg)$
raw work.
Consider a particular subexpression $e$ involving $\sw$ units of raw work,
and assume that the oracle is invoked on this subexpression.
Because the oracle is being invoked, $e$ 
must correspond to the branch of a parallel tuple.
By the regularity assumption, either $\sw$ is very
large compared with $\coff/(\cerr\creg)$, in which case
the conclusion holds immediately, or the branch $e$
is dominated by a branch $e_i$ that involves that 
involves $\sw'$ units of work, with $\sw' \leq \creg\sw$.
For the latter case, we need to establish $\sw \geq \coff/(\cerr\creg)$.
To that end, it suffices to prove that $\sw' \geq \coff/\cerr$,
which amounts to showing that the amount of raw
work associated with the dominating branch $e_i$
contains at least $\coff/\cerr$ raw work. 

We conclude the proof by establishing the inequality $\sw' \geq \coff/\cerr$.
Because the oracle is being invoked on the subexpression $e$,
it means that $e$ is being evaluated in the mode $\sorc$. Therefore,
the call to the oracle on the dominating branch $e_i$ must have 
predicted $e_i$ to contain more than $\coff$ raw work.
(Otherwise $e_i$ and its subexpression $e$ would have both
been executed in the sequential mode.)
Given that the oracle makes error by no more than a factor $\cerr$,
if $e_i$ is predicted to contain more than $\coff$ units of raw work, 
then $e_i$ must contain at least $\coff/\cerr$ units of raw work. 
So, $\sw' \geq \coff/\cerr$. \qed
\end{proof}
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->


</segment> <!-- subsection -->

</segment> <!-- section -->

<segment name='section'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Oracle Scheduling
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Oracle Scheduling
]]>
</field> <!-- title_src -->
<field name='label'>
sec:scheduling
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::_6_
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::_5_
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p> </p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\label{sec:scheduling}
\label{sec:schedule}
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::original
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::original
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[

]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\begin{comment}
The original theorem of Brent and our generalization assume a greedy
scheduler that can find available work (parallel tasks to execute)
immediately with no overhead.  This is unrealistic of course but many
schedulers can achieve similar bounds asymptotically for a reasonably
broad class of computations.  For example, a work-stealing scheduler
can execute a fully-strict computations with $\sws$ work and $\sds$
depth on $P$ processors in $O(\sws/P + \sds)$ expected
time~\cite{BlumofeWorkStealing}.  The class of fully-strict
computations (\textit{i.e.}, series-parallel computations) include all
fork-join programs, nested-parallel computations, and specifically
computations with parallel tuples, our focus here.
\uremark{Check the claim about fully strictness. }


Since our oracle semantics makes no assumptions about the specifics of
a scheduler (the semantics simply determines when to create parallel
tasks), the created parallel tasks can be scheduled by using any
suitable scheduler, \textit{e.g.}, a work-stealing scheduler.

In our experiments (\secreftwo{imp}{exp}), we use an implementation of
this estimator combined with a work-stealing scheduler
\end{comment}
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::describe
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::describe
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>As we describe in this section, we can realize the oracle semantics by using a <span><em><span class="math inline">\((\phi,\mu)\)</span>-estimator</em></span> that requires <span class="math inline">\(\phi\)</span> time to estimate actual run-time of parallel tasks within a factor of no more than <span class="math inline">\(\mu\)</span>. We refer to the combination of an estimator with a parallel scheduler as an <span><em><span class="math inline">\((\phi,\mu)\)</span>-oracle-scheduler</em></span>.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
As we
describe in this section, we can realize the oracle semantics by using
a {\em $(\corc,\cerr)$-estimator} that requires $\corc$ time to
estimate actual run-time of parallel tasks within a factor of no more
than $\cerr$.  We refer to the combination of an estimator with a
parallel scheduler as an {\em $(\corc,\cerr)$-oracle-scheduler}.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::type
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='figure'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
fig:estimator-sig
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>type<br />
type<br />
val : unit -&gt;<br />
val : <span class="math inline">\(\times\)</span> <span class="math inline">\(\times\)</span> float -&gt; unit<br />
val : <span class="math inline">\(\times\)</span> -&gt; float</p>
<p> <br />
<span>The signature of the estimator data structure </span> </p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\small
\begin{codeListing}
type \kwtypeofcost
\\
type \kwestimatortype
\\
val \kwestimatorinit: unit -> \kwestimatortype
\\
val \kwestimatordata: \kwestimatortype \!\!\!$\times$ \kwtypeofcost \!\!\!$\times$ float -> unit
\\
val \kwestimatorapp: \kwestimatortype \!\!\!$\times$ \kwtypeofcost -> float
\end{codeListing}
\caption{The signature of the estimator data structure
}
\label{fig:estimator-sig}
]]>
</field> <!-- body_src -->

</atom> <!-- figure -->

</segment> <!-- cluster -->


<segment name='paragraph'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Run-time estimators.
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Run-time estimators.
]]>
</field> <!-- title_src -->
<field name='label'>
sec:oracle::run-time-estimators
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::semantics
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::semantics
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>To realize the oracle semantics, we require the user to provide a <span> <em>cost function</em></span> for each function in the program and rely on an <span> <em>estimator</em></span> for estimating actual work using the user-provided cost information. When applied to an argument <code>v</code>, a cost function of <code>f</code> returns the abstract cost of the application of <code>f</code> to <code>v</code>. The cost is passed to the estimator, which uses the cost to compute an estimate of the actual execution time, that is, the raw work, of the application. Figure <a href="#fig:estimator-sig" data-reference-type="ref" data-reference="fig:estimator-sig">[fig:estimator-sig]</a> shows a signature for the estimator. To perform accurate estimates, the estimator utilizes profiling data obtained from actual execution times. The sampling operation <code>(t, c, e)</code> adds a cost <code>c</code> and an execution time <code>e</code> to the set of samples in an estimator <code>t</code>. An estimate of the actual execution time is obtained by calling <code>predict</code>. Given an estimator <code>t</code> and cost <code>c</code>, the call <code>(t, c)</code> returns a predicted execution time.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
To realize the oracle semantics, we require the user to provide a {\em
  cost function} for each function in the program and rely on an {\em
  estimator} for estimating actual work using the user-provided cost
information.  When applied to an argument \ttt{v}, a cost function of
\ttt{f} returns the abstract cost of the application of \ttt{f} to
\ttt{v}. The cost is passed to the estimator, which uses the cost to
compute an estimate of the actual execution time, that is, the raw
work, of the application.  \figref{estimator-sig} shows a signature
for the estimator. To perform accurate estimates, the estimator
utilizes profiling data obtained from actual execution times. The
sampling operation 
\ttt{\kwestimatordata(t, c, e)}
 adds a cost \ttt{c} and an execution time \ttt{e} to the set of
 samples in an estimator \ttt{t}. An estimate of the actual execution
 time is obtained by calling \ttt{predict}.  Given an estimator
 \ttt{t} and cost \ttt{c}, the call
\texttt{\kwestimatorapp(t, c)}
returns a predicted execution time.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->


</segment> <!-- paragraph -->

<segment name='paragraph'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Compilation.
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Compilation.
]]>
</field> <!-- title_src -->
<field name='label'>
sec:oracle::compilation
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::support
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::support
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>To support oracle scheduling with estimators, we need compilation support to associate an estimator with each function defined in the program code, to derive a sequential and an oracle version for each function, and to evaluate tuples sequentially or in parallel depending on the approximations performed by the estimator.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
To support oracle scheduling with estimators, we need compilation
support to associate an estimator with each function defined in the
program code, to derive a sequential and an oracle version for each
function, and to evaluate tuples sequentially or in parallel depending
on the approximations performed by the estimator.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::simplicity
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::simplicity
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>For simplicity, we assume that constituents of parallel tuples are function applications, <em>i.e.</em>, they are of the form <span class="math inline">\({\left(|{f_1\,v_1},{f_2\,v_2}|\right)}\)</span>. Note that this assumption does not cause loss of expressiveness, because a term <span class="math inline">\(e\)</span> can always be replaced by a trivial application of a “thunk”, a function that ignores its argument (typically of type “unit”) and evaluates <span class="math inline">\(e\)</span> to a dummy argument. Throughout, we write <span class="math inline">\(``{\mathtt{{fun}}~{f}.{x}.{e_b}~[{e_c}]}&#39;&#39;\)</span> to denote a function <span class="math inline">\(``{\mathtt{{fun}}~{f}.{x}.{e_b}}&#39;&#39;\)</span> for which the cost function for the body <span class="math inline">\(e_b\)</span> is described by the expression <span class="math inline">\(e_c\)</span>. This expression <span class="math inline">\(e_c\)</span>, which may refer to the argument <span class="math inline">\(x\)</span>, should be an expression whose evaluation always terminates and produces an cost of type .</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
For simplicity, we assume that constituents of parallel tuples are
function applications, \textit{i.e.}, they are of the form
$\kwpt{f_1\,v_1}{f_2\,v_2}$.  Note that this assumption does not cause
loss of expressiveness, because a term $e$ can always be replaced by a
trivial application of a ``thunk'', a function that ignores its
argument (typically of type ``unit'') and evaluates $e$ to a dummy
argument. Throughout, we write $\Q{\kwfuncost{f}{x}{e_b}{e_c}}$ to
denote a function $\Q{\kwfun{f}{x}{e_b}}$ for which the cost function
for the body $e_b$ is described by the expression $e_c$.  This
expression $e_c$, which may refer to the argument $x$, should be an
expression whose evaluation always terminates and produces an cost of
type \kwtypeofcost.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::associate
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::associate
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>To associate an estimator with each function, in a simple pass over the source code, we allocate and initialize an estimator for each syntactic function definition. For example, if the source code contains a function of the form <span class="math inline">\(``{\mathtt{{fun}}~{f}.{x}.{e_b}~[{e_c}]}&#39;&#39;\)</span>, then our compiler allocates an estimator specific to that function definition. Specifically, if the variable <span class="math inline">\(r\)</span> refers to the allocated estimator, then the translated function, written <span class="math inline">\(``{\mathtt{{fun}}~{f}.{x}.{e_b}~[{e_c}|{r}]}&#39;&#39;\)</span>, is annotated with <span class="math inline">\(r\)</span>.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
To associate an
estimator with each function, in a simple pass over the source code,
we allocate and initialize an estimator for each
syntactic function definition.  For
example, if the source code contains a function of the form
$\Q{\kwfuncost{f}{x}{e_b}{e_c}}$, then our compiler allocates an
estimator specific to that function definition.  Specifically, if the
variable~$r$ refers to the allocated estimator, then the translated
function, written $\Q{\kwfuncostced{f}{x}{e_b}{e_c}{r}}$, 
is annotated with $r$.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::makebranch
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='figure'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
fig:compilation
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p><span class="math display">\[\begin{array}{l@{\quad\equiv\quad}l}  
\llbracket x \rrbracket^{} &amp; x   
\vspace{2pt}\\
\llbracket \left({v_1},{v_2}\right) \rrbracket^{} &amp; \left({\llbracket v_1 \rrbracket^{}},{\llbracket v_2 \rrbracket^{}}\right)   
\vspace{2pt}\\
\llbracket \mathtt{{inl}}~{v} \rrbracket^{} &amp; \mathtt{{inl}}~{\llbracket v \rrbracket^{}}   
\vspace{2pt}\\
\llbracket \mathtt{{inr}}~{v} \rrbracket^{} &amp; \mathtt{{inr}}~{\llbracket v \rrbracket^{}}   
\vspace{2pt}\\
\llbracket \mathtt{{fun}}~{f}.{x}.{e_b}~[{e_c}|{r}] \rrbracket^{} &amp;   
(r ,(\mathtt{{fun}}~{\_}.{x}.{\llbracket e_c \rrbracket^{\textsf{seq}}}) ,(\mathtt{{fun}}~{f}.{x}.{\llbracket e_b \rrbracket^{\textsf{seq}}}) ,(\mathtt{{fun}}~{f}.{x}.{\llbracket e_b \rrbracket^{\textsf{orc}}}))   
\vspace{2pt}\\
\llbracket v \rrbracket^{\alpha} &amp; \llbracket v \rrbracket^{}   
\vspace{2pt}\\
\llbracket {v_1}~{v_2} \rrbracket^{\textsf{seq}} &amp; {\mathtt{{proj}}^{3}\,\llbracket v_1 \rrbracket^{}}~{\llbracket v_2 \rrbracket^{}}   
\vspace{2pt}\\
\llbracket {v_1}~{v_2} \rrbracket^{\textsf{orc}} &amp; {\mathtt{{proj}}^{4}\,\llbracket v_1 \rrbracket^{}}~{\llbracket v_2 \rrbracket^{}}   
\vspace{2pt}\\
\llbracket \left({e_1},{e_2}\right) \rrbracket^{\alpha} &amp; \left({\llbracket e_1 \rrbracket^{\alpha}},{\llbracket e_2 \rrbracket^{\alpha}}\right)   
\vspace{2pt}\\
\llbracket \mathtt{{let}}~{x}~=~{e_1}~\mathtt{{in}}~{e_2} \rrbracket^{\alpha} &amp; \mathtt{{let}}~{x}~=~{\llbracket e_1 \rrbracket^{\alpha}}~\mathtt{{in}}~{\llbracket e_2 \rrbracket^{\alpha}}   
\vspace{2pt}\\
\llbracket \mathtt{{fst}}~{v} \rrbracket^{\alpha} &amp; \mathtt{{fst}}~{\llbracket v \rrbracket^{}}   
\vspace{2pt}\\
\llbracket \mathtt{{snd}}~{v} \rrbracket^{\alpha} &amp; \mathtt{{snd}}~{\llbracket v \rrbracket^{}}   
\vspace{2pt}\\
\llbracket \mathtt{{case}}~{v}~\mathtt{{of}}~\{\mathtt{{inl}}~{x}.{e_1}, \mathtt{{inr}}~{x}.{e_2}\} \rrbracket^{\alpha} &amp; \mathtt{{case}}~{\llbracket v \rrbracket^{}}~\mathtt{{of}}~\{\mathtt{{inl}}~{x}.{\llbracket e_1 \rrbracket^{\alpha}}, \mathtt{{inr}}~{x}.{\llbracket e_2 \rrbracket^{\alpha}}\}   
\vspace{2pt}\\
\llbracket  {\left(|{ {f_1}~{v_1} },{ {f_2}~{v_2} }|\right)}  \rrbracket^{\textsf{seq}} &amp; \left({{\mathtt{{proj}}^{3}\,\llbracket f_1 \rrbracket^{}}~{\llbracket v_1 \rrbracket^{}}},{{\mathtt{{proj}}^{3}\,\llbracket f_2 \rrbracket^{}}~{\llbracket v_2 \rrbracket^{}}}\right)   
\vspace{2pt}\\
\llbracket  {\left(|{ {f_1}~{v_1} },{ {f_2}~{v_2} }|\right)}  \rrbracket^{\textsf{orc}} &amp;   
\left\{ \begin{array}{@{}l}  
\mathtt{{let}}~{(b_1,k_1)}~=~{\texttt{MakeBranch}(\llbracket f_1 \rrbracket^{},\llbracket v_1 \rrbracket^{})}~\mathtt{{in}} \\  
\mathtt{{let}}~{(b_2,k_2)}~=~{\texttt{MakeBranch}(\llbracket f_2 \rrbracket^{},\llbracket v_2 \rrbracket^{})}~\mathtt{{in}} \\  
\mathtt{{if}}~{(b_1~\mathtt{{\&amp;\&amp;}}~b_2)}~\mathtt{{then}}~{ {\left(|{{k_1}~{\mathtt{{()}}}},{{k_2}~{\mathtt{{()}}}}|\right)} }~\mathtt{{else}}~{ \left({{k_1}~{\mathtt{{()}}}},{{k_2}~{\mathtt{{()}}}}\right)  }  
\end{array} \right.  
\end{array}\]</span></p>
<p> <br />
<span>Translation for oracle scheduling.</span> </p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\begin{minipage}{\textwidth}
\newcommand{\kwApphere}[2]{\kwapp{#1}{#2}}
\newcommand{\compilskip}{\vspace{2pt}\\}
\[
\begin{array}{l@{\quad\equiv\quad}l}
\Fcompval{x} & x 
\compilskip
\Fcompval{\kwt{v_1}{v_2}} & \kwt{\Fcompval{v_1}}{\Fcompval{v_2}} 
\compilskip
\Fcompval{\kwinl{v}} & \kwinl{\Fcompval{v}} 
\compilskip
\Fcompval{\kwinr{v}} & \kwinr{\Fcompval{v}} 
\compilskip
\Fcompval{\kwfuncostced{f}{x}{e_b}{e_c}{r}} & 
\Lquadruple{r}
{\kwFun{\_}{x}{\Fcomp{\sseq}{e_c}}}
{\kwFun{f}{x}{\Fcomp{\sseq}{e_b}}}
{\kwFun{f}{x}{\Fcomp{\sorc}{e_b}}} 
\compilskip
\Fcomp{\Ga}{v} & \Fcompval{v} 
\compilskip
\Fcomp{\sseq}{\kwApphere{v_1}{v_2}} & \kwApphere{\kprojof{3}{\Fcompval{v_1}}}{\Fcompval{v_2}} 
\compilskip
\Fcomp{\sorc}{\kwApphere{v_1}{v_2}} & \kwApphere{\kprojof{4}{\Fcompval{v_1}}}{\Fcompval{v_2}} 
\compilskip
\Fcompga{\kwt{e_1}{e_2}} & \kwt{\Fcompga{e_1}}{\Fcompga{e_2}} 
\compilskip
\Fcompga{\kwletin{x}{e_1}{e_2}} & \kwletin{x}{\Fcompga{e_1}}{\Fcompga{e_2}} 
\compilskip
\Fcompga{\kwfst{v}} & \kwfst{\Fcompval{v}} 
\compilskip
\Fcompga{\kwsnd{v}} & \kwsnd{\Fcompval{v}} 
\compilskip
\Fcompga{\kwcase{v}{x}{e_1}{x}{e_2}} & \kwcase{\Fcompval{v}}{x}{\Fcompga{e_1}}{x}{\Fcompga{e_2}} 
\compilskip
\Fcomp{\sseq}{ \kwpt{ \kwApphere{f_1}{v_1} }{ \kwApphere{f_2}{v_2} } } & \kwt{\kwApphere{\kprojof{3}{\Fcompval{f_1}}}{\Fcompval{v_1}}}{\kwApphere{\kprojof{3}{\Fcompval{f_2}}}{\Fcompval{v_2}}} 
\compilskip
\Fcomp{\sorc}{ \kwpt{ \kwApphere{f_1}{v_1} }{ \kwApphere{f_2}{v_2} } } & 
\left\{ \begin{array}{@{}l}
\kwletins{(b_1,k_1)}{\ttt{MakeBranch}(\Fcompval{f_1},\Fcompval{v_1})} \\
\kwletins{(b_2,k_2)}{\ttt{MakeBranch}(\Fcompval{f_2},\Fcompval{v_2})} \\
\kwif{(b_1~\kw{\&\&}~b_2)}{ \kwpt{\kwApphere{k_1}{\kwunit}}{\kwApphere{k_2}{\kwunit}} }{ \kwt{\kwApphere{k_1}{\kwunit}}{\kwApphere{k_2}{\kwunit}}  }
\end{array} \right.
\end{array}
\]

\caption{Translation for oracle scheduling.}
\label{fig:compilation}
\end{minipage}
]]>
</field> <!-- body_src -->

</atom> <!-- figure -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::pass
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::pass
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>The second pass of our compilation scheme uses the allocated estimators to approximate the actual raw work of function applications and relies on an <code>MakeBranch</code> function to determine whether an application should be run in the oracle or in the sequential mode. Figure <a href="#fig:compilation" data-reference-type="ref" data-reference="fig:compilation">[fig:compilation]</a> defines more precisely the second pass. We write <span class="math inline">\(\llbracket v \rrbracket^{}\)</span> for the translation of a value <span class="math inline">\(v\)</span>, and we write <span class="math inline">\(\llbracket e \rrbracket^{\alpha}\)</span> for the translation of the expression <span class="math inline">\(e\)</span> according to the semantics <span class="math inline">\(\alpha\)</span>, which can be either <span class="math inline">\(\textsf{seq}\)</span> or <span class="math inline">\(\textsf{orc}\)</span>. When specifying the translation, we use triples, quadruples, projections, sequence, if-then-else statements, and unit value; these constructions can all be easily defined in our core programming language.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
The second pass of our compilation scheme uses the allocated
estimators to approximate the actual raw work of function applications
and relies on an \ttt{MakeBranch} function to determine whether an
application should be run in the oracle or in the sequential mode.
\figref{compilation} defines more precisely the second pass.  We write
$\Fcompval{v}$  for
 the translation of a value $v$, and we write
$\Fcomp{\Ga}{e}$ for the translation of the expression $e$ according to
the semantics $\Ga$, which can be either $\sseq$ or $\sorc$.  When
specifying the translation, we use triples, quadruples, projections,
sequence, if-then-else statements, and unit value; these constructions
can all be easily defined in our core programming language.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::translation
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::translation
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Translation of values other than functions does not depend on the mode and is relatively straightforward. We translate functions, which are of the form <span class="math inline">\(``{\mathtt{{fun}}~{f}.{x}.{e_b}~[{e_c}|{r}]}&#39;&#39;\)</span>, into a quadruple consisting of the estimator <code>r</code>, a sequential cost function, the sequential version of the function, and the oracle versions of the function. Translation of a function application depends on the mode. In the sequential mode, the sequential version of the function is selected (by projecting the third component of the function) and used in the application. Similarly, in the oracle mode, the oracle version of the function is selected and used in the application. To translate a tuple, we recursively translate the subexpression, while preserving the mode. Similarly, translation of the <code>let</code>, projections, and <code>case</code> constructs are entirely structural.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Translation of values other than functions does not depend on the mode
and is relatively straightforward.  We translate functions, which are
of the form $\Q{\kwfuncostced{f}{x}{e_b}{e_c}{r}}$, into a quadruple
consisting of the estimator \ttt{r}, a sequential cost function, the
sequential version of the function, and the oracle versions of the
function.  Translation of a function application depends on the
mode. In the sequential mode, the sequential version of the function
is selected (by projecting the third component of the function) and
used in the application.  Similarly, in the oracle mode, the oracle
version of the function is selected and used in the application.  To
translate a tuple, we recursively translate the subexpression, while
preserving the mode.  Similarly, translation of the \ttt{let},
projections, and \ttt{case} constructs are entirely structural.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::mode
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::mode
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>In the sequential mode, a parallel tuple is turned into a simple tuple. In the oracle mode, the translation applies the oracle-based scheduling policy with the aid of the meta-function <code>MakeBranch</code>. This meta-function, shown in Figure <a href="#fig:metafunctions" data-reference-type="ref" data-reference="fig:metafunctions">[fig:metafunctions]</a>, describes the template of the code generated for preparing the execution of a parallel tuple. <code>MakeBranch</code> expects a (translated) function <span class="math inline">\(f\)</span> and its (translated) argument <span class="math inline">\(v\)</span>, and it returns a boolean <span class="math inline">\(b\)</span> indicating whether the application of <span class="math inline">\(f\)</span> to <span class="math inline">\(v\)</span> is expected to take more or less time than the cutoff <span class="math inline">\(\kappa\)</span>, and a thunk <span class="math inline">\(t\)</span> to execute this application. On the one hand, if the application is predicted to take more time than the cutoff (in which case <span class="math inline">\(b\)</span> is true), then the thunk <span class="math inline">\(t\)</span> corresponds to the application of the oracle-semantics version of the function <span class="math inline">\(f\)</span>. On the other hand, if the application is predicted to take less time than the cutoff (in which case <span class="math inline">\(b\)</span> is false), then the thunk <span class="math inline">\(t\)</span> corresponds to the application of the sequential-semantics version of the function <span class="math inline">\(f\)</span>. Moreover, in the latter case, the time taken to execute the application sequentially is measured. This time measure is reported to the estimator by the auxiliary meta-function <code>MeasuredRun</code> (Figure <a href="#fig:metafunctions" data-reference-type="ref" data-reference="fig:metafunctions">[fig:metafunctions]</a>), so as to enable its approximations.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
In the sequential mode, a parallel tuple is turned into a simple tuple.
In the oracle mode, the translation applies the oracle-based scheduling
policy with the aid of the meta-function \ttt{MakeBranch}.  This
meta-function, shown in \figref{metafunctions}, describes
the template of the code generated for preparing the execution of a
parallel tuple.  \ttt{MakeBranch} expects a (translated) function $f$ and
its (translated) argument $v$, and it returns a boolean $b$ indicating
whether the application of $f$ to $v$ is expected to take more or less
time than the cutoff $\coff$, and a thunk $t$ to execute this application.  On
the one hand, if the application is predicted to take more time than
the cutoff (in which case $b$ is true), then the thunk $t$ corresponds
to the application of the 
oracle-semantics version of the function $f$.  On the other
hand, if the application is predicted to take less time than the
cutoff (in which case $b$ is false), then the thunk $t$ corresponds to
the application of the 
sequential-semantics version of the function $f$. Moreover, in the
latter case, the time taken to execute the application sequentially is
measured. This time measure is reported to the estimator by the auxiliary meta-function
\ttt{MeasuredRun} (\figref{metafunctions}), so as to enable
 its approximations.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::introduces
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::introduces
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Observe that the translation introduces many quadruples and applications of projection functions. However, in practice, the quadruples typically get inlined so most of the projections can be computed at compile time. Observe also that the compilation scheme involves some code duplication, because every function is translated once for the sequential mode and once for the oracle mode. In theory, the code could grow exponentially when the code involves functions defined inside the body of other functions. In practice, the code the growth is limited because functions are rarely deeply nested. If code duplication was a problem, then we can use flattening to eliminate deep nesting of local functions, or pass the mode <span class="math inline">\(\alpha\)</span> as an extra argument to functions.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Observe that the translation introduces many quadruples and
applications of projection functions. However, in practice, the
quadruples typically get inlined so most of the projections can be
computed at compile time.  Observe also that the compilation scheme
involves some code duplication, because every function is translated
once for the sequential mode and once for the oracle mode. In theory,
the code could grow exponentially when the code involves functions
defined inside the body of other functions. In practice, the code the
growth is limited because functions are rarely deeply nested.  If code
duplication was a problem, then we can use flattening to eliminate
deep nesting of local functions, or pass the mode $\Ga$ as an extra
argument to functions.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::measuredrun
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='figure'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
fig:metafunctions
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p><span class="math display">\[\begin{array}{l}   
\texttt{MakeBranch}\,(f,v) \, \equiv  \,\\  
\quad \begin{array}{l}   
\mathtt{{let}}~{r}~=~{\mathtt{{proj}}^{1}\,f}~\mathtt{{in}} \\  
\mathtt{{let}}~{m}~=~{{\mathtt{{proj}}^{2}\,f}~{v}}~\mathtt{{in}} \\  
\mathtt{{let}}~{b}~=~{\mathtt{{predict}}(r,m) &gt; \kappa}~\mathtt{{in}} \\  
\mathtt{{let}}~{\mathtt{{fun}}~k_{\textsf{seq}}\,\mathtt{{()}}}~=~{{\mathtt{{proj}}^{3}\,f}~{v} }~\mathtt{{in}} \\  
\mathtt{{let}}~{\mathtt{{fun}}~ k_{\textsf{seq}}&#39;\,\mathtt{{()}}}~=~{\texttt{MeasuredRun}(r,m,k_{\textsf{seq}})}~\mathtt{{in}} \\  
\mathtt{{let}}~{\mathtt{{fun}}~ k_{\textsf{orc}}\,\mathtt{{()}}}~=~{{\mathtt{{proj}}^{4}\,f}~{v} }~\mathtt{{in}} \\  
\mathtt{{let}}~{k}~=~{\mathtt{{if}}~{b}~\mathtt{{then}}~{k_{\textsf{orc}}}~\mathtt{{else}}~{k_{\textsf{seq}}&#39;}}~\mathtt{{in}} \\  
\left({b},{k}\right)  
 \end{array}   
\vspace{4pt}\\  
  
\texttt{MeasuredRun}\,(r,m,k) \, \equiv  \,\\  
\quad \begin{array}{l}   
\mathtt{{let}}~{t}~=~{{\texttt{get\_time}}~{\mathtt{{()}}}}~\mathtt{{in}} \\  
\mathtt{{let}}~{v}~=~{{k}~{\mathtt{{()}}}}~\mathtt{{in}} \\  
\mathtt{{let}}~{t&#39;}~=~{{\texttt{get\_time}}~{\mathtt{{()}}}}~\mathtt{{in}} \\  
{\mathtt{{report}}}~{(r,m,(t&#39;-t))}; \\  
v  
 \end{array}   
 \end{array}\]</span></p>
<p> <br />
<span>Auxiliary meta-functions used for compilation.</span> </p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
$$\begin{lines}
\ttt{MakeBranch}\,(f,v) \Sc\equiv \\
\quad\begin{lines}
\kwletins{r}{\kprojof{1}{f}} \\
\kwletins{m}{\kwapp{\kprojof{2}{f}}{v}} \\
\kwletins{b}{\kwestimatorapp(r,m) > \coff} \\
\kwletins{\kw{fun}~k_{\sseq}\,\kwunit}{\kwapp{\kprojof{3}{f}}{v} } \\
\kwletins{\kw{fun}~ k_{\sseq}'\,\kwunit}{\ttt{MeasuredRun}(r,m,k_{\sseq})} \\
\kwletins{\kw{fun}~ k_{\sorc}\,\kwunit}{\kwapp{\kprojof{4}{f}}{v} } \\
\kwletins{k}{\kwif{b}{k_{\sorc}}{k_{\sseq}'}} \\
\kwt{b}{k}
\end{lines}
\vspace{4pt}\\

\ttt{MeasuredRun}\,(r,m,k) \Sc\equiv \\
\quad\begin{lines}
\kwletins{t}{\kwappunit{\ttt{get\_time}}} \\
\kwletins{v}{\kwappunit{k}} \\
\kwletins{t'}{\kwappunit{\ttt{get\_time}}} \\
\kwapp{\kwestimatordata}{(r,m,(t'-t))}; \\
v
\end{lines}
\end{lines}$$

\vspace*{-4mm}

\caption{Auxiliary meta-functions used for compilation.}
\label{fig:metafunctions}
]]>
</field> <!-- body_src -->

</atom> <!-- figure -->

</segment> <!-- cluster -->


</segment> <!-- paragraph -->

<segment name='paragraph'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Cost as complexity functions.
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Cost as complexity functions.
]]>
</field> <!-- title_src -->
<field name='label'>
sec:oracle::cost-as-complexity-functions
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::techniques
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::techniques
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>The techniques described in this section require the programmer to annotate each function defined in the program with a cost function that, when applied to the argument, returns an abstract cost value. This abstract cost value is then used by an estimator, which is also left abstract, to approximate the actual raw work of a task. For our bounds to apply, complexity expressions should require constant time to evaluate.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
The techniques described in this section require the programmer to
annotate each function defined in the program with a cost function
that, when applied to the argument, returns an abstract cost value.
This abstract cost value is then used by an estimator, which is also
left abstract, to approximate the actual raw work of a task. 
For our bounds to apply, complexity expressions
should require constant time to evaluate.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::predicting
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::predicting
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Predicting the raw work is only needed for sequential tasks, so the estimator actually needs to return an approximation of the actual run time of a sequential task. A crucial property of the abstract cost is that it should be abstract enough that the programmer can write the cost functions without necessarily knowing the details of the hardware that the programs will be executed on. Yet, abstract costs should provide sufficient information to estimate the actual run times.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Predicting the raw work is only needed for sequential tasks,
so the estimator actually needs to return an approximation of the actual run
time of a sequential task.  A crucial property of the abstract cost is
that it should be abstract enough that the programmer can write the
cost functions without necessarily knowing the details of the hardware
that the programs will be executed on.  Yet, abstract costs should
provide sufficient information to estimate the actual run times.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::asymptotic
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::asymptotic
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Asymptotic complexity specifications serve as a natural cost function by satisfying both of these properties. Since they eliminate hardware specific constants, they can be specified easily. Using complexity functions, we can approximate the actual run time of sequentially executed functions by simply determining the constants hidden by the asymptotic complexity notation. Such an approximation can be performed by using the least squares method or similar techniques for data fitting from known samples.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Asymptotic complexity specifications serve as a natural cost function
by satisfying both of these properties.  Since they eliminate hardware
specific constants, they can be specified easily.  Using complexity
functions, we can approximate the actual run time of sequentially
executed functions by simply determining the constants hidden by the
asymptotic complexity notation.  Such an approximation can be
performed by using the least squares method or similar techniques for
data fitting from known samples.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::implementation
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::implementation
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>In our implementation described in Section <a href="#sec:imp" data-reference-type="ref" data-reference="sec:imp">[sec:imp]</a>, we implement an approach based on complexity functions. We define as an integer, which represents the application of the complexity function applied to the input size. We approximate the actual run time by calculating a single constant, assuming that the constants in all terms of the asymptotic complexity are the same. Although assuming a single constant can decrease the precision of the approximations, we believe that it suffices because we only have to compute lower bounds for our functions; <em>i.e.</em>, we only need to determine whether they are “big enough” for parallel execution.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
In our implementation described in \secref{imp}, we implement an
approach based on complexity functions.  We define \kwtypeofcost as an
integer, which represents the application of the complexity function
applied to the input size.  We approximate the actual run time by
calculating a single constant, assuming that the constants in all
terms of the asymptotic complexity are the same.  Although assuming a
single constant can decrease the precision of the approximations, we
believe that it suffices because we only have to compute lower bounds
for our functions; \textit{i.e.}, we only need to determine whether they are
``big enough'' for parallel execution.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::discuss
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::discuss
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[

]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\aremark{Discuss this last paragraph}
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::todo
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::todo
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[

]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\begin{comment}
[TODO: define substitution on language with cost expressions]
[TODO: define the semantics judgment on that language]
[TODO: theorems concern termination but could be extended to divergence]




\subsection{Correctness of the translation}

In this section, we outline the proof of correctness for our
translation. For this proof, we introduce a reference semantics whose
judgement takes the form
$$e \redb v$$ where $e$ is a term that evaluates to a value $v$. The
references semantics is similar to the dynamic cost semantics, except
for two cases: For simplicity, the reference semantics does not
measure costs and, as such, does not include oracle derivation rules.
For correctness, the reference semantics has a modified version of the
function-application rule where the cost function is applied to the
same argument as the function and the application is forced to
evaluate to a value. The correctness theorem below, as stated, would
be incorrect otherwise, because without the modification,
nonterminating cost functions could cause translated expressions to
diverge even though their untranslated counterpart expressions
converge.

The substitution lemma shows that our translation commutes with the
substitution of value for a variable inside a source expression.
\begin{lemma}[Substitution]
\label{lem:trans-substitution}
Let $e$ be an expression, $v$ be a value, $x$ be a 
variable and $\alpha$ be either $\sseq$ or $\sorc$.
Then,
$$\Fcomp{\alpha}{e[v/x]} \Sq\equiv
\Fcomp{\alpha}{e}[\Fcompval{v}/x]$$
\end{lemma}
The following two lemmas state the specifications of our metafunctions
$MeasuredRun$ and $MakeBranch$. The proof of each follows trivially from
the definitions.
\begin{lemma}[MeasuredRun]
\label{lem:trans-measured-run}
If $r$, $m$, and $k$ are \src values,
\begin{itemize}
\item $r$ is of type $\Tced$
\item $m$ is of type $\F{int}$
\item $k() \redb v$
\end{itemize}
then $MeasuredRun (r, m, k) \redb v$.
\end{lemma}
\begin{lemma}[Semantics of the function MeasureRun]
\label{lem:trans-run}
Let $r$ be a CED value, $m$ an integer value, $k$ a continuation
and $v$ a value.
If \W{\Jsemof{\kwapp{k}{\kwunit}}{v}}
then \W{\Jsemof{MeasureRun (r,m,k)}{v}}.
\end{lemma}

\begin{lemma}[Semantics of the function MakeBranch]
\label{lem:trans-oracle}
If $r$, $f_c$, $f_s$, $f_p$ are \src expressions, $v_a$ is a \src value,
and
\begin{itemize}
\item $r$ is of type $\Tced$
\item $f_c v_a \redb v_c$
\item $f_p v_a \redb v_r$
\item $f_s v_a \redb v_r$
\end{itemize}
and $(b, k)$ $=$ $Oracle ((r, f_c, f_s, f_p), v_a)$, then $k() \redb
v_r$.
\end{lemma}
Finally, the correctness theorem shows that our translation preserves
the meaning of the given source expression, if the source expression
terminates. It is straightforward to extend the argument to cover
expressions that diverge.
\begin{theorem}[Correctness of the translation]
If $e$ is an expression and $v$ a value such that \W{\Jsemof{e}{v}}, then,
for $\alpha$ equal to either $\sseq$ or $\sorc$,
we have \W{\Jsemof{\Fcomp{\Ga}{e}}{\Fcompval{v}}}.
\end{theorem}

\begin{proof}
By induction on the evaluation derivation $\redb$.
\end{proof}





\begin{figure}

$$\begin{lines}
\kw{val}\;default\_constant = 1.0
\vspace{4pt}\\
\kw{val}\;nb\_begin = 4
\vspace{4pt}\\
\kw{val}\;nb\_grouped = 20 * nb\_proc 
\vspace{4pt}\\
\kw{val}\;weight = 3
\vspace{4pt}\\

\kw{type}\;\Tced\Sc{=} 
\begin{array}[t]{@{}l@{\;}l@{}l} 
\{ & global & \Sc{:} \kw{float}; \\
& local & \Sc{:} \kw{(bool * int * float)\,array} \,\}  
\end{array}
\vspace{4pt}\\

\kw{val}\;ced\_initialize\,() =\\
\quad
\begin{array}[t]{@{}l@{\;}l@{}l} 
\{ & global & \Sc{=} default\_constant; \\
& local & \Sc{=} \kw{make\_array}\,(nb\_proc,\,(\kw{true},\, 0,\, 0.0)) \,\} 
\end{array}
\vspace{4pt}\\

\kw{val}\;ced\_estimate\,(r,m) = m * r.global
\vspace{4pt}\\

\kw{val}\;ced\_report\,(r,c) =\\
\quad \begin{lines}
r.shared \leftarrow (r.shared * weight + c) \Ss/ (weight + 1)
\end{lines}
\vspace{4pt}\\

\kw{val}\;ced\_measured\,(r,m,t) =\\
\quad \begin{lines}
\kwletins{p}{\kwappunit{get\_proc\_id}} \\
\kwletins{(g,n,s)}{ r.\kwarrget{local}{p}} \\
\kwletins{g'}{g~\kw{\&\&}~(n<nb\_begin)} \\
\kwletins{n'}{n+1} \\
\kwletins{s'}{s+(t / m)} \\
\kwletins{c}{s' \Sc/ n'} \\
\kw{if}~g'~\kw{then}~ced\_report\,(r,c); \\
\kw{if}~n' = nb\_grouped~\kw{then}~\kw{begin}\\
\qquad ced\_report\,(r,c); \\
\qquad r.\kwarrget{local}{p} \leftarrow (\kw{false},\, 0,\, 0.0); \\
\kw{end}~\kw{else} \\
\qquad r.\kwarrget{local}{p} \leftarrow (g',\,n',\,s'); \\
\end{lines}
\end{lines}$$

\aremark{Fix names and fonts}
\caption{Auxiliary meta-functions used for compilation}
\label{fig:ced-implementation}
\end{figure}

\end{comment}
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->


</segment> <!-- paragraph -->

</segment> <!-- section -->

<segment name='section'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Implementation
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Implementation
]]>
</field> <!-- title_src -->
<field name='label'>
sec:imp
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::scheduling
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::scheduling
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>In this section, we describe the implementation of our scheduling technique in an actual language and system. In our approach, source programs are written in our own dialect of the Caml language <span class="citation" data-cites="objective-caml"></span>, which is a strict functional language. Our Caml dialect corresponds to the core Caml language extended with syntax for parallel pairs and complexity annotations. Figure <a href="#fig:exampleprog" data-reference-type="ref" data-reference="fig:exampleprog">[fig:exampleprog]</a> shows a program implemented in our Caml dialect. This recursive program traverse a binary tree to compute the sum of the values stored in the leaves.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
In this section, we describe the implementation of our scheduling
technique in an actual language and system. In our approach, source
programs are written in our own dialect of the 
Caml language~\cite{objective-caml}, which is 
a strict functional language. 
Our Caml dialect corresponds to the core Caml language
extended with syntax for parallel pairs and complexity annotations. 
\figref{exampleprog} shows a program implemented in our Caml dialect.
This recursive program traverse a binary
tree to compute the sum of the values stored in the leaves.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::tree
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='figure'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
fig:exampleprog
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<pre><code>type tree =   
  | Leaf of int   
  | Node of int * tree * tree  
  
let size = function  
  | Leaf _ -&gt; 1  
  | Size (s,_,_) -&gt; s  
  
let rec sum t = Oracle.complexity (size t);  
  match t with  
  | Leaf n -&gt; n    
  | Node (size,t1,t2) -&gt;  
     let (n1,n2) = (| sum t1, sum t2 |) in  
     n1 + n2  </code></pre>
<p> <br />
<span>An example parallel program.</span> </p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\begin{verbatim}
type tree = 
  | Leaf of int 
  | Node of int * tree * tree

let size = function
  | Leaf _ -> 1
  | Size (s,_,_) -> s

let rec sum t = Oracle.complexity (size t);
  match t with
  | Leaf n -> n  
  | Node (size,t1,t2) ->
     let (n1,n2) = (| sum t1, sum t2 |) in
     n1 + n2
\end{verbatim}
\caption{An example parallel program.}
\label{fig:exampleprog}
]]>
</field> <!-- body_src -->

</atom> <!-- figure -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::caml
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::caml
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>We use the Caml type checker to obtain a typed syntax tree, on which we perform the oracle-scheduling translation defined in Figure <a href="#fig:compilation" data-reference-type="ref" data-reference="fig:compilation">[fig:compilation]</a>. We then produce code in the syntax of Parallel ML (PML) <span class="citation" data-cites="FluetRaReSh11"></span>, a parallel language close to Standard ML. The translation from Caml to PML is straightforward because the two languages are relatively similar. We compile our source programs to x86-64 binaries using Manticore, which is the optimizing PML compiler. The Manticore run-time system provides a parallel, generational garbage collector that is crucial for scaling to more than four processors, because functional programs, such as the ones we consider, often involve heavy garbage-collection loads. Further details on Manticore can be found elsewhere <span class="citation" data-cites="Fluet:2008:SFG:1411204.1411239"></span>. In the rest of this section, we explain how we compute the constant factors, and we also give a high-level description of the particular work-stealing scheduler on top of which we are building the implementation of our oracle scheduler.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
We use the Caml type checker to obtain a typed syntax tree, on which
we perform the oracle-scheduling translation defined in Figure
\ref{fig:compilation}. We then produce code in the syntax
of Parallel~ML (PML)~\cite{FluetRaReSh11}, a parallel 
language close to Standard~ML. The translation from Caml
to PML is straightforward because the two languages are 
relatively similar.
We compile our source programs to x86-64 binaries using Manticore,
which is the optimizing PML compiler. The Manticore run-time system
provides a parallel, generational garbage collector that is crucial
for scaling to more than four processors, because functional programs,
such as the ones we consider, often involve heavy garbage-collection
loads. Further details on Manticore can be found
elsewhere~\cite{Fluet:2008:SFG:1411204.1411239}.
In the rest of this section, we explain how we compute
the constant factors, and we also give a high-level description 
of the particular work-stealing scheduler on top of which 
we are building the implementation of our oracle scheduler.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->


<segment name='paragraph'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Run-time estimation of constants.
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Run-time estimation of constants.
]]>
</field> <!-- title_src -->
<field name='label'>
sec:oracle::run-time-estimation-of-constants
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::goal
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::goal
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>The goal of the oracle is to make relatively accurate execution time predictions at little cost. Our approach to implementing the oracle consists of evaluating a user-provided asymptotic complexity function, and then multiplying the result by an appropriate constant factor. Every function has its own constant factor, and the value for this constant factor is stored in the estimator data structure. In this section, we discuss the pratical implementation of the evaluation of constant factors.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
The goal of the oracle is to make relatively accurate execution 
time predictions at little cost. Our approach to implementing
the oracle consists of evaluating a user-provided asymptotic
complexity function, and then multiplying the result by an
appropriate constant factor. Every function has its own constant
factor, and the value for this constant factor is stored in the
estimator data structure.
In this section, we discuss the pratical implementation of the 
evaluation of constant factors.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::measurement
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::measurement
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>In order for the measurement of the constant to be lightweight, we simply compute average values of the constant. The constant might evolve over time, for example if the current program is sharing the machine with another program, a series of memory reads by the other program may slow down the current program. For this reason, we do not just compute the average across the entire history, but instead maintain a moving average, that is, an average of the values gathered across a certain number of runs.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
In order for the measurement of the constant to be lightweight, we
simply compute average values of the constant.  The constant might
evolve over time, for example if the current program is sharing the
machine with another program, a series of memory reads by the other
program may slow down the current program.  For this reason, we do not
just compute the average across the entire history, but instead
maintain a moving average, that is, an average of the values gathered
across a certain number of runs.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::maintaining
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::maintaining
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Maintaining averages is not entirely straightforward. One the one hand, storing data in a memory cell that is shared by all processors is not satisfying because it would involve some synchronization problems. On the other hand, using a different memory cell for every processor is not satisfying either, because it leads to slower updates of the constants when they change. In particular, in the beginning of the execution of a program it is important that all processors quickly share a relatively good estimate of the constant factors. For these reasons, we have opted for an approach that uses not only a shared memory cell but also one data structure local to every processor.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Maintaining averages is not entirely straightforward.
One the one hand, storing data in a memory cell 
that is shared by all processors is not satisfying because it would involve 
some synchronization problems. On the other hand, using a
different memory cell for every processor is not satisfying either,
because it leads to slower updates of the constants when they change.
In particular, in the beginning of the execution of a program
it is important that all processors quickly share a relatively good
estimate of the constant factors.
For these reasons, we have opted for an approach that
uses not only a shared memory cell but also one data structure
local to every processor.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::shared
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::shared
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>The shared memory cell associated with each estimator contains the estimated value for the constant that is read by all the processors when they need to predict execution times. The local data structures are used to accumulate statistics on the value of the constant. Those statistics are reported on a regular basis to the shared memory cell, by computing a weighted mean between the value previously stored in the shared memory cell and the value obtained out of the local data structure. We treat initializations somewhat specially: for the first few measures, a processor always begins by reporting its current average to the shared memory cell. This ensures a fast propagation of the information gathered from the first runs, so as to quickly improve the accuracy of the predictions.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
The shared memory cell associated with each estimator contains the
estimated value for the constant that is read by all the processors
when they need to predict execution times. The local data structures
are used to accumulate statistics on the value of the constant. Those
statistics are reported on a regular basis to the shared memory cell,
by computing a weighted mean between the value previously stored in
the shared memory cell and the value obtained out of the local data
structure. We treat initializations somewhat specially: for the first
few measures, a processor always begins by reporting its current
average to the shared memory cell. This ensures a fast propagation of
the information gathered from the first runs, so as to quickly improve
the accuracy of the predictions.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::implementing
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::implementing
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>When implementing the oracle, we faced three technical difficulties. First, we had to pay attention to the fact that the memory cells allocated for the different processors are not allocated next to each other. Otherwise, those cells would fall in the same cache line, in which case writing in one of these cells would make the other cells be removed from caches, making subsequent reads more costly. Second, we observed that the time measures typically yield a few outliers. Those are typically due to the activity of the garbage collector or of another program being scheduled by the operating system on the same processor. Fortunately, we have found detecting these outliers to be relatively easy because the measured times are at least one or two orders of magnitude greater than the cutoff value. Third, the default system function that reports the time is only accurate by one microsecond. This is good enough when the cutoff is greater than 10 microseconds. However, if one were to aim for a smaller cutoff, which could be useful for programs exhibiting only a limited amount of parallelism, then more accurate techniques would be required, for example using the specific processor instructions for counting the number of processor cycles.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
When implementing the oracle, we faced three technical difficulties.
First, we had to pay attention to the fact that the memory cells
allocated for the different processors are not allocated next to each
other. Otherwise, those cells would fall in the same cache line, in
which case writing in one of these cells would make the other cells be
removed from caches, making subsequent reads more costly.  Second, we
observed that the time measures typically yield a few outliers.  Those
are typically due to the activity of the garbage collector or of
another program being scheduled by the operating system on the same
processor.  Fortunately, we have found detecting these outliers to be
relatively easy because the measured times are at least one or two
orders of magnitude greater than the cutoff value.  Third, the default
system function that reports the time is only accurate by one
microsecond. This is good enough when the cutoff is greater than 10
microseconds.  However, if one were to aim for a smaller cutoff, which
could be useful for programs exhibiting only a limited amount of
parallelism, then more accurate techniques would be required, for
example using the specific processor instructions for counting the
number of processor cycles.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->


</segment> <!-- paragraph -->

<segment name='paragraph'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Work stealing.
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Work stealing.
]]>
</field> <!-- title_src -->
<field name='label'>
sec:oracle::work-stealing
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::implement
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::implement
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>We implement our oracle scheme on top of the work stealing scheduler <span class="citation" data-cites="BlumofeWorkStealing"></span>. In this section we outline the particular implementation of work stealing that we selected from the Manticore system. Our purpose is to understand what exactly contributes to the scheduling cost <span class="math inline">\(\tau\)</span> in our system.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
We implement our oracle scheme on top of the work stealing
scheduler~\cite{BlumofeWorkStealing}. In this section we outline the
particular implementation of work stealing that we selected from the
Manticore system. Our purpose is to understand what exactly
contributes to the scheduling cost $\tau$ in our system.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::manticore
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::manticore
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>In Manticore’s work-stealing scheduler, all system processors are assigned to collaborate on the computation. Each processor owns a deque (doubly-ended queue) of tasks represented as thunks. Processors treat their own deques like call stacks. When a processor starts to evaluate a parallel-pair expression, it creates a task for the second subexpression of the pair and pushes the task onto the bottom of the deque. Processors that have no work left try to <span><em>steal</em></span> tasks from others. More precisely, they repeatedly select a random processor and try to pop a task from this processor’s deque.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
In Manticore's work-stealing scheduler, all system processors are
assigned to collaborate on the computation. Each processor owns a
deque (doubly-ended queue) of tasks represented as thunks. Processors
treat their own deques like call stacks.  When a processor starts to
evaluate a parallel-pair expression, it creates a task for the second
subexpression of the pair and pushes the task onto the bottom of the
deque. Processors that have no work left try to 
{\em steal} tasks from others. More precisely, they repeatedly select 
a random processor and try to pop a task from this processor's deque.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::stealing
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::stealing
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Manticore’s implementation of work stealing <span class="citation" data-cites="rainey:phd"></span> adopts a code-specialization scheme, called clone translation, taken from Cilk-5’s implementation <span class="citation" data-cites="FrigoLeRa98"></span>.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> With clone translation, each parallel-pair expression is compiled into two versions: the fast clone and the slow clone. The purpose of a fast clone is to optimize the code that corresponds to evaluating on the local processor, whereas the slow clone is used when the second branch of a parallel-pair is migrated to another processor. A common aspect of between clone translation and our oracle translation (Figure <a href="#fig:compilation" data-reference-type="ref" data-reference="fig:compilation">[fig:compilation]</a>) is that both generate specialized code for the sequential case. But the clone translation differs in that there is no point at which parallelism is cut off entirely, as the fast clone may spawns subtasks.</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>In the Cilk-5 implementation, it is called clone compilation.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩</a></p></li>
</ol>
</section>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Manticore's implementation of work stealing~\cite{rainey:phd} adopts a
code-specialization scheme, called clone translation, taken from Cilk-5's
implementation~\cite{FrigoLeRa98}.\footnote{In the Cilk-5
implementation, it is called clone compilation.} With clone
translation, each parallel-pair expression is compiled into two
versions: the fast clone and the slow clone. 
The purpose of a fast clone is to optimize the code that corresponds
to evaluating on the local processor,
whereas the slow clone is used when the second branch 
of a parallel-pair is migrated to another processor.
A common aspect of between clone translation and our oracle
translation (Figure \ref{fig:compilation}) is that both generate
specialized code for the sequential case. But the clone translation
differs in that there is no point at which parallelism is cut off
entirely, as the fast clone may spawns subtasks.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::involved
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::involved
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>The scheduling cost involved in the fast clone is a (small) constant, because it involves just a few local operations, but the scheduling cost of the slow clone is variable, because it involves inter-processor communication. It is well established, both through analysis and experimentation, that (with high probability) no more than <span class="math inline">\(O(P\mathcal{D})\)</span> steals occur during the evaluation <span class="citation" data-cites="BlumofeWorkStealing"></span>. So, for programs that exhibit parallel slackness (<span class="math inline">\(\mathcal{W}\gg P \mathcal{D}\)</span>), we do not need to take into account the cost of slow clones because there are relatively few of them. We focus only on the cost of creating fast clones, which correspond to the cost <span class="math inline">\(\tau\)</span>. A fast clone needs to packages a task, push it onto the deque and later pop it from the deque. So, a fast clone is not quite as fast as the corresponding sequential code. The exact slowdown depend on the implementation, but in our case we have observed that a fast clone is 3 to 5 times slower than a simple function call.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
The scheduling cost involved in the fast clone is a (small) constant,
because it involves just a few local operations, but the scheduling
cost of the slow clone is variable, because it involves
inter-processor communication. 
It is well established, both through analysis and
experimentation, that (with high probability)
no more than $O(P\sds)$ steals occur during 
the evaluation~\cite{BlumofeWorkStealing}.
So, for programs that exhibit parallel slackness ($\sws \gg P \sds $),
we do not need to take into account the cost of slow clones
because there are relatively few of them.
We focus only on the cost of creating fast clones,
which correspond to the cost $\tau$.
A fast clone needs to packages a task, push it onto the deque
and later pop it from the deque. So, a fast clone is not quite
as fast as the corresponding sequential code.
The exact slowdown depend on the implementation, but in our
case we have observed that a fast clone is 3 to 5 
times slower than a simple function call.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->


</segment> <!-- paragraph -->

</segment> <!-- section -->

<segment name='section'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Empirical Evaluation
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Empirical Evaluation
]]>
</field> <!-- title_src -->
<field name='label'>
sec:exp
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::_8_
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::_7_
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p> </p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\label{sec:exp}
\label{sec:empirical-evaluation}
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::evaluate
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::evaluate
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>In this section, we evaluate the effectiveness of our implementation through several experiments. We consider results from a range of benchmarks run on two machines with different architectures. The results show that, in each case, our oracle implementation improves on the plain work-stealing implementation. Furthermore, the results show that the oracle implementation scales well with up to sixteen processors.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
In this section, we evaluate the effectiveness of our implementation
through several experiments. We consider results from a range of
benchmarks run on two machines with different architectures. The
results show that, in each case, our oracle implementation improves on
the plain work-stealing implementation. Furthermore, the results show
that the oracle implementation scales well with up to sixteen processors.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->


<segment name='paragraph'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Machines.
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Machines.
]]>
</field> <!-- title_src -->
<field name='label'>
sec:oracle::machines
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::machine
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::machine
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Our AMD machine has four quad-core AMD Opteron 8380 processors running at 2.5GHz. Each core has 64Kb each of L1 instruction and data cache, and a 512Kb L2 cache. Each processor has a 6Mb L3 cache that is shared with the four cores of the processor. The system has 32Gb of RAM and runs Debian Linux (kernel version 2.6.31.6-amd64).</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Our AMD machine has four quad-core AMD Opteron 8380 processors running
at 2.5GHz.  Each core has 64Kb each of L1 instruction and data cache,
and a 512Kb L2 cache. Each processor has a 6Mb L3 cache that is shared
with the four cores of the processor.  The system has 32Gb of RAM and
runs Debian Linux (kernel version 2.6.31.6-amd64).
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::intel
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::intel
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Our Intel machine has four eight-core Intel Xeon X7550 processors running at 2.0GHz. Each core has 32Kb each of L1 instruction and data cache and 256 Kb of L2 cache. Each processor has an 18Mb L3 cache that is shared by all eight cores. The system has 1Tb of RAM and runs Debian Linux (kernel version 2.6.32.22.1.amd64-smp). For uniformity, we consider results from just sixteen out of the thirty-two cores of the Intel machine.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Our Intel machine has four eight-core Intel Xeon X7550 processors
running at 2.0GHz. Each core has 32Kb each of L1 instruction and
data cache and 256 Kb of L2 cache. Each processor has an 18Mb L3 cache
that is shared by all eight cores. The system has 1Tb of RAM and runs
Debian Linux (kernel version 2.6.32.22.1.amd64-smp).
For uniformity, we consider results from just sixteen out of the
thirty-two cores of the Intel machine.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::rose
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::rose
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[

]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\uremark{Rose's comments fixes up to here.}
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->


</segment> <!-- paragraph -->

<segment name='paragraph'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Measuring scheduling costs.
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Measuring scheduling costs.
]]>
</field> <!-- title_src -->
<field name='label'>
sec:oracle::measuring-scheduling-costs
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::report
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::report
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>We report estimates of the task-creation overheads for each of our test machines. To estimate, we use a synthetic benchmark expression <span class="math inline">\(e\)</span> whose evaluation sums integers between zero and 30 million using a parallel divide-and-conquer computation. We chose this particular expression because most of its evaluation time is spent evaluating parallel pairs.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
We report estimates of the task-creation overheads for each of our test
machines. To estimate, we use a synthetic benchmark expression $e$
whose evaluation sums integers between zero and 30 million using a
parallel divide-and-conquer computation. We chose this particular
expression because most of its evaluation time is spent evaluating
parallel pairs.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::measure
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::measure
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>First, we measure <span class="math inline">\(w_s\)</span>: the time required for executing a sequentialized version of the program (a copy of the program where parallel tuples are systematically replaced with sequential tuples). This measure serves as the baseline. Second, we measure <span class="math inline">\(w_w\)</span>: the time required for executing the program using work stealing, on a single processor. This measure is used to evaluated <span class="math inline">\(\tau\)</span>. Third, we measure <span class="math inline">\(w_o\)</span>: the time required for executing a version of the program with parallel tuples replaced with ordinary tuples but where we still call the oracle. This measure is used to evaluate <span class="math inline">\(\phi\)</span>.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
First, we measure $w_s$: the time required for executing a
sequentialized version of the program (a copy of the program where
parallel tuples are systematically replaced with sequential tuples).
This measure serves as the baseline.  Second, we measure $w_w$: the
time required for executing the program using work stealing, on a
single processor.  This measure is used to evaluated $\csp$.  Third,
we measure $w_o$: the time required for executing a version of the
program with parallel tuples replaced with ordinary tuples but where
we still call the oracle. 
This measure is used to evaluate $\corc$.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::work-stealing
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::work-stealing
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>We then define the work-stealing overhead <span class="math inline">\(c_w = \frac{w_w}{w_s}\)</span>. We estimate the cost <span class="math inline">\(\tau\)</span> of creating a parallel task in work stealing by computing <span class="math inline">\(\frac{w_w - w_s}{n}\)</span>, where <span class="math inline">\(n\)</span> is the number of parallel pairs evaluated in the program. We also estimate the cost <span class="math inline">\(\phi\)</span> of invoking the oracle by computing <span class="math inline">\(\frac{w_o - w_s}{m}\)</span>, where <span class="math inline">\(m\)</span> is the number of times the oracle is invoked. Our measures are as follows.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Machine</th>
<th style="text-align: center;"><span class="math inline">\(c_w\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\tau\)</span> (<span class="math inline">\(\mu\)</span>s)</th>
<th style="text-align: center;"><span class="math inline">\(\phi\)</span> (<span class="math inline">\(\mu\)</span>s)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">AMD</td>
<td style="text-align: center;">4.86</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Intel</td>
<td style="text-align: center;">3.90</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
We then define the work-stealing overhead $c_w = \frac{w_w}{w_s}$.
We estimate the cost $\csp$ of creating a parallel task in work stealing
by computing $\frac{w_w - w_s}{n}$, where
$n$ is the number of parallel pairs evaluated in the program.
We also estimate the cost $\phi$ of invoking the oracle  by computing $\frac{w_o - w_s}{m}$,
where $m$ is the number of times the oracle is invoked.
Our measures are as follows.
\begin{center}
\begin{tabular}{l | c c c c}
Machine & $c_w$  &  $\csp$ ($\mu$s) & $\phi$ ($\mu$s) \\
\hline
AMD   & 4.86   & 0.09  & 0.18 \\
Intel & 3.90  &  0.18  & 0.94 \\
\end{tabular}
\end{center}
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::amd
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::amd
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[

]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\begin{comment}
\begin{center}
\begin{tabular}{l | c c c c}
Machine & $c_w$  & $c_o$ &  $\csp$ ($\mu$s) & $\phi$ ($\mu$s) \\
\hline
AMD   & 4.86  & 8.25 & 0.09  & 0.18 \\
Intel & 3.90 & 7.27 &  0.18  & 0.94 \\
\end{tabular}
\end{center}
\end{comment}
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::column
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::column
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>The first column indicates that work stealing alone can induce a slowdown by a factor of 4 or 5, for programs that create a huge number of parallel tuples. Column two indicates that the cost of creating parallel task <span class="math inline">\(\tau\)</span> is significant, taking roughly between 200 and 350 processor cycles. The last column suggests that the oracle cost <span class="math inline">\(\phi\)</span> is of the same order of magnitude (<span class="math inline">\(\phi\)</span> is 2 to 5 times larger than <span class="math inline">\(\tau\)</span>).</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\begin{comment}
   \begin{center}
   \begin{tabular}{l | c c }
   Machine & $c_w$ & $\csp$ ($\mu$s) \\
   \hline
   AMD   & 4.86 & 0.09 \\
   Intel & 3.90 & 0.18 \\
   \end{tabular}
   \end{center}
   \end{comment}
The first column indicates that work stealing alone can induce
a slowdown by a factor of 4 or 5, for programs that create a
huge number of parallel tuples. 
Column two indicates that the cost of creating parallel task $\csp$ 
is significant, taking roughly between 200 and 350 processor cycles. 
The last column suggests that the oracle cost $\corc$
is of the same order of magnitude ($\corc$ is 2 to 5 times larger than $\csp$).
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::determine
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::determine
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>To determine a value for <span class="math inline">\(\kappa\)</span>, we use the formula <span class="math inline">\(\frac{\mu(\tau+\gamma\phi)}{r}\)</span> from §<a href="#sec:estimate" data-reference-type="ref" data-reference="sec:estimate">[sec:estimate]</a>. Recall that <span class="math inline">\(r\)</span> is the targette overhead for scheduling costs. We aim for <span class="math inline">\(r =  
10\%\)</span>. Our oracle appears to be always accurate within a factor 2, so we set <span class="math inline">\(\mu= 2\)</span>. Our benchmark programs are fairly regular, so we take <span class="math inline">\(\gamma= 3\)</span>. We then use the values for <span class="math inline">\(\tau\)</span> and <span class="math inline">\(\phi\)</span> specific to the machine and evaluate the formula <span class="math inline">\(\frac{\mu(\tau+\gamma\phi)}{r}\)</span>. We obtain <span class="math inline">\(13\mu s\)</span> for the AMD machine and <span class="math inline">\(60\mu s\)</span> for the Intel machine. However, we were not able to use a cutoff as small as <span class="math inline">\(13\mu s\)</span> because the time function that we are using is only accurate up to <span class="math inline">\(1\mu s\)</span>. For this reason, we doubled the value to <span class="math inline">\(26\mu s\)</span>. (One possibility to achieve greater accuracy would be to use architecture-specific registers that are able to report on the number of processor cycles involved in the execution of a task.)</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
To determine a value for $\coff$, we use the formula
$\frac{\cerr(\csp+\creg\corc)}{r}$ from \sref{sec:estimate}.  Recall
that $r$ is the targette overhead for scheduling costs.  We aim for $r =
10\%$.  Our oracle appears to be always accurate within a factor 2, so
we set $\cerr = 2$.  Our benchmark programs are fairly regular, so we
take $\creg = 3$.  We then use the values for $\csp$ and $\corc$
specific to the machine and evaluate the formula
$\frac{\cerr(\csp+\creg\corc)}{r}$.  We obtain $13\mu s$ for the AMD
machine and $60\mu s$ for the Intel machine.  However, we were not able to
use a cutoff as small as $13\mu s$ because the time function that we
are using is only accurate up to $1\mu s$.  For this reason, we
doubled the value to $26\mu s$.  (One possibility to achieve greater
accuracy would be to use architecture-specific registers that are able
to report on the number of processor cycles involved in the execution
of a task.)
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::experiments
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::experiments
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>In our experiments, we used <span class="math inline">\(\kappa= 26\mu s\)</span> on the AMD machine and <span class="math inline">\(\kappa= 61\mu s\)</span> on the Intel machine.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
In our experiments, we used $\coff = 26\mu s$ on the AMD machine and $\coff = 61\mu s$ on the Intel machine.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::25in
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='figure'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
fig:comparison-oracle-work-stealing
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<table>
<tbody>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p> <br />
<span>Comparison of the speedup on sixteen processors. Higher bars are better.</span> </p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\begin{center}
  \begin{tabular}{p{3.25in}@{\hfil}p{3.25in}}
    \begin{center}
      AMD
      \medfig{pictures/work-stealing-vs-oracle--for--machine=hexi--proc=16.pdf}
    \end{center}
    &
    \begin{center}
      Intel
      \medfig{pictures/work-stealing-vs-oracle--for--machine=srv-53-07--proc=16.pdf}
    \end{center}
  \end{tabular}
\end{center}
\vspace*{-6mm}
\caption{Comparison of the speedup on sixteen processors. Higher bars
  are better.}
\label{fig:comparison-oracle-work-stealing}
]]>
</field> <!-- body_src -->

</atom> <!-- figure -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::sequential-vs-oracle-vs-work-stealing--for--machine
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='figure'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
fig:comparison-sequential-oracle-work-stealing
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<table>
<tbody>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p> <br />
<span>Comparison of execution times (normalized) on a single processor. Lower bars are better.</span> </p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\begin{center}
  \begin{tabular}{p{3.25in}@{\hfil}p{3.25in}}
    \begin{center}
      AMD
      \medfig{pictures/sequential-vs-oracle-vs-work-stealing--for--machine=hexi--proc=1.pdf}
    \end{center}
    &
    \begin{center}
      Intel
      \medfig{pictures/sequential-vs-oracle-vs-work-stealing--for--machine=srv-53-07--proc=1.pdf}
    \end{center}
  \end{tabular}
\end{center}
\vspace*{-6mm}
\caption{Comparison of execution times (normalized) on a single
  processor. Lower bars are better.}
\label{fig:comparison-sequential-oracle-work-stealing}
]]>
</field> <!-- body_src -->

</atom> <!-- figure -->

</segment> <!-- cluster -->


</segment> <!-- paragraph -->

<segment name='paragraph'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Benchmarks.
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Benchmarks.
]]>
</field> <!-- title_src -->
<field name='label'>
sec:oracle::benchmarks
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::used
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::used
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>We used five benchmarks in our empirical evaluation. Each benchmark program was originally written by other researchers and ported to our dialect of Caml.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
We used five benchmarks in our empirical evaluation. Each benchmark
program was originally written by other researchers and ported to our
dialect of Caml.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::quicksort
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::quicksort
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>The Quicksort benchmark sorts a sequence of 2 million integers. Our program is adapted from a functional, tree-based algorithm <span class="citation" data-cites="BlellochGr95"></span>. The algorithm runs with <span class="math inline">\(O(n  
\log n)\)</span> raw work and <span class="math inline">\(O(\log^2 n)\)</span> raw depth, where <span class="math inline">\(n\)</span> is the length of the sequence. Sequences of integers are represented as binary trees in which sequence elements are stored at leaf nodes and each internal node caches the number of leaves contained in its subtree.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
The Quicksort benchmark sorts a sequence of 2 million integers. Our
program is adapted from a functional, tree-based
algorithm~\cite{BlellochGr95}. The algorithm runs with $O(n
\log n)$ raw work and $O(\log^2 n)$ raw depth, where $n$ is the length
of the sequence. Sequences of integers are represented as binary trees
in which sequence elements are stored at leaf nodes and each internal
node caches the number of leaves contained in its subtree.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::quickhull
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::quickhull
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>The Quickhull benchmark calculates the convex hull of a sequence of 3 million points contained in 2-d space. The algorithm runs with <span class="math inline">\(O(n  
\log n)\)</span> raw work and <span class="math inline">\(O(\log^2 n)\)</span> raw depth, where <span class="math inline">\(n\)</span> is the length of the sequence. The representation of points is similar to that of Quicksort, except that leaves store 2-d points instead of integers.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
The Quickhull benchmark calculates the convex hull of a sequence of
3 million points contained in 2-d space. The algorithm runs with $O(n
\log n)$ raw work and $O(\log^2 n)$ raw depth, where $n$ is the length
of the sequence. The representation of points is similar to that of
Quicksort, except that leaves store 2-d points instead of integers.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::barnes-hut
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::barnes-hut
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>The Barnes-Hut benchmark is an <span class="math inline">\(n\)</span>-body simulation that calculates the gravitational forces between <span class="math inline">\(n\)</span> particles as they move through 2-d space <span class="citation" data-cites="barnes-hut"></span>. The Barnes-Hut computation consists of two phases. In the first, the simulation volume is divided into square cells via a quadtree, so that only particles from nearby cells need to be handled individually and particles from distant cells can be grouped together and treated as large particles. The second phase calculates gravitational forces using the quadtree to accelerate the computation. The algorithm runs with <span class="math inline">\(O(n \log n)\)</span> raw work and <span class="math inline">\(O(\log n)\)</span> raw depth. Our benchmark runs 10 iterations over 100,000 particles generated from a random Plummer distribution <span class="citation" data-cites="plummer-distribution"></span>. The program is adapted from a Data-Parallel Haskell program <span class="citation" data-cites="PeytonJonesHarnessingMulticores"></span>. The representation we use for sequences of particles is similar to that of Quicksort.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
The Barnes-Hut benchmark is an $n$-body simulation that calculates the
gravitational forces between $n$ particles as they move through 2-d
space~\cite{barnes-hut}. The Barnes-Hut computation consists of two
phases. In the first, the simulation volume is divided into square
cells via a quadtree, so that only particles from nearby cells need to
be handled individually and particles from distant cells can be
grouped together and treated as large particles. The second phase
calculates gravitational forces using the quadtree to accelerate the
computation. The algorithm runs with $O(n \log n)$ raw work and
$O(\log n)$ raw depth. Our benchmark runs 10 iterations over 100,000
particles generated from a random Plummer
distribution~\cite{plummer-distribution}. The program is adapted from
a Data-Parallel Haskell
program~\cite{PeytonJonesHarnessingMulticores}. The representation we
use for sequences of particles is similar to that of Quicksort.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::smvm
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::smvm
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>The SMVM benchmark multiplies an <span class="math inline">\(m \times n\)</span> matrix with an <span class="math inline">\(n \times  
1\)</span> dense vector. Our sparse matrix is stored in the compressed sparse-row format. The program contains parallelism both between dot products and within individual dot products. We use a sparse matrix of dimension <span class="math inline">\(m =\)</span> 500,000 and <span class="math inline">\(n =\)</span> 448,000, containing 50,400,000 nonzero values.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
The SMVM benchmark multiplies an $m \times n$ matrix with an $n \times
1$ dense vector. Our sparse matrix is stored in the compressed
sparse-row format. The program contains parallelism both between dot
products and within individual dot products. We use a sparse matrix
of dimension $m =$ 500,000 and $n =$ 448,000, containing 50,400,000
nonzero values.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::benchmark
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::benchmark
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>The DMM benchmark multiplies two dense, square <span class="math inline">\(n \times n\)</span> matrices using the recursive divide-and-conquer algorithm of Frens and Wise <span class="citation" data-cites="FrensWiseAutoblockingDMM"></span>. We have recursion go down to scalar elements. The algorithm runs with <span class="math inline">\(O(n^3)\)</span> raw work and <span class="math inline">\(O(\log  
n)\)</span> raw depth. We selected <span class="math inline">\(n=\)</span> 512.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
The DMM benchmark multiplies two dense, square $n \times n$ matrices
using the recursive divide-and-conquer algorithm of Frens and
Wise~\cite{FrensWiseAutoblockingDMM}. We have recursion go down to
scalar elements. The algorithm runs with $O(n^3)$ raw work and $O(\log
n)$ raw depth. We selected $n=$ 512.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->


</segment> <!-- paragraph -->

<segment name='paragraph'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Implementing complexity functions.
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Implementing complexity functions.
]]>
</field> <!-- title_src -->
<field name='label'>
sec:oracle::implementing-complexity-functions
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::make
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::make
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Our aim is to make complexity functions fast, ideally constant time, so that we can keep oracle costs low. But observe that, in order to complete in constant time, the complexity function needs access to the input size in constant time. For four of our benchmark programs, no modifications to the algorithm is necessary, because the relevant data structures are already decorated with sufficient size information. The only one for which we make special provisions is SMVM. The issue concerns a subproblem of SMVM called segmented sums <span class="citation" data-cites="cray-mvmult"></span>. In segmented sums, our input is an array of arrays of scalars, <em>e.g.</em>, <span class="math display">\[[[8, 3, 9], [2], [3, 1] [5]]\]</span> whose underlying representation is in segmented format. The segmented format consists of a pair of arrays, where the first array contains all the elements of the subarrays and second contains the lengths of the subarrays. <span class="math display">\[([8, 3, 9, 2, 3, 1, 5], [3, 1, 2, 1])\]</span> The second array is called the segment descriptor. The objective is to compute the sum of each subarray, <span class="math display">\[[20, 2, 4, 5],\]</span> There are two sources of parallelism in segmented sums: (1) within the summation of each subarray and (2) between different subarray sums. We use divide-and-conquer algorithms to solve each case. In the first case, our algorithm is just an array summation, and thus the complexity function is straightforward to compute in constant time from the segment descriptor. The second case is where we make the special provisions. For this case, we use a parallel array-map algorithm to compute all the subarray sums in parallel. The issue is that the complexity of performing a group of subarray sums is proportional to the sum of the sizes of those subarrays. So, to obtain this size information in constant time, we modify our segmented-array representation slightly so that we store a cached tree of subarray sizes rather than just a flat array of subarray sizes.</p>
<p><span class="math inline">\(([8, 3, 9, 2, 3, 1, 4, 5],\)</span> .4 [2 1 ].3 ] <span class="math inline">\()\)</span></p>
<p>To summarize, in order to write a constant-time complexity function, we changed the existing SMVM program to use a tree data structure, where originally there was an array data structure. Building the tree can be done in parallel, and the cost of building can be amortized away by reusing the sparse matrix multiple times, as is typically done in iterative solvers.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Our aim is to make complexity functions fast, ideally constant time,
so that we can keep oracle costs low. But observe that, in order to
complete in constant time, the complexity function needs access to the
input size in constant time. For four of our benchmark programs, no
modifications to the algorithm is necessary, because the relevant data
structures are already decorated with sufficient size information.
The only one for which we make special provisions is SMVM. The issue
concerns a subproblem of SMVM called segmented
sums~\cite{cray-mvmult}. In segmented sums, our input is an array of
arrays of scalars, \textit{e.g.},
\begin{displaymath}
[[8, 3, 9], [2], [3, 1] [5]]
\end{displaymath}
whose underlying representation is in segmented format. The segmented
format consists of a pair of arrays, where the first array contains
all the elements of the subarrays and second contains the lengths of
the subarrays.
\begin{displaymath}
([8, 3, 9, 2, 3, 1, 5], [3, 1, 2, 1])
\end{displaymath}
The second array is called the segment descriptor. The objective is to
compute the sum of each subarray,
\begin{displaymath}
[20, 2, 4, 5],
\end{displaymath}
There are two sources of parallelism in segmented sums: (1) within
the summation of each subarray and (2) between different subarray
sums. We use divide-and-conquer algorithms to solve each case. In the
first case, our algorithm is just an array summation, and
thus the complexity function is straightforward to
compute in constant time from the segment descriptor. The second
case is where we make the special provisions. For this case, we use a
parallel array-map algorithm to compute all the subarray sums in
parallel. The issue is that the complexity of performing a group of subarray
sums is proportional to the sum of the sizes of those subarrays. So,
to obtain this size information in constant time, we modify our
segmented-array representation slightly so that we store a cached tree
of subarray sizes rather than just a flat array of subarray sizes.
\begin{center}
$([8, 3, 9, 2, 3, 1, 4, 5],$ \Tree [.7 [ 3 1 ].4 [2 1 ].3 ] $)$
\end{center}
To summarize, in order to write a constant-time complexity function,
we changed the existing SMVM program to use a tree data structure,
where originally there was an array data structure. Building the tree
can be done in parallel, and the cost of building can be amortized
away by reusing the sparse matrix multiple times, as is typically done
in iterative solvers.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::speedup--vs--processors--for--barnes-hut--size
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='figure'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
fig:comparison-ws-oracle
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p><embed src="pictures/speedup--vs--processors--for--barnes-hut--size=100000--param=0--optim=0--machine=srv-53-07.pdf" style="width:2.5in" /><br />
<embed src="pictures/speedup--vs--processors--for--quicksort--size=2000000--param=0--optim=0--machine=srv-53-07.pdf" style="width:2.5in" /><br />
<embed src="pictures/speedup--vs--processors--for--quickhull--size=3000000--param=0--optim=0--machine=srv-53-07.pdf" style="width:2.5in" /><br />
<embed src="pictures/speedup--vs--processors--for--dmm--size=9--param=0--optim=0--machine=srv-53-07.pdf" style="width:2.5in" /><br />
<embed src="pictures/speedup--vs--processors--for--smvm--size=50000000--param=0--optim=0--machine=srv-53-07.pdf" style="width:2.5in" /><br />
 <br />
<span>Comparison between work stealing and oracle.</span> </p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\vspace*{-0.5in}
\centering

      \smallfig{pictures/speedup--vs--processors--for--barnes-hut--size=100000--param=0--optim=0--machine=srv-53-07.pdf}


      \smallfig{pictures/speedup--vs--processors--for--quicksort--size=2000000--param=0--optim=0--machine=srv-53-07.pdf}


      \smallfig{pictures/speedup--vs--processors--for--quickhull--size=3000000--param=0--optim=0--machine=srv-53-07.pdf}


      \smallfig{pictures/speedup--vs--processors--for--dmm--size=9--param=0--optim=0--machine=srv-53-07.pdf}


      \smallfig{pictures/speedup--vs--processors--for--smvm--size=50000000--param=0--optim=0--machine=srv-53-07.pdf}

\caption{Comparison between work stealing and oracle.}
\label{fig:comparison-ws-oracle}
]]>
</field> <!-- body_src -->

</atom> <!-- figure -->

</segment> <!-- cluster -->


</segment> <!-- paragraph -->

<segment name='paragraph'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Performance.
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Performance.
]]>
</field> <!-- title_src -->
<field name='label'>
sec:oracle::performance
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::every
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::every
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>For every benchmark, we measure several values. <span class="math inline">\(T_\textsf{seq}\)</span> denotes the time to execute the sequential version of the program. We obtain the sequential version of the program by replacing each parallel tuple with an ordinary tuple and erasing complexity functions, so that the sequential version includes none of the task-creation overheads. <span class="math inline">\(T_\textsf{par}^P\)</span> denotes the execution time with work stealing on <span class="math inline">\(P\)</span> processors. <span class="math inline">\(T_\textsf{orc}^P\)</span> denotes the execution time of our oracle-based work stealing on <span class="math inline">\(P\)</span> processors.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
For every benchmark, we measure several values.
$T_\sseq$ denotes
the time to execute the sequential version of the program.
We obtain the sequential version of the program by replacing each 
parallel tuple with an ordinary tuple and erasing complexity functions,
so that the sequential version includes none of the task-creation overheads.
$T_\spar^P$ denotes the execution time with work stealing on $P$ processors.
$T_\sorc^P$ denotes the execution time of our 
oracle-based work stealing on $P$ processors.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::results
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::results
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>The most important results of our experiments come from comparing plain work stealing and our oracle-based work stealing side by side. Figure <a href="#fig:comparison-oracle-work-stealing" data-reference-type="ref" data-reference="fig:comparison-oracle-work-stealing">[fig:comparison-oracle-work-stealing]</a> shows the speedup on sixteen processors for each of our benchmarks, that is, the values <span class="math inline">\(T_\textsf{par}^{16}/T_\textsf{seq}\)</span> and <span class="math inline">\(T_\textsf{orc}^{16}/T_\textsf{seq}\)</span>. The speedups show that, on sixteen cores, our oracle implementation is always between 4% and 76% faster than work stealing.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
The most important results of our experiments come from comparing
plain work stealing and our oracle-based work stealing side by
side. Figure \ref{fig:comparison-oracle-work-stealing} shows the
speedup on sixteen processors for each of our benchmarks,
that is, the values $T_\spar^{16}/T_\sseq$ and $T_\sorc^{16}/T_\sseq$.
The speedups show that, on sixteen cores, our oracle implementation is
always between 4\% and 76\% faster than work stealing.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::benchmarks
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::benchmarks
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>The fact that some benchmarks benefit more from our oracle implementation than others is explained by Figure <a href="#fig:comparison-sequential-oracle-work-stealing" data-reference-type="ref" data-reference="fig:comparison-sequential-oracle-work-stealing">[fig:comparison-sequential-oracle-work-stealing]</a>. This plot shows execution time for one processor, normalized with respect to the sequential execution times. In other words, the values plotted are 1, <span class="math inline">\(T_\textsf{orc}^{1}/T_\textsf{seq}\)</span> and <span class="math inline">\(T_\textsf{par}^{1}/T_\textsf{seq}\)</span>. The values <span class="math inline">\(T_\textsf{orc}^{1}/T_\textsf{seq}\)</span> range from 1.03 to 1.13 (with an average of <span class="math inline">\(1.07\)</span>), indicating that the task-creation overheads in the oracle implementation do not exceed 13% of the raw work in any benchmark. The cases where we observe large improvements in speedup are the same cases where there is a large difference bewteen sequential execution time and plain work-stealing execution time. When the difference is large, there is much room for our implementation to improve on work stealing, whereas when the difference is small we can only improve the execution time by a limited factor.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
The fact that some benchmarks benefit more from our oracle implementation 
than others is explained by Figure
\ref{fig:comparison-sequential-oracle-work-stealing}. This plot shows
execution time for one processor, normalized with respect to the  
sequential execution times. In other words, the values plotted
are 1, $T_\sorc^{1}/T_\sseq$ and $T_\spar^{1}/T_\sseq$.
The values $T_\sorc^{1}/T_\sseq$ range from 1.03 to 1.13 (with an average
of $1.07$), indicating that the task-creation overheads in the oracle 
implementation do not exceed 13\% of the raw work in any benchmark.
The cases where we observe large improvements in speedup are the same
cases where there is a large difference bewteen sequential execution
time and plain work-stealing execution time. When the difference is
large, there is much room for our implementation to improve on work
stealing, whereas when the difference is small
we can only improve the execution time by a limited factor.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::figure
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::figure
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Figure <a href="#fig:comparison-ws-oracle" data-reference-type="ref" data-reference="fig:comparison-ws-oracle">[fig:comparison-ws-oracle]</a> shows speedup curves for each of our experiments, that is, values of <span class="math inline">\(T_\textsf{par}^{P}/T_\textsf{seq}\)</span> and <span class="math inline">\(T_\textsf{orc}^{P}/T_\textsf{seq}\)</span> against the number of processors <span class="math inline">\(P\)</span> on our Intel machine; the measurements on the AMD machine show similar trends but quantitatively better results for the oracle versions.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Figure \ref{fig:comparison-ws-oracle} shows speedup curves for each of
our experiments, that is, values of $T_\spar^{P}/T_\sseq$ and 
$T_\sorc^{P}/T_\sseq$ against the number of processors~$P$ on our
Intel machine; the measurements on the AMD machine show similar trends
but quantitatively better results for the oracle versions.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::curves
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::curves
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>The curves show that our oracle implementation generally scales well up to sixteen processors.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
The curves show that our oracle implementation generally scales
well up to sixteen processors.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::exception
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::exception
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>There is one exception, which is the quickhull benchmark on the AMD machine. For this benchmark, the curve tails off after reaching twelve processors. We need to conduct further experiments to understand the cause, which is probably due to a lack of parallelism in the program. Notice, however, that our scheduler does not fall below work stealing.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
There is one exception, which is the quickhull benchmark on the AMD
machine. For this benchmark, the curve tails off after reaching twelve
processors. We need to conduct further experiments to understand the
cause, which is probably due to a lack of parallelism in the program.
Notice, however, that our scheduler does not fall below work stealing.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::speedup--vs--proc--for--param
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::pictures
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[

]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\begin{comment}

\begin{figure}
\begin{center}
\smallfig{pictures/speedup--vs--proc--for--param=0--optim=0--machine=srv-53-07.pdf}
\end{center}
\caption{Speedup curves}
\label{fig:speedups-all}
\end{figure}

~\\
Some graphs:
\begin{ul}
\item completion time against cut-off value
\item completion time against nb processors, comparing with other approaches
\item completion time against input size, comparing with sequential and sequential/P
\end{ul}

\paragraph{Selection of the cut-off constant.}
\label{sec:}

To ensure scheduling costs smaller than 5\%, we chose $\Ga$ to be 20.
In this case, we achieve 90\% of the theoretical linear speedup,
as soon as the parallel slackness $\frac{W}{PD}$ exceeds 20 times the constant $k$.
For example, if we have an algorithm with $n$ work and $\log n$ depth,
running on a 32-core machines where task creation costs 100 basic operations,
then we obtain the 90\% of the best possible speedup when $n$ becomes greater than 1 million. 

\begin{ul}

\item Running a task sequentially is more efficient with respect to cache performance
and garbage-collection performance. Moreover, it saves the need to compute cost functions.
\end{ul}

\end{comment}
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::nested
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::nested
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[

]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\begin{comment}
\item Algorithms with nested parallelism usually feature a very large amount of parallelism,
so even if we reduce the amount of parallelism it is likely that there remains enough 
parallelism to keep all the processors busy at all time.

In practice, the forking time $\Gt$ is roughly equivalent to the time of
computing $500$ basic operations, and we select a value of $\Ga$
equal to $20$. This leads to a cut-off at $10,000$ basic operations.

[Speculative idea: we could dynamically lower the value of $\Ga$ when we 
see that many processors are idle. To be tried out.]
\item Creating a parallel task always induce an immediate cost, whereas
reducing the number of opportunities for parallelism in a computation is only a potential cost.
This potential cost only becomes real when
(1) there is at least one idle processor at the time of running the current computation, and 
(2) the current computation belongs to a critical path for total completion time.
\end{comment}
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->


</segment> <!-- paragraph -->

</segment> <!-- section -->

<segment name='section'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Related Work
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Related Work
]]>
</field> <!-- title_src -->
<field name='label'>
sec:related-work
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->


<segment name='paragraph'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Cutting off excess parallelism.
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Cutting off excess parallelism.
]]>
</field> <!-- title_src -->
<field name='label'>
sec:oracle::cutting-off-excess-parallelism
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::study
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::study
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>This study is not the first to propose using cost prediction to determine when to cut off parallelism. One approach, developed in early work in functional programing, uses list size to determine cut offs <span class="citation" data-cites="HuelsbergenLaAi94"></span>. Using list size alone is limited, because the technique assumes linear work complexity for every parallel operation.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
This study is not the first to propose using cost prediction to
determine when to cut off parallelism.  One approach, developed in
early work in functional programing, uses list size to determine cut
offs~\cite{HuelsbergenLaAi94}. Using list size alone is limited,
because the technique assumes linear work complexity for every
parallel operation.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::handle
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::handle
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Another way to handle cost prediction is to use the depth and height of the recursion tree <span class="citation" data-cites="Weening89 PehoushekWe90"></span>. But depth and height are not, in general, the most direct means to predict the execution time of subcomputations. In our oracle scheduling, we ask for either the programmer or compiler to provide for each function a cost function that expresses the asymptotic cost of applying the function.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Another way to handle cost prediction is to use the depth and height
of the recursion tree~\cite{Weening89,PehoushekWe90}. But depth and
height are not, in general, the most direct means to predict the
execution time of subcomputations. In our oracle scheduling, we ask
for either the programmer or compiler to provide for each function a
cost function that expresses the asymptotic cost of applying the
function.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::lopez
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::lopez
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Lopez <em>et. al.</em> take this approach as well, but in the context of logic programming <span class="citation" data-cites="Lopez:1996:MGC:241129.241165"></span>. On the surface, their technique is similar to our oracle scheduling, except that their cost estimators do not utilize profiling to estimate constant factors. An approach without constant-factor estimation is overly simplistic for modern processors, because it relies on complexity function predicting execution time exactly. On modern processors, execution time depends heavily on factors such as caching, pipelining, <em>etc.</em> and it is not feasible in general to predict execution time from a complexity function alone.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Lopez \textit{et. al.} take this approach as well, but in the context
of logic programming~\cite{Lopez:1996:MGC:241129.241165}. On the
surface, their technique is similar to our oracle scheduling, except
that their cost estimators do not utilize profiling to estimate
constant factors. An approach without constant-factor estimation is
overly simplistic for modern processors, because it relies on
complexity function predicting execution time exactly. On modern
processors, execution time depends heavily on factors such as caching,
pipelining, \textit{etc.} and it is not feasible in general to predict
execution time from a complexity function alone.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->


</segment> <!-- paragraph -->

<segment name='paragraph'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Reducing per-task costs.
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Reducing per-task costs.
]]>
</field> <!-- title_src -->
<field name='label'>
sec:oracle::reducing-per-task-costs
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::approach
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::approach
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>One approach to the granularity problem is to focus on reducing the costs associated with tasks, rather than limiting how many tasks get created. This approach is taken by implementations of work stealing with lazy task creation <span class="citation" data-cites="lazy-task-creation message-passing-ltc FrigoLeRa98 rainey:phd backtracking-based-load-balancing SanchezADM"></span>. In lazy task creation, the work stealing scheduler is implemented so as to avoid, in the common case, the major scheduling costs, in particular, those of inter-processor communication. But, in even the most efficient lazy task creation, there is still a non-negligable scheduling cost for each implicit thread.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
One approach to the granularity problem is to focus on reducing
the costs associated with tasks, rather than limiting how many tasks
get created. This approach is taken by implementations of work
stealing with lazy task
creation~\cite{lazy-task-creation,message-passing-ltc,FrigoLeRa98,
  rainey:phd,backtracking-based-load-balancing,SanchezADM}. In lazy
task creation, the work stealing scheduler is implemented so as to
avoid, in the common case, the major scheduling costs, in particular,
those of inter-processor communication. But, in even the most
efficient lazy task creation, there is still a non-negligable
scheduling cost for each implicit thread.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::lazy
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::lazy
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Lazy Binary Splitting (LBS) is an improvement to lazy task creation that applies to parallel loops <span class="citation" data-cites="lazy-binary-splitting"></span>. The crucial optimization comes from extending the representation of a task so that multiple loop iterations can be packed into a single task. This representation enables the scheduler to both avoid creating closures and executing deque operations for most iterations. A limitation of LBS is that it addresses only parallel loops whose iteration space is over integers. Lazy Tree Splitting (LTS) generalizes LBS to handle parallel aggregate operations that produce and consume trees, such as map and reduce <span class="citation" data-cites="BergstromFlRaReSh10"></span>. LTS is limited, however, by the fact that it requires a special cursor data structure to be defined for each tree data structure.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Lazy Binary Splitting (LBS) is an improvement to lazy task creation
that applies to parallel loops~\cite{lazy-binary-splitting}. The
crucial optimization comes from extending the representation of a task
so that multiple loop iterations can be packed into a single
task. This representation enables the scheduler to both avoid creating
closures and executing deque operations for most iterations.
A limitation of LBS is that it addresses only parallel loops whose
iteration space is over integers.  Lazy Tree Splitting (LTS)
generalizes LBS to handle parallel aggregate operations that produce
and consume trees, such as map and
reduce~\cite{BergstromFlRaReSh10}. LTS is limited, however, by the
fact that it requires a special cursor data structure to be defined
for each tree data structure.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->


</segment> <!-- paragraph -->

<segment name='paragraph'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Amortizing per-task costs.
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Amortizing per-task costs.
]]>
</field> <!-- title_src -->
<field name='label'>
sec:oracle::amortizing-per-task-costs
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::feitelson
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::feitelson
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Feitelson <em>et al.</em> study the granularity problem in the setting of distributed computing <span class="citation" data-cites="aharoni92arun-time"></span>, where the crucial issue is how to minimize the cost of inter-processor communication. In their setting, the granularity problem is modeled as a staging problem, in which there are two stages. The first stage consists of a set of processor-local task pools and the second stage consists of a global task pool. Moving a task to the global task pool requires inter-processor communication. The crucial decision is how often each processor should promote tasks from its local task pool to the global task pool. We consider a different model of staging in which there is one stage for parallel evaluation and one for sequential evaluation.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Feitelson \textit{et al.} study the granularity problem in the setting
of distributed computing~\cite{aharoni92arun-time}, where the crucial
issue is how to minimize the cost of inter-processor communication. In
their setting, the granularity problem is modeled as a staging
problem, in which there are two stages. The first stage consists of a
set of processor-local task pools and the second stage consists of a
global task pool. Moving a task to the global task pool requires
inter-processor communication. The crucial decision is how often each
processor should promote tasks from its local task pool to the global
task pool. We consider a different model of staging in which there is
one stage for parallel evaluation and one for sequential
evaluation.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::proposed
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::proposed
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>The approach proposed by Feitelson <em>et al.</em> is based on an online algorithm called CG. In this approach, it is assumed that the cost of moving a task to the global task pool is an integer constant, called <span class="math inline">\(g\)</span>. The basic idea is to use amortization to reduce the scheduling total cost of moving tasks to the global task pool. In particular, for each task that is moved to the global task pool, CG ensures that there are at least <span class="math inline">\(g+1\)</span> tasks added to the local task pool. Narlikar describes a similar approach based on an algorithm called DFDeques <span class="citation" data-cites="Narlikar99"></span>. Just as with work stealing, even though the scheduler can avoid the communication costs in the common case, the scheduler still has to pay a non-negligable cost for each implicit thread.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
The approach proposed by Feitelson \textit{et al.} is based on an
online algorithm called CG. In this approach, it is assumed that the
cost of moving a task to the global task pool is an integer constant,
called $g$. The basic idea is to use amortization to reduce the
scheduling total cost of moving tasks to the global task pool. In
particular, for each task that is moved to the global task pool, CG
ensures that there are at least $g+1$ tasks added to the local task
pool. Narlikar describes a similar approach based on an algorithm
called DFDeques~\cite{Narlikar99}. Just as with work
stealing, even though the scheduler can avoid the communication costs
in the common case, the scheduler still has to pay a non-negligable
cost for each implicit thread.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->


</segment> <!-- paragraph -->

<segment name='paragraph'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Flattening and fusion.
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Flattening and fusion.
]]>
</field> <!-- title_src -->
<field name='label'>
sec:oracle::flattening-and-fusion
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::flattening
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::flattening
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Flattening is a well-known program transformation for nested parallel languages <span class="citation" data-cites="Blelloch:1990:CCL:78246.78250"></span>. Implementations of flattening include NESL <span class="citation" data-cites="nesl-implement"></span> and Data Parallel Haskell <span class="citation" data-cites="PeytonJones08"></span>. Flattening transforms the program into a form that maps well onto SIMD architectures. Flattened programs are typically much simpler to schedule at run time than nested programs, because much of the schedule is predetermined by the flattening <span class="citation" data-cites="spoonhower:phd"></span>. Controlling the granularity of such programs is correspondingly much simpler than in general. A limitation of existing flattening is that certain classes of programs generated by the translation suffer from space inefficiency <span class="citation" data-cites="BlellochGr96"></span>, as a consequence of the transformation making changes to data structures defined in the program. Our transformation involves no such changes.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Flattening is a well-known program transformation for nested parallel
languages~\cite{Blelloch:1990:CCL:78246.78250}. Implementations of
flattening include NESL~\cite{nesl-implement} and Data Parallel
Haskell~\cite{PeytonJones08}. Flattening transforms the program into a
form that maps well onto SIMD architectures. Flattened programs are
typically much simpler to schedule at run time than nested programs,
because much of the schedule is predetermined by the
flattening~\cite{spoonhower:phd}. Controlling the
granularity of such programs is correspondingly much simpler than in
general. A limitation of existing flattening is that certain classes
of programs generated by the translation suffer from space
inefficiency~\cite{BlellochGr96}, as a consequence of the
transformation making changes to data structures defined in the
program. Our transformation involves no such changes.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::nesl
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::nesl
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>The NESL <span class="citation" data-cites="nesl-implement"></span> and Data Parallel Haskell <span class="citation" data-cites="PeytonJones08"></span> compilers implement fusion transformation in order to increase granularity. Fusion transforms the program to eliminate redundant synchronization points and intermediate arrays. Although fusion reduces scheduling costs by combining adjacent parallel loops, it is not relevant to controlling granularity within loops. As such, fusion is orthogonal to our oracle based approach.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
The NESL~\cite{nesl-implement} and Data Parallel
Haskell~\cite{PeytonJones08} compilers implement fusion transformation
in order to increase granularity. Fusion transforms the program to
eliminate redundant synchronization points and intermediate
arrays. Although fusion reduces scheduling costs by combining adjacent
parallel loops, it is not relevant to controlling granularity within
loops. As such, fusion is orthogonal to our oracle based approach.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->


</segment> <!-- paragraph -->

<segment name='paragraph'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Cost Semantics.
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Cost Semantics.
]]>
</field> <!-- title_src -->
<field name='label'>
sec:oracle::cost
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::accurate
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::accurate
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>To give an accurate accounting of task-creation of overheads in implicitly parallel languages we use a cost semantics, where evaluation steps (derivation rules) are decorated with work and depth information or “costs”. This information can then be used to directly to bound running time on parallel computers by using standard scheduling theorems that realize Brent’s bound. Many previous approaches also use the same technique to study work-depth properties, some of which also make precise the relationship between cost semantics and the standard directed-acyclic-graph models <span class="citation" data-cites="BlellochGr95 BlellochGr96 SpoonhowerBlHaGi08"></span>. The idea of instrumenting evaluations to generate cost information goes back to the early 90s <span class="citation" data-cites="Sands-thesis Rosendahl89"></span>.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
To give an accurate accounting of task-creation of overheads in
implicitly parallel languages we use a cost semantics, where
evaluation steps (derivation rules) are decorated with work and depth
information or ``costs''.  This information can then be used to
directly to bound running time on parallel computers by using standard
scheduling theorems that realize Brent's bound.  Many previous
approaches also use the same technique to study work-depth properties,
some of which also make precise the relationship between cost
semantics and the standard directed-acyclic-graph
models~\cite{BlellochGr95,BlellochGr96,SpoonhowerBlHaGi08}. The idea
of instrumenting evaluations to generate cost information goes back to
the early 90s~\cite{Sands-thesis,Rosendahl89}.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->


</segment> <!-- paragraph -->

<segment name='paragraph'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Inferring Complexity Bounds.
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Inferring Complexity Bounds.
]]>
</field> <!-- title_src -->
<field name='label'>
sec:oracle::inferring-complexity-bounds
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::requires
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::requires
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Our implementation of oracle scheduling requires the programmer to enter complexity bounds for all parallel tasks. In some cases, these bounds can be inferred by various static analyses, for example, using type-based and other static analyses (e.g., <span class="citation" data-cites="CraryWe00 JostHaLoHo10"></span>), symbolic techniques (e.g., <span class="citation" data-cites="GoldsmithAiWi07 GulwaniMeCh09"></span>). Our approach can benefit from these approaches by reducing the programmer burden, making it ultimately easier to use the proposed techniques in practice.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Our implementation of oracle scheduling requires the programmer to
enter complexity bounds for all parallel tasks.  In some cases, these
bounds can be inferred by various static analyses, for example, using
type-based and other static analyses
(e.g.,~\cite{CraryWe00,JostHaLoHo10}), symbolic
techniques
(e.g.,~\cite{GoldsmithAiWi07,GulwaniMeCh09}). Our
approach can benefit from these approaches by reducing the programmer
burden, making it ultimately easier to use the proposed techniques in
practice.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::presented
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::presented
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[

]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\begin{comment}
We have presented a general solution to a fundamental problem
arising in parallel computations, namely the granularity problem.
The key contribution of this paper is the observation that 
complexity cost functions provided by the programmer are sufficient
for making good run-time decision about whether to exploit or ignore
the parallelism available in a program.
We have argued for the interest of our approach both
through the presentation of theorems establishing bounds
on the costs of scheduling and through experiments reporting
significant speedups compared with other scheduling techniques.
\end{comment}
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->


</segment> <!-- paragraph -->

</segment> <!-- section -->

<segment name='section'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Conclusion
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Conclusion
]]>
</field> <!-- title_src -->
<field name='label'>
sec:oracle::conclusion
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::propose
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::propose
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>In this paper, we propose a solution to the granularity-control problem. We prove that an oracle that can approximate the sizes of parallel tasks in constant time within a constant factor of accuracy can be used to reduce the task creation overheads to any desired constant fraction for a reasonably broad class of computations. We describe how such an oracle can be integrated with any scheduler to support what we call oracle scheduling. We realize oracle scheduling in practice by requiring the programmer to enter asymptotic complexity annotations for parallel tasks and by judicious use of run-time profiling. Consistently with our theoretical analysis, our experiments show that oracle scheduling can reduce task creation overheads to a small fraction of the sequential time without hurting parallel scalability.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
In this paper, we propose a solution to the granularity-control
problem.  We prove that an oracle that can approximate the sizes of
parallel tasks in constant time within a constant factor of accuracy
can be used to reduce the task creation overheads to any desired
constant fraction for a reasonably broad class of computations.  We
describe how such an oracle can be integrated with any scheduler to
support what we call oracle scheduling.  We realize oracle scheduling
in practice by requiring the programmer to enter asymptotic complexity
annotations for parallel tasks and by judicious use of run-time
profiling.  Consistently with our theoretical analysis, our
experiments show that oracle scheduling can reduce task creation
overheads to a small fraction of the sequential time without hurting
parallel scalability.
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:oracle::plain
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:oracle::plain
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[

]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\bibliographystyle{plain}
\bibliography{biblio,../../bibliography/main}
]]>
</field> <!-- body_src -->

</atom> <!-- gram -->

</segment> <!-- cluster -->


</segment> <!-- section -->

</segment> <!-- chapter -->
