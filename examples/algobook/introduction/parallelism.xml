<?xml version="1.0" encoding="UTF-8"?>
<segment name='chapter'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Parallelism
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Parallelism
]]>
</field> <!-- title_src -->
<field name='label'>
ch:introduction::parallelism
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:grm:introduction::parallelism::term
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='pl'>
...NOT.PROVIDED.PL...
</field> <!-- pl -->
<field name='pl_version'>
...NOT.PROVIDED.PL_VERSION...
</field> <!-- pl_version -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:introduction::parallelism::term
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>The term “parallelism” or “parallel computing” refers to the ability to run multiple computations (tasks) at the same time. Today parallelism is available in all computer systems, and at many different scales starting with parallelism in the nano-circuits that implement individual instructions, and working the way up to parallel systems that occupy large data centers.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
The term ``parallelism'' or ``parallel computing'' refers to the ability to run multiple computations (tasks) at the same time.
Today parallelism is available in all computer systems, and at many different scales starting with parallelism in the nano-circuits that implement individual instructions, and working the way up to parallel systems that occupy large data centers.
]]>
</field> <!-- body_src -->

<field name='caption'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption -->
<field name='caption_src'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption_src -->
</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:tch:introduction::parallelism::care
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='teachask'>
<field name='pl'>
...NOT.PROVIDED.PL...
</field> <!-- pl -->
<field name='pl_version'>
...NOT.PROVIDED.PL_VERSION...
</field> <!-- pl_version -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
tch:introduction::parallelism::care
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Why should we care about parallelism? Do we even have parallel computers today?</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Why should we care about parallelism?  
Do we even have parallel computers today?
]]>
</field> <!-- body_src -->

<field name='caption'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption -->
<field name='caption_src'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption_src -->
</atom> <!-- teachask -->

</segment> <!-- cluster -->


<segment name='section'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Parallel Hardware
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Parallel Hardware
]]>
</field> <!-- title_src -->
<field name='label'>
sec:introduction::parallelism::parallel-hardware
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:grm:introduction::parallelism::multicore-chips
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='pl'>
...NOT.PROVIDED.PL...
</field> <!-- pl -->
<field name='pl_version'>
...NOT.PROVIDED.PL_VERSION...
</field> <!-- pl_version -->
<field name='title'>
<![CDATA[
Multicore Chips
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Multicore Chips
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:introduction::parallelism::multicore-chips
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Since the early 2000s hardware manufacturers have been placing multiple processing units, often called “cores”, onto a single chip. These cores can be general purpose processors, or more special purpose processors, such as those found in  <strong><em>Graphics Processing Units</em></strong> (GPUs). Each core can run in parallel with the others. Today (in year 2018), multicore chips are used in essentially all computing devices ranging from mobile phones to desktop computers and servers.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Since the early 2000s
hardware manufacturers have been placing multiple processing units,
often called ``cores'', onto a single chip.  These cores can be
general purpose processors, or more special purpose processors, such as
those found in~\defn{Graphics Processing Units} (GPUs).  
Each core can run in parallel with the others.
Today (in year 2018), multicore chips are used in essentially all
computing devices ranging from mobile phones to desktop computers and
servers.
]]>
</field> <!-- body_src -->

<field name='caption'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption -->
<field name='caption_src'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption_src -->
</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:grm:introduction::parallelism::large-scale-parallelism
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='pl'>
...NOT.PROVIDED.PL...
</field> <!-- pl -->
<field name='pl_version'>
...NOT.PROVIDED.PL_VERSION...
</field> <!-- pl_version -->
<field name='title'>
<![CDATA[
Large-Scale Parallelism
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Large-Scale Parallelism
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:introduction::parallelism::large-scale-parallelism
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>At the larger scale, many computers can be connected by a network and used together to solve large problems. For example, when you perform a simple search on the Internet, you engage a data center with thousands of computers in some part of the world, likely near your geographic location. Many of these computers (perhaps as many as hundreds, if not thousands) take up your query and sift through data to give you an accurate response as quickly as possible. Because each computer can itself be parallel (e.g., built with multicore chips), the scale of parallelism can be quite large, e.g., in the thousands.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
At the larger scale, many computers can be connected by a network and
used together to solve large problems.  For example, when you perform
a simple search on the Internet, you engage a data center with
thousands of computers in some part of the world, likely near your
geographic location.
Many of these computers (perhaps as many as hundreds, if not
thousands) take up your query and sift through data to give you an
accurate response as quickly as possible.
Because each computer can itself be parallel (e.g., built with
multicore chips), the scale of parallelism can be quite large, e.g.,
in the thousands.
]]>
</field> <!-- body_src -->

<field name='caption'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption -->
<field name='caption_src'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption_src -->
</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:grm:introduction::parallelism::fundamental-reasons-for-why-parallelism-matters
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='pl'>
...NOT.PROVIDED.PL...
</field> <!-- pl -->
<field name='pl_version'>
...NOT.PROVIDED.PL_VERSION...
</field> <!-- pl_version -->
<field name='title'>
<![CDATA[
Fundamental Reasons for Why Parallelism Matters
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Fundamental Reasons for Why Parallelism Matters
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:introduction::parallelism::fundamental-reasons-for-why-parallelism-matters
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>There are several reasons for why parallelism has become prevalent over the past decade.</p>
<p>First, parallel computing is simply faster than sequential computing. This is important, because many tasks must be completed quickly to be of use. For example, to be useful, an Internet search should complete in “interactive speeds” (usually below <span class="math inline">\(100\)</span> milliseconds). Similarly, a weather-forecast simulation is essentially useless if it cannot be completed in time.</p>
<p>The second reason is efficiency in terms of energy usage. Due to basic physics, performing a computation twice as fast sequentially requires eight times as much energy (energy consumption is a cubic function of clock frequency). With parallelism we don’t need to use more energy than sequential computation, because energy is determined by the total amount of computation (work).</p>
<p>These two factors—time and energy—have become increasingly important in the last decade.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
There are several reasons for why parallelism has become prevalent
over the past decade.

First, parallel computing is simply faster than sequential computing.
This is important, because many tasks must be completed quickly to be
of use. 
For example, to be useful, an Internet search should complete in
``interactive speeds'' (usually below $100$ milliseconds).
Similarly, a weather-forecast simulation is essentially useless if it
cannot be completed in time.

The second reason is efficiency in terms of energy usage.
Due to basic physics, performing a computation twice as fast
sequentially requires eight times as much energy (energy consumption is a cubic function of clock frequency). 
With parallelism we don't need to use more energy than sequential
computation, because energy is determined by the total amount of
computation (work).

These two factors---time and energy---have become increasingly
important in the last decade.
]]>
</field> <!-- body_src -->

<field name='caption'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption -->
<field name='caption_src'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption_src -->
</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:xmpl:introduction::parallelism::using
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='example'>
<field name='pl'>
...NOT.PROVIDED.PL...
</field> <!-- pl -->
<field name='pl_version'>
...NOT.PROVIDED.PL_VERSION...
</field> <!-- pl_version -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
xmpl:introduction::parallelism::using
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Using two parallel computers, we can perform a computation in half the time of a sequential computer (operating at the same speed). To this end, we need to divide the computation into two parallel sub-computations, perform them in parallel and combine their results. This can require as little as half the time as the sequential computation. Because the total computation that we must do remains the same in both sequential and parallel cases, the total energy consumed is also the same.</p>
<p>The above reasoning holds in theory. In practice, there are overheads to parallelism: the speedup will be less than two-fold and more energy will be needed. For example, dividing the computation and combining the results could lead to additional overhead. Such overhead usually diminishes as the degree of parallelism increases but not always.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Using two parallel computers, we can perform a computation in half the
time of a sequential computer (operating at the same speed).
To this end, we need to divide the computation into two parallel
sub-computations, perform them in parallel and combine their results.
This can require as little as half the time as the sequential
computation.
Because the total computation that we must do remains the same in both
sequential and parallel cases, the total energy consumed is also the same.

The above reasoning holds in theory.
In practice, there are overheads to parallelism: the speedup will be
less than two-fold and more energy will be needed.
For example, dividing the computation and combining the results could
lead to additional overhead.  
Such overhead usually diminishes as the degree of parallelism
increases but not always.
]]>
</field> <!-- body_src -->

<field name='caption'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption -->
<field name='caption_src'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption_src -->
</atom> <!-- example -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:tch:introduction::parallelism::think
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='teachask'>
<field name='pl'>
...NOT.PROVIDED.PL...
</field> <!-- pl -->
<field name='pl_version'>
...NOT.PROVIDED.PL_VERSION...
</field> <!-- pl_version -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
tch:introduction::parallelism::think
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Can you think of consequences of these developments in hardware?</p>
<p>Answer: These developments in hardware make the specification, the design, and the implementation of parallel algorithms an important topic.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Can you think of consequences of these developments in hardware?  

Answer:
These developments in hardware make the specification, the design, and
the implementation of parallel algorithms an important topic.
]]>
</field> <!-- body_src -->

<field name='caption'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption -->
<field name='caption_src'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption_src -->
</atom> <!-- teachask -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:xmpl:introduction::parallelism::historically
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='example'>
<field name='pl'>
...NOT.PROVIDED.PL...
</field> <!-- pl -->
<field name='pl_version'>
...NOT.PROVIDED.PL_VERSION...
</field> <!-- pl_version -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
xmpl:introduction::parallelism::historically
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>As is historically popular in explaining algorithms, we can establish an analogy between parallel algorithms and cooking. As in a kitchen with multiple cooks, in parallel algorithms you can do things in parallel for faster turnaround time. For example, if you want to prepare 3 dishes with a team of cooks you can do so by asking each cook to prepare one. Doing so will often be faster that using one cook. But there are some overheads, for example, the work has to be divided as evenly as possible. Obviously, you also need more resources, e.g., each cook might need their own cooking pan.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
As is historically popular in explaining algorithms, we can establish
an analogy between parallel algorithms and cooking.  As in a kitchen
with multiple cooks, in parallel algorithms you can do things in
parallel for faster turnaround time.  For example, if you want to
prepare 3 dishes with a team of cooks you can do so by asking each
cook to prepare one.
Doing so will often be faster that using one cook.  But there are some
overheads, for example, the work has to be divided as evenly as
possible.  Obviously, you also need more resources, e.g., each cook
might need their own cooking pan.
]]>
</field> <!-- body_src -->

<field name='caption'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption -->
<field name='caption_src'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption_src -->
</atom> <!-- example -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:ex:intro::intro::example-runs
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='example'>
<field name='pl'>
...NOT.PROVIDED.PL...
</field> <!-- pl -->
<field name='pl_version'>
...NOT.PROVIDED.PL_VERSION...
</field> <!-- pl_version -->
<field name='title'>
<![CDATA[
Comparison to Sequential
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Comparison to Sequential
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
ex:intro::intro::example-runs
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p> One way to quantify the advantages of parallelism is to compare its performance to sequential computation. The table below illustrates the sort of performance improvements that can achieved today. These timings are taken on a 32 core commodity server machine. In the table, the sequential timings use sequential algorithms while the parallel timings use parallel algorithms. Notice that the  <strong><em>speedup</em></strong> for the parallel 32 core version relative to the sequential algorithm ranges from approximately 12 (minimum spanning tree) to approximately 32 (sorting).</p>
<table>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Application</strong></td>
<td style="text-align: center;"><strong>Sequential</strong></td>
<td style="text-align: center;"><strong>Parallel</strong></td>
<td style="text-align: center;"><strong>Parallel</strong></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>P = 1</strong></td>
<td style="text-align: center;"><strong>P = 32</strong></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Sort <span class="math inline">\(10^7\)</span> strings</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">.095</td>
</tr>
<tr class="even">
<td style="text-align: left;">Remove duplicates for <span class="math inline">\(10^7\)</span> strings</td>
<td style="text-align: center;">.66</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">.038</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Minimum spanning tree for <span class="math inline">\(10^7\)</span> edges</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">.14</td>
</tr>
<tr class="even">
<td style="text-align: left;">Breadth first search for <span class="math inline">\(10^7\)</span> edges</td>
<td style="text-align: center;">.82</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">.046</td>
</tr>
</tbody>
</table>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\label{ex:intro::intro::example-runs}
One way to quantify the advantages of parallelism is to compare
its performance to sequential computation.
The table below illustrates the sort of performance improvements
that can achieved today.  
These timings are taken on a 32 core commodity server machine.  In the
table, the sequential timings use sequential algorithms while the
parallel timings use parallel algorithms.  Notice that
the~\defn{speedup} for the parallel 32 core version relative to the
sequential algorithm ranges from approximately 12 (minimum spanning
tree) to approximately 32 (sorting).

  \begin{tabular}{l  c c c}
    \toprule
    \textbf{Application} & \textbf{Sequential} & \textbf{Parallel} &
    \textbf{Parallel}
\\
     & & \textbf{P = 1} & \textbf{P = 32}
\\
    \midrule
    Sort $10^7$ strings &        2.9 &  2.9 &  .095\\
    Remove duplicates for $10^7$ strings &      .66 &  1.0 & .038\\
    Minimum spanning tree for $10^7$ edges    &    1.6 & 2.5  & .14\\
    Breadth first search for $10^7$ edges  &   .82  & 1.2 &  .046\\
    \bottomrule
  \end{tabular}
]]>
</field> <!-- body_src -->

<field name='caption'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption -->
<field name='caption_src'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption_src -->
</atom> <!-- example -->

</segment> <!-- cluster -->


</segment> <!-- section -->

<segment name='section'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Parallel Software
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Parallel Software
]]>
</field> <!-- title_src -->
<field name='label'>
sec:introduction::parallelism::parallel-software
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:tch:introduction::parallelism::anything
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='teachask'>
<field name='pl'>
...NOT.PROVIDED.PL...
</field> <!-- pl -->
<field name='pl_version'>
...NOT.PROVIDED.PL_VERSION...
</field> <!-- pl_version -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
tch:introduction::parallelism::anything
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>But why after all, do we have to do anything differently to take advantage of parallelism?</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
But why after all, do we have to do anything differently to take
advantage of parallelism?
]]>
</field> <!-- body_src -->

<field name='caption'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption -->
<field name='caption_src'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption_src -->
</atom> <!-- teachask -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:gr:intro::intro::parallelism::challenges
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='pl'>
...NOT.PROVIDED.PL...
</field> <!-- pl -->
<field name='pl_version'>
...NOT.PROVIDED.PL_VERSION...
</field> <!-- pl_version -->
<field name='title'>
<![CDATA[
Challenges of Parallel Software
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Challenges of Parallel Software
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
gr:intro::intro::parallelism::challenges
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p> It would be convenient to use sequential algorithms on parallel computers, but this does not work well because parallel computing requires a different way of organizing the computation. The fundamental difference is that in parallel algorithms, computations must actually be  <strong><em>independent</em></strong> to be performed in parallel. By independent we mean that computations do not depend on each other. Thus when designing a parallel algorithm, we have to identify the underlying dependencies in the computation and avoid creating unnecessary dependencies. This design challenge is an important focus of this book.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\label{gr:intro::intro::parallelism::challenges}
It would be convenient to use sequential algorithms on parallel
computers, but this does not work well because parallel computing
requires a different way of organizing the computation.
The fundamental difference is that in parallel algorithms,
computations must actually be~\defn{independent} to be performed in
parallel.
By independent we mean that computations do not depend on each other.
Thus when designing a parallel algorithm, we have to identify the
underlying dependencies in the computation and avoid creating
unnecessary dependencies.
This design challenge is an important focus of this book.
]]>
</field> <!-- body_src -->

<field name='caption'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption -->
<field name='caption_src'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption_src -->
</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:xmpl:introduction::parallelism::going
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='example'>
<field name='pl'>
...NOT.PROVIDED.PL...
</field> <!-- pl -->
<field name='pl_version'>
...NOT.PROVIDED.PL_VERSION...
</field> <!-- pl_version -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
xmpl:introduction::parallelism::going
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Going back to our cooking example, suppose that we want to make a frittata in our kitchen with 4 cooks. Making a frittata is not easy. It involves cleaning and chopping vegetables, beating eggs, sauteeing, as well as baking. For the frittata to be good, the cooks must follow a specific receipe and pay attention to the dependencies between various tasks. For example, vegetables cannot be sauteed before they are washed, and the eggs cannot be fisked before they are broken!</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Going back to our cooking example, suppose that we want to make a
frittata in our kitchen with 4 cooks.
Making a frittata is not easy.
It involves cleaning and chopping vegetables, beating eggs,
sauteeing, as well as baking.
For the frittata to be good, the cooks must follow a specific receipe
and pay attention to the dependencies between various tasks.
For example,
vegetables cannot be sauteed before they are washed, and 
the eggs cannot be fisked before they are broken!
]]>
</field> <!-- body_src -->

<field name='caption'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption -->
<field name='caption_src'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption_src -->
</atom> <!-- example -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:gr:intro::intro::parallelism::challenges-software
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='pl'>
...NOT.PROVIDED.PL...
</field> <!-- pl -->
<field name='pl_version'>
...NOT.PROVIDED.PL_VERSION...
</field> <!-- pl_version -->
<field name='title'>
<![CDATA[
Coding Parallel Algorithms
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Coding Parallel Algorithms
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
gr:intro::intro::parallelism::challenges-software
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p> Another important challenge concerns the implementation and use of parallel algorithms in the real world. The many forms of parallelism, ranging from small to large scale, and from general to special purpose, have led to many different programming languages and systems for coding parallel algorithms. These different programming languages and systems often target a particular kind of hardware, and even a particular kind of problem domain. As it turns out, one can easily spend weeks or even months optimizing a parallel sorting algorithm on specific parallel hardware, such as a multicore chip, a GPU, or a large-scale massively parallel distributed system.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\label{gr:intro::intro::parallelism::challenges-software}
Another important challenge concerns the implementation and use of
parallel algorithms in the real world.
The many forms of parallelism, ranging from small to large scale, and
from general to special purpose, have led to many different programming
languages and systems for coding parallel algorithms.
These different programming languages and systems often target a
particular kind of hardware, and even a particular kind of problem
domain.  
As it turns out, one can easily spend weeks or even months optimizing a
parallel sorting algorithm on specific parallel hardware, such as a
multicore chip, a GPU, or a large-scale massively parallel distributed
system.
]]>
</field> <!-- body_src -->

<field name='caption'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption -->
<field name='caption_src'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption_src -->
</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:grm:introduction::parallelism::maximizing
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='pl'>
...NOT.PROVIDED.PL...
</field> <!-- pl -->
<field name='pl_version'>
...NOT.PROVIDED.PL_VERSION...
</field> <!-- pl_version -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:introduction::parallelism::maximizing
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Maximizing speedup by coding and optimizing an algorithm is not the goal of this book. Instead, our goal is to cover general design principles for parallel algorithms that can be applied in essentially all parallel systems, from the data center to the multicore chips on mobile phones. We will learn to think about parallelism at a high-level, learning general techniques for designing parallel algorithms and data structures, and learning how to approximately analyze their costs. The focus is on understanding when things can run in parallel, and when not due to dependencies. There is much more to learn about parallelism, and we hope you continue studying this subject.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Maximizing speedup by coding and optimizing an algorithm is not the
goal of this book.
Instead, our goal is to cover general design principles for parallel
algorithms that can be applied in essentially all parallel systems,
from the data center to the multicore chips on mobile phones.
We will learn to think about parallelism at a high-level, learning
general techniques for designing parallel algorithms and data
structures, and learning how to approximately analyze their costs.
The focus is on understanding when things can run in parallel, and
when not due to dependencies.  
There is much more to learn about parallelism, and we hope you
continue studying this subject.
]]>
</field> <!-- body_src -->

<field name='caption'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption -->
<field name='caption_src'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption_src -->
</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:xmpl:introduction::parallelism::separate
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='example'>
<field name='pl'>
...NOT.PROVIDED.PL...
</field> <!-- pl -->
<field name='pl_version'>
...NOT.PROVIDED.PL_VERSION...
</field> <!-- pl_version -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
xmpl:introduction::parallelism::separate
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>There are separate systems for coding parallel numerical algorithms on shared memory hardware, for coding graphics algorithms on Graphical Processing Units (GPUs), and for coding data-analytics software on a distributed system. Each such system tends to have its own programming interface, its own cost model, and its own optimizations, making it practically impossible to take a parallel algorithm and code it once and for all possible applications. Indeed, it can require a significant effort to implement even a simple algorithm and optimize it to run well on a particular parallel system.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
There are separate systems for coding parallel numerical
algorithms on shared memory hardware, for coding graphics algorithms
on Graphical Processing Units (GPUs), and for coding data-analytics
software on a distributed system.
Each such system tends to have its own programming interface, its own
cost model, and its own optimizations, making it practically
impossible to take a parallel algorithm and code it once and for all 
possible applications.
Indeed, it can require a significant effort to implement even a simple
algorithm and optimize it to run well on a particular parallel system.
]]>
</field> <!-- body_src -->

<field name='caption'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption -->
<field name='caption_src'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption_src -->
</atom> <!-- example -->

</segment> <!-- cluster -->


</segment> <!-- section -->

<segment name='section'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Work, Span, Parallel Time
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Work, Span, Parallel Time
]]>
</field> <!-- title_src -->
<field name='label'>
sec:introduction::parallelism::work-span
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:grm:introduction::parallelism::describes
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='pl'>
...NOT.PROVIDED.PL...
</field> <!-- pl -->
<field name='pl_version'>
...NOT.PROVIDED.PL_VERSION...
</field> <!-- pl_version -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:introduction::parallelism::describes
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>This section describes the two measures—work and span—that we use to analyze algorithms. Together these measures capture both the sequential time and the parallelism available in an algorithm. We typically analyze both of these asymptotically, using for example the big-O notation.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
This section describes the two measures---work and span---that we use
to analyze algorithms.  Together these measures capture both the
sequential time and the parallelism available in an algorithm.
We typically analyze both of these asymptotically, using
for example the big-O notation.
]]>
</field> <!-- body_src -->

<field name='caption'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption -->
<field name='caption_src'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption_src -->
</atom> <!-- gram -->

</segment> <!-- cluster -->


<segment name='subsection'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Work and Span
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Work and Span
]]>
</field> <!-- title_src -->
<field name='label'>
sec:introduction::parallelism::work-and-span
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:grm:introduction::parallelism::work
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='pl'>
...NOT.PROVIDED.PL...
</field> <!-- pl -->
<field name='pl_version'>
...NOT.PROVIDED.PL_VERSION...
</field> <!-- pl_version -->
<field name='title'>
<![CDATA[
Work
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Work
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:introduction::parallelism::work
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>The  <strong><em>work</em></strong> of an algorithm corresponds to the total number of primitive operations performed by an algorithm. If running on a sequential machine, it corresponds to the sequential time. On a parallel machine, however, work can be divided among multiple processors and thus does not necessarily correspond to time.</p>
<p>The interesting question is to what extent can the work be divided and performed in parallel. Ideally we would like to divide the work evenly. If we had <span class="math inline">\(W\)</span> work and <span class="math inline">\(P\)</span> processors to work on it in parallel, then even division would give each processor <span class="math inline">\(\frac{W}{P}\)</span> fraction of the work, and hence the total time would be <span class="math inline">\(\frac{W}{P}\)</span>. An algorithm that achieves such ideal division is said to have  <strong><em>perfect speedup</em></strong> . Perfect speedup, however, is not always possible.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
The~\defn{work} of an algorithm corresponds to the total number of
primitive operations performed by an algorithm.  If running on a
sequential machine, it corresponds to the sequential time.
On a parallel machine, however, work can be divided among multiple
processors and thus does not necessarily correspond to time.

The interesting question is to what extent can the work be divided and
performed in parallel.  Ideally we would like to divide the work
evenly.  If we had $W$ work and $P$ processors to work on it in
parallel, then even division would give each processor $\frac{W}{P}$
fraction of the work, and hence the total time would be $\frac{W}{P}$.
An algorithm that achieves such ideal division is said to
have~\defn{perfect speedup}.  Perfect speedup, however, is not always
possible.
]]>
</field> <!-- body_src -->

<field name='caption'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption -->
<field name='caption_src'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption_src -->
</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:xmpl:introduction::parallelism::fully
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='example'>
<field name='pl'>
...NOT.PROVIDED.PL...
</field> <!-- pl -->
<field name='pl_version'>
...NOT.PROVIDED.PL_VERSION...
</field> <!-- pl_version -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
xmpl:introduction::parallelism::fully
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>A fully sequential algorithm, where each operation depends on prior operations leaves no room for parallelism. We can only take advantage of one processor and the time would not be improved at all by adding more.</p>
<p>More generally, when executing an algorithm in parallel, we cannot break dependencies, if a task depends on another task, we have to complete them in order.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
A fully sequential algorithm, where each operation depends on prior
operations leaves no room for parallelism.
We can only take advantage of one processor and the time would not be
improved at all by adding more.  

More generally, when executing an algorithm in parallel, we cannot
break dependencies, if a task depends on another task, we have to
complete them in order.
]]>
</field> <!-- body_src -->

<field name='caption'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption -->
<field name='caption_src'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption_src -->
</atom> <!-- example -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:tch:introduction::parallelism::come
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='teachask'>
<field name='pl'>
...NOT.PROVIDED.PL...
</field> <!-- pl -->
<field name='pl_version'>
...NOT.PROVIDED.PL_VERSION...
</field> <!-- pl_version -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
tch:introduction::parallelism::come
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Can you come up with an example where perfect speedup is not possible?</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Can you come up with an example where perfect speedup is not possible?
]]>
</field> <!-- body_src -->

<field name='caption'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption -->
<field name='caption_src'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption_src -->
</atom> <!-- teachask -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:tch:introduction::parallelism::cooking
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='teachnote'>
<field name='pl'>
...NOT.PROVIDED.PL...
</field> <!-- pl -->
<field name='pl_version'>
...NOT.PROVIDED.PL_VERSION...
</field> <!-- pl_version -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
tch:introduction::parallelism::cooking
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>For example, when cooking a frittata, you cannot cook the egg that is not broken, nor can we add the eggs to the pan until the vegetables are sauteed.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
For example, when cooking a frittata, you cannot cook the egg that is
not broken, nor can we add the eggs to the pan until the vegetables
are sauteed.
]]>
</field> <!-- body_src -->

<field name='caption'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption -->
<field name='caption_src'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption_src -->
</atom> <!-- teachnote -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:grm:introduction::parallelism::span
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='pl'>
...NOT.PROVIDED.PL...
</field> <!-- pl -->
<field name='pl_version'>
...NOT.PROVIDED.PL_VERSION...
</field> <!-- pl_version -->
<field name='title'>
<![CDATA[
Span
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Span
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:introduction::parallelism::span
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>The second measure,  <strong><em>span</em></strong> , enables analyzing to what extent the work of an algorithm can be divided among processors. The  <strong><em>span</em></strong>  of an algorithm basically corresponds to the longest sequence of dependences in the computation. It can be thought of as the time an algorithm would take if we had an unlimited number of processors on an ideal machine.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
The second measure,~\defn{span}, enables analyzing to what extent the
work of an algorithm can be divided among processors.
The~\defn{span}~of an algorithm basically corresponds to the longest
sequence of dependences in the computation.  It can be thought of as
the time an algorithm would take if we had an unlimited number of
processors on an ideal machine.
]]>
</field> <!-- body_src -->

<field name='caption'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption -->
<field name='caption_src'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption_src -->
</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:tch:introduction::parallelism::calculate
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='teachask'>
<field name='pl'>
...NOT.PROVIDED.PL...
</field> <!-- pl -->
<field name='pl_version'>
...NOT.PROVIDED.PL_VERSION...
</field> <!-- pl_version -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
tch:introduction::parallelism::calculate
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>How do we calculate the work and span of an algorithm?</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
How do we calculate the work and span of an algorithm?
]]>
</field> <!-- body_src -->

<field name='caption'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption -->
<field name='caption_src'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption_src -->
</atom> <!-- teachask -->

</segment> <!-- cluster -->

<segment name='flex'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:def:introduction::parallelism::work-and-span
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='definition'>
<field name='pl'>
...NOT.PROVIDED.PL...
</field> <!-- pl -->
<field name='pl_version'>
...NOT.PROVIDED.PL_VERSION...
</field> <!-- pl_version -->
<field name='title'>
<![CDATA[
Work and Span
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Work and Span
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
def:introduction::parallelism::work-and-span
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>We calculate the work and span of algorithms in a very simple way that just involves composing costs across subcomputations. Basically we assume that sub-computations are either composed sequentially (one must be performed after the other) or in parallel (they can be performed at the same time). We then calculate the work as the sum of the work of the subcomputations. For span, we differentiate between sequential and parallel composition: we calculate span as the sum of the span of sequential subcomputations or maximum of the span of the parallel subcomputations. More concretely, given two subcomputations with work <span class="math inline">\(W_1\)</span> and <span class="math inline">\(W_2\)</span> and span <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span>, we can calculate the work and the span of their sequential and parallel composition as follows. In calculating the overall work and span, the unit cost <span class="math inline">\(1\)</span> accounts for the cost of (parallel or sequential) composition.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: center;"><strong><span class="math inline">\(W\)</span> (Work)</strong></th>
<th style="text-align: center;"><strong><span class="math inline">\(S\)</span> (span)</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Sequential composition</strong></td>
<td style="text-align: center;"><span class="math inline">\(1 + W_1 + W_2\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1 + S_1+ S_2\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Parallel composition</strong></td>
<td style="text-align: center;"><span class="math inline">\(1 + W_1 + W_2\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1 + \max(S_1, S_2)\)</span></td>
</tr>
</tbody>
</table>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
We calculate the work and span of algorithms in a very
simple way that just involves composing costs across subcomputations.
Basically we assume that sub-computations are either composed
sequentially (one must be performed after the other) or in parallel
(they can be performed at the same time).
We then calculate the work as the sum of the work of the
subcomputations.
For span, we differentiate between sequential and parallel composition:
we calculate span as the sum of the span of sequential
subcomputations or maximum of the span of the parallel
subcomputations.
More concretely, given two subcomputations with work $W_1$ and $W_2$
and span $S_1$ and $S_2$, we can calculate the work and the span of
their sequential and parallel composition as follows.
In calculating the overall work and span, the unit cost $1$ accounts
for the cost of (parallel or sequential) composition.


\begin{center}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{lcc}
\toprule
                          &  \bf $W$ (Work) & \bf $S$ (span)\\
\midrule
\bf Sequential composition & $1 + W_1 + W_2$ & $1 + S_1+ S_2$\\
\midrule
\bf Parallel composition   & $1 + W_1 + W_2$ & $1 + \max(S_1, S_2)$\\
\bottomrule
\end{tabular}
\end{center}
]]>
</field> <!-- body_src -->

<field name='caption'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption -->
<field name='caption_src'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption_src -->
</atom> <!-- definition -->

<atom name='note'>
<field name='pl'>
...NOT.PROVIDED.PL...
</field> <!-- pl -->
<field name='pl_version'>
...NOT.PROVIDED.PL_VERSION...
</field> <!-- pl_version -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
nt:introduction::parallelism::intuition
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>The intuition behind the definition of work and span is that work simply adds, whether we perform computations sequentially or in parallel. The span, however, only depends on the span of the maximum of the two parallel computations. It might help to think of work as the total energy consumed by a computation and span as the minimum possible time that the computation requires. Regardless of whether computations are performed serially or in parallel, energy is equally required; time, however, is determined only by the slowest computation.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
The intuition behind the definition of work and span is that work
simply adds, whether we perform computations sequentially or in
parallel.  The span, however, only depends on the span of the maximum
of the two parallel computations.  It might help to think of work as
the total energy consumed by a computation and span as the minimum
possible time that the computation requires.  Regardless of whether
computations are performed serially or in parallel, energy is equally
required; time, however, is determined only by the slowest
computation.
]]>
</field> <!-- body_src -->

<field name='caption'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption -->
<field name='caption_src'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption_src -->
</atom> <!-- note -->

<atom name='example'>
<field name='pl'>
...NOT.PROVIDED.PL...
</field> <!-- pl -->
<field name='pl_version'>
...NOT.PROVIDED.PL_VERSION...
</field> <!-- pl_version -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
xmpl:introduction::parallelism::suppose
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Suppose that we have <span class="math inline">\(30\)</span> eggs to cook using <span class="math inline">\(3\)</span> cooks. Whether all <span class="math inline">\(3\)</span> cooks to do the cooking or just one, the total work remains unchanged: <span class="math inline">\(30\)</span> eggs need to be cooked. Assuming that cooking an egg takes <span class="math inline">\(5\)</span> minutes, the total work therefore is <span class="math inline">\(150\)</span> minutes. The span of this job corresponds to the longest sequence of dependences that we must follow. Since we can, in principle, cook all the eggs at the same time, span is 5 minutes.</p>
<p>Given that we have <span class="math inline">\(3\)</span> cooks, how much time do we actually need? It should be clear that each cook can cook 10 eggs, for a total time of <span class="math inline">\(50\)</span> minutes. Later we will discuss the “the greedy scheduling principle” which tells us that given a task with <span class="math inline">\(W\)</span> work and <span class="math inline">\(S\)</span> span, and using a greedy schedule, the time is upper bounded by <span class="math inline">\(W/P+  
S\)</span>. In our case this would be <span class="math inline">\(150/3 + 5 = 55\)</span>.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Suppose that we have $30$ eggs to cook using $3$ cooks.  Whether all
$3$ cooks to do the cooking or just one, the total work remains
unchanged: $30$ eggs need to be cooked.
Assuming that cooking an egg takes $5$ minutes, the total work
therefore is $150$ minutes.
The span of this job corresponds to the longest sequence of
dependences that we must follow.
Since we can, in principle, cook all the eggs at the same time, 
span is 5 minutes.

Given that we have $3$ cooks, how much time do we actually need?
It should be clear that each cook can cook 10 eggs, for a total time of $50$
minutes.   Later we will discuss the ``the greedy scheduling
principle'' which tells us that given a task with $W$ work and $S$
span, and using a greedy schedule, the time is upper bounded by $W/P+
S$.    In our case this would be $150/3 + 5 = 55$.
]]>
</field> <!-- body_src -->

<field name='caption'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption -->
<field name='caption_src'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption_src -->
</atom> <!-- example -->

</segment> <!-- flex -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:ex:intro::intro::mergesort-cost
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='example'>
<field name='pl'>
...NOT.PROVIDED.PL...
</field> <!-- pl -->
<field name='pl_version'>
...NOT.PROVIDED.PL_VERSION...
</field> <!-- pl_version -->
<field name='title'>
<![CDATA[
Parallel Merge Sort
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Parallel Merge Sort
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
ex:intro::intro::mergesort-cost
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p> As an example, consider the parallel <span class="math inline">\(\texttt{mergeSort}\)</span> algorithm for sorting a sequence of length <span class="math inline">\(n\)</span>. The work is the same as the sequential time, which you might know is <span class="math display">\[W(n) = O(n \lg{n}).\]</span> We will see that the span for <span class="math inline">\(\texttt{mergeSort}\)</span> is <span class="math display">\[S(n) = O(\lg^2{n}).\]</span></p>
<p>Thus, when sorting a million keys and ignoring constant factors, work is <span class="math inline">\(10^6\lg (10^6) &gt; 10^7\)</span>, and span is <span class="math inline">\(\lg^2(10^6) &lt; 500.\)</span></p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\label{ex:intro::intro::mergesort-cost}
As an example, consider the parallel $\cd{mergeSort}$ algorithm for
sorting a sequence of length $n$.  The work is the same as the
sequential time, which you might know is
\[
W(n) = O(n \lg{n}).
\] 
We will see that the span for
$\cd{mergeSort}$ is
\[
S(n) = O(\lg^2{n}).
\]

Thus, when  sorting a million keys and ignoring constant factors, 
work is $10^6\lg (10^6) > 10^7$, and 
span is 
$\lg^2(10^6) < 500.$
]]>
</field> <!-- body_src -->

<field name='caption'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption -->
<field name='caption_src'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption_src -->
</atom> <!-- example -->

</segment> <!-- cluster -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:grm:introduction::parallelism::parallel-time
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='pl'>
...NOT.PROVIDED.PL...
</field> <!-- pl -->
<field name='pl_version'>
...NOT.PROVIDED.PL_VERSION...
</field> <!-- pl_version -->
<field name='title'>
<![CDATA[
Parallel Time
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Parallel Time
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:introduction::parallelism::parallel-time
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Even though work and span, are abstract measures of real costs, they can be used to predict the run-time on any number of processors. Specifically, if for an algorithm the work dominates, i.e., is much larger than, span, then we expect the algorithm to deliver good speedups.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Even though work and span, are abstract measures of real costs, they
can be used to predict the run-time on any number of processors.
Specifically, if for an algorithm the work dominates, i.e., is much
larger than, span, then we expect the algorithm to deliver good
speedups.
]]>
</field> <!-- body_src -->

<field name='caption'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption -->
<field name='caption_src'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption_src -->
</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='flex'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:xrcs:introduction::parallelism::would
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='exercise'>
<field name='pl'>
...NOT.PROVIDED.PL...
</field> <!-- pl -->
<field name='pl_version'>
...NOT.PROVIDED.PL_VERSION...
</field> <!-- pl_version -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
xrcs:introduction::parallelism::would
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>How would you expect the parallel mergesort algorithm, <span class="math inline">\(\texttt{mergeSort}\)</span>, mentioned in the example above to perform as we increase the number of processors dedicated to running it?</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
How would you expect the parallel mergesort algorithm, $\cd{mergeSort}$,
mentioned in the example above to perform as we increase the number of
processors dedicated to running it?
]]>
</field> <!-- body_src -->

<field name='caption'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption -->
<field name='caption_src'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption_src -->
</atom> <!-- exercise -->

<atom name='solution'>
<field name='pl'>
...NOT.PROVIDED.PL...
</field> <!-- pl -->
<field name='pl_version'>
...NOT.PROVIDED.PL_VERSION...
</field> <!-- pl_version -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
sol:introduction::parallelism::recall
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>Recall that the work of parallel merge sort is <span class="math inline">\(O(n\lg{n})\)</span>, whereas the span is <span class="math inline">\(O(\lg^2{n})\)</span>. Since span is much smaller than the work, we would expect to get good (close to perfect) speedups when using a small to moderate number of processors, e.g., couple of tens or hundreds, because the work term will dominate. We would expect for example the running time to halve when we double the number of processors. We should note that in practice, speedups tend to be more conservative due to natural overheads of parallel execution and due to other factors such as the memory subsystem that can limit parallelism.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
Recall that the work of parallel merge sort is $O(n\lg{n})$, whereas
the span is $O(\lg^2{n})$.  
Since span is much smaller than the work, we would expect to get good
(close to perfect) speedups when using a small to moderate number of
processors, e.g., couple of tens or hundreds, because the work term
will dominate.
We would expect for example the running time to halve when we double
the number of processors.
We should note that in practice, speedups tend to be more conservative
due to natural overheads of parallel execution and due to other
factors such as the memory subsystem that can limit parallelism.
]]>
</field> <!-- body_src -->

<field name='caption'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption -->
<field name='caption_src'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption_src -->
</atom> <!-- solution -->

</segment> <!-- flex -->


</segment> <!-- subsection -->

<segment name='subsection'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
Work Efficiency
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Work Efficiency
]]>
</field> <!-- title_src -->
<field name='label'>
sec:introduction::parallelism::work-efficiency
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<segment name='cluster'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:grm:introduction::parallelism::less
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='gram'>
<field name='pl'>
...NOT.PROVIDED.PL...
</field> <!-- pl -->
<field name='pl_version'>
...NOT.PROVIDED.PL_VERSION...
</field> <!-- pl_version -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
grm:introduction::parallelism::less
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p>If algorithm <span class="math inline">\(A\)</span> has less work than algorithm <span class="math inline">\(B\)</span>, but has greater span then which algorithm is better? In analyzing sequential algorithms there is only one measure so it is clear when one algorithm is asymptotically better than another, but now we have two measures. In general the work is more important than the span. This is because the work reflects the total cost of the computation (the processor-time product). Therefore typically the goal is to first reduce the work and then reduce the span by designing asymptotically work-efficient algorithms that perform no work than the best sequential algorithm for the same problem. However, sometimes it is worth giving up a little in work to gain a large improvement in span.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
If algorithm $A$ has less work than algorithm $B$, but has greater
span then which algorithm is better?  In analyzing sequential
algorithms there is only one measure so it is clear when one algorithm
is asymptotically better than another, but now we have two measures.
In general the work is more important than the span.  
This is because the work reflects the total cost of the computation
(the processor-time product).  Therefore typically the goal is to
first reduce the work and then reduce the span by designing
asymptotically work-efficient algorithms that perform no work
than the best sequential algorithm for the same problem. 
However, sometimes it is worth giving up a little in work to gain a
large improvement in span.
]]>
</field> <!-- body_src -->

<field name='caption'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption -->
<field name='caption_src'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption_src -->
</atom> <!-- gram -->

</segment> <!-- cluster -->

<segment name='flex'>
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='label'>
grp:def:intro::intro::work-efficiency
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->

<atom name='definition'>
<field name='pl'>
...NOT.PROVIDED.PL...
</field> <!-- pl -->
<field name='pl_version'>
...NOT.PROVIDED.PL_VERSION...
</field> <!-- pl_version -->
<field name='title'>
<![CDATA[
Work Efficiency
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
Work Efficiency
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
def:intro::intro::work-efficiency
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p> We say that a parallel algorithm is  <strong><em>(asymptotically) work efficient</em></strong> , if the work is asymptotically the same as the time for an optimal sequential algorithm that solves the same problem.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\label{def:intro::intro::work-efficiency}
We say that a parallel algorithm is~\defn{(asymptotically) work
  efficient}, if the work is asymptotically the same as the time for
an optimal sequential algorithm that solves the same problem.
]]>
</field> <!-- body_src -->

<field name='caption'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption -->
<field name='caption_src'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption_src -->
</atom> <!-- definition -->

<atom name='example'>
<field name='pl'>
...NOT.PROVIDED.PL...
</field> <!-- pl -->
<field name='pl_version'>
...NOT.PROVIDED.PL_VERSION...
</field> <!-- pl_version -->
<field name='title'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title -->
<field name='title_src'>
<![CDATA[
...NOT.PROVIDED.TITLE...
]]>
</field> <!-- title_src -->
<field name='cover'>
...NOT.PROVIDED.COVER...
</field> <!-- cover -->
<field name='sound'>
...NOT.PROVIDED.SOUND...
</field> <!-- sound -->
<field name='label'>
xmpl:introduction::parallelism::intro
</field> <!-- label -->
<field name='depend'>
...NOT.PROVIDED.DEPEND...
</field> <!-- depend -->
<field name='point_value'>
0.0
</field> <!-- point_value -->
<field name='body'>
<![CDATA[
<p><a href="ex:intro::intro::mergesort-cost">The parallel <span class="math inline">\(\texttt{mergeSort}\)</span> function</a> described in is work efficient since it does <span class="math inline">\(O(n \log n)\)</span> work, which optimal time for comparison based sorting.</p>
<p>In this course we will try to develop work-efficient or close to work-efficient algorithms.</p>
]]>
</field> <!-- body -->
<field name='body_src'>
<![CDATA[
\href{ex:intro::intro::mergesort-cost}{The parallel $\cd{mergeSort}$ function} described in is work efficient since it does $O(n \log n)$ work, which optimal time for comparison based sorting. 

In this course we will try to develop work-efficient or close to work-efficient algorithms.
]]>
</field> <!-- body_src -->

<field name='caption'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption -->
<field name='caption_src'>
<![CDATA[
...NOT.PROVIDED.CAPTION...
]]>
</field> <!-- caption_src -->
</atom> <!-- example -->

</segment> <!-- flex -->


</segment> <!-- subsection -->

</segment> <!-- section -->

</segment> <!-- chapter -->
