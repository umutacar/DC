
# JOURNAL

- [Dec 19]
  Trying to figure out how to generate xml code from tex.

  One question is whether to translate tex to tex_core, which we use to eliminate syntactic sugar.  This can probably be very helpful, especially when dealing with more complicated latex classes such as problem sets.  

  One problem with this approach is that it ends up generating lots of intermediate files.  But now we are using a real functional programming language.  So we can transform the ASTs but don't necessarily have to dump them.  I think this is the approach that I should follow.

- [Dec 18]
  * working on idempotent parsing

    Issue: headings for groups, atoms, sections, should be preserved as read from th
e file.<

  * rm'ed begin/end from keywords because we don't care about these.  they are not diderot keywords... 

  * completed a first pass on idempotent parsing.
    
  * do we need to distinguish comments can't we treat them as just words.  

    --> yes we do.  if we don't then, we have no way of skipping over commented atoms and other diderot keywords.  

   * resolved the kinding problem by "inlini" productions of menhir.

- [Dec 17]
  * One way to deal with all text that is inbetween atoms is to allow atoms and groups to have preambles and have a sequence of them to have a postamble.  So this is what I implemented.

  * The above idea works but it is not very uniform.  The problem is that who does that postamble belong to.  It is usually not clear because you can have a blocks of atoms that has a postamble and then a section that also has a postamble.  The chapter's postamble is now that of the block of the atoms, which is quite counterintuive.  It seems that a better approach is to allow the final atoms to have a post and a preamble, while allowing all other atoms to have preambles.  For clarity, i started calling these /intertext/ rather thas postamble.

  * Now working on idempotent parsing.
   ** Issue: whitespace are not being lined up.  WHat is our policy for dealing with white spaces.
      Resolved this issue it is not so much about whitespaces but more about making sure that we can push them through the parser carefully without dropping them somehow along the way as we extract various parts such as titles.


- [Dec 16]
  * working on comments

  * added atom preambles that can be associated with each atom but will not be "displayed".
  * added group preambles

  * thinking about what to about latex commands such as 
    \xyz where xyz could be \item \begin{enumerate} \% or \[
    These we should treat them as plain words.  But how? 

    The problem is that unless we treat \ specially we will not be able to
    match things like \begin{atom}.
    
    Thus we have to treat latex keywords as plain words in the parser.
    
    -> this approach seems to work.

  * Associating preambles with atoms and groups does not completely solve the stray text problem.

    Because we can also have stray text at the end of things that currently don't belonge anywhere

    For example:
    \section{xyz} 
 
     blocks

     stray text currently does not belong anywhere.

    The same applies to blocks and such.

    Thus if there is no atom group that the text will be assigned to we are kind of stuck.

   ** To solve this problem, one possibility would be to give sections (subsections) etc, a preamble as well.  This does not work, because when we see a preamble we don't know if it should belong to an atom or a section.  So this is a major difficulty.

    ** I wonder if a postamble will work....

      Postamble idea might work but it is quite tricy to make it work because of conflicts.  One conflict arises from preamble/postamble ambiguity.  Should postamble be preamble of the atom that we have not seen or is it postamble. So we have to differentiate between empty and nonempty blocks.  


      I will need to look into this and see if it is possible.


- [Dec 14, 15, mixed hacking in various meetings]

  * Learned about menhir parameterized rules.  they have some limitations but work generally well it seems.

  * Using parameteric menhir rules, I was able to express atom parsing rules

  * I was able to streamline all section definitions using parametric rules.

  * Made quite a bit of progress quickly. Next I need to deal with comments.

- [Dec 12, morning]
  * Copied over star contraction chapter and will try to make it through the parser.
  * Fleshed out atoms and the atom structure.

  * Added support for labeles into AST, chapters, and sections.  
 

- [Dec 11, morning]

Wrote some traversal functions for converting AST into strings and LaTeX code.  This seems to have worked well.  

Next steps are to start fleshing out the parser to include the full language.

To this end, the first thing I should figure out is whether I can deal with all the atoms in one swoop by using some kind of keywording mechanism so that we can keep the grammar neat and compact.

Ok. I have figured out how to deal with atoms by using a keyword hash table.  This is a good place to stop.  The next thing to do is to 


- [Dec 9, evening]

Working on develeping the AST.  Set up the AST and generated it. Next step is to write some traversal functions over AST.  First one should reconstruct the originial text.


- [Dec 9, morning]



Got a first version of the parser working. 

There were some shift/reduce conflicts but these were due to the redundant atoms definitions of the form:

atoms : empty | atom | atoms atom
The middle rule is redundant.  Deleting that rule eliminated the conflicts.


- [Dec 8]
  Resuming the work on the parser.
  
I am now investigating whether I can extend our definiton of words so to include whitespaces. The idea is to tokenize the input at special chars 

special characters: \ { } [ ] 

and to include into the tokens all the white space including newlines.

The method I am following is that each non-word token consist of whatever it wants to represent and then is followed by whitespace.

\begin{...} whitespace

the words themselved consist of everything except the special characters above.


The issue that I am not taking care of right now is that \ is a special character and words will not catch them.  So I need to account for it when boxing things.


- [Nov 30] I am now investigating whether it would be possible to extend our definiton of words so to include whitespaces. The idea is to tokenize the input at special chars 

\ { } [ ] 

and to include into the tokens all the white space including newlines.

I don't currently see any problem with this.  But we will be playing with it more.


- [Nov 30] The approach of splitting text into curly and square boxes has a problem:  it becomes difficult if not impossible to reconstruct the text back.  For example, we can have 
[something ]
\{ something else\}
now when we parse, we lose the spacing and the spacing might matter

  I think I need an more elementary method....

  Next I would like to try the following:
  Look for patterns of the form 
  \section{....}
  or 
  \begin{env}
  or
  \begin{env}[  ]

  the rest should be treated as CHAR's.  

  This parser does not elim white space but is somewhat sensitive to it. 

  In principle the above makes sense but it is difficult to do because we don't know when a paren ends (without parsing more deeply, which is the problem that i am trying to avoid).  So the easiest thing to do is to look for a newline.
   
  So look for patterns of the form 
  \section{....} whitespace \n
  or 
  \begin{env} whitespare \n
  or
  \begin{env}[  ] whitespace \n


  BUT this raises other issues: we can't reliably use the parser anymore.

  

- [Nov 28] Working on parameterizing the parser so that we can deal with many different kinds of env tokens and such
 
- [Nov 28] The idea of splitting text into square, curly, and plain boxes ([], {}, rest) seem to work.

- [Thanksgiving Break] 

  - The problem with the whitespace business is that it kills existing parsers.  The current parsers are all LR(1) variants, which means that they can only lookahead one token at a time.  This becomes a problem when taking whitespace into account, because we usually want to allow multiple whitespaces.

  - The files lexer.mll and parser.mly in this directory contain a take on the whitespace problem by uusing two newlines as a "separator" and ignoring whitespace otherwise.  This seems to "work" but it is clunky.  It is far away from the "Line by Line" parsing that I imagined would work nicely.

  - I think I will have to take a different take on this.  I see two options
1)  One way would be to write our own lexer that does the line by line parsing directly rather than relying on a parser, which is a bit rigid... 
2) Another way is to give up on the reliance on white space and write a superficial parser at the level of "boxes" consisting of words {box} and [box] objects and define a superficial grammar for diderot latex at this level.  This should be feasible.  One issue is the ambiguity in things like this, but should be fixable.  
\begin{example}[title][body] is ambigous because [title][body] can also be legal body.  We need a way to disambiguate this.

this approach has the disadvantage of not being able to deal with paragraphs but maybe i can deal with that by using two lines as breakers.  The problem with that though is that i have to account for it everywhere, which becomes annoying but it might be feasible.  another way to deal with this to write a separate simple "translator" that can gramify paragraphs.  



# Overview of the approach

The basic idea I am following here is to parse the document superficially, so superficially that we only want to care about what we need to identify blocks.  The rest, we treat them as blobs of text.

Specificially the lexer will break up the document into words and also keep the newlines and other spaces.  The parser will then extract what it needs and put the rest together.

Note: I had initially hoped that I could get away by parsing things only at the level of the lines but I decided agains it because I still have to parse arguments in curly braces and in square brackets.  This might get all too complicated by I am hoping to make it work. 



# Lexer

We are going to breax the text into tokens consisting of
1) spaces and special characters
- white space
- open close curly { }
- open close square [ ] 

2) various headings
- latex headings
- \begin{atom} \end{atom}, for all atoms
- \begin{group} \end{group}
- 

3) words will be the rest

The hope is that we will be able to put everything back together in the parser, except for the parts that we care about.
