# TODO
  - ProblemCluster is defunct: delete it...

  - Update the backend to support new groups

  - Testing: make sure that the updates did not break anything by running regression tests with existing chapters.

    Connect back with the diderot development. 

  - superblocks include blocks.  This leads to shift/reduce conflict.  Look into this.  Is this harmless?
  - diderot keywords such as labels can appear inside bodies.  How to deal with these?  We can include them as a word but this will likely lead to many conflicts.

    this suggests that "hijacking" latex commands as diderot commands can be a bit painful.  we have to be careful

   -- The major remaining gap: the older translation does some strictfication as an optimization to reduce the many nested section/subsection/subsubsection that would otherwise be needed.  I forget now how this was done.  I will need to look into it.
   
   -- see xml.ml, headers

 - Should we have a core language?  I think not.  It mostly bloat code.  So let's not unless we really need.  The AST should be good enough for most things.

 - clean up python crap from this directory when the parser is done. one possibility is to key dex/ for historical consistency and move relevant directories under it.

 - Add support for paragraphs

   One idea: sections will now have blocks followed by paragraphs and then transition into nested sections.  Basically, we are allowing paragraphs to occur at block level.  But of course once you have a paragraph, then the rest will be contained within that paragraph.  

 - Coordination with the python-based XML backend needs to be done carefully.

This coordinates seems to be going through several functions called
missing_field* in diderot/src/pervasives/syntax.py

These function are called from mlx_utils, which does nearly all work.

So whatever latex produces must be mediated through this. 

 

# JOURNAL
- [May 11]

  * multipart problems should have points.
    - added to lexer/parser.
    - added to xml
  * TODO: group points should be calculated from those of the atoms within 
  * TODO: what is the status of elaboration?

- [May 10]
  * Fixed up some bugs with lexing and parsing.  
  * tested tex2tex translation with different of groups: mproblem and flex.
  * tested xml translation


- [May 9]
  * Extend parser with multipart problems.  These would be another block, basically like a flex, but would use a different title.
    - Started doing this.  TODO: paremeterize group's so that I can use the same definitions, much like with atoms.  Working on this done with parser and lexer.


- [ May 7 ] 
  * Made a multichoice list. This works is late
  * TODO: current points scheme with optional titles and points, both being optional arguments doesn't work well with latex. There should not be more than one optional argument...
    Idea: detect the one that is a number and treat it specially.  
    OK This works.  Now carry it forward by fleshing it out.  If you don't want to care about the order of arguments, then you will need to be more careful about things in parsing also.  Maybe this is not the best thing.  It might be better to require the first argument always be poins.  Also for consistency.   

- [May 6]
  * Started working on XML translation.  The code has gotten unwieldy, so I want to reorg first...
    Reorg done.
  * Hint and explains fields created and translated to xml.

  * Worked on updating our latex sytle files to allow compiling problems to pdf.  It didn't work out by using the exam package, because of differences in style.  Not sure how to handle this.  I don't think it is going to be pretty.  I think the easiest thing is to define my own lists instead of using the exam package.

- [May 5]
  * Getting back to things...
  * Add \explain and \hint to atoms.
    The syntax would be
    \begin{atom}

    \hint
    ..
    \solution
    ..
    \explain (explain the solution).

   \end{atom}  
 
   The tricky things is that all of the above are optional, so when we find one, we would need to stop when we find another or the end of atom.  The end of atom in turn should (though doesn't have to be) passed out and returned to the parser.

   Working on this: figuring first the tex-to-tex translation.
   Ok this is done. 
  * Next TODO: extend XML translation to deal with hints, explanations, and solutions.


- [April 14]
  * Started testing lexing of \depend'ency lists and. Fixed some lexer issues.
  * Now extending ast to include depency lists for atoms.

- [April 13]
  * Added lexer and parser support for \depend{label1, label2, label3}...

    I don't think I can add depend to atoms without inflating the parser dramatically.  We will need to simplify things...
  * TODO: Ok, simplified the mk_atom function of the parser to accommodate \depend{} command.  Now I need to update the ast to include dependencies in addition to labels.

- [April 12]
  Got \input inlining to work by using OCAML's regular expressions.

- [April 11]
  * Added recognition for \input{filename} in branch pre_input.  The idea would be to read the contents of filename and include it in the current body of text.  The question is whether this should be done in a preprocessing step so that if there are diderot commands within the file, they could be handled properly.  I think the answer is obviously this is how it should be..

  I implemented this by extending the lexer but perhaps this is overkill.  It might be more straightforward to simply read in a file and substitute \input{filename} with the contents of the file.  I don't think it is necessary to parse the file.  It should be possible to do this much more simply, without having to do anything at all, and by simply using a string query and a replacement.

  I am therefore going to reset to quiz branch and implement preprocessing as a separate pass.

- [April 9]
  * Added elaboration to tex and xml generation. So this should be good to merge upstream.
    
- [April 8]
  - Working on making elobaration idempotent.  I need to be able to assign points to items optionally.
    * items now have points
    * clusters also should..
    * ok items and clusters now have points.  
    * BUT: suppose that the cluster sums don't match up the given sum.  What do we do? 
      One idea is to disallaw optional points this in the surface syntax.  Another is to scale. 
      This is a bit of a policy decision.  We will need to see...

- [April 7]  
  - Need a function to compute point values for clusters.
  - Renamed "block" to "element".  
    An element now is either a group or an atom.
  - Renamed "superblock to block"
    A block now is either a cluster or an element.
  - Wrote a set of  traversal functions that traverse the AST without doin anything.
    TODO: Need to customize this so that it can do traversals.
  - Started working on Elaboration.
    DOne: a basic elaboration for points.  Untested.

- [March 24]
  -> atoms now have points. if not specified, then they have 0 points.
     the syntax is \begin{atom}[pointval][title]
     pointval is optional
   
     
  -> added points to clusters.  but clusters are not given points at the syntax level. 
     the question now is how to compute this stuff for clusters.
- [March 23]
  -> Updated beamer to recognize clusters.
  -> Allow point scores to be added to clusters.  

  How should we do this? What should the syntax be? 
  Maybe recreate a 210 exam to see how.  

  -> Played with the exam package and latex.  The conclusion is problems will support two optional arguments: titles and points.  The integer numbers will become points, the other will become title.

  -> It seems that it will be cleanest to have a solution field within a problem. This will indicate whether the problem is free response or not.  And if so, whether there should be a textbox.

  -> Allow points to have point values.  See ilists an an example, the idea is to match the point value as part of the begin{atom}.  For example \begin{atom}[20]

- [Marh 22]
  -> Chapters can now have blocks.  One problem is that this leads to an additional shift/reduce conflict, because a superblock can be a block. So I  will have to look into this.
  -> Superblocks and clusters are now good.

- [Marh 21]
  -> Extended the parser and AST/XML to support clusters.
  -> Cluster are currently not hooked up to the syntax. That is the next thing to do.
  --> Will need to test and cluster, when not used don't change anything.

- [March 15] 
  -> Starting to work on free responses.

  --> One question is how do we deal with the fact that the meaninng of "solution" is different in the exam package?  I think the solution to this for us to have a separate diderot->latex mapping for exam package environments.  This is easy so let's not worry about this.
 
      Update: I was able to set up our "problem bank" to do this.  there were no problems.  so this will work.

  --> added \titledquestion{} section
      format: 
      \titledquestions{Question 1}
      \begin{atom}
      \end{atom}
      \begin{parts}
      \part[2]
      \begin{problem}
      \end{problem}
      \begin{solution}
      \end{solution}
      \part[3]
      \end{parts}

      I want to support this but somehow I need to ignore parts.

  --> Ignoring parts and pushing also points to problems works.  There is some redudancy but don't think it is a big deal. 


- [March 14] 
  -> completed an initial working of lists.

- [March 13] 
  -> Extend to support checkboxes and other types of lists.

- [March 12] 
  -> completed  an initial version of the beamer to support itemized lists.
- [March 11]
  Goal: XML generation for lists.
  -> superficial generation complete but need separate xml for list items.
  -> now generating xml for individual items.
  -> moving onto beamer.


- [March 10]
  Goal: add diderot itemized lists, e.g.,
  \begin{choices}
  \choice latex blobs
  \correctchoice latex blobs
  \choice 
  \end{choices}

  TODO: how do we want to incorporate ilists?  should they occur as part of an atom?
  For example, a problem could be like this
  \titledquestion{questions title}
  
  Pick any any of the following
  \part

  \begin{choices}
  \choice latex blobs
  \correctchoice latex blobs
  \choice 
  \end{choices}

  \part

  --> This may be too general.  we have to proceed incrementally.  let's just focus on getting multiple choice questions in.  But I don't think this would be impossible to do. we just have to create a structure to parse "section-like" lists.  This should not be that different from parsing sections.

  * Done with the parsing,  now move on the xml genaration. 

- [March 9]
  Goal: treat latex environments as "words".

  The idea is to start skipping over text when we encounter a latex env.
  Because these can be nested, we have to count the depth.

  When we encounter a verbatim, we will still need to skip over everything up to the next verbatim. this is important because the body of verbatim can have arbitrary including unbalanced latex code, which could confuse our compiler. 

  * Done: latex env's sketched.  not worrying about verbatim at this time.
  * Have a separate ENV token for environments.  "word" is not the right thing.
  * Introduced blobs as word and environments.
  * Allowing nesting of verbatims inside latex env's.

- [March 8]
  Goal: Working on recognizing labels in the lexer so that we can treat the follow-up text as an blob if the label occurs within an atom.
  * Done: recognize label in the lexer and make sure parser works as the same.
    Done: checked that the tex generated is the same as the source (idempotence).
    Done: checked that the xml generated is the same as with the prior compiler.

- [March 5]
  Working on treating \begin{verbatim} .. \end{verbatim} as a block. 
  This is done.


- [Dec 29]

  - Found a way to streamline the defitinions of atoms.  The basic idea is to use variables to capture the atom kind matched so that the atom can be returned as part of a matched token.  This means that we don't have to create separate tokens for each atom and can treat them all as part of instances of a single atom token.  This reduces the number of lines of codes in the parser quite dramatically and simplifies things a lot.  The only *slight* downside is that it does not bugs of the sort

  \begin{example}
  \end{defition}

  This is because begin and end are both well defined atoms.  this atom would be treated as an example.  This "bug" might even be a feature.  But it would be nice to produce a warning in this case.

- [Dec 30]
  - Added levels to blocks and atoms.

- [Dec 28]
  - Fixed a label printing bug
  - Dropped the Core library in favor of standard ocaml libraries in lexer.

    This is because lexer internally uses standard libraries (it seems) and core is not consistent with standard.

  - Extended the lexer and parser to support diderotXYZ atoms where XYZ can be any custom atom.
  - Allowing chapters to have preambles much like atoms.

- [Dec 23]
  - Cleanedup up the ocaml code a bit 

  - Starting running the parser through 15210 sources.  
    Was able to compile and upload the first 5-6 chapters but encountered a problem in language/functional.tex.
    Not sure what the problem is.  Have to look into it more carefully.
   - Resolved this issue, it was due to \% character being special in latex.

- [Dec 22]

  * Completed a first working version of OCaml parser.  It is able to produce xml code from .tex files. I used star.tex (from graph contraction as an example).

    There are still some issues left. The major one being how to strictify: the older translation does some strictfication as an optimization to reduce the many nested section/subsection/subsubsection that would otherwise be needed.  I forget now how this was done.  I will need to look into it.

    I looked into this.  The basic idea is to wrap the "Blocks" in each section into another section.  Here is one way to proceed.

    Create a separate AST-Strict data type.  This would be pretty similar to current AST but its types are more streamlined.  It would only allow strict nesting.  Then wriwe a function that translates AST into AST-Strict.  AST strict would then have functions that write it out as LaTeX or XML. Voila.  This should work...

 * Modulo the strictification above, I was able to upload a XML generated by the OCML parser on diderot. 
   

 
- [Dec 21]

  * Need a way to pass the translation function into atom's of the AST.  This requires threading the translation through.  I think this is fine, because it is pure.

  * Generated xml successfully using pandoc translation method. Yay!!!
  * Now comes the tedious part.  I need to carefully go through the xml and make sure that I am not missing any fields.  This is a bit tedious.  I have to study the dex translation because there isn't really a spec.  The implementation is the spec.  Ouch.

  * OCaml Str.regexp stuff is broken.  Groups are not identified correctly.


- [Dec 20]
  * Two ways to do the translation of latex to xml.  One is to continue reusing our python code.  This will work but the problem is that it will prevent us from distributing an executable that can do this without having to rely on the user having installed python.  This is pretty bad.  Another way is to rewrite this stuff in OCAML.  I think this is the right direction.  But maybe for now the first path is OK because it doesn't require writing new code.    

  * I completed a first version of an ocaml module for translating tex to html by using pandoc externally via systems calls.  This module is currently in tex/tex2html.ml.  I plan on trying it soon.


- [Dec 19]
  * Trying to figure out how to generate xml code from tex.

  One question is whether to translate tex to tex_core, which we use to eliminate syntactic sugar.  This can probably be very helpful, especially when dealing with more complicated latex classes such as problem sets.  

  One problem with this approach is that it ends up generating lots of intermediate files.  But now we are using a real functional programming language.  So we can transform the ASTs but don't necessarily have to dump them.  I think this is the approach that I should follow.

  * Doing some of the "infrastructure" work needed to translatex tex to xml such as creating a vocabulary for the core language and such

  * Coordination with XML backend is important, see tex/COORDINATE

  *  One question is what to do with various missing fields. I think the backend is robust enough to deal with their absence so maybe we should just omit them. 

  * completed a skeloten of the xml generation.

- [Dec 18]
  * working on idempotent parsing

    Issue: headings for groups, atoms, sections, should be preserved as read from th
e file.<

  * rm'ed begin/end from keywords because we don't care about these.  they are not diderot keywords... 

  * completed a first pass on idempotent parsing.
    
  * do we need to distinguish comments can't we treat them as just words.  

    --> yes we do.  if we don't then, we have no way of skipping over commented atoms and other diderot keywords.  

   * resolved the kinding problem by "inlini" productions of menhir.

- [Dec 17]
  * One way to deal with all text that is inbetween atoms is to allow atoms and groups to have preambles and have a sequence of them to have a postamble.  So this is what I implemented.

  * The above idea works but it is not very uniform.  The problem is that who does that postamble belong to.  It is usually not clear because you can have a blocks of atoms that has a postamble and then a section that also has a postamble.  The chapter's postamble is now that of the block of the atoms, which is quite counterintuive.  It seems that a better approach is to allow the final atoms to have a post and a preamble, while allowing all other atoms to have preambles.  For clarity, i started calling these /intertext/ rather thas postamble.

  * Now working on idempotent parsing.
   ** Issue: whitespace are not being lined up.  WHat is our policy for dealing with white spaces.
      Resolved this issue it is not so much about whitespaces but more about making sure that we can push them through the parser carefully without dropping them somehow along the way as we extract various parts such as titles.


- [Dec 16]
  * working on comments

  * added atom preambles that can be associated with each atom but will not be "displayed".
  * added group preambles

  * thinking about what to about latex commands such as 
    \xyz where xyz could be \item \begin{enumerate} \% or \[
    These we should treat them as plain words.  But how? 

    The problem is that unless we treat \ specially we will not be able to
    match things like \begin{atom}.
    
    Thus we have to treat latex keywords as plain words in the parser.
    
    -> this approach seems to work.

  * Associating preambles with atoms and groups does not completely solve the stray text problem.

    Because we can also have stray text at the end of things that currently don't belonge anywhere

    For example:
    \section{xyz} 
 
     blocks

     stray text currently does not belong anywhere.

    The same applies to blocks and such.

    Thus if there is no atom group that the text will be assigned to we are kind of stuck.

   ** To solve this problem, one possibility would be to give sections (subsections) etc, a preamble as well.  This does not work, because when we see a preamble we don't know if it should belong to an atom or a section.  So this is a major difficulty.

    ** I wonder if a postamble will work....

      Postamble idea might work but it is quite tricy to make it work because of conflicts.  One conflict arises from preamble/postamble ambiguity.  Should postamble be preamble of the atom that we have not seen or is it postamble. So we have to differentiate between empty and nonempty blocks.  


      I will need to look into this and see if it is possible.


- [Dec 14, 15, mixed hacking in various meetings]

  * Learned about menhir parameterized rules.  they have some limitations but work generally well it seems.

  * Using parameteric menhir rules, I was able to express atom parsing rules

  * I was able to streamline all section definitions using parametric rules.

  * Made quite a bit of progress quickly. Next I need to deal with comments.

- [Dec 12, morning]
  * Copied over star contraction chapter and will try to make it through the parser.
  * Fleshed out atoms and the atom structure.

  * Added support for labeles into AST, chapters, and sections.  
 

- [Dec 11, morning]

Wrote some traversal functions for converting AST into strings and LaTeX code.  This seems to have worked well.  

Next steps are to start fleshing out the parser to include the full language.

To this end, the first thing I should figure out is whether I can deal with all the atoms in one swoop by using some kind of keywording mechanism so that we can keep the grammar neat and compact.

Ok. I have figured out how to deal with atoms by using a keyword hash table.  This is a good place to stop.  The next thing to do is to 


- [Dec 9, evening]

Working on develeping the AST.  Set up the AST and generated it. Next step is to write some traversal functions over AST.  First one should reconstruct the originial text.


- [Dec 9, morning]



Got a first version of the parser working. 

There were some shift/reduce conflicts but these were due to the redundant atoms definitions of the form:

atoms : empty | atom | atoms atom
The middle rule is redundant.  Deleting that rule eliminated the conflicts.


- [Dec 8]
  Resuming the work on the parser.
  
I am now investigating whether I can extend our definiton of words so to include whitespaces. The idea is to tokenize the input at special chars 

special characters: \ { } [ ] 

and to include into the tokens all the white space including newlines.

The method I am following is that each non-word token consist of whatever it wants to represent and then is followed by whitespace.

\begin{...} whitespace

the words themselved consist of everything except the special characters above.


The issue that I am not taking care of right now is that \ is a special character and words will not catch them.  So I need to account for it when boxing things.


- [Nov 30] I am now investigating whether it would be possible to extend our definiton of words so to include whitespaces. The idea is to tokenize the input at special chars 

\ { } [ ] 

and to include into the tokens all the white space including newlines.

I don't currently see any problem with this.  But we will be playing with it more.


- [Nov 30] The approach of splitting text into curly and square boxes has a problem:  it becomes difficult if not impossible to reconstruct the text back.  For example, we can have 
[something ]
\{ something else\}
now when we parse, we lose the spacing and the spacing might matter

  I think I need an more elementary method....

  Next I would like to try the following:
  Look for patterns of the form 
  \section{....}
  or 
  \begin{env}
  or
  \begin{env}[  ]

  the rest should be treated as CHAR's.  

  This parser does not elim white space but is somewhat sensitive to it. 

  In principle the above makes sense but it is difficult to do because we don't know when a paren ends (without parsing more deeply, which is the problem that i am trying to avoid).  So the easiest thing to do is to look for a newline.
   
  So look for patterns of the form 
  \section{....} whitespace \n
  or 
  \begin{env} whitespare \n
  or
  \begin{env}[  ] whitespace \n


  BUT this raises other issues: we can't reliably use the parser anymore.

  

- [Nov 28] Working on parameterizing the parser so that we can deal with many different kinds of env tokens and such
 
- [Nov 28] The idea of splitting text into square, curly, and plain boxes ([], {}, rest) seem to work.

- [Thanksgiving Break] 

  - The problem with the whitespace business is that it kills existing parsers.  The current parsers are all LR(1) variants, which means that they can only lookahead one token at a time.  This becomes a problem when taking whitespace into account, because we usually want to allow multiple whitespaces.

  - The files lexer.mll and parser.mly in this directory contain a take on the whitespace problem by uusing two newlines as a "separator" and ignoring whitespace otherwise.  This seems to "work" but it is clunky.  It is far away from the "Line by Line" parsing that I imagined would work nicely.

  - I think I will have to take a different take on this.  I see two options
1)  One way would be to write our own lexer that does the line by line parsing directly rather than relying on a parser, which is a bit rigid... 
2) Another way is to give up on the reliance on white space and write a superficial parser at the level of "boxes" consisting of words {box} and [box] objects and define a superficial grammar for diderot latex at this level.  This should be feasible.  One issue is the ambiguity in things like this, but should be fixable.  
\begin{example}[title][body] is ambigous because [title][body] can also be legal body.  We need a way to disambiguate this.

this approach has the disadvantage of not being able to deal with paragraphs but maybe i can deal with that by using two lines as breakers.  The problem with that though is that i have to account for it everywhere, which becomes annoying but it might be feasible.  another way to deal with this to write a separate simple "translator" that can gramify paragraphs.  



# Overview of the approach

The basic idea I am following here is to parse the document superficially, so superficially that we only want to care about what we need to identify blocks.  The rest, we treat them as blobs of text.

Specificially the lexer will break up the document into words and also keep the newlines and other spaces.  The parser will then extract what it needs and put the rest together.

Note: I had initially hoped that I could get away by parsing things only at the level of the lines but I decided agains it because I still have to parse arguments in curly braces and in square brackets.  This might get all too complicated by I am hoping to make it work. 



# Lexer

We are going to breax the text into tokens consisting of
1) spaces and special characters
- white space
- open close curly { }
- open close square [ ] 

2) various headings
- latex headings
- \begin{atom} \end{atom}, for all atoms
- \begin{group} \end{group}
- 

3) words will be the rest

The hope is that we will be able to put everything back together in the parser, except for the parts that we care about.
