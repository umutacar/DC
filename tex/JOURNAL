# TODO
  - diderot keywords such as labels can appear inside bodies.  How to deal with these?  We can include them as a word but this will likely lead to many conflicts.

    this suggests that "hijacking" latex commands as diderot commands can be a bit painful.  we have to be careful

 - Clean up the parser code (shared constants and so on) and upload several chapters (on staging) to test its robustness.

 - Test the parser on 210 files.

 - Extend XML translation to match older MLX translation.


   -- The major remaining gap: the older translation does some strictfication as an optimization to reduce the many nested section/subsection/subsubsection that would otherwise be needed.  I forget now how this was done.  I will need to look into it.
   
   -- see xml.ml, headers

 - Should we have a core language?  I think not.  It mostly bloat code.  So let's not unless we really need.  The AST should be good enough for most things.

 - clean up python crap from this directory when the parser is done. one possibility is to key dex/ for historical consistency and move relevant directories under it.

 - Add support for paragraphs

   One idea: sections will now have blocks followed by paragraphs and then transition into nested sections.  Basically, we are allowing paragraphs to occur at block level.  But of course once you have a paragraph, then the rest will be contained within that paragraph.  

 - Coordination with the python-based XML backend needs to be done carefully.

This coordinates seems to be going through several functions called
missing_field* in diderot/src/pervasives/syntax.py

These function are called from mlx_utils, which does nearly all work.

So whatever latex produces must be mediated through this. 

 

# JOURNAL

- [Dec 29]

  - Found a way to streamline the defitinions of atoms.  The basic idea is to use variables to capture the atom kind matched so that the atom can be returned as part of a matched token.  This means that we don't have to create separate tokens for each atom and can treat them all as part of instances of a single atom token.  This reduces the number of lines of codes in the parser quite dramatically and simplifies things a lot.  The only *slight* downside is that it does not bugs of the sort

  \begin{example}
  \end{defition}

  This is because begin and end are both well defined atoms.  this atom would be treated as an example.  This "bug" might even be a feature.  But it would be nice to produce a warning in this case.

- [Dec 28]
  - Fixed a label printing bug
  - Dropped the Core library in favor of standard ocaml libraries in lexer.

    This is because lexer internally uses standard libraries (it seems) and core is not consistent with standard.

  - Extended the lexer and parser to support diderotXYZ atoms where XYZ can be any custom atom.

- [Dec 23]
  - Cleanedup up the ocaml code a bit 

  - Starting running the parser through 15210 sources.  
    Was able to compile and upload the first 5-6 chapters but encountered a problem in language/functional.tex.
    Not sure what the problem is.  Have to look into it more carefully.
   - Resolved this issue, it was due to \% character being special in latex.

- [Dec 22]

  * Completed a first working version of OCaml parser.  It is able to produce xml code from .tex files. I used star.tex (from graph contraction as an example).

    There are still some issues left. The major one being how to strictify: the older translation does some strictfication as an optimization to reduce the many nested section/subsection/subsubsection that would otherwise be needed.  I forget now how this was done.  I will need to look into it.

    I looked into this.  The basic idea is to wrap the "Blocks" in each section into another section.  Here is one way to proceed.

    Create a separate AST-Strict data type.  This would be pretty similar to current AST but its types are more streamlined.  It would only allow strict nesting.  Then wriwe a function that translates AST into AST-Strict.  AST strict would then have functions that write it out as LaTeX or XML. Voila.  This should work...

 * Modulo the strictification above, I was able to upload a XML generated by the OCML parser on diderot. 
   

 
- [Dec 21]

  * Need a way to pass the translation function into atom's of the AST.  This requires threading the translation through.  I think this is fine, because it is pure.

  * Generated xml successfully using pandoc translation method. Yay!!!
  * Now comes the tedious part.  I need to carefully go through the xml and make sure that I am not missing any fields.  This is a bit tedious.  I have to study the dex translation because there isn't really a spec.  The implementation is the spec.  Ouch.

  * OCaml Str.regexp stuff is broken.  Groups are not identified correctly.


- [Dec 20]
  * Two ways to do the translation of latex to xml.  One is to continue reusing our python code.  This will work but the problem is that it will prevent us from distributing an executable that can do this without having to rely on the user having installed python.  This is pretty bad.  Another way is to rewrite this stuff in OCAML.  I think this is the right direction.  But maybe for now the first path is OK because it doesn't require writing new code.    

  * I completed a first version of an ocaml module for translating tex to html by using pandoc externally via systems calls.  This module is currently in tex/tex2html.ml.  I plan on trying it soon.


- [Dec 19]
  * Trying to figure out how to generate xml code from tex.

  One question is whether to translate tex to tex_core, which we use to eliminate syntactic sugar.  This can probably be very helpful, especially when dealing with more complicated latex classes such as problem sets.  

  One problem with this approach is that it ends up generating lots of intermediate files.  But now we are using a real functional programming language.  So we can transform the ASTs but don't necessarily have to dump them.  I think this is the approach that I should follow.

  * Doing some of the "infrastructure" work needed to translatex tex to xml such as creating a vocabulary for the core language and such

  * Coordination with XML backend is important, see tex/COORDINATE

  *  One question is what to do with various missing fields. I think the backend is robust enough to deal with their absence so maybe we should just omit them. 

  * completed a skeloten of the xml generation.

- [Dec 18]
  * working on idempotent parsing

    Issue: headings for groups, atoms, sections, should be preserved as read from th
e file.<

  * rm'ed begin/end from keywords because we don't care about these.  they are not diderot keywords... 

  * completed a first pass on idempotent parsing.
    
  * do we need to distinguish comments can't we treat them as just words.  

    --> yes we do.  if we don't then, we have no way of skipping over commented atoms and other diderot keywords.  

   * resolved the kinding problem by "inlini" productions of menhir.

- [Dec 17]
  * One way to deal with all text that is inbetween atoms is to allow atoms and groups to have preambles and have a sequence of them to have a postamble.  So this is what I implemented.

  * The above idea works but it is not very uniform.  The problem is that who does that postamble belong to.  It is usually not clear because you can have a blocks of atoms that has a postamble and then a section that also has a postamble.  The chapter's postamble is now that of the block of the atoms, which is quite counterintuive.  It seems that a better approach is to allow the final atoms to have a post and a preamble, while allowing all other atoms to have preambles.  For clarity, i started calling these /intertext/ rather thas postamble.

  * Now working on idempotent parsing.
   ** Issue: whitespace are not being lined up.  WHat is our policy for dealing with white spaces.
      Resolved this issue it is not so much about whitespaces but more about making sure that we can push them through the parser carefully without dropping them somehow along the way as we extract various parts such as titles.


- [Dec 16]
  * working on comments

  * added atom preambles that can be associated with each atom but will not be "displayed".
  * added group preambles

  * thinking about what to about latex commands such as 
    \xyz where xyz could be \item \begin{enumerate} \% or \[
    These we should treat them as plain words.  But how? 

    The problem is that unless we treat \ specially we will not be able to
    match things like \begin{atom}.
    
    Thus we have to treat latex keywords as plain words in the parser.
    
    -> this approach seems to work.

  * Associating preambles with atoms and groups does not completely solve the stray text problem.

    Because we can also have stray text at the end of things that currently don't belonge anywhere

    For example:
    \section{xyz} 
 
     blocks

     stray text currently does not belong anywhere.

    The same applies to blocks and such.

    Thus if there is no atom group that the text will be assigned to we are kind of stuck.

   ** To solve this problem, one possibility would be to give sections (subsections) etc, a preamble as well.  This does not work, because when we see a preamble we don't know if it should belong to an atom or a section.  So this is a major difficulty.

    ** I wonder if a postamble will work....

      Postamble idea might work but it is quite tricy to make it work because of conflicts.  One conflict arises from preamble/postamble ambiguity.  Should postamble be preamble of the atom that we have not seen or is it postamble. So we have to differentiate between empty and nonempty blocks.  


      I will need to look into this and see if it is possible.


- [Dec 14, 15, mixed hacking in various meetings]

  * Learned about menhir parameterized rules.  they have some limitations but work generally well it seems.

  * Using parameteric menhir rules, I was able to express atom parsing rules

  * I was able to streamline all section definitions using parametric rules.

  * Made quite a bit of progress quickly. Next I need to deal with comments.

- [Dec 12, morning]
  * Copied over star contraction chapter and will try to make it through the parser.
  * Fleshed out atoms and the atom structure.

  * Added support for labeles into AST, chapters, and sections.  
 

- [Dec 11, morning]

Wrote some traversal functions for converting AST into strings and LaTeX code.  This seems to have worked well.  

Next steps are to start fleshing out the parser to include the full language.

To this end, the first thing I should figure out is whether I can deal with all the atoms in one swoop by using some kind of keywording mechanism so that we can keep the grammar neat and compact.

Ok. I have figured out how to deal with atoms by using a keyword hash table.  This is a good place to stop.  The next thing to do is to 


- [Dec 9, evening]

Working on develeping the AST.  Set up the AST and generated it. Next step is to write some traversal functions over AST.  First one should reconstruct the originial text.


- [Dec 9, morning]



Got a first version of the parser working. 

There were some shift/reduce conflicts but these were due to the redundant atoms definitions of the form:

atoms : empty | atom | atoms atom
The middle rule is redundant.  Deleting that rule eliminated the conflicts.


- [Dec 8]
  Resuming the work on the parser.
  
I am now investigating whether I can extend our definiton of words so to include whitespaces. The idea is to tokenize the input at special chars 

special characters: \ { } [ ] 

and to include into the tokens all the white space including newlines.

The method I am following is that each non-word token consist of whatever it wants to represent and then is followed by whitespace.

\begin{...} whitespace

the words themselved consist of everything except the special characters above.


The issue that I am not taking care of right now is that \ is a special character and words will not catch them.  So I need to account for it when boxing things.


- [Nov 30] I am now investigating whether it would be possible to extend our definiton of words so to include whitespaces. The idea is to tokenize the input at special chars 

\ { } [ ] 

and to include into the tokens all the white space including newlines.

I don't currently see any problem with this.  But we will be playing with it more.


- [Nov 30] The approach of splitting text into curly and square boxes has a problem:  it becomes difficult if not impossible to reconstruct the text back.  For example, we can have 
[something ]
\{ something else\}
now when we parse, we lose the spacing and the spacing might matter

  I think I need an more elementary method....

  Next I would like to try the following:
  Look for patterns of the form 
  \section{....}
  or 
  \begin{env}
  or
  \begin{env}[  ]

  the rest should be treated as CHAR's.  

  This parser does not elim white space but is somewhat sensitive to it. 

  In principle the above makes sense but it is difficult to do because we don't know when a paren ends (without parsing more deeply, which is the problem that i am trying to avoid).  So the easiest thing to do is to look for a newline.
   
  So look for patterns of the form 
  \section{....} whitespace \n
  or 
  \begin{env} whitespare \n
  or
  \begin{env}[  ] whitespace \n


  BUT this raises other issues: we can't reliably use the parser anymore.

  

- [Nov 28] Working on parameterizing the parser so that we can deal with many different kinds of env tokens and such
 
- [Nov 28] The idea of splitting text into square, curly, and plain boxes ([], {}, rest) seem to work.

- [Thanksgiving Break] 

  - The problem with the whitespace business is that it kills existing parsers.  The current parsers are all LR(1) variants, which means that they can only lookahead one token at a time.  This becomes a problem when taking whitespace into account, because we usually want to allow multiple whitespaces.

  - The files lexer.mll and parser.mly in this directory contain a take on the whitespace problem by uusing two newlines as a "separator" and ignoring whitespace otherwise.  This seems to "work" but it is clunky.  It is far away from the "Line by Line" parsing that I imagined would work nicely.

  - I think I will have to take a different take on this.  I see two options
1)  One way would be to write our own lexer that does the line by line parsing directly rather than relying on a parser, which is a bit rigid... 
2) Another way is to give up on the reliance on white space and write a superficial parser at the level of "boxes" consisting of words {box} and [box] objects and define a superficial grammar for diderot latex at this level.  This should be feasible.  One issue is the ambiguity in things like this, but should be fixable.  
\begin{example}[title][body] is ambigous because [title][body] can also be legal body.  We need a way to disambiguate this.

this approach has the disadvantage of not being able to deal with paragraphs but maybe i can deal with that by using two lines as breakers.  The problem with that though is that i have to account for it everywhere, which becomes annoying but it might be feasible.  another way to deal with this to write a separate simple "translator" that can gramify paragraphs.  



# Overview of the approach

The basic idea I am following here is to parse the document superficially, so superficially that we only want to care about what we need to identify blocks.  The rest, we treat them as blobs of text.

Specificially the lexer will break up the document into words and also keep the newlines and other spaces.  The parser will then extract what it needs and put the rest together.

Note: I had initially hoped that I could get away by parsing things only at the level of the lines but I decided agains it because I still have to parse arguments in curly braces and in square brackets.  This might get all too complicated by I am hoping to make it work. 



# Lexer

We are going to breax the text into tokens consisting of
1) spaces and special characters
- white space
- open close curly { }
- open close square [ ] 

2) various headings
- latex headings
- \begin{atom} \end{atom}, for all atoms
- \begin{group} \end{group}
- 

3) words will be the rest

The hope is that we will be able to put everything back together in the parser, except for the parts that we care about.
